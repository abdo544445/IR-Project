# IR Project with Web Crawler Integration

[![Java](https://img.shields.io/badge/Java-8-red.svg)](https://www.oracle.com/java/)
[![Python](https://img.shields.io/badge/Python-3.6+-blue.svg)](https://www.python.org/)
[![Maven](https://img.shields.io/badge/Maven-3.8+-green.svg)](https://maven.apache.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

This project implements an Information Retrieval (IR) system with a web crawler integration. It combines a Java-based IR application using Apache Lucene with a Python web crawler to collect and index academic papers from the internet.

## ğŸŒŸ Features

- **Web Crawler**
  - Fetches academic papers from specified URLs
  - Extracts title, author, and content
  - Formats documents in Cranfield format
  - Generates sample queries and relevance judgments

- **IR System**
  - Document indexing using Apache Lucene
  - Multiple analyzer options (Standard, Whitespace, English)
  - Different similarity models (VSM, BM25)
  - Query processing and result evaluation
  - Performance metrics calculation (Precision, Recall, MAP)

## ğŸ› ï¸ Technology Stack

### Java Application
- Java 8 or higher
- Apache Lucene for indexing and searching
- Maven for dependency management
- JUnit for testing

### Python Web Crawler
- Python 3.6 or higher
- BeautifulSoup4 for HTML parsing
- Requests for HTTP requests
- Concurrent processing for efficient crawling

## ğŸ“‹ Prerequisites

Before you begin, ensure you have the following installed:
- Java JDK 8 or higher
- Python 3.6 or higher
- Maven 3.8 or higher
- Git (for cloning the repository)

## ğŸš€ Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/abdo544445/IR-Project.git
   cd IR-Project
   ```

2. Set up Python virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Linux/Mac
   # or
   .\venv\Scripts\activate  # On Windows
   ```

3. Install Python dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Install Java dependencies:
   ```bash
   mvn clean install
   ```

## ğŸ’» Usage

### Running the Web Crawler

Run the crawler independently:
```bash
python web_crawler.py --urls https://arxiv.org/list/cs.IR/recent --max-pages 50
```

#### Crawler Options
- `--urls`: Starting URLs (space-separated)
- `--output`: Output directory (default: src/main/resources/crawled)
- `--max-pages`: Maximum pages to crawl (default: 100)
- `--delay`: Request delay in seconds (default: 1.0)
- `--workers`: Concurrent workers (default: 5)

### Running with Java Integration

Run crawler and Java application together:
```bash
python run_crawler.py --urls https://arxiv.org/list/cs.IR/recent --max-pages 50 --run-java
```

### Running Java Application Manually

```bash
mvn clean compile exec:java -Dexec.mainClass=IR.App
```

## ğŸ“ Project Structure

```
IR-Project/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main/
â”‚       â”œâ”€â”€ java/
â”‚       â”‚   â””â”€â”€ IR/
â”‚       â”‚       â”œâ”€â”€ App.java                 # Main application entry point
â”‚       â”‚       â”œâ”€â”€ CranQuery.java           # Query parsing and handling
â”‚       â”‚       â”œâ”€â”€ CreateIndex.java         # Document indexing
â”‚       â”‚       â”œâ”€â”€ Evaluation.java          # Search result evaluation
â”‚       â”‚       â”œâ”€â”€ EvaluationMetrics.java   # Evaluation metrics calculation
â”‚       â”‚       â”œâ”€â”€ QueryIndex.java          # Query processing and results
â”‚       â”‚       â””â”€â”€ SearchResult.java        # Search result representation
â”‚       â””â”€â”€ resources/
â”‚           â”œâ”€â”€ cran/                        # Standard Cranfield dataset
â”‚           â”œâ”€â”€ crawled/                     # Web crawler output
â”‚           â””â”€â”€ results/                     # Search results
â”œâ”€â”€ web_crawler.py                          # Main crawler implementation
â”œâ”€â”€ run_crawler.py                          # Integration script
â”œâ”€â”€ requirements.txt                        # Python dependencies
â””â”€â”€ pom.xml                                # Maven configuration
```

## ğŸ”§ Configuration

### Java Application
- Analyzer options:
  1. Standard Analyzer
  2. Whitespace Analyzer
  3. English Analyzer

- Similarity methods:
  1. Vector Space Model (VSM)
  2. BM25 Similarity

### Web Crawler
- Customize crawling behavior in `web_crawler.py`
- Adjust output formats in `run_crawler.py`

## ğŸ“Š Performance Metrics

The system evaluates search results using:
- Precision@25
- Recall
- Mean Average Precision (MAP)

## ğŸ”„ Recent Updates

### Java Application Updates
- Added automatic creation of results directory
- Improved error handling
- Enhanced evaluation metrics

## ğŸ› Troubleshooting

### Common Issues
1. **Missing Files**
   - Check output directory for crawler results
   - Verify file permissions

2. **Java Application Errors**
   - Ensure Maven dependencies are installed
   - Check Java version compatibility

3. **Crawler Issues**
   - Review crawler.log for detailed error messages
   - Check network connectivity
   - Verify URL accessibility

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¥ Authors

- Your Name - Initial work

## ğŸ™ Acknowledgments

- Apache Lucene team for the excellent IR framework
- BeautifulSoup4 and Requests libraries for web scraping capabilities
- The academic community for providing open access to research papers 