[
  {
    "url": "https://arxiv.org/list/cs.IR/recent",
    "title": "Information Retrieval",
    "author": "Sejong Kim, Hyunseo Song, Hyunwoo Seo, Hyunjun Kim",
    "body": "Help | Advanced Search See today's new changes arXiv Operational Status Get status notifications via email or slack"
  },
  {
    "url": "https://info.arxiv.org/about/donate.html",
    "title": "Donate to arXiv",
    "author": "Unknown",
    "body": "Thank you for supporting arXiv! 100% of your contribution will fund new initiatives and ongoing operations that benefit the global scientific community. Donate Donations to arXiv are processed by Cornell University. Corporate Employer Matching Funds can be made through Benevity. Member Institutions: Membership contributions made by credit card can be processed through the donation link above. Please be sure to include the institution\u00e2\u0080\u0099s name (not an individual\u00e2\u0080\u0099s name). Authors: Donations will have no impact on submission decisions. For questions related to donations, institutional membership, or arXiv's funding: Please contact arXiv Membership & Giving at membership@arxiv.org.Thank you for supporting arXiv! 100% of your contribution will fund new initiatives and ongoing operations that benefit the global scientific community. Donate Donations to arXiv are processed by Cornell University. Corporate Employer Matching Funds can be made through Benevity. Member Institutions: Membership contributions made by credit card can be processed through the donation link above. Please be sure to include the institution\u00e2\u0080\u0099s name (not an individual\u00e2\u0080\u0099s name). Authors: Donations will have no impact on submission decisions. For questions related to donations, institutional membership, or arXiv's funding: Please contact arXiv Membership & Giving at membership@arxiv.org."
  },
  {
    "url": "https://www.cornell.edu/",
    "title": "Cornell University",
    "author": "Office of Web Communications, Cornell University",
    "body": "Art and science overlap for nature illustrators Energy Warriors program provides paths to green jobs, and hope How SARS-CoV-2 evolved through the pandemic Chaotic springs, long summers mean uncertainty for NY grape growers New biodegradable graft could help cardiovascular patients Bacterial changes point to new therapies for Legionnaires\u2019 disease Cornell research informs cybersecurity executive order New telescope to set sail for monthlong journey to Chile Mice use their tongues to \u2018see\u2019 tactile targets Dogs sniff out devastating spotted lanternflies for early detection \u201cI would found an institution where any person can find instruction in any study.\u201d Ezra Cornell, 1868"
  },
  {
    "url": "https://info.arxiv.org/about/ourmembers.html",
    "title": "Our Members: 2023 - 2024",
    "author": "Unknown",
    "body": "We are so thankful for our members \u00e2\u0080\u0093 arXiv would not be possible without their support. arXiv members are universities, research labs, libraries, and consortia that contribute funding to arXiv to help advance the cause of open science. Other funding for arXiv comes from foundations, affiliates, and sponsors. Interested in supporting arXiv and expanding the possibilities of open science? Learn more about arXiv's funding and contact us at membership@arXiv.org. arXiv gratefully acknowledges the consortia across the globe who represent the best interests of their constituents by collectively supporting arXiv and open science. Champion consortia support arXiv by giving above and beyond in the name of open science. Champion members with arXiv are institutions whose researchers are top users of arXiv and whose goal is to lead the global research community in innovation, education, and promoting open science. Top Submitters are institutions who are globally ranked as the top users of arXiv. Frequent Submitters are members whose researchers frequently submit scholarly articles to arXiv. Regular Submitters are members whose researchers regularly submit scholarly articles to arXiv. Community members are institutions who primarily use arXiv for reading articles, rather than submitting, or who may be resource-limited, but would still like to give to arXiv.We are so thankful for our members \u00e2\u0080\u0093 arXiv would not be possible without their support. arXiv members are universities, research labs, libraries, and consortia that contribute funding to arXiv to help advance the cause of open science. Other funding for arXiv comes from foundations, affiliates, and sponsors. Interested in supporting arXiv and expanding the possibilities of open science? Learn more about arXiv's funding and contact us at membership@arXiv.org. arXiv gratefully acknowledges the consortia across the globe who represent the best interests of their constituents by collectively supporting arXiv and open science. Champion consortia support arXiv by giving above and beyond in the name of open science. Champion members with arXiv are institutions whose researchers are top users of arXiv and whose goal is to lead the global research community in innovation, education, and promoting open science. Top Submitters are institutions who are globally ranked as the top users of arXiv. Frequent Submitters are members whose researchers frequently submit scholarly articles to arXiv. Regular Submitters are members whose researchers regularly submit scholarly articles to arXiv. Community members are institutions who primarily use arXiv for reading articles, rather than submitting, or who may be resource-limited, but would still like to give to arXiv."
  },
  {
    "url": "https://arxiv.org/login",
    "title": "Log in to arXiv.org",
    "author": "Unknown",
    "body": "We gratefully acknowledge support from the Simons Foundation and member institutions. Help | Advanced Search The arXiv Privacy Policy has changed. By continuing to use arxiv.org, you are agreeing to the privacy policy. Forgot your password? Register for the first time Registration is required to submit or update papers, but is not necessary to view them. arXiv Operational Status Get status notifications via email or slack"
  },
  {
    "url": "https://arxiv.org/list/cs.IR/recent?skip=0&show=50",
    "title": "Information Retrieval",
    "author": "Sejong Kim, Hyunseo Song, Hyunwoo Seo, Hyunjun Kim",
    "body": "Help | Advanced Search See today's new changes arXiv Operational Status Get status notifications via email or slack"
  },
  {
    "url": "https://info.arxiv.org/about",
    "title": "About arXiv",
    "author": "Unknown",
    "body": "arXiv is a curated research-sharing platform open to anyone. As a pioneer in digital open access, arXiv.org now hosts more than two million scholarly articles in eight subject areas, curated by our strong community of volunteer moderators. arXiv offers researchers a broad range of services: article submission, compilation, production, retrieval, search and discovery, web distribution for human readers, and API access for machines, together with content curation and preservation. Our emphasis on openness, collaboration, and scholarship provide the strong foundation on which arXiv thrives. arXiv currently serves the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics. arXiv was founded by Paul Ginsparg in 1991 and is now maintained and operated by Cornell Tech. Operations are maintained by the arXiv Leadership Team and arXiv staff at Cornell University. Governance of arXiv is led by the Leadership Team with guidance from the arXiv Editorial Advisory Council, Institutions Advisory Council, and Science Advisory Council. arXiv is a community-supported resource funded by Cornell University, the Simons Foundation, member institutions, and donors. Registered users may submit articles to be announced by arXiv. There are no fees or costs for article submission. Submissions to arXiv are subject to a moderation process that classifies material as topical to the subject area and checks for scholarly value. Material is not peer-reviewed by arXiv - the contents of arXiv submissions are wholly the responsibility of the submitter and are presented \u00e2\u0080\u009cas is\u00e2\u0080\u009d without any warranty or guarantee. By hosting works and other materials on this site, arXiv, Cornell University, and their agents do not in any way convey implied approval of the assumptions, methods, results, or conclusions of the work.arXiv is a curated research-sharing platform open to anyone. As a pioneer in digital open access, arXiv.org now hosts more than two million scholarly articles in eight subject areas, curated by our strong community of volunteer moderators. arXiv offers researchers a broad range of services: article submission, compilation, production, retrieval, search and discovery, web distribution for human readers, and API access for machines, together with content curation and preservation. Our emphasis on openness, collaboration, and scholarship provide the strong foundation on which arXiv thrives. arXiv currently serves the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics. arXiv was founded by Paul Ginsparg in 1991 and is now maintained and operated by Cornell Tech. Operations are maintained by the arXiv Leadership Team and arXiv staff at Cornell University. Governance of arXiv is led by the Leadership Team with guidance from the arXiv Editorial Advisory Council, Institutions Advisory Council, and Science Advisory Council. arXiv is a community-supported resource funded by Cornell University, the Simons Foundation, member institutions, and donors. Registered users may submit articles to be announced by arXiv. There are no fees or costs for article submission. Submissions to arXiv are subject to a moderation process that classifies material as topical to the subject area and checks for scholarly value. Material is not peer-reviewed by arXiv - the contents of arXiv submissions are wholly the responsibility of the submitter and are presented \u00e2\u0080\u009cas is\u00e2\u0080\u009d without any warranty or guarantee. By hosting works and other materials on this site, arXiv, Cornell University, and their agents do not in any way convey implied approval of the assumptions, methods, results, or conclusions of the work."
  },
  {
    "url": "https://info.arxiv.org/help",
    "title": "arXiv Help Contents",
    "author": "Unknown",
    "body": "If you could not find an answer to your question, please contact our User Support team.If you could not find an answer to your question, please contact our User Support team."
  },
  {
    "url": "https://arxiv.org/search/advanced",
    "title": "Advanced Search",
    "author": "Unknown",
    "body": "You may filter on either submission date or announcement date. Note that announcement date supports only year and month granularity. Wildcards: Expressions: Phrases: Dates: Journal References:"
  },
  {
    "url": "https://arxiv.org/list/cs.IR/recent?skip=28&show=50",
    "title": "Information Retrieval",
    "author": "Pengfei Luo, Jingbo Zhou, Tong Xu, Yuan Xia, Linli Xu, Enhong Chen",
    "body": "Help | Advanced Search See today's new changes arXiv Operational Status Get status notifications via email or slack"
  },
  {
    "url": "https://arxiv.org/list/cs.IR/recent?skip=20&show=50",
    "title": "Information Retrieval",
    "author": "Yi Xu, Zhiyuan Lu, Xiaochen Li, Jinxin Hu, Hong Wen, Zulong Chen, Yu Zhang, Jing Zhang",
    "body": "Help | Advanced Search See today's new changes arXiv Operational Status Get status notifications via email or slack"
  },
  {
    "url": "https://arxiv.org/list/cs.IR/recent?skip=6&show=50",
    "title": "Information Retrieval",
    "author": "Yu Feng, Puzhen Zhang, Guohui Xiao, Linfang Ding, Liqiu Meng",
    "body": "Help | Advanced Search See today's new changes arXiv Operational Status Get status notifications via email or slack"
  },
  {
    "url": "https://arxiv.org/list/cs.IR/new",
    "title": "Information Retrieval",
    "author": "Wenqi Jiang, Suvinay Subramanian, Cat Graves, Gustavo Alonso, Amir Yazdanbakhsh, Vidushi Dadu",
    "body": "Help | Advanced Search See recent articles Retrieval-augmented generation (RAG), which combines large language models (LLMs) with retrievals from external knowledge databases, is emerging as a popular approach for reliable LLM serving. However, efficient RAG serving remains an open challenge due to the rapid emergence of many RAG variants and the substantial differences in workload characteristics across them. In this paper, we make three fundamental contributions to advancing RAG serving. First, we introduce RAGSchema, a structured abstraction that captures the wide range of RAG algorithms, serving as a foundation for performance optimization. Second, we analyze several representative RAG workloads with distinct RAGSchema, revealing significant performance variability across these workloads. Third, to address this variability and meet diverse performance requirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a system optimization framework for efficient RAG serving. Our evaluation shows that RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in time-to-first-token latency compared to RAG systems built on LLM-system extensions. Effective long-term memory management is crucial for language models handling extended contexts. We introduce a novel framework that dynamically ranks memory entries based on relevance. Unlike previous works, our model introduces a novel relevance scoring and a pointwise re-ranking model for key-value embeddings, inspired by learning-to-rank techniques in information retrieval. Enhanced Ranked Memory Augmented Retrieval ERMAR achieves state-of-the-art results on standard benchmarks. Knowledge graphs have emerged to be promising datastore candidates for context augmentation during Retrieval Augmented Generation (RAG). As a result, techniques in graph representation learning have been simultaneously explored alongside principal neural information retrieval approaches, such as two-phased retrieval, also known as re-ranking. While Graph Neural Networks (GNNs) have been proposed to demonstrate proficiency in graph learning for re-ranking, there are ongoing limitations in modeling and evaluating input graph structures for training and evaluation for passage and document ranking tasks. In this survey, we review emerging GNN-based ranking model architectures along with their corresponding graph representation construction methodologies. We conclude by providing recommendations on future research based on community-wide challenges and opportunities. Pseudo-relevance feedback (PRF) refines queries by leveraging initially retrieved documents to improve retrieval effectiveness. In this paper, we investigate how large language models (LLMs) can facilitate PRF for zero-shot LLM-based dense retrieval, extending the recently proposed PromptReps method. Specifically, our approach uses LLMs to extract salient passage features-such as keywords and summaries-from top-ranked documents, which are then integrated into PromptReps to produce enhanced query representations. Experiments on passage retrieval benchmarks demonstrate that incorporating PRF significantly boosts retrieval performance. Notably, smaller rankers with PRF can match the effectiveness of larger rankers without PRF, highlighting PRF's potential to improve LLM-driven search while maintaining an efficient balance between effectiveness and resource usage. Retrieval-Augmented Generation (RAG) has emerged as a promising framework to mitigate hallucinations in Large Language Models (LLMs), yet its overall performance is dependent on the underlying retrieval system. In the finance domain, documents such as 10-K reports pose distinct challenges due to domain-specific vocabulary and multi-hierarchical tabular data. In this work, we introduce an efficient, end-to-end RAG pipeline that enhances retrieval for financial documents through a three-phase approach: pre-retrieval, retrieval, and post-retrieval. In the pre-retrieval phase, various query and corpus preprocessing techniques are employed to enrich input data. During the retrieval phase, we fine-tuned state-of-the-art (SOTA) embedding models with domain-specific knowledge and implemented a hybrid retrieval strategy that combines dense and sparse representations. Finally, the post-retrieval phase leverages Direct Preference Optimization (DPO) training and document selection methods to further refine the results. Evaluations on seven financial question answering datasets-FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA, and MultiHiertt-demonstrate substantial improvements in retrieval performance, leading to more accurate and contextually appropriate generation. These findings highlight the critical role of tailored retrieval techniques in advancing the effectiveness of RAG systems for financial applications. A fully replicable pipeline is available on GitHub: this https URL. Swine disease surveillance is critical to the sustainability of global agriculture, yet its effectiveness is frequently undermined by limited veterinary resources, delayed identification of cases, and variability in diagnostic accuracy. To overcome these barriers, we introduce a novel AI-powered, multi-agent diagnostic system that leverages Retrieval-Augmented Generation (RAG) to deliver timely, evidence-based disease detection and clinical guidance. By automatically classifying user inputs into either Knowledge Retrieval Queries or Symptom-Based Diagnostic Queries, the system ensures targeted information retrieval and facilitates precise diagnostic reasoning. An adaptive questioning protocol systematically collects relevant clinical signs, while a confidence-weighted decision fusion mechanism integrates multiple diagnostic hypotheses to generate robust disease predictions and treatment recommendations. Comprehensive evaluations encompassing query classification, disease diagnosis, and knowledge retrieval demonstrate that the system achieves high accuracy, rapid response times, and consistent reliability. By providing a scalable, AI-driven diagnostic framework, this approach enhances veterinary decision-making, advances sustainable livestock management practices, and contributes substantively to the realization of global food security. The issue of data sparsity poses a significant challenge to recommender systems. In response to this, algorithms that leverage side information such as review texts have been proposed. Furthermore, Cross-Domain Recommendation (CDR), which captures domain-shareable knowledge and transfers it from a richer domain (source) to a sparser one (target), has received notable attention. Nevertheless, the majority of existing methodologies assume a Euclidean embedding space, encountering difficulties in accurately representing richer text information and managing complex interactions between users and items. This paper advocates a hyperbolic CDR approach based on review texts for modeling user-item relationships. We first emphasize that conventional distance-based domain alignment techniques may cause problems because small modifications in hyperbolic geometry result in magnified perturbations, ultimately leading to the collapse of hierarchical structures. To address this challenge, we propose hierarchy-aware embedding and domain alignment schemes that adjust the scale to extract domain-shareable information without disrupting structural forms. The process involves the initial embedding of review texts in hyperbolic space, followed by feature extraction incorporating degree-based normalization and structure alignment. We conducted extensive experiments to substantiate the efficiency, robustness, and scalability of our proposed model in comparison to state-of-the-art baselines. The state-of-the-art recommendation systems have shifted the attention to efficient recommendation, e.g., on-device recommendation, under memory constraints. To this end, the existing methods either focused on the lightweight embeddings for both users and items, or involved on-device systems enjoying the compact embeddings to enhance reusability and reduces space complexity. However, they focus solely on the coarse granularity of embedding, while overlook the fine-grained semantic nuances, to adversarially downgrade the efficacy of meta-embeddings in capturing the intricate relationship over both user and item, consequently resulting into the suboptimal recommendations. In this paper, we aim to study how the meta-embedding can efficiently learn varied grained semantics, together with how the fine-grained meta-embedding can strengthen the representation of coarse-grained meta-embedding. To answer these questions, we develop a novel graph neural networks (GNNs) based recommender where each user and item serves as the node, linked directly to coarse-grained virtual nodes and indirectly to fine-grained virtual nodes, ensuring different grained semantic learning, while disclosing: 1) In contrast to coarse-grained semantics, fine-grained semantics are well captured through sparse meta-embeddings, which adaptively 2) balance the embedding uniqueness and memory constraint. Additionally, the initialization method come up upon SparsePCA, along with a soft thresholding activation function to render the sparseness of the meta-embeddings. We propose a weight bridging update strategy that focuses on matching each coarse-grained meta-embedding with several fine-grained meta-embeddings based on the users/items' semantics. Extensive experiments substantiate our method's superiority over existing baselines. Our code is available at this https URL. This paper introduces an approach for training o1-like RAG models that retrieve and reason over relevant information step by step before generating the final answer. Conventional RAG methods usually perform a single retrieval step before the generation process, which limits their effectiveness in addressing complex queries due to imperfect retrieval results. In contrast, our proposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the model to dynamically reformulate the query based on the evolving state. To train CoRAG effectively, we utilize rejection sampling to automatically generate intermediate retrieval chains, thereby augmenting existing RAG datasets that only provide the correct final answer. At test time, we propose various decoding strategies to scale the model's test-time compute by controlling the length and number of sampled retrieval chains. Experimental results across multiple benchmarks validate the efficacy of CoRAG, particularly in multi-hop question answering tasks, where we observe more than 10 points improvement in EM score compared to strong baselines. On the KILT benchmark, CoRAG establishes a new state-of-the-art performance across a diverse range of knowledge-intensive tasks. Furthermore, we offer comprehensive analyses to understand the scaling behavior of CoRAG, laying the groundwork for future research aimed at developing factual and grounded foundation models. Methods for relation extraction from text mostly focus on high precision, at the cost of limited recall. High recall is crucial, though, to populate long lists of object entities that stand in a specific relation with a given subject. Cues for relevant objects can be spread across many passages in long texts. This poses the challenge of extracting long lists from long texts. We present the L3X method which tackles the problem in two stages: (1) recall-oriented generation using a large language model (LLM) with judicious techniques for retrieval augmentation, and (2) precision-oriented scrutinization to validate or prune candidates. Our L3X method outperforms LLM-only generations by a substantial margin. We introduce a bootstrapping approach to train long-context language models by exploiting their short-context capabilities only. Our method utilizes a simple agent workflow to synthesize diverse long-context instruction tuning data, thereby eliminating the necessity for manual data collection and annotation. The proposed data synthesis workflow requires only a short-context language model, a text retriever, and a document collection, all of which are readily accessible within the open-source ecosystem. Subsequently, language models are fine-tuned using the synthesized data to extend their context lengths. In this manner, we effectively transfer the short-context capabilities of language models to long-context scenarios through a bootstrapping process. We conduct experiments with the open-source Llama-3 family of models and demonstrate that our method can successfully extend the context length to up to 1M tokens, achieving superior performance across various benchmarks. Web refresh crawling is the problem of keeping a cache of web pages fresh, that is, having the most recent copy available when a page is requested, given a limited bandwidth available to the crawler. Under the assumption that the change and request events, resp., to each web page follow independent Poisson processes, the optimal scheduling policy was derived by Azar et al. 2018. In this paper, we study an extension of this problem where side information indicating content changes, such as various types of web pings, for example, signals from sitemaps, content delivery networks, etc., is available. Incorporating such side information into the crawling policy is challenging, because (i) the signals can be noisy with false positive events and with missing change events; and (ii) the crawler should achieve a fair performance over web pages regardless of the quality of the side information, which might differ from web page to web page. We propose a scalable crawling algorithm which (i) uses the noisy side information in an optimal way under mild assumptions; (ii) can be deployed without heavy centralized computation; (iii) is able to crawl web pages at a constant total rate without spikes in the total bandwidth usage over any time interval, and automatically adapt to the new optimal solution when the total bandwidth changes without centralized computation. Experiments clearly demonstrate the versatility of our approach. arXiv Operational Status Get status notifications via email or slack"
  },
  {
    "url": "https://arxiv.org/list/cs.IR/recent?skip=10&show=50",
    "title": "Information Retrieval",
    "author": "Yu Liu, Hanbin Jiang, Lei Zhu, Yu Zhang, Yuqi Mao, Jiangxia Cao, Shuchao Pang",
    "body": "Help | Advanced Search See today's new changes arXiv Operational Status Get status notifications via email or slack"
  },
  {
    "url": "https://arxiv.org/format/2503.15191",
    "title": "arXiv.org >\n2503.15191 > format",
    "author": "Unknown",
    "body": "Help | Advanced Search Note: Many of the formats above are served gzipped (Content-Encoding: x-gzip). Your browser may silently uncompress after downloading so the files you see saved may appear uncompressed. Please report any problems to arXiv technical support and include the paper identifier. arXiv Operational Status Get status notifications via email or slack"
  },
  {
    "url": "https://arxiv.org/list/cs.IR/recent?skip=0&show=25",
    "title": "Information Retrieval",
    "author": "Sejong Kim, Hyunseo Song, Hyunwoo Seo, Hyunjun Kim",
    "body": "Help | Advanced Search See today's new changes arXiv Operational Status Get status notifications via email or slack"
  },
  {
    "url": "https://arxiv.org/abs/2503.15191",
    "title": "Computer Science > Information Retrieval",
    "author": "Authors:Sejong Kim, Hyunseo Song, Hyunwoo Seo, Hyunjun Kim",
    "body": "Help | Advanced Search arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. arXiv Operational Status Get status notifications via email or slack"
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Seo,+H",
    "title": "Showing 1\u201350 of 107 results for author: Seo, H",
    "author": "Authors:\nSejong Kim, \n      \n      Hyunseo Song, \n      \n      Hyunwoo Seo, \n      \n      Hyunjun Kim",
    "body": "arXiv:2503.15191 [pdf, other] Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems Authors: Sejong Kim, Hyunseo Song, Hyunwoo Seo, Hyunjun Kim Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising framework to mitigate hallucinations in Large Language Models (LLMs), yet its overall performance is dependent on the underlying retrieval system. In the finance domain, documents such as 10-K reports pose distinct challenges due to domain-specific vocabulary and multi-hierarchical tabular data. In this work, we introduce an efficient\u2026 \u25bd More Retrieval-Augmented Generation (RAG) has emerged as a promising framework to mitigate hallucinations in Large Language Models (LLMs), yet its overall performance is dependent on the underlying retrieval system. In the finance domain, documents such as 10-K reports pose distinct challenges due to domain-specific vocabulary and multi-hierarchical tabular data. In this work, we introduce an efficient, end-to-end RAG pipeline that enhances retrieval for financial documents through a three-phase approach: pre-retrieval, retrieval, and post-retrieval. In the pre-retrieval phase, various query and corpus preprocessing techniques are employed to enrich input data. During the retrieval phase, we fine-tuned state-of-the-art (SOTA) embedding models with domain-specific knowledge and implemented a hybrid retrieval strategy that combines dense and sparse representations. Finally, the post-retrieval phase leverages Direct Preference Optimization (DPO) training and document selection methods to further refine the results. Evaluations on seven financial question answering datasets-FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA, and MultiHiertt-demonstrate substantial improvements in retrieval performance, leading to more accurate and contextually appropriate generation. These findings highlight the critical role of tailored retrieval techniques in advancing the effectiveness of RAG systems for financial applications. A fully replicable pipeline is available on GitHub: https://github.com/seohyunwoo-0407/GAR. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. Comments: 15 pages, 3 figures, 11 tables. Accepted at ICLR 2025 Workshop on Advances in Financial AI. Code available at https://github.com/seohyunwoo-0407/GAR arXiv:2503.14868 [pdf, other] Efficient Personalization of Quantized Diffusion Model without Backpropagation Authors: Hoigi Seo, Wongi Jeong, Kyungryeol Lee, Se Young Chun Abstract: Diffusion models have shown remarkable performance in image synthesis, but they demand extensive computational and memory resources for training, fine-tuning and inference. Although advanced quantization techniques have successfully minimized memory usage for inference, training and fine-tuning these quantized models still require large memory possibly due to dequantization for accurate computatio\u2026 \u25bd More Diffusion models have shown remarkable performance in image synthesis, but they demand extensive computational and memory resources for training, fine-tuning and inference. Although advanced quantization techniques have successfully minimized memory usage for inference, training and fine-tuning these quantized models still require large memory possibly due to dequantization for accurate computation of gradients and/or backpropagation for gradient-based algorithms. However, memory-efficient fine-tuning is particularly desirable for applications such as personalization that often must be run on edge devices like mobile phones with private data. In this work, we address this challenge by quantizing a diffusion model with personalization via Textual Inversion and by leveraging a zeroth-order optimization on personalization tokens without dequantization so that it does not require gradient and activation storage for backpropagation that consumes considerable memory. Since a gradient estimation using zeroth-order optimization is quite noisy for a single or a few images in personalization, we propose to denoise the estimated gradient by projecting it onto a subspace that is constructed with the past history of the tokens, dubbed Subspace Gradient. In addition, we investigated the influence of text embedding in image generation, leading to our proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for sampling with effective diffusion timesteps. Our method achieves comparable performance to prior methods in image and text alignment scores for personalizing Stable Diffusion with only forward passes while reducing training memory demand up to $8.2\\times$. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. arXiv:2503.11439 [pdf, other] COIN: Confidence Score-Guided Distillation for Annotation-Free Cell Segmentation Authors: Sanghyun Jo, Seo Jin Lee, Seungwoo Lee, Seohyung Hong, Hyungseok Seo, Kyungsu Kim Abstract: Cell instance segmentation (CIS) is crucial for identifying individual cell morphologies in histopathological images, providing valuable insights for biological and medical research. While unsupervised CIS (UCIS) models aim to reduce the heavy reliance on labor-intensive image annotations, they fail to accurately capture cell boundaries, causing missed detections and poor performance. Recognizing\u2026 \u25bd More Cell instance segmentation (CIS) is crucial for identifying individual cell morphologies in histopathological images, providing valuable insights for biological and medical research. While unsupervised CIS (UCIS) models aim to reduce the heavy reliance on labor-intensive image annotations, they fail to accurately capture cell boundaries, causing missed detections and poor performance. Recognizing the absence of error-free instances as a key limitation, we present COIN (COnfidence score-guided INstance distillation), a novel annotation-free framework with three key steps: (1) Increasing the sensitivity for the presence of error-free instances via unsupervised semantic segmentation with optimal transport, leveraging its ability to discriminate spatially minor instances, (2) Instance-level confidence scoring to measure the consistency between model prediction and refined mask and identify highly confident instances, offering an alternative to ground truth annotations, and (3) Progressive expansion of confidence with recursive self-distillation. Extensive experiments across six datasets show COIN outperforming existing UCIS methods, even surpassing semi- and weakly-supervised approaches across all metrics on the MoNuSeg and TNBC datasets. The code is available at https://github.com/shjo-april/COIN. \u25b3 Less Submitted 16 March, 2025; v1 submitted 14 March, 2025; originally announced March 2025. arXiv:2502.08690 [pdf, other] Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation Authors: Hoigi Seo, Wongi Jeong, Jae-sun Seo, Se Young Chun Abstract: Large-scale text encoders in text-to-image (T2I) diffusion models have demonstrated exceptional performance in generating high-quality images from textual prompts. Unlike denoising modules that rely on multiple iterative steps, text encoders require only a single forward pass to produce text embeddings. However, despite their minimal contribution to total inference time and floating-point operatio\u2026 \u25bd More Large-scale text encoders in text-to-image (T2I) diffusion models have demonstrated exceptional performance in generating high-quality images from textual prompts. Unlike denoising modules that rely on multiple iterative steps, text encoders require only a single forward pass to produce text embeddings. However, despite their minimal contribution to total inference time and floating-point operations (FLOPs), text encoders demand significantly higher memory usage, up to eight times more than denoising modules. To address this inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet effective pruning strategy specifically designed for text encoders in T2I diffusion models. Skrr exploits the inherent redundancy in transformer blocks by selectively skipping or reusing certain layers in a manner tailored for T2I tasks, thereby reducing memory consumption without compromising performance. Extensive experiments demonstrate that Skrr maintains image quality comparable to the original model even under high sparsity levels, outperforming existing blockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory efficiency while preserving performance across multiple evaluation metrics, including the FID, CLIP, DreamSim, and GenEval scores. \u25b3 Less Submitted 12 February, 2025; originally announced February 2025. arXiv:2502.06139 [pdf, other] LCIRC: A Recurrent Compression Approach for Efficient Long-form Context and Query Dependent Modeling in LLMs Authors: Sumin An, Junyoung Sung, Wonpyo Park, Chanjun Park, Paul Hongsuck Seo Abstract: While large language models (LLMs) excel in generating coherent and contextually rich outputs, their capacity to efficiently handle long-form contexts is limited by fixed-length position embeddings. Additionally, the computational cost of processing long sequences increases quadratically, making it challenging to extend context length. To address these challenges, we propose Long-form Context Inje\u2026 \u25bd More While large language models (LLMs) excel in generating coherent and contextually rich outputs, their capacity to efficiently handle long-form contexts is limited by fixed-length position embeddings. Additionally, the computational cost of processing long sequences increases quadratically, making it challenging to extend context length. To address these challenges, we propose Long-form Context Injection with Recurrent Compression (LCIRC), a method that enables the efficient processing long-form sequences beyond the model's length limit through recurrent compression without retraining the entire model. We further introduce query dependent context modeling, which selectively compresses query-relevant information, ensuring that the model retains the most pertinent content. Our empirical results demonstrate that Query Dependent LCIRC (QD-LCIRC) significantly improves LLM's ability to manage extended contexts, making it well-suited for tasks that require both comprehensive context understanding and query relevance. \u25b3 Less Submitted 9 February, 2025; originally announced February 2025. Comments: Accepted to NAACL 2025 Main arXiv:2501.10455 [pdf, other] PhyDeformer: High-Quality Non-Rigid Garment Registration with Physics-Awareness Authors: Boyang Yu, Frederic Cordier, Hyewon Seo Abstract: We present PhyDeformer, a new deformation method for high-quality garment mesh registration. It operates in two phases: In the first phase, a garment grading is performed to achieve a coarse 3D alignment between the mesh template and the target mesh, accounting for proportional scaling and fit (e.g. length, size). Then, the graded mesh is refined to align with the fine-grained details of the 3D ta\u2026 \u25bd More We present PhyDeformer, a new deformation method for high-quality garment mesh registration. It operates in two phases: In the first phase, a garment grading is performed to achieve a coarse 3D alignment between the mesh template and the target mesh, accounting for proportional scaling and fit (e.g. length, size). Then, the graded mesh is refined to align with the fine-grained details of the 3D target through an optimization coupled with the Jacobian-based deformation framework. Both quantitative and qualitative evaluations on synthetic and real garments highlight the effectiveness of our method. \u25b3 Less Submitted 24 January, 2025; v1 submitted 14 January, 2025; originally announced January 2025. arXiv:2501.06442 [pdf, other] ARES: Auxiliary Range Expansion for Outlier Synthesis Authors: Eui-Soo Jung, Hae-Hun Seo, Hyun-Woo Jung, Je-Geon Oh, Yoon-Yeong Kim Abstract: Recent successes of artificial intelligence and deep learning often depend on the well-collected training dataset which is assumed to have an identical distribution with the test dataset. However, this assumption, which is called closed-set learning, is hard to meet in realistic scenarios for deploying deep learning models. As one of the solutions to mitigate this assumption, research on out-of-di\u2026 \u25bd More Recent successes of artificial intelligence and deep learning often depend on the well-collected training dataset which is assumed to have an identical distribution with the test dataset. However, this assumption, which is called closed-set learning, is hard to meet in realistic scenarios for deploying deep learning models. As one of the solutions to mitigate this assumption, research on out-of-distribution (OOD) detection has been actively explored in various domains. In OOD detection, we assume that we are given the data of a new class that was not seen in the training phase, i.e., outlier, at the evaluation phase. The ultimate goal of OOD detection is to detect and classify such unseen outlier data as a novel \"unknown\" class. Among various research branches for OOD detection, generating a virtual outlier during the training phase has been proposed. However, conventional generation-based methodologies utilize in-distribution training dataset to imitate outlier instances, which limits the quality of the synthesized virtual outlier instance itself. In this paper, we propose a novel methodology for OOD detection named Auxiliary Range Expansion for Outlier Synthesis, or ARES. ARES models the region for generating out-of-distribution instances by escaping from the given in-distribution region; instead of remaining near the boundary of in-distribution region. Various stages consists ARES to ultimately generate valuable OOD-like virtual instances. The energy score-based discriminator is then trained to effectively separate in-distribution data and outlier data. Quantitative experiments on broad settings show the improvement of performance by our method, and qualitative results provide logical explanations of the mechanism behind it. \u25b3 Less Submitted 11 January, 2025; originally announced January 2025. arXiv:2412.14223 [pdf, other] Towards Precise Prediction Uncertainty in GNNs: Refining GNNs with Topology-grouping Strategy Authors: Hyunjin Seo, Kyusung Seo, Joonhyung Park, Eunho Yang Abstract: Recent advancements in graph neural networks (GNNs) have highlighted the critical need of calibrating model predictions, with neighborhood prediction similarity recognized as a pivotal component. Existing studies suggest that nodes with analogous neighborhood prediction similarity often exhibit similar calibration characteristics. Building on this insight, recent approaches incorporate neighborhoo\u2026 \u25bd More Recent advancements in graph neural networks (GNNs) have highlighted the critical need of calibrating model predictions, with neighborhood prediction similarity recognized as a pivotal component. Existing studies suggest that nodes with analogous neighborhood prediction similarity often exhibit similar calibration characteristics. Building on this insight, recent approaches incorporate neighborhood similarity into node-wise temperature scaling techniques. However, our analysis reveals that this assumption does not hold universally. Calibration errors can differ significantly even among nodes with comparable neighborhood similarity, depending on their confidence levels. This necessitates a re-evaluation of existing GNN calibration methods, as a single, unified approach may lead to sub-optimal calibration. In response, we introduce **Simi-Mailbox**, a novel approach that categorizes nodes by both neighborhood similarity and their own confidence, irrespective of proximity or connectivity. Our method allows fine-grained calibration by employing *group-specific* temperature scaling, with each temperature tailored to address the specific miscalibration level of affiliated nodes, rather than adhering to a uniform trend based on neighborhood similarity. Extensive experiments demonstrate the effectiveness of our **Simi-Mailbox** across diverse datasets on different GNN architectures, achieving up to 13.79\\% error reduction compared to uncalibrated GNN predictions. \u25b3 Less Submitted 23 February, 2025; v1 submitted 18 December, 2024; originally announced December 2024. Comments: Accepted at AAAI 2025 arXiv:2412.13705 [pdf, other] Mitigating Adversarial Attacks in LLMs through Defensive Suffix Generation Authors: Minkyoung Kim, Yunha Kim, Hyeram Seo, Heejung Choi, Jiye Han, Gaeun Kee, Soyoung Ko, HyoJe Jung, Byeolhee Kim, Young-Hak Kim, Sanghyun Park, Tae Joon Jun Abstract: Large language models (LLMs) have exhibited outstanding performance in natural language processing tasks. However, these models remain susceptible to adversarial attacks in which slight input perturbations can lead to harmful or misleading outputs. A gradient-based defensive suffix generation algorithm is designed to bolster the robustness of LLMs. By appending carefully optimized defensive suffix\u2026 \u25bd More Large language models (LLMs) have exhibited outstanding performance in natural language processing tasks. However, these models remain susceptible to adversarial attacks in which slight input perturbations can lead to harmful or misleading outputs. A gradient-based defensive suffix generation algorithm is designed to bolster the robustness of LLMs. By appending carefully optimized defensive suffixes to input prompts, the algorithm mitigates adversarial influences while preserving the models' utility. To enhance adversarial understanding, a novel total loss function ($L_{\\text{total}}$) combining defensive loss ($L_{\\text{def}}$) and adversarial loss ($L_{\\text{adv}}$) generates defensive suffixes more effectively. Experimental evaluations conducted on open-source LLMs such as Gemma-7B, mistral-7B, Llama2-7B, and Llama2-13B show that the proposed method reduces attack success rates (ASR) by an average of 11\\% compared to models without defensive suffixes. Additionally, the perplexity score of Gemma-7B decreased from 6.57 to 3.93 when applying the defensive suffix generated by openELM-270M. Furthermore, TruthfulQA evaluations demonstrate consistent improvements with Truthfulness scores increasing by up to 10\\% across tested configurations. This approach significantly enhances the security of LLMs in critical applications without requiring extensive retraining. \u25b3 Less Submitted 18 December, 2024; originally announced December 2024. Comments: 9 pages, 2 figures arXiv:2412.01471 [pdf, other] Multi-Granularity Video Object Segmentation Authors: Sangbeom Lim, Seongchan Kim, Seungjun An, Seokju Cho, Paul Hongsuck Seo, Seungryong Kim Abstract: Current benchmarks for video segmentation are limited to annotating only salient objects (i.e., foreground instances). Despite their impressive architectural designs, previous works trained on these benchmarks have struggled to adapt to real-world scenarios. Thus, developing a new video segmentation dataset aimed at tracking multi-granularity segmentation target in the video scene is necessary. In\u2026 \u25bd More Current benchmarks for video segmentation are limited to annotating only salient objects (i.e., foreground instances). Despite their impressive architectural designs, previous works trained on these benchmarks have struggled to adapt to real-world scenarios. Thus, developing a new video segmentation dataset aimed at tracking multi-granularity segmentation target in the video scene is necessary. In this work, we aim to generate multi-granularity video segmentation dataset that is annotated for both salient and non-salient masks. To achieve this, we propose a large-scale, densely annotated multi-granularity video object segmentation (MUG-VOS) dataset that includes various types and granularities of mask annotations. We automatically collected a training set that assists in tracking both salient and non-salient objects, and we also curated a human-annotated test set for reliable evaluation. In addition, we present memory-based mask propagation model (MMPM), trained and evaluated on MUG-VOS dataset, which leads to the best performance among the existing video object segmentation methods and Segment SAM-based video segmentation methods. Project page is available at https://cvlab-kaist.github.io/MUG-VOS. \u25b3 Less Submitted 3 December, 2024; v1 submitted 2 December, 2024; originally announced December 2024. Comments: Project Page: https://cvlab-kaist.github.io/MUG-VOS arXiv:2410.14696 [pdf, other] REBIND: Enhancing ground-state molecular conformation via force-based graph rewiring Authors: Taewon Kim, Hyunjin Seo, Sungsoo Ahn, Eunho Yang Abstract: Predicting the ground-state 3D molecular conformations from 2D molecular graphs is critical in computational chemistry due to its profound impact on molecular properties. Deep learning (DL) approaches have recently emerged as promising alternatives to computationally-heavy classical methods such as density functional theory (DFT). However, we discover that existing DL methods inadequately model in\u2026 \u25bd More Predicting the ground-state 3D molecular conformations from 2D molecular graphs is critical in computational chemistry due to its profound impact on molecular properties. Deep learning (DL) approaches have recently emerged as promising alternatives to computationally-heavy classical methods such as density functional theory (DFT). However, we discover that existing DL methods inadequately model inter-atomic forces, particularly for non-bonded atomic pairs, due to their naive usage of bonds and pairwise distances. Consequently, significant prediction errors occur for atoms with low degree (i.e., low coordination numbers) whose conformations are primarily influenced by non-bonded interactions. To address this, we propose REBIND, a novel framework that rewires molecular graphs by adding edges based on the Lennard-Jones potential to capture non-bonded interactions for low-degree atoms. Experimental results demonstrate that REBIND significantly outperforms state-of-the-art methods across various molecular sizes, achieving up to a 20\\% reduction in prediction error. \u25b3 Less Submitted 4 October, 2024; originally announced October 2024. Comments: 17 pages, 4 figures, 5 tables arXiv:2410.02303 [pdf, other] Semantic Communication and Control Co-Design for Multi-Objective Correlated Dynamics Authors: Abanoub M. Girgis, Hyowoon Seo, Mehdi Bennis Abstract: This letter introduces a machine-learning approach to learning the semantic dynamics of correlated systems with different control rules and dynamics. By leveraging the Koopman operator in an autoencoder (AE) framework, the system's state evolution is linearized in the latent space using a dynamic semantic Koopman (DSK) model, capturing the baseline semantic dynamics. Signal temporal logic (STL) is\u2026 \u25bd More This letter introduces a machine-learning approach to learning the semantic dynamics of correlated systems with different control rules and dynamics. By leveraging the Koopman operator in an autoencoder (AE) framework, the system's state evolution is linearized in the latent space using a dynamic semantic Koopman (DSK) model, capturing the baseline semantic dynamics. Signal temporal logic (STL) is incorporated through a logical semantic Koopman (LSK) model to encode system-specific control rules. These models form the proposed logical Koopman AE framework that reduces communication costs while improving state prediction accuracy and control performance, showing a 91.65% reduction in communication samples and significant performance gains in simulation. \u25b3 Less Submitted 3 October, 2024; originally announced October 2024. arXiv:2409.19846 [pdf, other] Towards Open-Vocabulary Semantic Segmentation Without Semantic Labels Authors: Heeseong Shin, Chaehyun Kim, Sunghwan Hong, Seokju Cho, Anurag Arnab, Paul Hongsuck Seo, Seungryong Kim Abstract: Large-scale vision-language models like CLIP have demonstrated impressive open-vocabulary capabilities for image-level tasks, excelling in recognizing what objects are present. However, they struggle with pixel-level recognition tasks like semantic segmentation, which additionally require understanding where the objects are located. In this work, we propose a novel method, PixelCLIP, to adapt the\u2026 \u25bd More Large-scale vision-language models like CLIP have demonstrated impressive open-vocabulary capabilities for image-level tasks, excelling in recognizing what objects are present. However, they struggle with pixel-level recognition tasks like semantic segmentation, which additionally require understanding where the objects are located. In this work, we propose a novel method, PixelCLIP, to adapt the CLIP image encoder for pixel-level understanding by guiding the model on where, which is achieved using unlabeled images and masks generated from vision foundation models such as SAM and DINO. To address the challenges of leveraging masks without semantic labels, we devise an online clustering algorithm using learnable class names to acquire general semantic concepts. PixelCLIP shows significant performance improvements over CLIP and competitive results compared to caption-supervised methods in open-vocabulary semantic segmentation. Project page is available at https://cvlab-kaist.github.io/PixelCLIP \u25b3 Less Submitted 29 September, 2024; originally announced September 2024. Comments: To appear at NeurIPS 2024. Project page is available at https://cvlab-kaist.github.io/PixelCLIP arXiv:2409.06210 [pdf, other] INTRA: Interaction Relationship-aware Weakly Supervised Affordance Grounding Authors: Ji Ha Jang, Hoigi Seo, Se Young Chun Abstract: Affordance denotes the potential interactions inherent in objects. The perception of affordance can enable intelligent agents to navigate and interact with new environments efficiently. Weakly supervised affordance grounding teaches agents the concept of affordance without costly pixel-level annotations, but with exocentric images. Although recent advances in weakly supervised affordance grounding\u2026 \u25bd More Affordance denotes the potential interactions inherent in objects. The perception of affordance can enable intelligent agents to navigate and interact with new environments efficiently. Weakly supervised affordance grounding teaches agents the concept of affordance without costly pixel-level annotations, but with exocentric images. Although recent advances in weakly supervised affordance grounding yielded promising results, there remain challenges including the requirement for paired exocentric and egocentric image dataset, and the complexity in grounding diverse affordances for a single object. To address them, we propose INTeraction Relationship-aware weakly supervised Affordance grounding (INTRA). Unlike prior arts, INTRA recasts this problem as representation learning to identify unique features of interactions through contrastive learning with exocentric images only, eliminating the need for paired datasets. Moreover, we leverage vision-language model embeddings for performing affordance grounding flexibly with any text, designing text-conditioned affordance map generation to reflect interaction relationship for contrastive learning and enhancing robustness with our text synonym augmentation. Our method outperformed prior arts on diverse datasets such as AGD20K, IIT-AFF, CAD and UMD. Additionally, experimental results demonstrate that our method has remarkable domain scalability for synthesized images / illustrations and is capable of performing affordance grounding for novel interactions and objects. \u25b3 Less Submitted 10 September, 2024; originally announced September 2024. arXiv:2409.04598 [pdf, other] A Novel Dataset for Video-Based Autism Classification Leveraging Extra-Stimulatory Behavior Authors: Manuel Serna-Aguilera, Xuan Bac Nguyen, Han-Seok Seo, Khoa Luu Abstract: Autism Spectrum Disorder (ASD) can affect individuals at varying degrees of intensity, from challenges in overall health, communication, and sensory processing, and this often begins at a young age. Thus, it is critical for medical professionals to be able to accurately diagnose ASD in young children, but doing so is difficult. Deep learning can be responsibly leveraged to improve productivity in\u2026 \u25bd More Autism Spectrum Disorder (ASD) can affect individuals at varying degrees of intensity, from challenges in overall health, communication, and sensory processing, and this often begins at a young age. Thus, it is critical for medical professionals to be able to accurately diagnose ASD in young children, but doing so is difficult. Deep learning can be responsibly leveraged to improve productivity in addressing this task. The availability of data, however, remains a considerable obstacle. Hence, in this work, we introduce the Video ASD dataset--a dataset that contains video frame convolutional and attention map feature data--to foster further progress in the task of ASD classification. The original videos showcase children reacting to chemo-sensory stimuli, among auditory, touch, and vision This dataset contains the features of the frames spanning 2,467 videos, for a total of approximately 1.4 million frames. Additionally, head pose angles are included to account for head movement noise, as well as full-sentence text labels for the taste and smell videos that describe how the facial expression changes before, immediately after, and long after interaction with the stimuli. In addition to providing features, we also test foundation models on this data to showcase how movement noise affects performance and the need for more data and more complex labels. \u25b3 Less Submitted 6 September, 2024; originally announced September 2024. arXiv:2409.02883 [pdf] Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test Authors: Junyoung Park, Eun Hyun Seo, Sunjun Kim, SangHak Yi, Kun Ho Lee, Sungho Won Abstract: Drawing tests like the Rey Complex Figure Test (RCFT) are widely used to assess cognitive functions such as visuospatial skills and memory, making them valuable tools for detecting mild cognitive impairment (MCI). Despite their utility, existing predictive models based on these tests often suffer from limitations like small sample sizes and lack of external validation, which undermine their reliab\u2026 \u25bd More Drawing tests like the Rey Complex Figure Test (RCFT) are widely used to assess cognitive functions such as visuospatial skills and memory, making them valuable tools for detecting mild cognitive impairment (MCI). Despite their utility, existing predictive models based on these tests often suffer from limitations like small sample sizes and lack of external validation, which undermine their reliability. We developed a multi-stream deep learning framework that integrates two distinct processing streams: a multi-head self-attention based spatial stream using raw RCFT images and a scoring stream employing a previously developed automated scoring system. Our model was trained on data from 1,740 subjects in the Korean cohort and validated on an external hospital dataset of 222 subjects from Korea. The proposed multi-stream model demonstrated superior performance over baseline models (AUC = 0.872, Accuracy = 0.781) in external validation. The integration of both spatial and scoring streams enables the model to capture intricate visual details from the raw images while also incorporating structured scoring data, which together enhance its ability to detect subtle cognitive impairments. This dual approach not only improves predictive accuracy but also increases the robustness of the model, making it more reliable in diverse clinical settings. Our model has practical implications for clinical settings, where it could serve as a cost-effective tool for early MCI screening. \u25b3 Less Submitted 4 September, 2024; originally announced September 2024. Comments: 20 pages, 3 figures, 2 tables arXiv:2407.18442 [pdf, other] Guidance-Based Prompt Data Augmentation in Specialized Domains for Named Entity Recognition Authors: Hyeonseok Kang, Hyein Seo, Jeesu Jung, Sangkeun Jung, Du-Seong Chang, Riwoo Chung Abstract: While the abundance of rich and vast datasets across numerous fields has facilitated the advancement of natural language processing, sectors in need of specialized data types continue to struggle with the challenge of finding quality data. Our study introduces a novel guidance data augmentation technique utilizing abstracted context and sentence structures to produce varied sentences while maintai\u2026 \u25bd More While the abundance of rich and vast datasets across numerous fields has facilitated the advancement of natural language processing, sectors in need of specialized data types continue to struggle with the challenge of finding quality data. Our study introduces a novel guidance data augmentation technique utilizing abstracted context and sentence structures to produce varied sentences while maintaining context-entity relationships, addressing data scarcity challenges. By fostering a closer relationship between context, sentence structure, and role of entities, our method enhances data augmentation's effectiveness. Consequently, by showcasing diversification in both entity-related vocabulary and overall sentence structure, and simultaneously improving the training performance of named entity recognition task. \u25b3 Less Submitted 25 July, 2024; originally announced July 2024. arXiv:2407.07412 [pdf, other] Pseudo-RIS: Distinctive Pseudo-supervision Generation for Referring Image Segmentation Authors: Seonghoon Yu, Paul Hongsuck Seo, Jeany Son Abstract: We propose a new framework that automatically generates high-quality segmentation masks with their referring expressions as pseudo supervisions for referring image segmentation (RIS). These pseudo supervisions allow the training of any supervised RIS methods without the cost of manual labeling. To achieve this, we incorporate existing segmentation and image captioning foundation models, leveraging\u2026 \u25bd More We propose a new framework that automatically generates high-quality segmentation masks with their referring expressions as pseudo supervisions for referring image segmentation (RIS). These pseudo supervisions allow the training of any supervised RIS methods without the cost of manual labeling. To achieve this, we incorporate existing segmentation and image captioning foundation models, leveraging their broad generalization capabilities. However, the naive incorporation of these models may generate non-distinctive expressions that do not distinctively refer to the target masks. To address this challenge, we propose two-fold strategies that generate distinctive captions: 1) 'distinctive caption sampling', a new decoding method for the captioning model, to generate multiple expression candidates with detailed words focusing on the target. 2) 'distinctiveness-based text filtering' to further validate the candidates and filter out those with a low level of distinctiveness. These two strategies ensure that the generated text supervisions can distinguish the target from other objects, making them appropriate for the RIS annotations. Our method significantly outperforms both weakly and zero-shot SoTA methods on the RIS benchmark datasets. It also surpasses fully supervised methods in unseen domains, proving its capability to tackle the open-world challenge within RIS. Furthermore, integrating our method with human annotations yields further improvements, highlighting its potential in semi-supervised learning applications. \u25b3 Less Submitted 17 July, 2024; v1 submitted 10 July, 2024; originally announced July 2024. Comments: Accepted to ECCV 2024 arXiv:2406.08718 [pdf, other] Enhancing Psychotherapy Counseling: A Data Augmentation Pipeline Leveraging Large Language Models for Counseling Conversations Authors: Jun-Woo Kim, Ji-Eun Han, Jun-Seok Koh, Hyeon-Tae Seo, Du-Seong Chang Abstract: We introduce a pipeline that leverages Large Language Models (LLMs) to transform single-turn psychotherapy counseling sessions into multi-turn interactions. While AI-supported online counseling services for individuals with mental disorders exist, they are often constrained by the limited availability of multi-turn training datasets and frequently fail to fully utilize therapists' expertise. Our p\u2026 \u25bd More We introduce a pipeline that leverages Large Language Models (LLMs) to transform single-turn psychotherapy counseling sessions into multi-turn interactions. While AI-supported online counseling services for individuals with mental disorders exist, they are often constrained by the limited availability of multi-turn training datasets and frequently fail to fully utilize therapists' expertise. Our proposed pipeline effectively addresses these limitations. The pipeline comprises two main steps: 1) Information Extraction and 2) Multi-turn Counseling Generation. Each step is meticulously designed to extract and generate comprehensive multi-turn counseling conversations from the available datasets. Experimental results from both zero-shot and few-shot generation scenarios demonstrate that our approach significantly enhances the ability of LLMs to produce higher quality multi-turn dialogues in the context of mental health counseling. Our pipeline and dataset are publicly available https://github.com/jwkim-chat/A-Data-Augmentation-Pipeline-Leveraging-Large-Language-Models-for-Counseling-Conversations. \u25b3 Less Submitted 12 June, 2024; originally announced June 2024. Comments: IJCAI 2024 AI4Research workshop arXiv:2405.18581 [pdf, other] Unleashing the Potential of Text-attributed Graphs: Automatic Relation Decomposition via Large Language Models Authors: Hyunjin Seo, Taewon Kim, June Yong Yang, Eunho Yang Abstract: Recent advancements in text-attributed graphs (TAGs) have significantly improved the quality of node features by using the textual modeling capabilities of language models. Despite this success, utilizing text attributes to enhance the predefined graph structure remains largely unexplored. Our extensive analysis reveals that conventional edges on TAGs, treated as a single relation (e.g., hyperlink\u2026 \u25bd More Recent advancements in text-attributed graphs (TAGs) have significantly improved the quality of node features by using the textual modeling capabilities of language models. Despite this success, utilizing text attributes to enhance the predefined graph structure remains largely unexplored. Our extensive analysis reveals that conventional edges on TAGs, treated as a single relation (e.g., hyperlinks) in previous literature, actually encompass mixed semantics (e.g., \"advised by\" and \"participates in\"). This simplification hinders the representation learning process of Graph Neural Networks (GNNs) on downstream tasks, even when integrated with advanced node features. In contrast, we discover that decomposing these edges into distinct semantic relations significantly enhances the performance of GNNs. Despite this, manually identifying and labeling of edges to corresponding semantic relations is labor-intensive, often requiring domain expertise. To this end, we introduce RoSE (Relation-oriented Semantic Edge-decomposition), a novel framework that leverages the capability of Large Language Models (LLMs) to decompose the graph structure by analyzing raw text attributes - in a fully automated manner. RoSE operates in two stages: (1) identifying meaningful relations using an LLM-based generator and discriminator, and (2) categorizing each edge into corresponding relations by analyzing textual contents associated with connected nodes via an LLM-based decomposer. Extensive experiments demonstrate that our model-agnostic framework significantly enhances node classification performance across various datasets, with improvements of up to 16% on the Wisconsin dataset. \u25b3 Less Submitted 28 May, 2024; originally announced May 2024. arXiv:2405.06778 [pdf, other] Shape Conditioned Human Motion Generation with Diffusion Model Authors: Kebing Xue, Hyewon Seo Abstract: Human motion synthesis is an important task in computer graphics and computer vision. While focusing on various conditioning signals such as text, action class, or audio to guide the generation process, most existing methods utilize skeleton-based pose representation, requiring additional skinning to produce renderable meshes. Given that human motion is a complex interplay of bones, joints, and mu\u2026 \u25bd More Human motion synthesis is an important task in computer graphics and computer vision. While focusing on various conditioning signals such as text, action class, or audio to guide the generation process, most existing methods utilize skeleton-based pose representation, requiring additional skinning to produce renderable meshes. Given that human motion is a complex interplay of bones, joints, and muscles, considering solely the skeleton for generation may neglect their inherent interdependency, which can limit the variability and precision of the generated results. To address this issue, we propose a Shape-conditioned Motion Diffusion model (SMD), which enables the generation of motion sequences directly in mesh format, conditioned on a specified target mesh. In SMD, the input meshes are transformed into spectral coefficients using graph Laplacian, to efficiently represent meshes. Subsequently, we propose a Spectral-Temporal Autoencoder (STAE) to leverage cross-temporal dependencies within the spectral domain. Extensive experimental evaluations show that SMD not only produces vivid and realistic motions but also achieves competitive performance in text-to-motion and action-to-motion tasks when compared to state-of-the-art methods. \u25b3 Less Submitted 10 May, 2024; originally announced May 2024. arXiv:2404.14664 [pdf, ps, other] Employing Layerwised Unsupervised Learning to Lessen Data and Loss Requirements in Forward-Forward Algorithms Authors: Taewook Hwang, Hyein Seo, Sangkeun Jung Abstract: Recent deep learning models such as ChatGPT utilizing the back-propagation algorithm have exhibited remarkable performance. However, the disparity between the biological brain processes and the back-propagation algorithm has been noted. The Forward-Forward algorithm, which trains deep learning models solely through the forward pass, has emerged to address this. Although the Forward-Forward algorit\u2026 \u25bd More Recent deep learning models such as ChatGPT utilizing the back-propagation algorithm have exhibited remarkable performance. However, the disparity between the biological brain processes and the back-propagation algorithm has been noted. The Forward-Forward algorithm, which trains deep learning models solely through the forward pass, has emerged to address this. Although the Forward-Forward algorithm cannot replace back-propagation due to limitations such as having to use special input and loss functions, it has the potential to be useful in special situations where back-propagation is difficult to use. To work around this limitation and verify usability, we propose an Unsupervised Forward-Forward algorithm. Using an unsupervised learning model enables training with usual loss functions and inputs without restriction. Through this approach, we lead to stable learning and enable versatile utilization across various datasets and tasks. From a usability perspective, given the characteristics of the Forward-Forward algorithm and the advantages of the proposed method, we anticipate its practical application even in scenarios such as federated learning, where deep learning layers need to be trained separately in physically distributed environments. \u25b3 Less Submitted 22 April, 2024; originally announced April 2024. Comments: 8 pages, 6 figures arXiv:2404.05144 [pdf, other] Enhancing Clinical Efficiency through LLM: Discharge Note Generation for Cardiac Patients Authors: HyoJe Jung, Yunha Kim, Heejung Choi, Hyeram Seo, Minkyoung Kim, JiYe Han, Gaeun Kee, Seohyun Park, Soyoung Ko, Byeolhee Kim, Suyeon Kim, Tae Joon Jun, Young-Hak Kim Abstract: Medical documentation, including discharge notes, is crucial for ensuring patient care quality, continuity, and effective medical communication. However, the manual creation of these documents is not only time-consuming but also prone to inconsistencies and potential errors. The automation of this documentation process using artificial intelligence (AI) represents a promising area of innovation in\u2026 \u25bd More Medical documentation, including discharge notes, is crucial for ensuring patient care quality, continuity, and effective medical communication. However, the manual creation of these documents is not only time-consuming but also prone to inconsistencies and potential errors. The automation of this documentation process using artificial intelligence (AI) represents a promising area of innovation in healthcare. This study directly addresses the inefficiencies and inaccuracies in creating discharge notes manually, particularly for cardiac patients, by employing AI techniques, specifically large language model (LLM). Utilizing a substantial dataset from a cardiology center, encompassing wide-ranging medical records and physician assessments, our research evaluates the capability of LLM to enhance the documentation process. Among the various models assessed, Mistral-7B distinguished itself by accurately generating discharge notes that significantly improve both documentation efficiency and the continuity of care for patients. These notes underwent rigorous qualitative evaluation by medical expert, receiving high marks for their clinical relevance, completeness, readability, and contribution to informed decision-making and care planning. Coupled with quantitative analyses, these results confirm Mistral-7B's efficacy in distilling complex medical information into concise, coherent summaries. Overall, our findings illuminate the considerable promise of specialized LLM, such as Mistral-7B, in refining healthcare documentation workflows and advancing patient care. This study lays the groundwork for further integrating advanced AI technologies in healthcare, demonstrating their potential to revolutionize patient documentation and support better care outcomes. \u25b3 Less Submitted 7 April, 2024; originally announced April 2024. Comments: 10 pages, 1 figure, 3 tables, conference arXiv:2404.04544 [pdf, other] BeyondScene: Higher-Resolution Human-Centric Scene Generation With Pretrained Diffusion Authors: Gwanghyun Kim, Hayeon Kim, Hoigi Seo, Dong Un Kang, Se Young Chun Abstract: Generating higher-resolution human-centric scenes with details and controls remains a challenge for existing text-to-image diffusion models. This challenge stems from limited training image size, text encoder capacity (limited tokens), and the inherent difficulty of generating complex scenes involving multiple humans. While current methods attempted to address training size limit only, they often\u2026 \u25bd More Generating higher-resolution human-centric scenes with details and controls remains a challenge for existing text-to-image diffusion models. This challenge stems from limited training image size, text encoder capacity (limited tokens), and the inherent difficulty of generating complex scenes involving multiple humans. While current methods attempted to address training size limit only, they often yielded human-centric scenes with severe artifacts. We propose BeyondScene, a novel framework that overcomes prior limitations, generating exquisite higher-resolution (over 8K) human-centric scenes with exceptional text-image correspondence and naturalness using existing pretrained diffusion models. BeyondScene employs a staged and hierarchical approach to initially generate a detailed base image focusing on crucial elements in instance creation for multiple humans and detailed descriptions beyond token limit of diffusion model, and then to seamlessly convert the base image to a higher-resolution output, exceeding training image size and incorporating details aware of text and instances via our novel instance-aware hierarchical enlargement process that consists of our proposed high-frequency injected forward diffusion and adaptive joint diffusion. BeyondScene surpasses existing methods in terms of correspondence with detailed text descriptions and naturalness, paving the way for advanced applications in higher-resolution human-centric scene creation beyond the capacity of pretrained diffusion models without costly retraining. Project page: https://janeyeon.github.io/beyond-scene. \u25b3 Less Submitted 6 April, 2024; originally announced April 2024. Comments: Project page: https://janeyeon.github.io/beyond-scene arXiv:2404.03924 [pdf, other] Learning Correlation Structures for Vision Transformers Authors: Manjin Kim, Paul Hongsuck Seo, Cordelia Schmid, Minsu Cho Abstract: We introduce a new attention mechanism, dubbed structural self-attention (StructSA), that leverages rich correlation patterns naturally emerging in key-query interactions of attention. StructSA generates attention maps by recognizing space-time structures of key-query correlations via convolution and uses them to dynamically aggregate local contexts of value features. This effectively leverages ri\u2026 \u25bd More We introduce a new attention mechanism, dubbed structural self-attention (StructSA), that leverages rich correlation patterns naturally emerging in key-query interactions of attention. StructSA generates attention maps by recognizing space-time structures of key-query correlations via convolution and uses them to dynamically aggregate local contexts of value features. This effectively leverages rich structural patterns in images and videos such as scene layouts, object motion, and inter-object relations. Using StructSA as a main building block, we develop the structural vision transformer (StructViT) and evaluate its effectiveness on both image and video classification tasks, achieving state-of-the-art results on ImageNet-1K, Kinetics-400, Something-Something V1 & V2, Diving-48, and FineGym. \u25b3 Less Submitted 5 April, 2024; originally announced April 2024. Comments: Accepted to CVPR 2024 arXiv:2404.03745 [pdf, other] Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations Authors: Mahjabin Nahar, Haeseung Seo, Eun-Ju Lee, Aiping Xiong, Dongwon Lee Abstract: The widespread adoption and transformative effects of large language models (LLMs) have sparked concerns regarding their capacity to produce inaccurate and fictitious content, referred to as `hallucinations'. Given the potential risks associated with hallucinations, humans should be able to identify them. This research aims to understand the human perception of LLM hallucinations by systematically\u2026 \u25bd More The widespread adoption and transformative effects of large language models (LLMs) have sparked concerns regarding their capacity to produce inaccurate and fictitious content, referred to as `hallucinations'. Given the potential risks associated with hallucinations, humans should be able to identify them. This research aims to understand the human perception of LLM hallucinations by systematically varying the degree of hallucination (genuine, minor hallucination, major hallucination) and examining its interaction with warning (i.e., a warning of potential inaccuracies: absent vs. present). Participants (N=419) from Prolific rated the perceived accuracy and engaged with content (e.g., like, dislike, share) in a Q/A format. Participants ranked content as truthful in the order of genuine, minor hallucination, and major hallucination, and user engagement behaviors mirrored this pattern. More importantly, we observed that warning improved the detection of hallucination without significantly affecting the perceived truthfulness of genuine content. We conclude by offering insights for future tools to aid human detection of hallucinations. All survey materials, demographic questions, and post-session questions are available at: https://github.com/MahjabinNahar/fakes-of-varying-shades-survey-materials \u25b3 Less Submitted 12 August, 2024; v1 submitted 4 April, 2024; originally announced April 2024. Comments: Accepted at COLM 2024 arXiv:2404.01954 [pdf, other] HyperCLOVA X Technical Report Authors: Kang Min Yoo, Jaegeun Han, Sookyo In, Heewon Jeon, Jisu Jeong, Jaewook Kang, Hyunwook Kim, Kyung-Min Kim, Munhyong Kim, Sungju Kim, Donghyun Kwak, Hanock Kwak, Se Jung Kwon, Bado Lee, Dongsoo Lee, Gichang Lee, Jooho Lee, Baeseong Park, Seongjin Shin, Joonsang Yu, Seolki Baek, Sumin Byeon, Eungsup Cho, Dooseok Choe, Jeesung Han , et al. (371 additional authors not shown) Abstract: We introduce HyperCLOVA X, a family of large language models (LLMs) tailored to the Korean language and culture, along with competitive capabilities in English, math, and coding. HyperCLOVA X was trained on a balanced mix of Korean, English, and code data, followed by instruction-tuning with high-quality human-annotated datasets while abiding by strict safety guidelines reflecting our commitment t\u2026 \u25bd More We introduce HyperCLOVA X, a family of large language models (LLMs) tailored to the Korean language and culture, along with competitive capabilities in English, math, and coding. HyperCLOVA X was trained on a balanced mix of Korean, English, and code data, followed by instruction-tuning with high-quality human-annotated datasets while abiding by strict safety guidelines reflecting our commitment to responsible AI. The model is evaluated across various benchmarks, including comprehensive reasoning, knowledge, commonsense, factuality, coding, math, chatting, instruction-following, and harmlessness, in both Korean and English. HyperCLOVA X exhibits strong reasoning capabilities in Korean backed by a deep understanding of the language and cultural nuances. Further analysis of the inherent bilingual nature and its extension to multilingualism highlights the model's cross-lingual proficiency and strong generalization ability to untargeted languages, including machine translation between several language pairs and cross-lingual inference tasks. We believe that HyperCLOVA X can provide helpful guidance for regions or countries in developing their sovereign LLMs. \u25b3 Less Submitted 13 April, 2024; v1 submitted 2 April, 2024; originally announced April 2024. Comments: 44 pages; updated authors list and fixed author names arXiv:2404.01339 [pdf, other] Humane Speech Synthesis through Zero-Shot Emotion and Disfluency Generation Authors: Rohan Chaudhury, Mihir Godbole, Aakash Garg, Jinsil Hwaryoung Seo Abstract: Contemporary conversational systems often present a significant limitation: their responses lack the emotional depth and disfluent characteristic of human interactions. This absence becomes particularly noticeable when users seek more personalized and empathetic interactions. Consequently, this makes them seem mechanical and less relatable to human users. Recognizing this gap, we embarked on a jou\u2026 \u25bd More Contemporary conversational systems often present a significant limitation: their responses lack the emotional depth and disfluent characteristic of human interactions. This absence becomes particularly noticeable when users seek more personalized and empathetic interactions. Consequently, this makes them seem mechanical and less relatable to human users. Recognizing this gap, we embarked on a journey to humanize machine communication, to ensure AI systems not only comprehend but also resonate. To address this shortcoming, we have designed an innovative speech synthesis pipeline. Within this framework, a cutting-edge language model introduces both human-like emotion and disfluencies in a zero-shot setting. These intricacies are seamlessly integrated into the generated text by the language model during text generation, allowing the system to mirror human speech patterns better, promoting more intuitive and natural user interactions. These generated elements are then adeptly transformed into corresponding speech patterns and emotive sounds using a rule-based approach during the text-to-speech phase. Based on our experiments, our novel system produces synthesized speech that's almost indistinguishable from genuine human communication, making each interaction feel more personal and authentic. \u25b3 Less Submitted 30 March, 2024; originally announced April 2024. Comments: 10 pages, 1 figure, for associated code and media files, see https://github.com/Rohan-Chaudhury/Humane-Speech-Synthesis-through-Zero-Shot-Emotion-and-Disfluency-Generation arXiv:2404.00930 [pdf, other] PSYDIAL: Personality-based Synthetic Dialogue Generation using Large Language Models Authors: Ji-Eun Han, Jun-Seok Koh, Hyeon-Tae Seo, Du-Seong Chang, Kyung-Ah Sohn Abstract: We present a novel end-to-end personality-based synthetic dialogue data generation pipeline, specifically designed to elicit responses from large language models via prompting. We design the prompts to generate more human-like dialogues considering real-world scenarios when users engage with chatbots. We introduce PSYDIAL, the first Korean dialogue dataset focused on personality-based dialogues, c\u2026 \u25bd More We present a novel end-to-end personality-based synthetic dialogue data generation pipeline, specifically designed to elicit responses from large language models via prompting. We design the prompts to generate more human-like dialogues considering real-world scenarios when users engage with chatbots. We introduce PSYDIAL, the first Korean dialogue dataset focused on personality-based dialogues, curated using our proposed pipeline. Notably, we focus on the Extraversion dimension of the Big Five personality model in our research. Experimental results indicate that while pre-trained models and those fine-tuned with a chit-chat dataset struggle to generate responses reflecting personality, models trained with PSYDIAL show significant improvements. The versatility of our pipeline extends beyond dialogue tasks, offering potential for other non-dialogue related applications. This research opens doors for more nuanced, personality-driven conversational AI in Korean and potentially other languages. Our code is publicly available at https://github.com/jiSilverH/psydial. \u25b3 Less Submitted 1 April, 2024; originally announced April 2024. Comments: LREC-COLING 2024 Main arXiv:2403.19105 [pdf, ps, other] Pilot Signal and Channel Estimator Co-Design for Hybrid-Field XL-MIMO Authors: Yoonseong Kang, Hyowoon Seo, Wan Choi Abstract: This paper addresses the intricate task of hybrid-field channel estimation in extremely large-scale MIMO (XL-MIMO) systems, critical for the progression of 6G communications. Within these systems, comprising a line-of-sight (LoS) channel component alongside far-field and near-field scattering channel components, our objective is to tackle the channel estimation challenge. We encounter two central\u2026 \u25bd More This paper addresses the intricate task of hybrid-field channel estimation in extremely large-scale MIMO (XL-MIMO) systems, critical for the progression of 6G communications. Within these systems, comprising a line-of-sight (LoS) channel component alongside far-field and near-field scattering channel components, our objective is to tackle the channel estimation challenge. We encounter two central hurdles for ensuring dependable sparse channel recovery: the design of pilot signals and channel estimators tailored for hybrid-field communications. To overcome the first challenge, we propose a method to derive optimal pilot signals, aimed at minimizing the mutual coherence of the sensing matrix within the context of compressive sensing (CS) problems. These optimal signals are derived using the alternating direction method of multipliers (ADMM), ensuring robust performance in sparse channel recovery. Additionally, leveraging the acquired optimal pilot signal, we introduce a two-stage channel estimation approach that sequentially estimates the LoS channel component and the hybrid-field scattering channel components. Simulation results attest to the superiority of our co-designed approach for pilot signal and channel estimation over conventional CS-based methods, providing more reliable sparse channel recovery in practical scenarios. \u25b3 Less Submitted 27 March, 2024; originally announced March 2024. arXiv:2403.13756 [pdf, other] Enhancing Gait Video Analysis in Neurodegenerative Diseases by Knowledge Augmentation in Vision Language Model Authors: Diwei Wang, Kun Yuan, Candice Muller, Fr\u00e9d\u00e9ric Blanc, Nicolas Padoy, Hyewon Seo Abstract: We present a knowledge augmentation strategy for assessing the diagnostic groups and gait impairment from monocular gait videos. Based on a large-scale pre-trained Vision Language Model (VLM), our model learns and improves visual, textual, and numerical representations of patient gait videos, through a collective learning across three distinct modalities: gait videos, class-specific descriptions,\u2026 \u25bd More We present a knowledge augmentation strategy for assessing the diagnostic groups and gait impairment from monocular gait videos. Based on a large-scale pre-trained Vision Language Model (VLM), our model learns and improves visual, textual, and numerical representations of patient gait videos, through a collective learning across three distinct modalities: gait videos, class-specific descriptions, and numerical gait parameters. Our specific contributions are two-fold: First, we adopt a knowledge-aware prompt tuning strategy to utilize the class-specific medical description in guiding the text prompt learning. Second, we integrate the paired gait parameters in the form of numerical texts to enhance the numeracy of the textual representation. Results demonstrate that our model not only significantly outperforms state-of-the-art methods in video-based classification tasks but also adeptly decodes the learned class-specific text features into natural language descriptions using the vocabulary of quantitative gait parameters. The code and the model will be made available at our project page: https://lisqzqng.github.io/GaitAnalysisVLM/. \u25b3 Less Submitted 15 October, 2024; v1 submitted 20 March, 2024; originally announced March 2024. Comments: MICCAI 2024 arXiv:2403.06841 [pdf, other] Inverse Garment and Pattern Modeling with a Differentiable Simulator Authors: Boyang Yu, Frederic Cordier, Hyewon Seo Abstract: The capability to generate simulation-ready garment models from 3D shapes of clothed humans will significantly enhance the interpretability of captured geometry of real garments, as well as their faithful reproduction in the virtual world. This will have notable impact on fields like shape capture in social VR, and virtual try-on in the fashion industry. To align with the garment modeling process\u2026 \u25bd More The capability to generate simulation-ready garment models from 3D shapes of clothed humans will significantly enhance the interpretability of captured geometry of real garments, as well as their faithful reproduction in the virtual world. This will have notable impact on fields like shape capture in social VR, and virtual try-on in the fashion industry. To align with the garment modeling process standardized by the fashion industry as well as cloth simulation softwares, it is required to recover 2D patterns. This involves an inverse garment design problem, which is the focus of our work here: Starting with an arbitrary target garment geometry, our system estimates an animatable garment model by automatically adjusting its corresponding 2D template pattern, along with the material parameters of the physics-based simulation (PBS). Built upon a differentiable cloth simulator, the optimization process is directed towards minimizing the deviation of the simulated garment shape from the target geometry. Moreover, our produced patterns meet manufacturing requirements such as left-to-right-symmetry, making them suited for reverse garment fabrication. We validate our approach on examples of different garment types, and show that our method faithfully reproduces both the draped garment shape and the sewing pattern. \u25b3 Less Submitted 21 October, 2024; v1 submitted 11 March, 2024; originally announced March 2024. arXiv:2403.05093 [pdf, other] Spectrum Translation for Refinement of Image Generation (STIG) Based on Contrastive Learning and Spectral Filter Profile Authors: Seokjun Lee, Seung-Won Jung, Hyunseok Seo Abstract: Currently, image generation and synthesis have remarkably progressed with generative models. Despite photo-realistic results, intrinsic discrepancies are still observed in the frequency domain. The spectral discrepancy appeared not only in generative adversarial networks but in diffusion models. In this study, we propose a framework to effectively mitigate the disparity in frequency domain of the\u2026 \u25bd More Currently, image generation and synthesis have remarkably progressed with generative models. Despite photo-realistic results, intrinsic discrepancies are still observed in the frequency domain. The spectral discrepancy appeared not only in generative adversarial networks but in diffusion models. In this study, we propose a framework to effectively mitigate the disparity in frequency domain of the generated images to improve generative performance of both GAN and diffusion models. This is realized by spectrum translation for the refinement of image generation (STIG) based on contrastive learning. We adopt theoretical logic of frequency components in various generative networks. The key idea, here, is to refine the spectrum of the generated image via the concept of image-to-image translation and contrastive learning in terms of digital signal processing. We evaluate our framework across eight fake image datasets and various cutting-edge models to demonstrate the effectiveness of STIG. Our framework outperforms other cutting-edges showing significant decreases in FID and log frequency distance of spectrum. We further emphasize that STIG improves image quality by decreasing the spectral anomaly. Additionally, validation results present that the frequency-based deepfake detector confuses more in the case where fake spectrums are manipulated by STIG. \u25b3 Less Submitted 8 March, 2024; originally announced March 2024. Comments: Accepted to AAAI 2024 arXiv:2402.16774 [pdf, ps, other] Video-Based Autism Detection with Deep Learning Authors: M. Serna-Aguilera, X. B. Nguyen, A. Singh, L. Rockers, S. Park, L. Neely, H. Seo, K. Luu Abstract: Individuals with Autism Spectrum Disorder (ASD) often experience challenges in health, communication, and sensory processing; therefore, early diagnosis is necessary for proper treatment and care. In this work, we consider the problem of detecting or classifying ASD children to aid medical professionals in early diagnosis. We develop a deep learning model that analyzes video clips of children reac\u2026 \u25bd More Individuals with Autism Spectrum Disorder (ASD) often experience challenges in health, communication, and sensory processing; therefore, early diagnosis is necessary for proper treatment and care. In this work, we consider the problem of detecting or classifying ASD children to aid medical professionals in early diagnosis. We develop a deep learning model that analyzes video clips of children reacting to sensory stimuli, with the intent of capturing key differences in reactions and behavior between ASD and non-ASD participants. Unlike many recent studies in ASD classification with MRI data, which require expensive specialized equipment, our method utilizes a powerful but relatively affordable GPU, a standard computer setup, and a video camera for inference. Results show that our model effectively generalizes and understands key differences in the distinct movements of the children. It is noteworthy that our model exhibits successful classification performance despite the limited amount of data for a deep learning problem and limited temporal information available for learning, even with the motion artifacts. \u25b3 Less Submitted 30 March, 2024; v1 submitted 26 February, 2024; originally announced February 2024. Comments: Poster Abstract. Accepted into 2024 IEEE Green Technologies Conference arXiv:2402.10457 [pdf, other] Learning-Augmented Search Data Structures Authors: Chunkai Fu, Brandon G. Nguyen, Jung Hoon Seo, Ryan Zesch, Samson Zhou Abstract: We study the integration of machine learning advice to improve upon traditional data structure designed for efficient search queries. Although there has been recent effort in improving the performance of binary search trees using machine learning advice, e.g., Lin et. al. (ICML 2022), the resulting constructions nevertheless suffer from inherent weaknesses of binary search trees, such as complexit\u2026 \u25bd More We study the integration of machine learning advice to improve upon traditional data structure designed for efficient search queries. Although there has been recent effort in improving the performance of binary search trees using machine learning advice, e.g., Lin et. al. (ICML 2022), the resulting constructions nevertheless suffer from inherent weaknesses of binary search trees, such as complexity of maintaining balance across multiple updates and the inability to handle partially-ordered or high-dimensional datasets. For these reasons, we focus on skip lists and KD trees in this work. Given access to a possibly erroneous oracle that outputs estimated fractional frequencies for search queries on a set of items, we construct skip lists and KD trees that provably provides the optimal expected search time, within nearly a factor of two. In fact, our learning-augmented skip lists and KD trees are still optimal up to a constant factor, even if the oracle is only accurate within a constant factor. We also demonstrate robustness by showing that our data structures achieves an expected search time that is within a constant factor of an oblivious skip list/KD tree construction even when the predictions are arbitrarily incorrect. Finally, we empirically show that our learning-augmented search data structures outperforms their corresponding traditional analogs on both synthetic and real-world datasets. \u25b3 Less Submitted 7 March, 2025; v1 submitted 16 February, 2024; originally announced February 2024. Comments: ICLR 2025 arXiv:2402.09784 [pdf, other] Sequential Recommendation on Temporal Proximities with Contrastive Learning and Self-Attention Authors: Hansol Jung, Hyunwoo Seo, Chiehyeon Lim Abstract: Sequential recommender systems identify user preferences from their past interactions to predict subsequent items optimally. Although traditional deep-learning-based models and modern transformer-based models in previous studies capture unidirectional and bidirectional patterns within user-item interactions, the importance of temporal contexts, such as individual behavioral and societal trend patt\u2026 \u25bd More Sequential recommender systems identify user preferences from their past interactions to predict subsequent items optimally. Although traditional deep-learning-based models and modern transformer-based models in previous studies capture unidirectional and bidirectional patterns within user-item interactions, the importance of temporal contexts, such as individual behavioral and societal trend patterns, remains underexplored. Notably, recent models often neglect similarities in users' actions that occur implicitly among users during analogous timeframes-a concept we term vertical temporal proximity. These models primarily adapt the self-attention mechanisms of the transformer to consider the temporal context in individual user actions. Meanwhile, this adaptation still remains limited in considering the horizontal temporal proximity within item interactions, like distinguishing between subsequent item purchases within a week versus a month. To address these gaps, we propose a sequential recommendation model called TemProxRec, which includes contrastive learning and self-attention methods to consider temporal proximities both across and within user-item interactions. The proposed contrastive learning method learns representations of items selected in close temporal periods across different users to be close. Simultaneously, the proposed self-attention mechanism encodes temporal and positional contexts in a user sequence using both absolute and relative embeddings. This way, our TemProxRec accurately predicts the relevant items based on the user-item interactions within a specific timeframe. We validate this work through comprehensive experiments on TemProxRec, consistently outperforming existing models on benchmark datasets as well as showing the significance of considering the vertical and horizontal temporal proximities into sequential recommendation. \u25b3 Less Submitted 17 February, 2024; v1 submitted 15 February, 2024; originally announced February 2024. Comments: 10 pages, 9 figures arXiv:2402.04563 [pdf, other] Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention Authors: Saebom Leem, Hyunseok Seo Abstract: Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structu\u2026 \u25bd More Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test. \u25b3 Less Submitted 6 February, 2024; originally announced February 2024. Comments: AAAI2024. Code available at https://github.com/LeemSaebom/Attention-Guided-CAM-Visual-Explanations-of-Vision-Transformer-Guided-by-Self-Attention.git arXiv:2402.01261 [pdf, other] TEDDY: Trimming Edges with Degree-based Discrimination strategY Authors: Hyunjin Seo, Jihun Yun, Eunho Yang Abstract: Since the pioneering work on the lottery ticket hypothesis for graph neural networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph lottery tickets (GLT) has become one of the pivotal focus in the GNN community, inspiring researchers to discover sparser GLT while achieving comparable performance to original dense networks. In parallel, the graph structure has gained substant\u2026 \u25bd More Since the pioneering work on the lottery ticket hypothesis for graph neural networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph lottery tickets (GLT) has become one of the pivotal focus in the GNN community, inspiring researchers to discover sparser GLT while achieving comparable performance to original dense networks. In parallel, the graph structure has gained substantial attention as a crucial factor in GNN training dynamics, also elucidated by several recent studies. Despite this, contemporary studies on GLT, in general, have not fully exploited inherent pathways in the graph structure and identified tickets in an iterative manner, which is time-consuming and inefficient. To address these limitations, we introduce TEDDY, a one-shot edge sparsification framework that leverages structural information by incorporating edge-degree information. Following edge sparsification, we encourage the parameter sparsity during training via simple projected gradient descent on the $\\ell_0$ ball. Given the target sparsity levels for both the graph structure and the model parameters, our TEDDY facilitates efficient and rapid realization of GLT within a single training. Remarkably, our experimental results demonstrate that TEDDY significantly surpasses conventional iterative approaches in generalization, even when conducting one-shot sparsification that solely utilizes graph structures, without taking feature information into account. \u25b3 Less Submitted 15 March, 2024; v1 submitted 2 February, 2024; originally announced February 2024. arXiv:2312.04861 [pdf, other] Exploring Radar Data Representations in Autonomous Driving: A Comprehensive Review Authors: Shanliang Yao, Runwei Guan, Zitian Peng, Chenhang Xu, Yilu Shi, Weiping Ding, Eng Gee Lim, Yong Yue, Hyungjoon Seo, Ka Lok Man, Jieming Ma, Xiaohui Zhu, Yutao Yue Abstract: With the rapid advancements of sensor technology and deep learning, autonomous driving systems are providing safe and efficient access to intelligent vehicles as well as intelligent transportation. Among these equipped sensors, the radar sensor plays a crucial role in providing robust perception information in diverse environmental conditions. This review focuses on exploring different radar data\u2026 \u25bd More With the rapid advancements of sensor technology and deep learning, autonomous driving systems are providing safe and efficient access to intelligent vehicles as well as intelligent transportation. Among these equipped sensors, the radar sensor plays a crucial role in providing robust perception information in diverse environmental conditions. This review focuses on exploring different radar data representations utilized in autonomous driving systems. Firstly, we introduce the capabilities and limitations of the radar sensor by examining the working principles of radar perception and signal processing of radar measurements. Then, we delve into the generation process of five radar representations, including the ADC signal, radar tensor, point cloud, grid map, and micro-Doppler signature. For each radar representation, we examine the related datasets, methods, advantages and limitations. Furthermore, we discuss the challenges faced in these data representations and propose potential research directions. Above all, this comprehensive review offers an in-depth insight into how these representations enhance autonomous system capabilities, providing guidance for radar perception researchers. To facilitate retrieval and comparison of different data representations, datasets and methods, we provide an interactive website at https://radar-camera-fusion.github.io/radar. \u25b3 Less Submitted 19 April, 2024; v1 submitted 8 December, 2023; originally announced December 2023. Comments: 24 pages, 10 figures, 5 tables. arXiv admin note: text overlap with arXiv:2304.10410 arXiv:2311.18654 [pdf, other] Detailed Human-Centric Text Description-Driven Large Scene Synthesis Authors: Gwanghyun Kim, Dong Un Kang, Hoigi Seo, Hayeon Kim, Se Young Chun Abstract: Text-driven large scene image synthesis has made significant progress with diffusion models, but controlling it is challenging. While using additional spatial controls with corresponding texts has improved the controllability of large scene synthesis, it is still challenging to faithfully reflect detailed text descriptions without user-provided controls. Here, we propose DetText2Scene, a novel tex\u2026 \u25bd More Text-driven large scene image synthesis has made significant progress with diffusion models, but controlling it is challenging. While using additional spatial controls with corresponding texts has improved the controllability of large scene synthesis, it is still challenging to faithfully reflect detailed text descriptions without user-provided controls. Here, we propose DetText2Scene, a novel text-driven large-scale image synthesis with high faithfulness, controllability, and naturalness in a global context for the detailed human-centric text description. Our DetText2Scene consists of 1) hierarchical keypoint-box layout generation from the detailed description by leveraging large language model (LLM), 2) view-wise conditioned joint diffusion process to synthesize a large scene from the given detailed text with LLM-generated grounded keypoint-box layout and 3) pixel perturbation-based pyramidal interpolation to progressively refine the large scene for global coherence. Our DetText2Scene significantly outperforms prior arts in text-to-large scene synthesis qualitatively and quantitatively, demonstrating strong faithfulness with detailed descriptions, superior controllability, and excellent naturalness in a global context. \u25b3 Less Submitted 30 November, 2023; originally announced November 2023. arXiv:2310.16112 [pdf, other] Towards long-tailed, multi-label disease classification from chest X-ray: Overview of the CXR-LT challenge Authors: Gregory Holste, Yiliang Zhou, Song Wang, Ajay Jaiswal, Mingquan Lin, Sherry Zhuge, Yuzhe Yang, Dongkyun Kim, Trong-Hieu Nguyen-Mau, Minh-Triet Tran, Jaehyup Jeong, Wongi Park, Jongbin Ryu, Feng Hong, Arsh Verma, Yosuke Yamagishi, Changhyun Kim, Hyeryeong Seo, Myungjoo Kang, Leo Anthony Celi, Zhiyong Lu, Ronald M. Summers, George Shih, Zhangyang Wang, Yifan Peng Abstract: Many real-world image recognition problems, such as diagnostic medical imaging exams, are \"long-tailed\" $\\unicode{x2013}$ there are a few common findings followed by many more relatively rare conditions. In chest radiography, diagnosis is both a long-tailed and multi-label problem, as patients often present with multiple findings simultaneously. While researchers have begun to study the problem of\u2026 \u25bd More Many real-world image recognition problems, such as diagnostic medical imaging exams, are \"long-tailed\" $\\unicode{x2013}$ there are a few common findings followed by many more relatively rare conditions. In chest radiography, diagnosis is both a long-tailed and multi-label problem, as patients often present with multiple findings simultaneously. While researchers have begun to study the problem of long-tailed learning in medical image recognition, few have studied the interaction of label imbalance and label co-occurrence posed by long-tailed, multi-label disease classification. To engage with the research community on this emerging topic, we conducted an open challenge, CXR-LT, on long-tailed, multi-label thorax disease classification from chest X-rays (CXRs). We publicly release a large-scale benchmark dataset of over 350,000 CXRs, each labeled with at least one of 26 clinical findings following a long-tailed distribution. We synthesize common themes of top-performing solutions, providing practical recommendations for long-tailed, multi-label medical image classification. Finally, we use these insights to propose a path forward involving vision-language foundation models for few- and zero-shot disease classification. \u25b3 Less Submitted 1 April, 2024; v1 submitted 24 October, 2023; originally announced October 2023. Comments: Update after major revision arXiv:2309.16936 [pdf, other] PC-Adapter: Topology-Aware Adapter for Efficient Domain Adaption on Point Clouds with Rectified Pseudo-label Authors: Joonhyung Park, Hyunjin Seo, Eunho Yang Abstract: Understanding point clouds captured from the real-world is challenging due to shifts in data distribution caused by varying object scales, sensor angles, and self-occlusion. Prior works have addressed this issue by combining recent learning principles such as self-supervised learning, self-training, and adversarial training, which leads to significant computational overhead.Toward succinct yet pow\u2026 \u25bd More Understanding point clouds captured from the real-world is challenging due to shifts in data distribution caused by varying object scales, sensor angles, and self-occlusion. Prior works have addressed this issue by combining recent learning principles such as self-supervised learning, self-training, and adversarial training, which leads to significant computational overhead.Toward succinct yet powerful domain adaptation for point clouds, we revisit the unique challenges of point cloud data under domain shift scenarios and discover the importance of the global geometry of source data and trends of target pseudo-labels biased to the source label distribution. Motivated by our observations, we propose an adapter-guided domain adaptation method, PC-Adapter, that preserves the global shape information of the source domain using an attention-based adapter, while learning the local characteristics of the target domain via another adapter equipped with graph convolution. Additionally, we propose a novel pseudo-labeling strategy resilient to the classifier bias by adjusting confidence scores using their class-wise confidence distributions to consider relative confidences. Our method demonstrates superiority over baselines on various domain shift settings in benchmark datasets - PointDA, GraspNetPC, and PointSegDA. \u25b3 Less Submitted 28 September, 2023; originally announced September 2023. Comments: 11 pages; Accepted to ICCV 2023 arXiv:2308.13564 [pdf, other] SGMM: Stochastic Approximation to Generalized Method of Moments Authors: Xiaohong Chen, Sokbae Lee, Yuan Liao, Myung Hwan Seo, Youngki Shin, Myunghyun Song Abstract: We introduce a new class of algorithms, Stochastic Generalized Method of Moments (SGMM), for estimation and inference on (overidentified) moment restriction models. Our SGMM is a novel stochastic approximation alternative to the popular Hansen (1982) (offline) GMM, and offers fast and scalable implementation with the ability to handle streaming datasets in real time. We establish the almost sure c\u2026 \u25bd More We introduce a new class of algorithms, Stochastic Generalized Method of Moments (SGMM), for estimation and inference on (overidentified) moment restriction models. Our SGMM is a novel stochastic approximation alternative to the popular Hansen (1982) (offline) GMM, and offers fast and scalable implementation with the ability to handle streaming datasets in real time. We establish the almost sure convergence, and the (functional) central limit theorem for the inefficient online 2SLS and the efficient SGMM. Moreover, we propose online versions of the Durbin-Wu-Hausman and Sargan-Hansen tests that can be seamlessly integrated within the SGMM framework. Extensive Monte Carlo simulations show that as the sample size increases, the SGMM matches the standard (offline) GMM in terms of estimation accuracy and gains over computational efficiency, indicating its practical value for both large-scale and online datasets. We demonstrate the efficacy of our approach by a proof of concept using two well known empirical examples with large sample sizes. \u25b3 Less Submitted 30 October, 2023; v1 submitted 24 August, 2023; originally announced August 2023. Comments: 46 pages, 4 tables, 2 figures arXiv:2307.06505 [pdf, other] WaterScenes: A Multi-Task 4D Radar-Camera Fusion Dataset and Benchmarks for Autonomous Driving on Water Surfaces Authors: Shanliang Yao, Runwei Guan, Zhaodong Wu, Yi Ni, Zile Huang, Ryan Wen Liu, Yong Yue, Weiping Ding, Eng Gee Lim, Hyungjoon Seo, Ka Lok Man, Jieming Ma, Xiaohui Zhu, Yutao Yue Abstract: Autonomous driving on water surfaces plays an essential role in executing hazardous and time-consuming missions, such as maritime surveillance, survivors rescue, environmental monitoring, hydrography mapping and waste cleaning. This work presents WaterScenes, the first multi-task 4D radar-camera fusion dataset for autonomous driving on water surfaces. Equipped with a 4D radar and a monocular camer\u2026 \u25bd More Autonomous driving on water surfaces plays an essential role in executing hazardous and time-consuming missions, such as maritime surveillance, survivors rescue, environmental monitoring, hydrography mapping and waste cleaning. This work presents WaterScenes, the first multi-task 4D radar-camera fusion dataset for autonomous driving on water surfaces. Equipped with a 4D radar and a monocular camera, our Unmanned Surface Vehicle (USV) proffers all-weather solutions for discerning object-related information, including color, shape, texture, range, velocity, azimuth, and elevation. Focusing on typical static and dynamic objects on water surfaces, we label the camera images and radar point clouds at pixel-level and point-level, respectively. In addition to basic perception tasks, such as object detection, instance segmentation and semantic segmentation, we also provide annotations for free-space segmentation and waterline segmentation. Leveraging the multi-task and multi-modal data, we conduct benchmark experiments on the uni-modality of radar and camera, as well as the fused modalities. Experimental results demonstrate that 4D radar-camera fusion can considerably improve the accuracy and robustness of perception on water surfaces, especially in adverse lighting and weather conditions. WaterScenes dataset is public on https://waterscenes.github.io. \u25b3 Less Submitted 15 June, 2024; v1 submitted 12 July, 2023; originally announced July 2023. Comments: Accepted by IEEE Transactions on Intelligent Transportation Systems arXiv:2307.01753 [pdf, other] Local primordial non-Gaussianity from the large-scale clustering of photometric DESI luminous red galaxies Authors: Mehdi Rezaie, Ashley J. Ross, Hee-Jong Seo, Hui Kong, Anna Porredon, Lado Samushia, Edmond Chaussidon, Alex Krolewski, Arnaud de Mattia, Florian Beutler, Jessica Nicole Aguilar, Steven Ahlen, Shadab Alam, Santiago Avila, Benedict Bahr-Kalus, Jose Bermejo-Climent, David Brooks, Todd Claybaugh, Shaun Cole, Kyle Dawson, Axel de la Macorra, Peter Doel, Andreu Font-Ribera, Jaime E. Forero-Romero, Satya Gontcho A Gontcho , et al. (24 additional authors not shown) Abstract: We use angular clustering of luminous red galaxies from the Dark Energy Spectroscopic Instrument (DESI) imaging surveys to constrain the local primordial non-Gaussianity parameter $\\fnl$. Our sample comprises over 12 million targets, covering 14,000 square degrees of the sky, with redshifts in the range $0.2< z < 1.35$. We identify Galactic extinction, survey depth, and astronomical seeing as the\u2026 \u25bd More We use angular clustering of luminous red galaxies from the Dark Energy Spectroscopic Instrument (DESI) imaging surveys to constrain the local primordial non-Gaussianity parameter $\\fnl$. Our sample comprises over 12 million targets, covering 14,000 square degrees of the sky, with redshifts in the range $0.2< z < 1.35$. We identify Galactic extinction, survey depth, and astronomical seeing as the primary sources of systematic error, and employ linear regression and artificial neural networks to alleviate non-cosmological excess clustering on large scales. Our methods are tested against simulations with and without $\\fnl$ and systematics, showing superior performance of the neural network treatment. The neural network with a set of nine imaging property maps passes our systematic null test criteria, and is chosen as the fiducial treatment. Assuming the universality relation, we find $\\fnl = 34^{+24(+50)}_{-44(-73)}$ at 68\\%(95\\%) confidence. We apply a series of robustness tests (e.g., cuts on imaging, declination, or scales used) that show consistency in the obtained constraints. We study how the regression method biases the measured angular power-spectrum and degrades the $\\fnl$ constraining power. The use of the nine maps more than doubles the uncertainty compared to using only the three primary maps in the regression. Our results thus motivate the development of more efficient methods that avoid over-correction, protect large-scale clustering information, and preserve constraining power. Additionally, our results encourage further studies of $\\fnl$ with DESI spectroscopic samples, where the inclusion of 3D clustering modes should help separate imaging systematics and lessen the degradation in the $\\fnl$ uncertainty. \u25b3 Less Submitted 25 June, 2024; v1 submitted 4 July, 2023; originally announced July 2023. Comments: 21 pages, 17 figures, 7 tables (Appendix excluded). Published in MNRAS arXiv:2306.06403 [pdf, other] Bayesian Inverse Contextual Reasoning for Heterogeneous Semantics-Native Communication Authors: Hyowoon Seo, Yoonseong Kang, Mehdi Bennis, Wan Choi Abstract: This work deals with the heterogeneous semantic-native communication (SNC) problem. When agents do not share the same communication context, the effectiveness of contextual reasoning (CR) is compromised calling for agents to infer other agents' context. This article proposes a novel framework for solving the inverse problem of CR in SNC using two Bayesian inference methods, namely: Bayesian invers\u2026 \u25bd More This work deals with the heterogeneous semantic-native communication (SNC) problem. When agents do not share the same communication context, the effectiveness of contextual reasoning (CR) is compromised calling for agents to infer other agents' context. This article proposes a novel framework for solving the inverse problem of CR in SNC using two Bayesian inference methods, namely: Bayesian inverse CR (iCR) and Bayesian inverse linearized CR (iLCR). The first proposed Bayesian iCR method utilizes Markov Chain Monte Carlo (MCMC) sampling to infer the agent's context while being computationally expensive. To address this issue, a Bayesian iLCR method is leveraged which obtains a linearized CR (LCR) model by training a linear neural network. Experimental results show that the Bayesian iLCR method requires less computation and achieves higher inference accuracy compared to Bayesian iCR. Additionally, heterogeneous SNC based on the context obtained through the Bayesian iLCR method shows better communication effectiveness than that of Bayesian iCR. Overall, this work provides valuable insights and methods to improve the effectiveness of SNC in situations where agents have different contexts. \u25b3 Less Submitted 10 June, 2023; originally announced June 2023. Comments: 14 pages, 7 figures, submitted for possible publication arXiv:2305.06310 [pdf, other] SoGAR: Self-supervised Spatiotemporal Attention-based Social Group Activity Recognition Authors: Naga VS Raviteja Chappa, Pha Nguyen, Alexander H Nelson, Han-Seok Seo, Xin Li, Page Daniel Dobbs, Khoa Luu Abstract: This paper introduces a novel approach to Social Group Activity Recognition (SoGAR) using Self-supervised Transformers network that can effectively utilize unlabeled video data. To extract spatio-temporal information, we created local and global views with varying frame rates. Our self-supervised objective ensures that features extracted from contrasting views of the same video were consistent acr\u2026 \u25bd More This paper introduces a novel approach to Social Group Activity Recognition (SoGAR) using Self-supervised Transformers network that can effectively utilize unlabeled video data. To extract spatio-temporal information, we created local and global views with varying frame rates. Our self-supervised objective ensures that features extracted from contrasting views of the same video were consistent across spatio-temporal domains. Our proposed approach is efficient in using transformer-based encoders to alleviate the weakly supervised setting of group activity recognition. By leveraging the benefits of transformer models, our approach can model long-term relationships along spatio-temporal dimensions. Our proposed SoGAR method achieved state-of-the-art results on three group activity recognition benchmarks, namely JRDB-PAR, NBA, and Volleyball datasets, surpassing the current numbers in terms of F1-score, MCA, and MPCA metrics. \u25b3 Less Submitted 18 November, 2024; v1 submitted 26 April, 2023; originally announced May 2023. Comments: Under review for IEEE Access journal; 12 pages, 7 figures. arXiv admin note: text overlap with arXiv:2303.12149 arXiv:2304.10410 [pdf, other] Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review Authors: Shanliang Yao, Runwei Guan, Xiaoyu Huang, Zhuoxiao Li, Xiangyu Sha, Yong Yue, Eng Gee Lim, Hyungjoon Seo, Ka Lok Man, Xiaohui Zhu, Yutao Yue Abstract: Driven by deep learning techniques, perception technology in autonomous driving has developed rapidly in recent years, enabling vehicles to accurately detect and interpret surrounding environment for safe and efficient navigation. To achieve accurate and robust perception capabilities, autonomous vehicles are often equipped with multiple sensors, making sensor fusion a crucial part of the percepti\u2026 \u25bd More Driven by deep learning techniques, perception technology in autonomous driving has developed rapidly in recent years, enabling vehicles to accurately detect and interpret surrounding environment for safe and efficient navigation. To achieve accurate and robust perception capabilities, autonomous vehicles are often equipped with multiple sensors, making sensor fusion a crucial part of the perception system. Among these fused sensors, radars and cameras enable a complementary and cost-effective perception of the surrounding environment regardless of lighting and weather conditions. This review aims to provide a comprehensive guideline for radar-camera fusion, particularly concentrating on perception tasks related to object detection and semantic segmentation.Based on the principles of the radar and camera sensors, we delve into the data processing process and representations, followed by an in-depth analysis and summary of radar-camera fusion datasets. In the review of methodologies in radar-camera fusion, we address interrogative questions, including \"why to fuse\", \"what to fuse\", \"where to fuse\", \"when to fuse\", and \"how to fuse\", subsequently discussing various challenges and potential research directions within this domain. To ease the retrieval and comparison of datasets and fusion methods, we also provide an interactive website: https://radar-camera-fusion.github.io. \u25b3 Less Submitted 23 August, 2023; v1 submitted 20 April, 2023; originally announced April 2023. Comments: Accepted by IEEE Transactions on Intelligent Vehicles (T-IV) Journal ref: IEEE Transactions on Intelligent Vehicles 2023 arXiv:2304.03195 [pdf, other] Micron-BERT: BERT-based Facial Micro-Expression Recognition Authors: Xuan-Bac Nguyen, Chi Nhan Duong, Xin Li, Susan Gauch, Han-Seok Seo, Khoa Luu Abstract: Micro-expression recognition is one of the most challenging topics in affective computing. It aims to recognize tiny facial movements difficult for humans to perceive in a brief period, i.e., 0.25 to 0.5 seconds. Recent advances in pre-training deep Bidirectional Transformers (BERT) have significantly improved self-supervised learning tasks in computer vision. However, the standard BERT in vision\u2026 \u25bd More Micro-expression recognition is one of the most challenging topics in affective computing. It aims to recognize tiny facial movements difficult for humans to perceive in a brief period, i.e., 0.25 to 0.5 seconds. Recent advances in pre-training deep Bidirectional Transformers (BERT) have significantly improved self-supervised learning tasks in computer vision. However, the standard BERT in vision problems is designed to learn only from full images or videos, and the architecture cannot accurately detect details of facial micro-expressions. This paper presents Micron-BERT ($\u03bc$-BERT), a novel approach to facial micro-expression recognition. The proposed method can automatically capture these movements in an unsupervised manner based on two key ideas. First, we employ Diagonal Micro-Attention (DMA) to detect tiny differences between two frames. Second, we introduce a new Patch of Interest (PoI) module to localize and highlight micro-expression interest regions and simultaneously reduce noisy backgrounds and distractions. By incorporating these components into an end-to-end deep network, the proposed $\u03bc$-BERT significantly outperforms all previous work in various micro-expression tasks. $\u03bc$-BERT can be trained on a large-scale unlabeled dataset, i.e., up to 8 million images, and achieves high accuracy on new unseen facial micro-expression datasets. Empirical experiments show $\u03bc$-BERT consistently outperforms state-of-the-art performance on four micro-expression benchmarks, including SAMM, CASME II, SMIC, and CASME3, by significant margins. Code will be available at \\url{https://github.com/uark-cviu/Micron-BERT} \u25b3 Less Submitted 6 April, 2023; originally announced April 2023. Comments: Accepted by CVPR2023 arXiv:2304.02827 [pdf, other] DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model Authors: Hoigi Seo, Hayeon Kim, Gwanghyun Kim, Se Young Chun Abstract: The increasing demand for high-quality 3D content creation has motivated the development of automated methods for creating 3D object models from a single image and/or from a text prompt. However, the reconstructed 3D objects using state-of-the-art image-to-3D methods still exhibit low correspondence to the given image and low multi-view consistency. Recent state-of-the-art text-to-3D methods are a\u2026 \u25bd More The increasing demand for high-quality 3D content creation has motivated the development of automated methods for creating 3D object models from a single image and/or from a text prompt. However, the reconstructed 3D objects using state-of-the-art image-to-3D methods still exhibit low correspondence to the given image and low multi-view consistency. Recent state-of-the-art text-to-3D methods are also limited, yielding 3D samples with low diversity per prompt with long synthesis time. To address these challenges, we propose DITTO-NeRF, a novel pipeline to generate a high-quality 3D NeRF model from a text prompt or a single image. Our DITTO-NeRF consists of constructing high-quality partial 3D object for limited in-boundary (IB) angles using the given or text-generated 2D image from the frontal view and then iteratively reconstructing the remaining 3D NeRF using inpainting latent diffusion model. We propose progressive 3D object reconstruction schemes in terms of scales (low to high resolution), angles (IB angles initially to outer-boundary (OB) later), and masks (object to background boundary) in our DITTO-NeRF so that high-quality information on IB can be propagated into OB. Our DITTO-NeRF outperforms state-of-the-art methods in terms of fidelity and diversity qualitatively and quantitatively with much faster training times than prior arts on image/text-to-3D such as DreamFusion, and NeuralLift-360. \u25b3 Less Submitted 5 April, 2023; originally announced April 2023. Comments: Project page: https://janeyeon.github.io/ditto-nerf/"
  },
  {
    "url": "https://github.com/seohyunwoo-0407/GAR",
    "title": "Search code, repositories, users, issues, pull requests...",
    "author": "seohyunwoo-0407",
    "body": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation.We read every piece of feedback, and take your input very seriously.To see all available qualifiers, see our documentation.This project is a RAG-based (Retrieval-Augmented Generation) answer generation and refinement pipeline specifically designed for question answering in the finance domain. GAR/ \u251c\u2500\u2500 task1/ # Hybrid Search implementation \u2502 \u2514\u2500\u2500 Embedder_Finetuning.py \u2502 \u2514\u2500\u2500 hybrid_search.py \u2502 \u2514\u2500\u2500 run_finetuning.py \u2502 \u2514\u2500\u2500 run_hybrid_search.py \u2502 \u2514\u2500\u2500 main.py # Main pipeline execution \u2514\u2500\u2500 requirements.txt # List of required packages Embedder_Finetuning.py: Fine-tuning retrieval model hybrid_search.py: Execute hybrid search, and find optimal alpha for each task run_finetuning.py: Run Embedder_Finetuning.py run_hybrid_search.py: Run hybrid_search.py Clone the repository git clone https://github.com/seohyunwoo-0407/GAR.git cd GAR Create and activate a virtual environment python -m venv .venv source .venv/bin/activate # Linux/Mac .venv\\Scripts\\activate # Windows Install required packages pip install -r requirements.txt FinanceRAG project by KAIST students. Advanced Retrieval-Augmented Generation (RAG) system designed for the financial domain.This project is a RAG-based (Retrieval-Augmented Generation) answer generation and refinement pipeline specifically designed for question answering in the finance domain. GAR/ \u251c\u2500\u2500 task1/ # Hybrid Search implementation \u2502 \u2514\u2500\u2500 Embedder_Finetuning.py \u2502 \u2514\u2500\u2500 hybrid_search.py \u2502 \u2514\u2500\u2500 run_finetuning.py \u2502 \u2514\u2500\u2500 run_hybrid_search.py \u2502 \u2514\u2500\u2500 main.py # Main pipeline execution \u2514\u2500\u2500 requirements.txt # List of required packages Embedder_Finetuning.py: Fine-tuning retrieval model hybrid_search.py: Execute hybrid search, and find optimal alpha for each task run_finetuning.py: Run Embedder_Finetuning.py run_hybrid_search.py: Run hybrid_search.py Clone the repository git clone https://github.com/seohyunwoo-0407/GAR.git cd GAR Create and activate a virtual environment python -m venv .venv source .venv/bin/activate # Linux/Mac .venv\\Scripts\\activate # Windows Install required packages pip install -r requirements.txt"
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Song,+H",
    "title": "Showing 1\u201350 of 524 results for author: Song, H",
    "author": "Authors:\nSejong Kim, \n      \n      Hyunseo Song, \n      \n      Hyunwoo Seo, \n      \n      Hyunjun Kim",
    "body": "arXiv:2503.15191 [pdf, other] Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems Authors: Sejong Kim, Hyunseo Song, Hyunwoo Seo, Hyunjun Kim Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising framework to mitigate hallucinations in Large Language Models (LLMs), yet its overall performance is dependent on the underlying retrieval system. In the finance domain, documents such as 10-K reports pose distinct challenges due to domain-specific vocabulary and multi-hierarchical tabular data. In this work, we introduce an efficient\u2026 \u25bd More Retrieval-Augmented Generation (RAG) has emerged as a promising framework to mitigate hallucinations in Large Language Models (LLMs), yet its overall performance is dependent on the underlying retrieval system. In the finance domain, documents such as 10-K reports pose distinct challenges due to domain-specific vocabulary and multi-hierarchical tabular data. In this work, we introduce an efficient, end-to-end RAG pipeline that enhances retrieval for financial documents through a three-phase approach: pre-retrieval, retrieval, and post-retrieval. In the pre-retrieval phase, various query and corpus preprocessing techniques are employed to enrich input data. During the retrieval phase, we fine-tuned state-of-the-art (SOTA) embedding models with domain-specific knowledge and implemented a hybrid retrieval strategy that combines dense and sparse representations. Finally, the post-retrieval phase leverages Direct Preference Optimization (DPO) training and document selection methods to further refine the results. Evaluations on seven financial question answering datasets-FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA, and MultiHiertt-demonstrate substantial improvements in retrieval performance, leading to more accurate and contextually appropriate generation. These findings highlight the critical role of tailored retrieval techniques in advancing the effectiveness of RAG systems for financial applications. A fully replicable pipeline is available on GitHub: https://github.com/seohyunwoo-0407/GAR. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. Comments: 15 pages, 3 figures, 11 tables. Accepted at ICLR 2025 Workshop on Advances in Financial AI. Code available at https://github.com/seohyunwoo-0407/GAR arXiv:2503.13402 [pdf, other] Toward Generative 6G Simulation: An Experimental Multi-Agent LLM and ns-3 Integration Authors: Farhad Rezazadeh, Amir Ashtari Gargari, Sandra Lagen, Houbing Song, Dusit Niyato, Lingjia Liu Abstract: The move toward open Sixth-Generation (6G) networks necessitates a novel approach to full-stack simulation environments for evaluating complex technology developments before prototyping and real-world implementation. This paper introduces an innovative approach\\footnote{A lightweight, mock version of the code is available on GitHub at that combines a multi-agent framework with the Network Simulato\u2026 \u25bd More The move toward open Sixth-Generation (6G) networks necessitates a novel approach to full-stack simulation environments for evaluating complex technology developments before prototyping and real-world implementation. This paper introduces an innovative approach\\footnote{A lightweight, mock version of the code is available on GitHub at that combines a multi-agent framework with the Network Simulator 3 (ns-3) to automate and optimize the generation, debugging, execution, and analysis of complex 5G network scenarios. Our framework orchestrates a suite of specialized agents -- namely, the Simulation Generation Agent, Test Designer Agent, Test Executor Agent, and Result Interpretation Agent -- using advanced LangChain coordination. The Simulation Generation Agent employs a structured chain-of-thought (CoT) reasoning process, leveraging LLMs and retrieval-augmented generation (RAG) to translate natural language simulation specifications into precise ns-3 scripts. Concurrently, the Test Designer Agent generates comprehensive automated test suites by integrating knowledge retrieval techniques with dynamic test case synthesis. The Test Executor Agent dynamically deploys and runs simulations, managing dependencies and parsing detailed performance metrics. At the same time, the Result Interpretation Agent utilizes LLM-driven analysis to extract actionable insights from the simulation outputs. By integrating external resources such as library documentation and ns-3 testing frameworks, our experimental approach can enhance simulation accuracy and adaptability, reducing reliance on extensive programming expertise. A detailed case study using the ns-3 5G-LENA module validates the effectiveness of the proposed approach. The code generation process converges in an average of 1.8 iterations, has a syntax error rate of 17.0%, a mean response time of 7.3 seconds, and receives a human evaluation score of 7.5. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. Comments: 6 pages, 4 figures, 4 tables arXiv:2503.13025 [pdf, other] PoseSyn: Synthesizing Diverse 3D Pose Data from In-the-Wild 2D Data Authors: ChangHee Yang, Hyeonseop Song, Seokhun Choi, Seungwoo Lee, Jaechul Kim, Hoseok Do Abstract: Despite considerable efforts to enhance the generalization of 3D pose estimators without costly 3D annotations, existing data augmentation methods struggle in real world scenarios with diverse human appearances and complex poses. We propose PoseSyn, a novel data synthesis framework that transforms abundant in the wild 2D pose dataset into diverse 3D pose image pairs. PoseSyn comprises two key comp\u2026 \u25bd More Despite considerable efforts to enhance the generalization of 3D pose estimators without costly 3D annotations, existing data augmentation methods struggle in real world scenarios with diverse human appearances and complex poses. We propose PoseSyn, a novel data synthesis framework that transforms abundant in the wild 2D pose dataset into diverse 3D pose image pairs. PoseSyn comprises two key components: Error Extraction Module (EEM), which identifies challenging poses from the 2D pose datasets, and Motion Synthesis Module (MSM), which synthesizes motion sequences around the challenging poses. Then, by generating realistic 3D training data via a human animation model aligned with challenging poses and appearances PoseSyn boosts the accuracy of various 3D pose estimators by up to 14% across real world benchmarks including various backgrounds and occlusions, challenging poses, and multi view scenarios. Extensive experiments further confirm that PoseSyn is a scalable and effective approach for improving generalization without relying on expensive 3D annotations, regardless of the pose estimator's model size or design. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. Comments: The first three authors contributed equally to this work arXiv:2503.12383 [pdf, other] VRsketch2Gaussian: 3D VR Sketch Guided 3D Object Generation with Gaussian Splatting Authors: Songen Gu, Haoxuan Song, Binjie Liu, Qian Yu, Sanyi Zhang, Haiyong Jiang, Jin Huang, Feng Tian Abstract: We propose VRSketch2Gaussian, a first VR sketch-guided, multi-modal, native 3D object generation framework that incorporates a 3D Gaussian Splatting representation. As part of our work, we introduce VRSS, the first large-scale paired dataset containing VR sketches, text, images, and 3DGS, bridging the gap in multi-modal VR sketch-based generation. Our approach features the following key innovation\u2026 \u25bd More We propose VRSketch2Gaussian, a first VR sketch-guided, multi-modal, native 3D object generation framework that incorporates a 3D Gaussian Splatting representation. As part of our work, we introduce VRSS, the first large-scale paired dataset containing VR sketches, text, images, and 3DGS, bridging the gap in multi-modal VR sketch-based generation. Our approach features the following key innovations: 1) Sketch-CLIP feature alignment. We propose a two-stage alignment strategy that bridges the domain gap between sparse VR sketch embeddings and rich CLIP embeddings, facilitating both VR sketch-based retrieval and generation tasks. 2) Fine-Grained multi-modal conditioning. We disentangle the 3D generation process by using explicit VR sketches for geometric conditioning and text descriptions for appearance control. To facilitate this, we propose a generalizable VR sketch encoder that effectively aligns different modalities. 3) Efficient and high-fidelity 3D native generation. Our method leverages a 3D-native generation approach that enables fast and texture-rich 3D object synthesis. Experiments conducted on our VRSS dataset demonstrate that our method achieves high-quality, multi-modal VR sketch-based 3D generation. We believe our VRSS dataset and VRsketch2Gaussian method will be beneficial for the 3D generation community. \u25b3 Less Submitted 16 March, 2025; originally announced March 2025. arXiv:2503.12035 [pdf, other] MOS: Modeling Object-Scene Associations in Generalized Category Discovery Authors: Zhengyuan Peng, Jinpeng Ma, Zhimin Sun, Ran Yi, Haichuan Song, Xin Tan, Lizhuang Ma Abstract: Generalized Category Discovery (GCD) is a classification task that aims to classify both base and novel classes in unlabeled images, using knowledge from a labeled dataset. In GCD, previous research overlooks scene information or treats it as noise, reducing its impact during model training. However, in this paper, we argue that scene information should be viewed as a strong prior for inferring no\u2026 \u25bd More Generalized Category Discovery (GCD) is a classification task that aims to classify both base and novel classes in unlabeled images, using knowledge from a labeled dataset. In GCD, previous research overlooks scene information or treats it as noise, reducing its impact during model training. However, in this paper, we argue that scene information should be viewed as a strong prior for inferring novel classes. We attribute the misinterpretation of scene information to a key factor: the Ambiguity Challenge inherent in GCD. Specifically, novel objects in base scenes might be wrongly classified into base categories, while base objects in novel scenes might be mistakenly recognized as novel categories. Once the ambiguity challenge is addressed, scene information can reach its full potential, significantly enhancing the performance of GCD models. To more effectively leverage scene information, we propose the Modeling Object-Scene Associations (MOS) framework, which utilizes a simple MLP-based scene-awareness module to enhance GCD performance. It achieves an exceptional average accuracy improvement of 4% on the challenging fine-grained datasets compared to state-of-the-art methods, emphasizing its superior performance in fine-grained GCD. The code is publicly available at https://github.com/JethroPeng/MOS \u25b3 Less Submitted 17 March, 2025; v1 submitted 15 March, 2025; originally announced March 2025. Comments: Accepted to CVPR 2025.The code is available at https://github.com/JethroPeng/MOS arXiv:2503.11116 [pdf, other] Trust in Disinformation Narratives: a Trust in the News Experiment Authors: Hanbyul Song, Miguel F. Santos Silva, Jaume Suau, Luis Espinosa-Anke Abstract: Understanding why people trust or distrust one another, institutions, or information is a complex task that has led scholars from various fields of study to employ diverse epistemological and methodological approaches. Despite the challenges, it is generally agreed that the antecedents of trust (and distrust) encompass a multitude of emotional and cognitive factors, including a general disposition\u2026 \u25bd More Understanding why people trust or distrust one another, institutions, or information is a complex task that has led scholars from various fields of study to employ diverse epistemological and methodological approaches. Despite the challenges, it is generally agreed that the antecedents of trust (and distrust) encompass a multitude of emotional and cognitive factors, including a general disposition to trust and an assessment of trustworthiness factors. In an era marked by increasing political polarization, cultural backlash, widespread disinformation and fake news, and the use of AI software to produce news content, the need to study trust in the news has gained significant traction. This study presents the findings of a trust in the news experiment designed in collaboration with Spanish and UK journalists, fact-checkers, and the CardiffNLP Natural Language Processing research group. The purpose of this experiment, conducted in June 2023, was to examine the extent to which people trust a set of fake news articles based on previously identified disinformation narratives related to gender, climate change, and COVID-19. The online experiment participants (801 in Spain and 800 in the UK) were asked to read three fake news items and rate their level of trust on a scale from 1 (not true) to 8 (true). The pieces used a combination of factors, including stance (favourable, neutral, or against the narrative), presence of toxic expressions, clickbait titles, and sources of information to test which elements influenced people's responses the most. Half of the pieces were produced by humans and the other half by ChatGPT. The results show that the topic of news articles, stance, people's age, gender, and political ideologies significantly affected their levels of trust in the news, while the authorship (humans or ChatGPT) does not have a significant impact. \u25b3 Less Submitted 14 March, 2025; originally announced March 2025. arXiv:2503.09959 [pdf, other] RMG: Real-Time Expressive Motion Generation with Self-collision Avoidance for 6-DOF Companion Robotic Arms Authors: Jiansheng Li, Haotian Song, Jinni Zhou, Qiang Nie, Yi Cai Abstract: The six-degree-of-freedom (6-DOF) robotic arm has gained widespread application in human-coexisting environments. While previous research has predominantly focused on functional motion generation, the critical aspect of expressive motion in human-robot interaction remains largely unexplored. This paper presents a novel real-time motion generation planner that enhances interactivity by creating exp\u2026 \u25bd More The six-degree-of-freedom (6-DOF) robotic arm has gained widespread application in human-coexisting environments. While previous research has predominantly focused on functional motion generation, the critical aspect of expressive motion in human-robot interaction remains largely unexplored. This paper presents a novel real-time motion generation planner that enhances interactivity by creating expressive robotic motions between arbitrary start and end states within predefined time constraints. Our approach involves three key contributions: first, we develop a mapping algorithm to construct an expressive motion dataset derived from human dance movements; second, we train motion generation models in both Cartesian and joint spaces using this dataset; third, we introduce an optimization algorithm that guarantees smooth, collision-free motion while maintaining the intended expressive style. Experimental results demonstrate the effectiveness of our method, which can generate expressive and generalized motions in under 0.5 seconds while satisfying all specified constraints. \u25b3 Less Submitted 12 March, 2025; originally announced March 2025. arXiv:2503.06291 [pdf, other] IteRABRe: Iterative Recovery-Aided Block Reduction Authors: Haryo Akbarianto Wibowo, Haiyue Song, Hideki Tanaka, Masao Utiyama, Alham Fikri Aji, Raj Dabre Abstract: Large Language Models (LLMs) have grown increasingly expensive to deploy, driving the need for effective model compression techniques. While block pruning offers a straightforward approach to reducing model size, existing methods often struggle to maintain performance or require substantial computational resources for recovery. We present IteRABRe, a simple yet effective iterative pruning method t\u2026 \u25bd More Large Language Models (LLMs) have grown increasingly expensive to deploy, driving the need for effective model compression techniques. While block pruning offers a straightforward approach to reducing model size, existing methods often struggle to maintain performance or require substantial computational resources for recovery. We present IteRABRe, a simple yet effective iterative pruning method that achieves superior compression results while requiring minimal computational resources. Using only 2.5M tokens for recovery, our method outperforms baseline approaches by ~3% on average when compressing the Llama3.1-8B and Qwen2.5-7B models. IteRABRe demonstrates particular strength in the preservation of linguistic capabilities, showing an improvement 5% over the baselines in language-related tasks. Our analysis reveals distinct pruning characteristics between these models, while also demonstrating preservation of multilingual capabilities. \u25b3 Less Submitted 8 March, 2025; originally announced March 2025. Comments: 8 pages MSC Class: 68T50 arXiv:2503.05836 [pdf, other] Safe Distributed Learning-Enhanced Predictive Control for Multiple Quadrupedal Robots Authors: Weishu Zhan, Zheng Liang, Hongyu Song, Wei Pan Abstract: Quadrupedal robots exhibit remarkable adaptability in unstructured environments, making them well-suited for formation control in real-world applications. However, keeping stable formations while ensuring collision-free navigation presents significant challenges due to dynamic obstacles, communication constraints, and the complexity of legged locomotion. This paper proposes a distributed model pre\u2026 \u25bd More Quadrupedal robots exhibit remarkable adaptability in unstructured environments, making them well-suited for formation control in real-world applications. However, keeping stable formations while ensuring collision-free navigation presents significant challenges due to dynamic obstacles, communication constraints, and the complexity of legged locomotion. This paper proposes a distributed model predictive control framework for multi-quadruped formation control, integrating Control Lyapunov Functions to ensure formation stability and Control Barrier Functions for decentralized safety enforcement. To address the challenge of dynamically changing team structures, we introduce Scale-Adaptive Permutation-Invariant Encoding (SAPIE), which enables robust feature encoding of neighboring robots while preserving permutation invariance. Additionally, we develop a low-latency Data Distribution Service-based communication protocol and an event-triggered deadlock resolution mechanism to enhance real-time coordination and prevent motion stagnation in constrained spaces. Our framework is validated through high-fidelity simulations in NVIDIA Omniverse Isaac Sim and real-world experiments using our custom quadrupedal robotic system, XG. Results demonstrate stable formation control, real-time feasibility, and effective collision avoidance, validating its potential for large-scale deployment. \u25b3 Less Submitted 6 March, 2025; originally announced March 2025. arXiv:2503.05592 [pdf, other] R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning Authors: Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen Abstract: Existing Large Reasoning Models (LRMs) have shown the potential of reinforcement learning (RL) to enhance the complex reasoning capabilities of Large Language Models~(LLMs). While they achieve remarkable performance on challenging tasks such as mathematics and coding, they often rely on their internal knowledge to solve problems, which can be inadequate for time-sensitive or knowledge-intensive qu\u2026 \u25bd More Existing Large Reasoning Models (LRMs) have shown the potential of reinforcement learning (RL) to enhance the complex reasoning capabilities of Large Language Models~(LLMs). While they achieve remarkable performance on challenging tasks such as mathematics and coding, they often rely on their internal knowledge to solve problems, which can be inadequate for time-sensitive or knowledge-intensive questions, leading to inaccuracies and hallucinations. To address this, we propose \\textbf{R1-Searcher}, a novel two-stage outcome-based RL approach designed to enhance the search capabilities of LLMs. This method allows LLMs to autonomously invoke external search systems to access additional knowledge during the reasoning process. Our framework relies exclusively on RL, without requiring process rewards or distillation for a cold start. % effectively generalizing to out-of-domain datasets and supporting both Base and Instruct models. Our experiments demonstrate that our method significantly outperforms previous strong RAG methods, even when compared to the closed-source GPT-4o-mini. \u25b3 Less Submitted 18 March, 2025; v1 submitted 7 March, 2025; originally announced March 2025. arXiv:2503.04789 [pdf, other] Ext2Gen: Alignment through Unified Extraction and Generation for Robust Retrieval-Augmented Generation Authors: Hwanjun Song, Jeonghwan Choi, Minseok Kim Abstract: Retrieval-augmented generation (RAG) enhances LLMs by integrating external knowledge, but generation remains fragile due to the uncertain placement of relevant chunks and retrieval-induced information overload, leading to hallucinations. We propose Ext2Gen, a novel extract-then-generate model that enhances RAG robustness by first extracting query-relevant sentences before generating answers. To op\u2026 \u25bd More Retrieval-augmented generation (RAG) enhances LLMs by integrating external knowledge, but generation remains fragile due to the uncertain placement of relevant chunks and retrieval-induced information overload, leading to hallucinations. We propose Ext2Gen, a novel extract-then-generate model that enhances RAG robustness by first extracting query-relevant sentences before generating answers. To optimize this model, we employ preference alignment through pairwise feedback learning, enabling the model to generate robust answers regardless of variations in retrieval results. Extensive experiments demonstrate that Ext2Gen effectively identifies query-relevant sentences with high precision and recall, leading to highly reliable answers. Furthermore, deploying our model in a RAG environment reveals that it not only boosts the performance of the base LLM but also synergizes with advanced retrieval strategies like query expansion. The model is available at https://huggingface.co/DISLab/Ext2Gen-8B-R2. \u25b3 Less Submitted 12 March, 2025; v1 submitted 28 February, 2025; originally announced March 2025. arXiv:2503.01754 [pdf, other] SDRT: Enhance Vision-Language Models by Self-Distillation with Diverse Reasoning Traces Authors: Guande Wu, Huan Song, Yawei Wang, Qiaojing Yan, Yijun Tian, Lin Lee Cheong, Panpan Xu Abstract: Reasoning is increasingly crucial for various tasks. While chain-of-thought prompting enables large language models to leverage reasoning effectively, harnessing the reasoning capabilities of Vision-Language Models (VLMs) remains challenging. To solve this problem, we propose a novel self-distillation framework that enhances the reasoning capabilities of the model. The proposed framework introduce\u2026 \u25bd More Reasoning is increasingly crucial for various tasks. While chain-of-thought prompting enables large language models to leverage reasoning effectively, harnessing the reasoning capabilities of Vision-Language Models (VLMs) remains challenging. To solve this problem, we propose a novel self-distillation framework that enhances the reasoning capabilities of the model. The proposed framework introduces several key innovations. We start by employing a prompt library tailored to visual reasoning tasks to generate diverse in-context questions and utilize a two-step reasoning procedure to derive reasoning-guided responses. These responses are then used for self-distillation, enabling the model to internalize the reasoning process. Additionally, we improve the model architecture with several innovative components, including an intervention adapter for efficient parameter updates, a cross-modal skip connection to facilitate information exchange between modalities, and an ensemble learning algorithm to integrate diverse reasoning from multiple in-context questions. Extensive experiments show that our method significantly improves the baseline performance across five VQA datasets. \u25b3 Less Submitted 18 March, 2025; v1 submitted 3 March, 2025; originally announced March 2025. arXiv:2503.00761 [pdf, other] TRACE: A Self-Improving Framework for Robot Behavior Forecasting with Vision-Language Models Authors: Gokul Puthumanaillam, Paulo Padrao, Jose Fuentes, Pranay Thangeda, William E. Schafer, Jae Hyuk Song, Karan Jagdale, Leonardo Bobadilla, Melkior Ornik Abstract: Predicting the near-term behavior of a reactive agent is crucial in many robotic scenarios, yet remains challenging when observations of that agent are sparse or intermittent. Vision-Language Models (VLMs) offer a promising avenue by integrating textual domain knowledge with visual cues, but their one-shot predictions often miss important edge cases and unusual maneuvers. Our key insight is that i\u2026 \u25bd More Predicting the near-term behavior of a reactive agent is crucial in many robotic scenarios, yet remains challenging when observations of that agent are sparse or intermittent. Vision-Language Models (VLMs) offer a promising avenue by integrating textual domain knowledge with visual cues, but their one-shot predictions often miss important edge cases and unusual maneuvers. Our key insight is that iterative, counterfactual exploration--where a dedicated module probes each proposed behavior hypothesis, explicitly represented as a plausible trajectory, for overlooked possibilities--can significantly enhance VLM-based behavioral forecasting. We present TRACE (Tree-of-thought Reasoning And Counterfactual Exploration), an inference framework that couples tree-of-thought generation with domain-aware feedback to refine behavior hypotheses over multiple rounds. Concretely, a VLM first proposes candidate trajectories for the agent; a counterfactual critic then suggests edge-case variations consistent with partial observations, prompting the VLM to expand or adjust its hypotheses in the next iteration. This creates a self-improving cycle where the VLM progressively internalizes edge cases from previous rounds, systematically uncovering not only typical behaviors but also rare or borderline maneuvers, ultimately yielding more robust trajectory predictions from minimal sensor data. We validate TRACE on both ground-vehicle simulations and real-world marine autonomous surface vehicles. Experimental results show that our method consistently outperforms standard VLM-driven and purely model-based baselines, capturing a broader range of feasible agent behaviors despite sparse sensing. Evaluation videos and code are available at trace-robotics.github.io. \u25b3 Less Submitted 2 March, 2025; originally announced March 2025. arXiv:2503.00615 [pdf, other] xIDS-EnsembleGuard: An Explainable Ensemble Learning-based Intrusion Detection System Authors: Muhammad Adil, Mian Ahmad Jan, Safayat Bin Hakim, Houbing Herbert Song, Zhanpeng Jin Abstract: In this paper, we focus on addressing the challenges of detecting malicious attacks in networks by designing an advanced Explainable Intrusion Detection System (xIDS). The existing machine learning and deep learning approaches have invisible limitations, such as potential biases in predictions, a lack of interpretability, and the risk of overfitting to training data. These issues can create doubt\u2026 \u25bd More In this paper, we focus on addressing the challenges of detecting malicious attacks in networks by designing an advanced Explainable Intrusion Detection System (xIDS). The existing machine learning and deep learning approaches have invisible limitations, such as potential biases in predictions, a lack of interpretability, and the risk of overfitting to training data. These issues can create doubt about their usefulness, transparency, and a decrease in trust among stakeholders. To overcome these challenges, we propose an ensemble learning technique called \"EnsembleGuard.\" This approach uses the predicted outputs of multiple models, including tree-based methods (LightGBM, GBM, Bagging, XGBoost, CatBoost) and deep learning models such as LSTM (long short-term memory) and GRU (gated recurrent unit), to maintain a balance and achieve trustworthy results. Our work is unique because it combines both tree-based and deep learning models to design an interpretable and explainable meta-model through model distillation. By considering the predictions of all individual models, our meta-model effectively addresses key challenges and ensures both explainable and reliable results. We evaluate our model using well-known datasets, including UNSW-NB15, NSL-KDD, and CIC-IDS-2017, to assess its reliability against various types of attacks. During analysis, we found that our model outperforms both tree-based models and other comparative approaches in different attack scenarios. \u25b3 Less Submitted 1 March, 2025; originally announced March 2025. Comments: Accepted in, 23rd IEEE International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom-2024) arXiv:2503.00565 [pdf, other] Semi-Parametric Batched Global Multi-Armed Bandits with Covariates Authors: Sakshi Arya, Hyebin Song Abstract: The multi-armed bandits (MAB) framework is a widely used approach for sequential decision-making, where a decision-maker selects an arm in each round with the goal of maximizing long-term rewards. Moreover, in many practical applications, such as personalized medicine and recommendation systems, feedback is provided in batches, contextual information is available at the time of decision-making, an\u2026 \u25bd More The multi-armed bandits (MAB) framework is a widely used approach for sequential decision-making, where a decision-maker selects an arm in each round with the goal of maximizing long-term rewards. Moreover, in many practical applications, such as personalized medicine and recommendation systems, feedback is provided in batches, contextual information is available at the time of decision-making, and rewards from different arms are related rather than independent. We propose a novel semi-parametric framework for batched bandits with covariates and a shared parameter across arms, leveraging the single-index regression (SIR) model to capture relationships between arm rewards while balancing interpretability and flexibility. Our algorithm, Batched single-Index Dynamic binning and Successive arm elimination (BIDS), employs a batched successive arm elimination strategy with a dynamic binning mechanism guided by the single-index direction. We consider two settings: one where a pilot direction is available and another where the direction is estimated from data, deriving theoretical regret bounds for both cases. When a pilot direction is available with sufficient accuracy, our approach achieves minimax-optimal rates (with $d = 1$) for nonparametric batched bandits, circumventing the curse of dimensionality. Extensive experiments on simulated and real-world datasets demonstrate the effectiveness of our algorithm compared to the nonparametric batched bandit method introduced by \\cite{jiang2024batched}. \u25b3 Less Submitted 1 March, 2025; originally announced March 2025. MSC Class: 62L05; 62G05 arXiv:2502.18083 [pdf, other] A Fusion Model for Artwork Identification Based on Convolutional Neural Networks and Transformers Authors: Zhenyu Wang, Heng Song Abstract: The identification of artwork is crucial in areas like cultural heritage protection, art market analysis, and historical research. With the advancement of deep learning, Convolutional Neural Networks (CNNs) and Transformer models have become key tools for image classification. While CNNs excel in local feature extraction, they struggle with global context, and Transformers are strong in capturing\u2026 \u25bd More The identification of artwork is crucial in areas like cultural heritage protection, art market analysis, and historical research. With the advancement of deep learning, Convolutional Neural Networks (CNNs) and Transformer models have become key tools for image classification. While CNNs excel in local feature extraction, they struggle with global context, and Transformers are strong in capturing global dependencies but weak in fine-grained local details. To address these challenges, this paper proposes a fusion model combining CNNs and Transformers for artwork identification. The model first extracts local features using CNNs, then captures global context with a Transformer, followed by a feature fusion mechanism to enhance classification accuracy. Experiments on Chinese and oil painting datasets show the fusion model outperforms individual CNN and Transformer models, improving classification accuracy by 9.7% and 7.1%, respectively, and increasing F1 scores by 0.06 and 0.05. The results demonstrate the model's effectiveness and potential for future improvements, such as multimodal integration and architecture optimization. \u25b3 Less Submitted 26 February, 2025; v1 submitted 25 February, 2025; originally announced February 2025. arXiv:2502.17761 [pdf, other] AI-driven 3D Spatial Transcriptomics Authors: Cristina Almagro-P\u00e9rez, Andrew H. Song, Luca Weishaupt, Ahrong Kim, Guillaume Jaume, Drew F. K. Williamson, Konstantin Hemker, Ming Y. Lu, Kritika Singh, Bowen Chen, Long Phi Le, Alexander S. Baras, Sizun Jiang, Ali Bashashati, Jonathan T. C. Liu, Faisal Mahmood Abstract: A comprehensive three-dimensional (3D) map of tissue architecture and gene expression is crucial for illuminating the complexity and heterogeneity of tissues across diverse biomedical applications. However, most spatial transcriptomics (ST) approaches remain limited to two-dimensional (2D) sections of tissue. Although current 3D ST methods hold promise, they typically require extensive tissue sect\u2026 \u25bd More A comprehensive three-dimensional (3D) map of tissue architecture and gene expression is crucial for illuminating the complexity and heterogeneity of tissues across diverse biomedical applications. However, most spatial transcriptomics (ST) approaches remain limited to two-dimensional (2D) sections of tissue. Although current 3D ST methods hold promise, they typically require extensive tissue sectioning, are complex, are not compatible with non-destructive 3D tissue imaging technologies, and often lack scalability. Here, we present VOlumetrically Resolved Transcriptomics EXpression (VORTEX), an AI framework that leverages 3D tissue morphology and minimal 2D ST to predict volumetric 3D ST. By pretraining on diverse 3D morphology-transcriptomic pairs from heterogeneous tissue samples and then fine-tuning on minimal 2D ST data from a specific volume of interest, VORTEX learns both generic tissue-related and sample-specific morphological correlates of gene expression. This approach enables dense, high-throughput, and fast 3D ST, scaling seamlessly to large tissue volumes far beyond the reach of existing 3D ST techniques. By offering a cost-effective and minimally destructive route to obtaining volumetric molecular insights, we anticipate that VORTEX will accelerate biomarker discovery and our understanding of morphomolecular associations and cell states in complex tissues. Interactive 3D ST volumes can be viewed at https://vortex-demo.github.io/ \u25b3 Less Submitted 24 February, 2025; originally announced February 2025. arXiv:2502.14541 [pdf, other] LLM-based User Profile Management for Recommender System Authors: Seunghwan Bang, Hwanjun Song Abstract: The rapid advancement of Large Language Models (LLMs) has opened new opportunities in recommender systems by enabling zero-shot recommendation without conventional training. Despite their potential, most existing works rely solely on users' purchase histories, leaving significant room for improvement by incorporating user-generated textual data, such as reviews and product descriptions. Addressing\u2026 \u25bd More The rapid advancement of Large Language Models (LLMs) has opened new opportunities in recommender systems by enabling zero-shot recommendation without conventional training. Despite their potential, most existing works rely solely on users' purchase histories, leaving significant room for improvement by incorporating user-generated textual data, such as reviews and product descriptions. Addressing this gap, we propose PURE, a novel LLM-based recommendation framework that builds and maintains evolving user profiles by systematically extracting and summarizing key information from user reviews. PURE consists of three core components: a Review Extractor for identifying user preferences and key product features, a Profile Updater for refining and updating user profiles, and a Recommender for generating personalized recommendations using the most current profile. To evaluate PURE, we introduce a continuous sequential recommendation task that reflects real-world scenarios by adding reviews over time and updating predictions incrementally. Our experimental results on Amazon datasets demonstrate that PURE outperforms existing LLM-based methods, effectively leveraging long-term user information while managing token limitations. \u25b3 Less Submitted 20 February, 2025; originally announced February 2025. Comments: Submitted to ACL 2025 arXiv:2502.13181 [pdf, other] RingFormer: Rethinking Recurrent Transformer with Adaptive Level Signals Authors: Jaemu Heo, Eldor Fozilov, Hyunmin Song, Taehwan Kim Abstract: Transformers have achieved great success in effectively processing sequential data such as text. Their architecture consisting of several attention and feedforward blocks can model relations between elements of a sequence in parallel manner, which makes them very efficient to train and effective in sequence modeling. Even though they have shown strong performance in processing sequential data, the\u2026 \u25bd More Transformers have achieved great success in effectively processing sequential data such as text. Their architecture consisting of several attention and feedforward blocks can model relations between elements of a sequence in parallel manner, which makes them very efficient to train and effective in sequence modeling. Even though they have shown strong performance in processing sequential data, the size of their parameters is considerably larger when compared to other architectures such as RNN and CNN based models. Therefore, several approaches have explored parameter sharing and recurrence in Transformer models to address their computational demands. However, such methods struggle to maintain high performance compared to the original transformer model. To address this challenge, we propose our novel approach, RingFormer, which employs one Transformer layer that processes input repeatedly in a circular, ring-like manner, while utilizing low-rank matrices to generate input-dependent level signals. This allows us to reduce the model parameters substantially while maintaining high performance in a variety of tasks such as translation and image classification, as validated in the experiments. \u25b3 Less Submitted 18 February, 2025; originally announced February 2025. arXiv:2502.13177 [pdf, other] KL Penalty Control via Perturbation for Direct Preference Optimization Authors: Sangkyu Lee, Janghoon Han, Hosung Song, Stanley Jungkyu Choi, Honglak Lee, Youngjae Yu Abstract: Direct Preference Optimization (DPO) demonstrates the advantage of aligning a large language model with human preference using only an offline dataset. However, DPO has the limitation that the KL penalty, which prevents excessive deviation from the reference model, is static throughout the training process. Several methods try to turn this static KL penalty into a dynamic one, but no approach can\u2026 \u25bd More Direct Preference Optimization (DPO) demonstrates the advantage of aligning a large language model with human preference using only an offline dataset. However, DPO has the limitation that the KL penalty, which prevents excessive deviation from the reference model, is static throughout the training process. Several methods try to turn this static KL penalty into a dynamic one, but no approach can adaptively assign different KL penalties for each preference pair. In this paper, we propose $\\varepsilon$-Direct Preference Optimization ($\\varepsilon$-DPO), which allows adaptive control of the KL penalty strength $\u03b2$ for each preference pair. Specifically, $\\varepsilon$-DPO adaptively controls $\u03b2$ for each preference pair based on the monotonicity of logits as a preference model under the perturbation of $\u03b2$ during training by simply reusing the logit of the current policy and the reference policy. Experimental results show that $\\varepsilon$-DPO outperforms existing direct alignment algorithms and KL penalty relaxation methods on general chatbot benchmarks, highlighting the significance of adaptive KL penalty relaxation at the instance-level in DPO. \u25b3 Less Submitted 18 February, 2025; originally announced February 2025. Comments: Preprint; Under review arXiv:2502.11003 [pdf, other] FeaKM: Robust Collaborative Perception under Noisy Pose Conditions Authors: Jiuwu Hao, Liguo Sun, Ti Xiang, Yuting Wan, Haolin Song, Pin Lv Abstract: Collaborative perception is essential for networks of agents with limited sensing capabilities, enabling them to work together by exchanging information to achieve a robust and comprehensive understanding of their environment. However, localization inaccuracies often lead to significant spatial message displacement, which undermines the effectiveness of these collaborative efforts. To tackle this\u2026 \u25bd More Collaborative perception is essential for networks of agents with limited sensing capabilities, enabling them to work together by exchanging information to achieve a robust and comprehensive understanding of their environment. However, localization inaccuracies often lead to significant spatial message displacement, which undermines the effectiveness of these collaborative efforts. To tackle this challenge, we introduce FeaKM, a novel method that employs Feature-level Keypoints Matching to effectively correct pose discrepancies among collaborating agents. Our approach begins by utilizing a confidence map to identify and extract salient points from intermediate feature representations, allowing for the computation of their descriptors. This step ensures that the system can focus on the most relevant information, enhancing the matching process. We then implement a target-matching strategy that generates an assignment matrix, correlating the keypoints identified by different agents. This is critical for establishing accurate correspondences, which are essential for effective collaboration. Finally, we employ a fine-grained transformation matrix to synchronize the features of all agents and ascertain their relative statuses, ensuring coherent communication among them. Our experimental results demonstrate that FeaKM significantly outperforms existing methods on the DAIR-V2X dataset, confirming its robustness even under severe noise conditions. The code and implementation details are available at https://github.com/uestchjw/FeaKM. \u25b3 Less Submitted 16 February, 2025; originally announced February 2025. Comments: Accepted by JCRAI 2024 arXiv:2502.10514 [pdf] Applying Deep Learning to Ads Conversion Prediction in Last Mile Delivery Marketplace Authors: Di Li, Xiaochang Miao, Huiyu Song, Chao Chu, Hao Xu, Mandar Rahurkar Abstract: Deep neural networks (DNNs) have revolutionized web-scale ranking systems, enabling breakthroughs in capturing complex user behaviors and driving performance gains. At DoorDash, we first harnessed this transformative power by transitioning our homepage Ads ranking system from traditional tree based models to cutting edge multi task DNNs. This evolution sparked advancements in data foundations, mod\u2026 \u25bd More Deep neural networks (DNNs) have revolutionized web-scale ranking systems, enabling breakthroughs in capturing complex user behaviors and driving performance gains. At DoorDash, we first harnessed this transformative power by transitioning our homepage Ads ranking system from traditional tree based models to cutting edge multi task DNNs. This evolution sparked advancements in data foundations, model design, training efficiency, evaluation rigor, and online serving, delivering substantial business impact and reshaping our approach to machine learning. In this paper, we talk about our problem driven journey, from identifying the right problems and crafting targeted solutions to overcoming the complexity of developing and scaling a deep learning recommendation system. Through our successes and learned lessons, we aim to share insights and practical guidance to teams pursuing similar advancements in machine learning systems. \u25b3 Less Submitted 27 February, 2025; v1 submitted 14 February, 2025; originally announced February 2025. Comments: 10 pages, 5 figures, submitted to KDD arXiv:2502.09776 [pdf, other] Investigating the Role of Situational Disruptors in Engagement with Digital Mental Health Tools Authors: Ananya Bhattacharjee, Joseph Jay Williams, Miranda Beltzer, Jonah Meyerhoff, Harsh Kumar, Haochen Song, David C. Mohr, Alex Mariakakis, Rachel Kornfield Abstract: Challenges in engagement with digital mental health (DMH) tools are commonly addressed through technical enhancements and algorithmic interventions. This paper shifts the focus towards the role of users' broader social context as a significant factor in engagement. Through an eight-week text messaging program aimed at enhancing psychological wellbeing, we recruited 20 participants to help us ident\u2026 \u25bd More Challenges in engagement with digital mental health (DMH) tools are commonly addressed through technical enhancements and algorithmic interventions. This paper shifts the focus towards the role of users' broader social context as a significant factor in engagement. Through an eight-week text messaging program aimed at enhancing psychological wellbeing, we recruited 20 participants to help us identify situational engagement disruptors (SEDs), including personal responsibilities, professional obligations, and unexpected health issues. In follow-up design workshops with 25 participants, we explored potential solutions that address such SEDs: prioritizing self-care through structured goal-setting, alternative framings for disengagement, and utilization of external resources. Our findings challenge conventional perspectives on engagement and offer actionable design implications for future DMH tools. \u25b3 Less Submitted 25 February, 2025; v1 submitted 13 February, 2025; originally announced February 2025. arXiv:2502.08514 [pdf, other] Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation Authors: Mahnaz Koupaee, Jake W. Vincent, Saab Mansour, Igor Shalyminov, Han He, Hwanjun Song, Raphael Shu, Jianfeng He, Yi Nian, Amy Wing-mei Wong, Kyu J. Han, Hang Su Abstract: Faithfulness evaluators based on large language models (LLMs) are often fooled by the fluency of the text and struggle with identifying errors in the summaries. We propose an approach to summary faithfulness evaluation in which multiple LLM-based agents are assigned initial stances (regardless of what their belief might be) and forced to come up with a reason to justify the imposed belief, thus en\u2026 \u25bd More Faithfulness evaluators based on large language models (LLMs) are often fooled by the fluency of the text and struggle with identifying errors in the summaries. We propose an approach to summary faithfulness evaluation in which multiple LLM-based agents are assigned initial stances (regardless of what their belief might be) and forced to come up with a reason to justify the imposed belief, thus engaging in a multi-round debate to reach an agreement. The uniformly distributed initial assignments result in a greater diversity of stances leading to more meaningful debates and ultimately more errors identified. Furthermore, by analyzing the recent faithfulness evaluation datasets, we observe that naturally, it is not always the case for a summary to be either faithful to the source document or not. We therefore introduce a new dimension, ambiguity, and a detailed taxonomy to identify such special cases. Experiments demonstrate our approach can help identify ambiguities, and have even a stronger performance on non-ambiguous summaries. \u25b3 Less Submitted 13 February, 2025; v1 submitted 12 February, 2025; originally announced February 2025. arXiv:2502.05609 [pdf, other] Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding Authors: Sukmin Cho, Sangjin Choi, Taeho Hwang, Jeongyeon Seo, Soyeong Jeong, Huije Lee, Hoyun Song, Jong C. Park, Youngjin Kwon Abstract: Accelerating inference in Large Language Models (LLMs) is critical for real-time interactions, as they have been widely incorporated into real-world services. Speculative decoding, a fully algorithmic solution, has gained attention for improving inference speed by drafting and verifying tokens, thereby generating multiple tokens in a single forward pass. However, current drafting strategies usuall\u2026 \u25bd More Accelerating inference in Large Language Models (LLMs) is critical for real-time interactions, as they have been widely incorporated into real-world services. Speculative decoding, a fully algorithmic solution, has gained attention for improving inference speed by drafting and verifying tokens, thereby generating multiple tokens in a single forward pass. However, current drafting strategies usually require significant fine-tuning or have inconsistent performance across tasks. To address these challenges, we propose Hierarchy Drafting (HD), a novel lossless drafting approach that organizes various token sources into multiple databases in a hierarchical framework based on temporal locality. In the drafting step, HD sequentially accesses multiple databases to obtain draft tokens from the highest to the lowest locality, ensuring consistent acceleration across diverse tasks and minimizing drafting latency. Our experiments on Spec-Bench using LLMs with 7B and 13B parameters demonstrate that HD outperforms existing database drafting methods, achieving robust inference speedups across model sizes, tasks, and temperatures. \u25b3 Less Submitted 8 February, 2025; originally announced February 2025. Comments: Findings of NAACL 2025 arXiv:2502.03493 [pdf, other] MetaFE-DE: Learning Meta Feature Embedding for Depth Estimation from Monocular Endoscopic Images Authors: Dawei Lu, Deqiang Xiao, Danni Ai, Jingfan Fan, Tianyu Fu, Yucong Lin, Hong Song, Xujiong Ye, Lei Zhang, Jian Yang Abstract: Depth estimation from monocular endoscopic images presents significant challenges due to the complexity of endoscopic surgery, such as irregular shapes of human soft tissues, as well as variations in lighting conditions. Existing methods primarily estimate the depth information from RGB images directly, and often surffer the limited interpretability and accuracy. Given that RGB and depth images ar\u2026 \u25bd More Depth estimation from monocular endoscopic images presents significant challenges due to the complexity of endoscopic surgery, such as irregular shapes of human soft tissues, as well as variations in lighting conditions. Existing methods primarily estimate the depth information from RGB images directly, and often surffer the limited interpretability and accuracy. Given that RGB and depth images are two views of the same endoscopic surgery scene, in this paper, we introduce a novel concept referred as ``meta feature embedding (MetaFE)\", in which the physical entities (e.g., tissues and surgical instruments) of endoscopic surgery are represented using the shared features that can be alternatively decoded into RGB or depth image. With this concept, we propose a two-stage self-supervised learning paradigm for the monocular endoscopic depth estimation. In the first stage, we propose a temporal representation learner using diffusion models, which are aligned with the spatial information through the cross normalization to construct the MetaFE. In the second stage, self-supervised monocular depth estimation with the brightness calibration is applied to decode the meta features into the depth image. Extensive evaluation on diverse endoscopic datasets demonstrates that our approach outperforms the state-of-the-art method in depth estimation, achieving superior accuracy and generalization. The source code will be publicly available. \u25b3 Less Submitted 4 February, 2025; originally announced February 2025. arXiv:2502.01946 [pdf, other] HeRCULES: Heterogeneous Radar Dataset in Complex Urban Environment for Multi-session Radar SLAM Authors: Hanjun Kim, Minwoo Jung, Chiyun Noh, Sangwoo Jung, Hyunho Song, Wooseong Yang, Hyesu Jang, Ayoung Kim Abstract: Recently, radars have been widely featured in robotics for their robustness in challenging weather conditions. Two commonly used radar types are spinning radars and phased-array radars, each offering distinct sensor characteristics. Existing datasets typically feature only a single type of radar, leading to the development of algorithms limited to that specific kind. In this work, we highlight tha\u2026 \u25bd More Recently, radars have been widely featured in robotics for their robustness in challenging weather conditions. Two commonly used radar types are spinning radars and phased-array radars, each offering distinct sensor characteristics. Existing datasets typically feature only a single type of radar, leading to the development of algorithms limited to that specific kind. In this work, we highlight that combining different radar types offers complementary advantages, which can be leveraged through a heterogeneous radar dataset. Moreover, this new dataset fosters research in multi-session and multi-robot scenarios where robots are equipped with different types of radars. In this context, we introduce the HeRCULES dataset, a comprehensive, multi-modal dataset with heterogeneous radars, FMCW LiDAR, IMU, GPS, and cameras. This is the first dataset to integrate 4D radar and spinning radar alongside FMCW LiDAR, offering unparalleled localization, mapping, and place recognition capabilities. The dataset covers diverse weather and lighting conditions and a range of urban traffic scenarios, enabling a comprehensive analysis across various environments. The sequence paths with multiple revisits and ground truth pose for each sensor enhance its suitability for place recognition research. We expect the HeRCULES dataset to facilitate odometry, mapping, place recognition, and sensor fusion research. The dataset and development tools are available at https://sites.google.com/view/herculesdataset. \u25b3 Less Submitted 21 February, 2025; v1 submitted 3 February, 2025; originally announced February 2025. Comments: 2025 IEEE International Conference on Robotics and Automation (ICRA 2025) arXiv:2502.01680 [pdf, other] Neurosymbolic AI for Travel Demand Prediction: Integrating Decision Tree Rules into Neural Networks Authors: Kamal Acharya, Mehul Lad, Liang Sun, Houbing Song Abstract: Travel demand prediction is crucial for optimizing transportation planning, resource allocation, and infrastructure development, ensuring efficient mobility and economic sustainability. This study introduces a Neurosymbolic Artificial Intelligence (Neurosymbolic AI) framework that integrates decision tree (DT)-based symbolic rules with neural networks (NNs) to predict travel demand, leveraging the\u2026 \u25bd More Travel demand prediction is crucial for optimizing transportation planning, resource allocation, and infrastructure development, ensuring efficient mobility and economic sustainability. This study introduces a Neurosymbolic Artificial Intelligence (Neurosymbolic AI) framework that integrates decision tree (DT)-based symbolic rules with neural networks (NNs) to predict travel demand, leveraging the interpretability of symbolic reasoning and the predictive power of neural learning. The framework utilizes data from diverse sources, including geospatial, economic, and mobility datasets, to build a comprehensive feature set. DTs are employed to extract interpretable if-then rules that capture key patterns, which are then incorporated as additional features into a NN to enhance its predictive capabilities. Experimental results show that the combined dataset, enriched with symbolic rules, consistently outperforms standalone datasets across multiple evaluation metrics, including Mean Absolute Error (MAE), \\(R^2\\), and Common Part of Commuters (CPC). Rules selected at finer variance thresholds (e.g., 0.0001) demonstrate superior effectiveness in capturing nuanced relationships, reducing prediction errors, and aligning with observed commuter patterns. By merging symbolic and neural learning paradigms, this Neurosymbolic approach achieves both interpretability and accuracy. \u25b3 Less Submitted 2 February, 2025; originally announced February 2025. Comments: 9 pages, 5 figures, this paper is under review in the conference arXiv:2501.16652 [pdf, other] Molecular-driven Foundation Model for Oncologic Pathology Authors: Anurag Vaidya, Andrew Zhang, Guillaume Jaume, Andrew H. Song, Tong Ding, Sophia J. Wagner, Ming Y. Lu, Paul Doucet, Harry Robertson, Cristina Almagro-Perez, Richard J. Chen, Dina ElHarouni, Georges Ayoub, Connor Bossi, Keith L. Ligon, Georg Gerber, Long Phi Le, Faisal Mahmood Abstract: Foundation models are reshaping computational pathology by enabling transfer learning, where models pre-trained on vast datasets can be adapted for downstream diagnostic, prognostic, and therapeutic response tasks. Despite these advances, foundation models are still limited in their ability to encode the entire gigapixel whole-slide images without additional training and often lack complementary m\u2026 \u25bd More Foundation models are reshaping computational pathology by enabling transfer learning, where models pre-trained on vast datasets can be adapted for downstream diagnostic, prognostic, and therapeutic response tasks. Despite these advances, foundation models are still limited in their ability to encode the entire gigapixel whole-slide images without additional training and often lack complementary multimodal data. Here, we introduce Threads, a slide-level foundation model capable of generating universal representations of whole-slide images of any size. Threads was pre-trained using a multimodal learning approach on a diverse cohort of 47,171 hematoxylin and eosin (H&E)-stained tissue sections, paired with corresponding genomic and transcriptomic profiles - the largest such paired dataset to be used for foundation model development to date. This unique training paradigm enables Threads to capture the tissue's underlying molecular composition, yielding powerful representations applicable to a wide array of downstream tasks. In extensive benchmarking across 54 oncology tasks, including clinical subtyping, grading, mutation prediction, immunohistochemistry status determination, treatment response prediction, and survival prediction, Threads outperformed all baselines while demonstrating remarkable generalizability and label efficiency. It is particularly well suited for predicting rare events, further emphasizing its clinical utility. We intend to make the model publicly available for the broader community. \u25b3 Less Submitted 27 January, 2025; originally announced January 2025. arXiv:2501.16591 [pdf] Applying Ensemble Models based on Graph Neural Network and Reinforcement Learning for Wind Power Forecasting Authors: Hongjin Song, Qianrun Chen, Tianqi Jiang, Yongfeng Li, Xusheng Li, Wenjun Xi, Songtao Huang Abstract: Accurately predicting the wind power output of a wind farm across various time scales utilizing Wind Power Forecasting (WPF) is a critical issue in wind power trading and utilization. The WPF problem remains unresolved due to numerous influencing variables, such as wind speed, temperature, latitude, and longitude. Furthermore, achieving high prediction accuracy is crucial for maintaining electric\u2026 \u25bd More Accurately predicting the wind power output of a wind farm across various time scales utilizing Wind Power Forecasting (WPF) is a critical issue in wind power trading and utilization. The WPF problem remains unresolved due to numerous influencing variables, such as wind speed, temperature, latitude, and longitude. Furthermore, achieving high prediction accuracy is crucial for maintaining electric grid stability and ensuring supply security. In this paper, we model all wind turbines within a wind farm as graph nodes in a graph built by their geographical locations. Accordingly, we propose an ensemble model based on graph neural networks and reinforcement learning (EMGRL) for WPF. Our approach includes: (1) applying graph neural networks to capture the time-series data from neighboring wind farms relevant to the target wind farm; (2) establishing a general state embedding that integrates the target wind farm's data with the historical performance of base models on the target wind farm; (3) ensembling and leveraging the advantages of all base models through an actor-critic reinforcement learning framework for WPF. \u25b3 Less Submitted 27 January, 2025; originally announced January 2025. arXiv:2501.16361 [pdf] Large Language Models Meet Graph Neural Networks for Text-Numeric Graph Reasoning Authors: Haoran Song, Jiarui Feng, Guangfu Li, Michael Province, Philip Payne, Yixin Chen, Fuhai Li Abstract: In real-world scientific discovery, human beings always make use of the accumulated prior knowledge with imagination pick select one or a few most promising hypotheses from large and noisy data analysis results. In this study, we introduce a new type of graph structure, the text-numeric graph (TNG), which is defined as graph entities and associations have both text-attributed information and numer\u2026 \u25bd More In real-world scientific discovery, human beings always make use of the accumulated prior knowledge with imagination pick select one or a few most promising hypotheses from large and noisy data analysis results. In this study, we introduce a new type of graph structure, the text-numeric graph (TNG), which is defined as graph entities and associations have both text-attributed information and numeric information. The TNG is an ideal data structure model for novel scientific discovery via graph reasoning because it integrates human-understandable textual annotations or prior knowledge, with numeric values that represent the observed or activation levels of graph entities or associations in different samples. Together both the textual information and numeric values determine the importance of graph entities and associations in graph reasoning for novel scientific knowledge discovery. We further propose integrating large language models (LLMs) and graph neural networks (GNNs) to analyze the TNGs for graph understanding and reasoning. To demonstrate the utility, we generated the text-omic(numeric) signaling graphs (TOSG), as one type of TNGs, in which all graphs have the same entities, associations and annotations, but have sample-specific entity numeric (omic) values using single cell RNAseq (scRNAseq) datasets of different diseases. We proposed joint LLM-GNN models for key entity mining and signaling pathway mining on the TOSGs. The evaluation results showed the LLM-GNN and TNGs models significantly improve classification accuracy and network inference. In conclusion, the TNGs and joint LLM-GNN models are important approaches for scientific discovery. \u25b3 Less Submitted 20 January, 2025; originally announced January 2025. Comments: 29 pages, 6 figures arXiv:2501.15830 [pdf, other] SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model Authors: Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, Xuelong Li Abstract: In this paper, we claim that spatial understanding is the keypoint in robot manipulation, and propose SpatialVLA to explore effective spatial representations for the robot foundation model. Specifically, we introduce Ego3D Position Encoding to inject 3D information into the input observations of the visual-language-action model, and propose Adaptive Action Grids to represent spatial robot movement\u2026 \u25bd More In this paper, we claim that spatial understanding is the keypoint in robot manipulation, and propose SpatialVLA to explore effective spatial representations for the robot foundation model. Specifically, we introduce Ego3D Position Encoding to inject 3D information into the input observations of the visual-language-action model, and propose Adaptive Action Grids to represent spatial robot movement actions with adaptive discretized action grids, facilitating learning generalizable and transferrable spatial action knowledge for cross-robot control. SpatialVLA is first pre-trained on top of a vision-language model with 1.1 Million real-world robot episodes, to learn a generalist manipulation policy across multiple robot environments and tasks. After pre-training, SpatialVLA is directly applied to perform numerous tasks in a zero-shot manner. The superior results in both simulation and real-world robots demonstrate its advantage of inferring complex robot motion trajectories and its strong in-domain multi-task generalization ability. We further show the proposed Adaptive Action Grids offer a new and effective way to fine-tune the pre-trained SpatialVLA model for new simulation and real-world setups, where the pre-learned action grids are re-discretized to capture robot-specific spatial action movements of new setups. The superior results from extensive evaluations demonstrate the exceptional in-distribution generalization and out-of-distribution adaptation capability, highlighting the crucial benefit of the proposed spatial-aware representations for generalist robot policy learning. All the details and codes will be open-sourced. \u25b3 Less Submitted 7 March, 2025; v1 submitted 27 January, 2025; originally announced January 2025. arXiv:2501.11283 [pdf, other] Large Language Model Agents for Radio Map Generation and Wireless Network Planning Authors: Hongye Quan, Wanli Ni, Tong Zhang, Xiangyu Ye, Ziyi Xie, Shuai Wang, Yuanwei Liu, Hui Song Abstract: Using commercial software for radio map generation and wireless network planning often require complex manual operations, posing significant challenges in terms of scalability, adaptability, and user-friendliness, due to heavy manual operations. To address these issues, we propose an automated solution that employs large language model (LLM) agents. These agents are designed to autonomously genera\u2026 \u25bd More Using commercial software for radio map generation and wireless network planning often require complex manual operations, posing significant challenges in terms of scalability, adaptability, and user-friendliness, due to heavy manual operations. To address these issues, we propose an automated solution that employs large language model (LLM) agents. These agents are designed to autonomously generate radio maps and facilitate wireless network planning for specified areas, thereby minimizing the necessity for extensive manual intervention. To validate the effectiveness of our proposed solution, we develop a software platform that integrates LLM agents. Experimental results demonstrate that a large amount manual operations can be saved via the proposed LLM agent, and the automated solutions can achieve an enhanced coverage and signal-to-interference-noise ratio (SINR), especially in urban environments. \u25b3 Less Submitted 13 February, 2025; v1 submitted 20 January, 2025; originally announced January 2025. Comments: 5 pages, 7 figures arXiv:2501.10151 [pdf, other] Topology-Driven Attribute Recovery for Attribute Missing Graph Learning in Social Internet of Things Authors: Mengran Li, Junzhou Chen, Chenyun Yu, Guanying Jiang, Ronghui Zhang, Yanming Shen, Houbing Herbert Song Abstract: With the advancement of information technology, the Social Internet of Things (SIoT) has fostered the integration of physical devices and social networks, deepening the study of complex interaction patterns. Text Attribute Graphs (TAGs) capture both topological structures and semantic attributes, enhancing the analysis of complex interactions within the SIoT. However, existing graph learning metho\u2026 \u25bd More With the advancement of information technology, the Social Internet of Things (SIoT) has fostered the integration of physical devices and social networks, deepening the study of complex interaction patterns. Text Attribute Graphs (TAGs) capture both topological structures and semantic attributes, enhancing the analysis of complex interactions within the SIoT. However, existing graph learning methods are typically designed for complete attributed graphs, and the common issue of missing attributes in Attribute Missing Graphs (AMGs) increases the difficulty of analysis tasks. To address this, we propose the Topology-Driven Attribute Recovery (TDAR) framework, which leverages topological data for AMG learning. TDAR introduces an improved pre-filling method for initial attribute recovery using native graph topology. Additionally, it dynamically adjusts propagation weights and incorporates homogeneity strategies within the embedding space to suit AMGs' unique topological structures, effectively reducing noise during information propagation. Extensive experiments on public datasets demonstrate that TDAR significantly outperforms state-of-the-art methods in attribute reconstruction and downstream tasks, offering a robust solution to the challenges posed by AMGs. The code is available at https://github.com/limengran98/TDAR. \u25b3 Less Submitted 17 January, 2025; originally announced January 2025. Comments: Accepted by IEEE Internet of Things Journal arXiv:2501.08610 [pdf, other] Multi-view Correlation-aware Network Traffic Detection on Flow Hypergraph Authors: Jiajun Zhou, Wentao Fu, Hao Song, Shanqing Yu, Qi Xuan, Xiaoniu Yang Abstract: As the Internet rapidly expands, the increasing complexity and diversity of network activities pose significant challenges to effective network governance and security regulation. Network traffic, which serves as a crucial data carrier of network activities, has become indispensable in this process. Network traffic detection aims to monitor, analyze, and evaluate the data flows transmitted across\u2026 \u25bd More As the Internet rapidly expands, the increasing complexity and diversity of network activities pose significant challenges to effective network governance and security regulation. Network traffic, which serves as a crucial data carrier of network activities, has become indispensable in this process. Network traffic detection aims to monitor, analyze, and evaluate the data flows transmitted across the network to ensure network security and optimize performance. However, existing network traffic detection methods generally suffer from several limitations: 1) a narrow focus on characterizing traffic features from a single perspective; 2) insufficient exploration of discriminative features for different traffic; 3) poor generalization to different traffic scenarios. To address these issues, we propose a multi-view correlation-aware framework named FlowID for network traffic detection. FlowID captures multi-view traffic features via temporal and interaction awareness, while a hypergraph encoder further explores higher-order relationships between flows. To overcome the challenges of data imbalance and label scarcity, we design a dual-contrastive proxy task, enhancing the framework's ability to differentiate between various traffic flows through traffic-to-traffic and group-to-group contrast. Extensive experiments on five real-world datasets demonstrate that FlowID significantly outperforms existing methods in accuracy, robustness, and generalization across diverse network scenarios, particularly in detecting malicious traffic. \u25b3 Less Submitted 15 January, 2025; originally announced January 2025. arXiv:2501.08561 [pdf, other] ANSR-DT: An Adaptive Neuro-Symbolic Learning and Reasoning Framework for Digital Twins Authors: Safayat Bin Hakim, Muhammad Adil, Alvaro Velasquez, Houbing Herbert Song Abstract: In this paper, we propose an Adaptive Neuro-Symbolic Learning Framework for digital twin technology called ``ANSR-DT.\" Our approach combines pattern recognition algorithms with reinforcement learning and symbolic reasoning to enable real-time learning and adaptive intelligence. This integration enhances the understanding of the environment and promotes continuous learning, leading to better and mo\u2026 \u25bd More In this paper, we propose an Adaptive Neuro-Symbolic Learning Framework for digital twin technology called ``ANSR-DT.\" Our approach combines pattern recognition algorithms with reinforcement learning and symbolic reasoning to enable real-time learning and adaptive intelligence. This integration enhances the understanding of the environment and promotes continuous learning, leading to better and more effective decision-making in real-time for applications that require human-machine collaboration. We evaluated the \\textit{ANSR-DT} framework for its ability to learn and adapt to dynamic patterns, observing significant improvements in decision accuracy, reliability, and interpretability when compared to existing state-of-the-art methods. However, challenges still exist in extracting and integrating symbolic rules in complex environments, which limits the full potential of our framework in heterogeneous settings. Moreover, our ongoing research aims to address this issue in the future by ensuring seamless integration of neural models at large. In addition, our open-source implementation promotes reproducibility and encourages future research to build on our foundational work. \u25b3 Less Submitted 14 January, 2025; originally announced January 2025. arXiv:2501.06459 [pdf, other] Enhancing The Open Network: Definition and Automated Detection of Smart Contract Defects Authors: Hao Song, Teng Li, Jiachi Chen, Ting Chen, Beibei Li, Zhangyan Lin, Yi Lu, Pan Li, Xihan Zhou Abstract: The Open Network (TON), designed to support Telegram's extensive user base of hundreds of millions, has garnered considerable attention since its launch in 2022. FunC is the most popular programming language for writing smart contracts on TON. It is distinguished by a unique syntax compared to other smart contract languages. Despite growing interest, research on the practical defects of TON smart\u2026 \u25bd More The Open Network (TON), designed to support Telegram's extensive user base of hundreds of millions, has garnered considerable attention since its launch in 2022. FunC is the most popular programming language for writing smart contracts on TON. It is distinguished by a unique syntax compared to other smart contract languages. Despite growing interest, research on the practical defects of TON smart contracts is still in its early stages. In this paper, we summarize eight smart contract defects identified from TON's official blogs and audit reports, each with detailed definitions and code examples. Furthermore, we propose a static analysis framework called TONScanner to facilitate the detection of these defects. Specifically, TONScanner reuses FunC compiler's frontend code to transform the FunC source code into FunC intermediate representation (IR) in the form of a directed acyclic graph (DAG). Based on this IR, TONScanner constructs a control flow graph (CFG), then transforms it into a static single assignment (SSA) form to simplify further analysis. TONScanner also integrates Data Dependency, Call Graph, Taint Analysis, and Cell Construct, which are specifically tailored for TON blockchain's unique data structures. These components finally facilitate the identification of the eight defects. We evaluate the effectiveness of TONScanner by applying it to 1,640 smart contracts and find a total of 14,995 defects. Through random sampling and manual labeling, we find that TONScanner achieves an overall precision of 97.49%. The results reveal that current TON contracts contain numerous defects, indicating that developers are prone to making errors. TONScanner has proven its ability to accurately identify these defects, thereby aiding in their correction. \u25b3 Less Submitted 11 January, 2025; originally announced January 2025. Comments: The paper has been accepted for presentation at the 47th IEEE/ACM International Conference on Software Engineering (ICSE 2025) arXiv:2501.04161 [pdf, other] KGIF: Optimizing Relation-Aware Recommendations with Knowledge Graph Information Fusion Authors: Dong Hyun Jeon, Wenbo Sun, Houbing Herbert Song, Dongfang Liu, Velasquez Alvaro, Yixin Chloe Xie, Shuteng Niu Abstract: While deep-learning-enabled recommender systems demonstrate strong performance benchmarks, many struggle to adapt effectively in real-world environments due to limited use of user-item relationship data and insufficient transparency in recommendation generation. Traditional collaborative filtering approaches fail to integrate multifaceted item attributes, and although Factorization Machines accoun\u2026 \u25bd More While deep-learning-enabled recommender systems demonstrate strong performance benchmarks, many struggle to adapt effectively in real-world environments due to limited use of user-item relationship data and insufficient transparency in recommendation generation. Traditional collaborative filtering approaches fail to integrate multifaceted item attributes, and although Factorization Machines account for item-specific details, they overlook broader relational patterns. Collaborative knowledge graph-based models have progressed by embedding user-item interactions with item-attribute relationships, offering a holistic perspective on interconnected entities. However, these models frequently aggregate attribute and interaction data in an implicit manner, leaving valuable relational nuances underutilized. This study introduces the Knowledge Graph Attention Network with Information Fusion (KGIF), a specialized framework designed to merge entity and relation embeddings explicitly through a tailored self-attention mechanism. The KGIF framework integrates reparameterization via dynamic projection vectors, enabling embeddings to adaptively represent intricate relationships within knowledge graphs. This explicit fusion enhances the interplay between user-item interactions and item-attribute relationships, providing a nuanced balance between user-centric and item-centric representations. An attentive propagation mechanism further optimizes knowledge graph embeddings, capturing multi-layered interaction patterns. The contributions of this work include an innovative method for explicit information fusion, improved robustness for sparse knowledge graphs, and the ability to generate explainable recommendations through interpretable path visualization. \u25b3 Less Submitted 7 January, 2025; originally announced January 2025. Comments: Published at IEEE Big Data 2024 arXiv:2501.03999 [pdf, other] Adaptive Experiments Under High-Dimensional and Data Sparse Settings: Applications for Educational Platforms Authors: Haochen Song, Ilya Musabirov, Ananya Bhattacharjee, Audrey Durand, Meredith Franklin, Anna Rafferty, Joseph Jay Williams Abstract: In online educational platforms, adaptive experiment designs play a critical role in personalizing learning pathways, instructional sequencing, and content recommendations. Traditional adaptive policies, such as Thompson Sampling, struggle with scalability in high-dimensional and sparse settings such as when there are large amount of treatments (arms) and limited resources such as funding and time\u2026 \u25bd More In online educational platforms, adaptive experiment designs play a critical role in personalizing learning pathways, instructional sequencing, and content recommendations. Traditional adaptive policies, such as Thompson Sampling, struggle with scalability in high-dimensional and sparse settings such as when there are large amount of treatments (arms) and limited resources such as funding and time to conduct to a classroom constraint student size. Furthermore, the issue of under-exploration in large-scale educational interventions can lead to suboptimal learning recommendations. To address these challenges, we build upon the concept of lenient regret, which tolerates limited suboptimal selections to enhance exploratory learning, and propose a framework for determining the feasible number of treatments given a sample size. We illustrate these ideas with a case study in online educational learnersourcing examples, where adaptive algorithms dynamically allocate peer-crafted interventions to other students under active recall exercise. Our proposed Weighted Allocation Probability Adjusted Thompson Sampling (WAPTS) algorithm enhances the efficiency of treatment allocation by adjusting sampling weights to balance exploration and exploitation in data-sparse environments. We present comparative evaluations of WAPTS across various sample sizes (N=50, 300, 1000) and treatment conditions, demonstrating its ability to mitigate under-exploration while optimizing learning outcomes. \u25b3 Less Submitted 24 February, 2025; v1 submitted 7 January, 2025; originally announced January 2025. arXiv:2412.20064 [pdf, other] VELoRA: A Low-Rank Adaptation Approach for Efficient RGB-Event based Recognition Authors: Lan Chen, Haoxiang Yang, Pengpeng Shao, Haoyu Song, Xiao Wang, Zhicheng Zhao, Yaowei Wang, Yonghong Tian Abstract: Pattern recognition leveraging both RGB and Event cameras can significantly enhance performance by deploying deep neural networks that utilize a fine-tuning strategy. Inspired by the successful application of large models, the introduction of such large models can also be considered to further enhance the performance of multi-modal tasks. However, fully fine-tuning these models leads to inefficien\u2026 \u25bd More Pattern recognition leveraging both RGB and Event cameras can significantly enhance performance by deploying deep neural networks that utilize a fine-tuning strategy. Inspired by the successful application of large models, the introduction of such large models can also be considered to further enhance the performance of multi-modal tasks. However, fully fine-tuning these models leads to inefficiency and lightweight fine-tuning methods such as LoRA and Adapter have been proposed to achieve a better balance between efficiency and performance. To our knowledge, there is currently no work that has conducted parameter-efficient fine-tuning (PEFT) for RGB-Event recognition based on pre-trained foundation models. To address this issue, this paper proposes a novel PEFT strategy to adapt the pre-trained foundation vision models for the RGB-Event-based classification. Specifically, given the RGB frames and event streams, we extract the RGB and event features based on the vision foundation model ViT with a modality-specific LoRA tuning strategy. The frame difference of the dual modalities is also considered to capture the motion cues via the frame difference backbone network. These features are concatenated and fed into high-level Transformer layers for efficient multi-modal feature learning via modality-shared LoRA tuning. Finally, we concatenate these features and feed them into a classification head to achieve efficient fine-tuning. The source code and pre-trained models will be released on \\url{https://github.com/Event-AHU/VELoRA}. \u25b3 Less Submitted 28 December, 2024; originally announced December 2024. Comments: In Peer Review arXiv:2412.18720 [pdf, other] Effective and Lightweight Representation Learning for Link Sign Prediction in Signed Bipartite Graphs Authors: Gyeongmin Gu, Minseo Jeon, Hyun-Je Song, Jinhong Jung Abstract: How can we effectively and efficiently learn node representations in signed bipartite graphs? A signed bipartite graph is a graph consisting of two nodes sets where nodes of different types are positively or negative connected, and it has been extensively used to model various real-world relationships such as e-commerce, etc. To analyze such a graph, previous studies have focused on designing meth\u2026 \u25bd More How can we effectively and efficiently learn node representations in signed bipartite graphs? A signed bipartite graph is a graph consisting of two nodes sets where nodes of different types are positively or negative connected, and it has been extensively used to model various real-world relationships such as e-commerce, etc. To analyze such a graph, previous studies have focused on designing methods for learning node representations using graph neural networks. In particular, these methods insert edges between nodes of the same type based on balance theory, enabling them to leverage augmented structures in their learning. However, the existing methods rely on a naive message passing design, which is prone to over-smoothing and susceptible to noisy interactions in real-world graphs. Furthermore, they suffer from computational inefficiency due to their heavy design and the significant increase in the number of added edges. In this paper, we propose ELISE, an effective and lightweight GNN-based approach for learning signed bipartite graphs. We first extend personalized propagation to a signed bipartite graph, incorporating signed edges during message passing. This extension adheres to balance theory without introducing additional edges, mitigating the over-smoothing issue and enhancing representation power. We then jointly learn node embeddings on a low-rank approximation of the signed bipartite graph, which reduces potential noise and emphasizes its global structure, further improving expressiveness without significant loss of efficiency. We encapsulate these ideas into ELISE, designing it to be lightweight, unlike the previous methods that add too many edges and cause inefficiency. Through extensive experiments on real-world signed bipartite graphs, we demonstrate that ELISE outperforms its competitors for predicting link signs while providing faster training and inference time. \u25b3 Less Submitted 24 December, 2024; originally announced December 2024. arXiv:2412.18170 [pdf, other] Unlocking the Hidden Treasures: Enhancing Recommendations with Unlabeled Data Authors: Yuhan Zhao, Rui Chen, Qilong Han, Hongtao Song, Li Chen Abstract: Collaborative filtering (CF) stands as a cornerstone in recommender systems, yet effectively leveraging the massive unlabeled data presents a significant challenge. Current research focuses on addressing the challenge of unlabeled data by extracting a subset that closely approximates negative samples. Regrettably, the remaining data are overlooked, failing to fully integrate this valuable informat\u2026 \u25bd More Collaborative filtering (CF) stands as a cornerstone in recommender systems, yet effectively leveraging the massive unlabeled data presents a significant challenge. Current research focuses on addressing the challenge of unlabeled data by extracting a subset that closely approximates negative samples. Regrettably, the remaining data are overlooked, failing to fully integrate this valuable information into the construction of user preferences. To address this gap, we introduce a novel positive-neutral-negative (PNN) learning paradigm. PNN introduces a neutral class, encompassing intricate items that are challenging to categorize directly as positive or negative samples. By training a model based on this triple-wise partial ranking, PNN offers a promising solution to learning complex user preferences. Through theoretical analysis, we connect PNN to one-way partial AUC (OPAUC) to validate its efficacy. Implementing the PNN paradigm is, however, technically challenging because: (1) it is difficult to classify unlabeled data into neutral or negative in the absence of supervised signals; (2) there does not exist any loss function that can handle set-level triple-wise ranking relationships. To address these challenges, we propose a semi-supervised learning method coupled with a user-aware attention model for knowledge acquisition and classification refinement. Additionally, a novel loss function with a two-step centroid ranking approach enables handling set-level rankings. Extensive experiments on four real-world datasets demonstrate that, when combined with PNN, a wide range of representative CF models can consistently and significantly boost their performance. Even with a simple matrix factorization, PNN can achieve comparable performance to sophisticated graph neutral networks. \u25b3 Less Submitted 24 December, 2024; originally announced December 2024. arXiv:2412.18168 [pdf, other] From Pairwise to Ranking: Climbing the Ladder to Ideal Collaborative Filtering with Pseudo-Ranking Authors: Yuhan Zhao, Rui Chen, Li Chen, Shuang Zhang, Qilong Han, Hongtao Song Abstract: Intuitively, an ideal collaborative filtering (CF) model should learn from users' full rankings over all items to make optimal top-K recommendations. Due to the absence of such full rankings in practice, most CF models rely on pairwise loss functions to approximate full rankings, resulting in an immense performance gap. In this paper, we provide a novel analysis using the multiple ordinal classifi\u2026 \u25bd More Intuitively, an ideal collaborative filtering (CF) model should learn from users' full rankings over all items to make optimal top-K recommendations. Due to the absence of such full rankings in practice, most CF models rely on pairwise loss functions to approximate full rankings, resulting in an immense performance gap. In this paper, we provide a novel analysis using the multiple ordinal classification concept to reveal the inevitable gap between a pairwise approximation and the ideal case. However, bridging the gap in practice encounters two formidable challenges: (1) none of the real-world datasets contains full ranking information; (2) there does not exist a loss function that is capable of consuming ranking information. To overcome these challenges, we propose a pseudo-ranking paradigm (PRP) that addresses the lack of ranking information by introducing pseudo-rankings supervised by an original noise injection mechanism. Additionally, we put forward a new ranking loss function designed to handle ranking information effectively. To ensure our method's robustness against potential inaccuracies in pseudo-rankings, we equip the ranking loss function with a gradient-based confidence mechanism to detect and mitigate abnormal gradients. Extensive experiments on four real-world datasets demonstrate that PRP significantly outperforms state-of-the-art methods. \u25b3 Less Submitted 24 December, 2024; originally announced December 2024. arXiv:2412.17743 [pdf, other] YuLan-Mini: An Open Data-efficient Language Model Authors: Yiwen Hu, Huatong Song, Jia Deng, Jiapeng Wang, Jie Chen, Kun Zhou, Yutao Zhu, Jinhao Jiang, Zican Dong, Wayne Xin Zhao, Ji-Rong Wen Abstract: Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-tier performance among models of similar parameter scale. Our pre-training approach focuses on enhanc\u2026 \u25bd More Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-tier performance among models of similar parameter scale. Our pre-training approach focuses on enhancing training efficacy through three key technical contributions: an elaborate data pipeline combines data cleaning with data schedule strategies, a robust optimization method to mitigate training instability, and an effective annealing approach that incorporates targeted data selection and long context training. Remarkably, YuLan-Mini, trained on 1.08T tokens, achieves performance comparable to industry-leading models that require significantly more data. To facilitate reproduction, we release the full details of the data composition for each training phase. Project details can be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini. \u25b3 Less Submitted 24 December, 2024; v1 submitted 23 December, 2024; originally announced December 2024. arXiv:2412.12559 [pdf, other] EXIT: Context-Aware Extractive Compression for Enhancing Retrieval-Augmented Generation Authors: Taeho Hwang, Sukmin Cho, Soyeong Jeong, Hoyun Song, SeungYoon Han, Jong C. Park Abstract: We introduce EXIT, an extractive context compression framework that enhances both the effectiveness and efficiency of retrieval-augmented generation (RAG) in question answering (QA). Current RAG systems often struggle when retrieval models fail to rank the most relevant documents, leading to the inclusion of more context at the expense of latency and accuracy. While abstractive compression methods\u2026 \u25bd More We introduce EXIT, an extractive context compression framework that enhances both the effectiveness and efficiency of retrieval-augmented generation (RAG) in question answering (QA). Current RAG systems often struggle when retrieval models fail to rank the most relevant documents, leading to the inclusion of more context at the expense of latency and accuracy. While abstractive compression methods can drastically reduce token counts, their token-by-token generation process significantly increases end-to-end latency. Conversely, existing extractive methods reduce latency but rely on independent, non-adaptive sentence selection, failing to fully utilize contextual information. EXIT addresses these limitations by classifying sentences from retrieved documents - while preserving their contextual dependencies - enabling parallelizable, context-aware extraction that adapts to query complexity and retrieval quality. Our evaluations on both single-hop and multi-hop QA tasks show that EXIT consistently surpasses existing compression methods and even uncompressed baselines in QA accuracy, while also delivering substantial reductions in inference time and token count. By improving both effectiveness and efficiency, EXIT provides a promising direction for developing scalable, high-quality QA solutions in RAG pipelines. Our code is available at https://github.com/ThisIsHwang/EXIT \u25b3 Less Submitted 18 December, 2024; v1 submitted 17 December, 2024; originally announced December 2024. Comments: Under Review arXiv:2412.10689 [pdf, other] Learning to Verify Summary Facts with Fine-Grained LLM Feedback Authors: Jihwan Oh, Jeonghwan Choi, Nicole Hee-Yeon Kim, Taewon Yun, Hwanjun Song Abstract: Training automatic summary fact verifiers often faces the challenge of a lack of human-labeled data. In this paper, we explore alternative way of leveraging Large Language Model (LLM) generated feedback to address the inherent limitation of using human-labeled data. We introduce FineSumFact, a large-scale dataset containing fine-grained factual feedback on summaries. We employ 10 distinct LLMs for\u2026 \u25bd More Training automatic summary fact verifiers often faces the challenge of a lack of human-labeled data. In this paper, we explore alternative way of leveraging Large Language Model (LLM) generated feedback to address the inherent limitation of using human-labeled data. We introduce FineSumFact, a large-scale dataset containing fine-grained factual feedback on summaries. We employ 10 distinct LLMs for diverse summary generation and Llama-3-70B-Instruct for feedback. We utilize this dataset to fine-tune the lightweight open-source model Llama-3-8B-Instruct, optimizing resource efficiency while maintaining high performance. Our experimental results reveal that the model trained on extensive LLM-generated datasets surpasses that trained on smaller human-annotated datasets when evaluated using human-generated test sets. Fine-tuning fact verification models with LLM feedback can be more effective and cost-efficient than using human feedback. The dataset is available at https://github.com/DISL-Lab/FineSumFact. \u25b3 Less Submitted 14 December, 2024; originally announced December 2024. Comments: Accepted at COLING 2025 arXiv:2412.09413 [pdf, other] Imitate, Explore, and Self-Improve: A Reproduction Report on Slow-thinking Reasoning Systems Authors: Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen Abstract: Recently, slow-thinking reasoning systems, such as o1, have demonstrated remarkable capabilities in solving complex reasoning tasks. These systems typically engage in an extended thinking process before responding to a query, allowing them to generate more thorough, accurate, and well-reasoned solutions. These systems are primarily developed and maintained by industry, with their core techniques n\u2026 \u25bd More Recently, slow-thinking reasoning systems, such as o1, have demonstrated remarkable capabilities in solving complex reasoning tasks. These systems typically engage in an extended thinking process before responding to a query, allowing them to generate more thorough, accurate, and well-reasoned solutions. These systems are primarily developed and maintained by industry, with their core techniques not publicly disclosed. In response, an increasing number of studies from the research community aim to explore the technical foundations underlying these powerful reasoning systems. Building on these prior efforts, this paper presents a reproduction report on implementing o1-like reasoning systems. We introduce an ``imitate, explore, and self-improve'' framework, denoted as \\textbf{STILL-2}, as our primary technical approach to train the reasoning model. In the initial phase, we use distilled long-form thought data to fine-tune the reasoning model, enabling it to invoke a slow-thinking mode. The model is then encouraged to explore challenging problems by generating multiple rollouts, which can result in increasingly more high-quality trajectories that lead to correct answers. Furthermore, the model undergoes self-improvement by iteratively refining its training dataset. To verify the effectiveness of this approach, we conduct extensive experiments on three challenging benchmarks. The experimental results demonstrate that our approach achieves competitive performance compared to industry-level reasoning systems on these benchmarks. \u25b3 Less Submitted 22 December, 2024; v1 submitted 12 December, 2024; originally announced December 2024. Comments: Technical Report on Slow Thinking with LLMs: Part II arXiv:2412.06807 [pdf, other] Demand Modeling for Advanced Air Mobility Authors: Kamal Acharya, Mehul Lad, Liang Sun, Houbing Song Abstract: In recent years, the rapid pace of urbanization has posed profound challenges globally, exacerbating environmental concerns and escalating traffic congestion in metropolitan areas. To mitigate these issues, Advanced Air Mobility (AAM) has emerged as a promising transportation alternative. However, the effective implementation of AAM requires robust demand modeling. This study delves into the deman\u2026 \u25bd More In recent years, the rapid pace of urbanization has posed profound challenges globally, exacerbating environmental concerns and escalating traffic congestion in metropolitan areas. To mitigate these issues, Advanced Air Mobility (AAM) has emerged as a promising transportation alternative. However, the effective implementation of AAM requires robust demand modeling. This study delves into the demand dynamics of AAM by analyzing employment based trip data across Tennessee's census tracts, employing statistical techniques and machine learning models to enhance accuracy in demand forecasting. Drawing on datasets from the Bureau of Transportation Statistics (BTS), the Internal Revenue Service (IRS), the Federal Aviation Administration (FAA), and additional sources, we perform cost, time, and risk assessments to compute the Generalized Cost of Trip (GCT). Our findings indicate that trips are more likely to be viable for AAM if air transportation accounts for over 70\\% of the GCT and the journey spans more than 250 miles. The study not only refines the understanding of AAM demand but also guides strategic planning and policy formulation for sustainable urban mobility solutions. The data and code can be accessed on GitHub.{https://github.com/lotussavy/IEEEBigData-2024.git } \u25b3 Less Submitted 25 November, 2024; originally announced December 2024. Comments: 9 pages, 8 figures, Accepted in IEEE BigData 2024 conference as workshop paper arXiv:2412.06507 [pdf, other] BATseg: Boundary-aware Multiclass Spinal Cord Tumor Segmentation on 3D MRI Scans Authors: Hongkang Song, Zihui Zhang, Yanpeng Zhou, Jie Hu, Zishuo Wang, Hou Him Chan, Chon Lok Lei, Chen Xu, Yu Xin, Bo Yang Abstract: Spinal cord tumors significantly contribute to neurological morbidity and mortality. Precise morphometric quantification, encompassing the size, location, and type of such tumors, holds promise for optimizing treatment planning strategies. Although recent methods have demonstrated excellent performance in medical image segmentation, they primarily focus on discerning shapes with relatively large m\u2026 \u25bd More Spinal cord tumors significantly contribute to neurological morbidity and mortality. Precise morphometric quantification, encompassing the size, location, and type of such tumors, holds promise for optimizing treatment planning strategies. Although recent methods have demonstrated excellent performance in medical image segmentation, they primarily focus on discerning shapes with relatively large morphology such as brain tumors, ignoring the challenging problem of identifying spinal cord tumors which tend to have tiny sizes, diverse locations, and shapes. To tackle this hard problem of multiclass spinal cord tumor segmentation, we propose a new method, called BATseg, to learn a tumor surface distance field by applying our new multiclass boundary-aware loss function. To verify the effectiveness of our approach, we also introduce the first and large-scale spinal cord tumor dataset. It comprises gadolinium-enhanced T1-weighted 3D MRI scans from 653 patients and contains the four most common spinal cord tumor types: astrocytomas, ependymomas, hemangioblastomas, and spinal meningiomas. Extensive experiments on our dataset and another public kidney tumor segmentation dataset show that our proposed method achieves superior performance for multiclass tumor segmentation. \u25b3 Less Submitted 9 December, 2024; originally announced December 2024. Comments: ECCV 2024 Workshop on BioImage Computing. Code and data are available at: https://github.com/vLAR-group/BATseg arXiv:2412.04484 [pdf, other] Epinet for Content Cold Start Authors: Hong Jun Jeon, Songbin Liu, Yuantong Li, Jie Lyu, Hunter Song, Ji Liu, Peng Wu, Zheqing Zhu Abstract: The exploding popularity of online content and its user base poses an evermore challenging matching problem for modern recommendation systems. Unlike other frontiers of machine learning such as natural language, recommendation systems are responsible for collecting their own data. Simply exploiting current knowledge can lead to pernicious feedback loops but naive exploration can detract from user\u2026 \u25bd More The exploding popularity of online content and its user base poses an evermore challenging matching problem for modern recommendation systems. Unlike other frontiers of machine learning such as natural language, recommendation systems are responsible for collecting their own data. Simply exploiting current knowledge can lead to pernicious feedback loops but naive exploration can detract from user experience and lead to reduced engagement. This exploration-exploitation trade-off is exemplified in the classic multi-armed bandit problem for which algorithms such as upper confidence bounds (UCB) and Thompson sampling (TS) demonstrate effective performance. However, there have been many challenges to scaling these approaches to settings which do not exhibit a conjugate prior structure. Recent scalable approaches to uncertainty quantification via epinets have enabled efficient approximations of Thompson sampling even when the learning model is a complex neural network. In this paper, we demonstrate the first application of epinets to an online recommendation system. Our experiments demonstrate improvements in both user traffic and engagement efficiency on the Facebook Reels online video platform. \u25b3 Less Submitted 20 November, 2024; originally announced December 2024."
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Kim,+S",
    "title": "Showing 1\u201350 of 2,415 results for author: Kim, S",
    "author": "Authors:\nJisu Nam, \n      \n      Soowon Son, \n      \n      Zhan Xu, \n      \n      Jing Shi, \n      \n      Difan Liu, \n      \n      Feng Liu, \n      \n      Aashish Misraa, \n      \n      Seungryong Kim, \n      \n      Yang Zhou",
    "body": "arXiv:2503.15406 [pdf, other] Visual Persona: Foundation Model for Full-Body Human Customization Authors: Jisu Nam, Soowon Son, Zhan Xu, Jing Shi, Difan Liu, Feng Liu, Aashish Misraa, Seungryong Kim, Yang Zhou Abstract: We introduce Visual Persona, a foundation model for text-to-image full-body human customization that, given a single in-the-wild human image, generates diverse images of the individual guided by text descriptions. Unlike prior methods that focus solely on preserving facial identity, our approach captures detailed full-body appearance, aligning with text descriptions for body structure and scene va\u2026 \u25bd More We introduce Visual Persona, a foundation model for text-to-image full-body human customization that, given a single in-the-wild human image, generates diverse images of the individual guided by text descriptions. Unlike prior methods that focus solely on preserving facial identity, our approach captures detailed full-body appearance, aligning with text descriptions for body structure and scene variations. Training this model requires large-scale paired human data, consisting of multiple images per individual with consistent full-body identities, which is notoriously difficult to obtain. To address this, we propose a data curation pipeline leveraging vision-language models to evaluate full-body appearance consistency, resulting in Visual Persona-500K, a dataset of 580k paired human images across 100k unique identities. For precise appearance transfer, we introduce a transformer encoder-decoder architecture adapted to a pre-trained text-to-image diffusion model, which augments the input image into distinct body regions, encodes these regions as local appearance features, and projects them into dense identity embeddings independently to condition the diffusion model for synthesizing customized images. Visual Persona consistently surpasses existing approaches, generating high-quality, customized images from in-the-wild inputs. Extensive ablation studies validate design choices, and we demonstrate the versatility of Visual Persona across various downstream tasks. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. Comments: CVPR 2025, Project page is available at https://cvlab-kaist.github.io/Visual-Persona arXiv:2503.15191 [pdf, other] Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems Authors: Sejong Kim, Hyunseo Song, Hyunwoo Seo, Hyunjun Kim Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising framework to mitigate hallucinations in Large Language Models (LLMs), yet its overall performance is dependent on the underlying retrieval system. In the finance domain, documents such as 10-K reports pose distinct challenges due to domain-specific vocabulary and multi-hierarchical tabular data. In this work, we introduce an efficient\u2026 \u25bd More Retrieval-Augmented Generation (RAG) has emerged as a promising framework to mitigate hallucinations in Large Language Models (LLMs), yet its overall performance is dependent on the underlying retrieval system. In the finance domain, documents such as 10-K reports pose distinct challenges due to domain-specific vocabulary and multi-hierarchical tabular data. In this work, we introduce an efficient, end-to-end RAG pipeline that enhances retrieval for financial documents through a three-phase approach: pre-retrieval, retrieval, and post-retrieval. In the pre-retrieval phase, various query and corpus preprocessing techniques are employed to enrich input data. During the retrieval phase, we fine-tuned state-of-the-art (SOTA) embedding models with domain-specific knowledge and implemented a hybrid retrieval strategy that combines dense and sparse representations. Finally, the post-retrieval phase leverages Direct Preference Optimization (DPO) training and document selection methods to further refine the results. Evaluations on seven financial question answering datasets-FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA, and MultiHiertt-demonstrate substantial improvements in retrieval performance, leading to more accurate and contextually appropriate generation. These findings highlight the critical role of tailored retrieval techniques in advancing the effectiveness of RAG systems for financial applications. A fully replicable pipeline is available on GitHub: https://github.com/seohyunwoo-0407/GAR. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. Comments: 15 pages, 3 figures, 11 tables. Accepted at ICLR 2025 Workshop on Advances in Financial AI. Code available at https://github.com/seohyunwoo-0407/GAR arXiv:2503.15185 [pdf, other] 3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware View Transformation Authors: Gyeongrok Oh, Sungjune Kim, Heeju Ko, Hyung-gun Chi, Jinkyu Kim, Dongwook Lee, Daehyun Ji, Sungjoon Choi, Sujin Jang, Sangpil Kim Abstract: The resolution of voxel queries significantly influences the quality of view transformation in camera-based 3D occupancy prediction. However, computational constraints and the practical necessity for real-time deployment require smaller query resolutions, which inevitably leads to an information loss. Therefore, it is essential to encode and preserve rich visual details within limited query sizes\u2026 \u25bd More The resolution of voxel queries significantly influences the quality of view transformation in camera-based 3D occupancy prediction. However, computational constraints and the practical necessity for real-time deployment require smaller query resolutions, which inevitably leads to an information loss. Therefore, it is essential to encode and preserve rich visual details within limited query sizes while ensuring a comprehensive representation of 3D occupancy. To this end, we introduce ProtoOcc, a novel occupancy network that leverages prototypes of clustered image segments in view transformation to enhance low-resolution context. In particular, the mapping of 2D prototypes onto 3D voxel queries encodes high-level visual geometries and complements the loss of spatial information from reduced query resolutions. Additionally, we design a multi-perspective decoding strategy to efficiently disentangle the densely compressed visual cues into a high-dimensional 3D occupancy scene. Experimental results on both Occ3D and SemanticKITTI benchmarks demonstrate the effectiveness of the proposed method, showing clear improvements over the baselines. More importantly, ProtoOcc achieves competitive performance against the baselines even with 75\\% reduced voxel resolution. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. Comments: Accepted to CVPR2025 arXiv:2503.14427 [pdf, other] VisEscape: A Benchmark for Evaluating Exploration-driven Decision-making in Virtual Escape Rooms Authors: Seungwon Lim, Sungwoong Kim, Jihwan Yu, Sungjae Lee, Jiwan Chung, Youngjae Yu Abstract: Escape rooms present a unique cognitive challenge that demands exploration-driven planning: players should actively search their environment, continuously update their knowledge based on new discoveries, and connect disparate clues to determine which elements are relevant to their objectives. Motivated by this, we introduce VisEscape, a benchmark of 20 virtual escape rooms specifically designed to\u2026 \u25bd More Escape rooms present a unique cognitive challenge that demands exploration-driven planning: players should actively search their environment, continuously update their knowledge based on new discoveries, and connect disparate clues to determine which elements are relevant to their objectives. Motivated by this, we introduce VisEscape, a benchmark of 20 virtual escape rooms specifically designed to evaluate AI models under these challenging conditions, where success depends not only on solving isolated puzzles but also on iteratively constructing and refining spatial-temporal knowledge of a dynamically changing environment. On VisEscape, we observed that even state-of-the-art multimodal models generally fail to escape the rooms, showing considerable variation in their levels of progress and trajectories. To address this issue, we propose VisEscaper, which effectively integrates Memory, Feedback, and ReAct modules, demonstrating significant improvements by performing 3.7 times more effectively and 5.0 times more efficiently on average. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. arXiv:2503.14035 [pdf, other] A Revisit to the Decoder for Camouflaged Object Detection Authors: Seung Woo Ko, Joopyo Hong, Suyoung Kim, Seungjai Bang, Sungzoon Cho, Nojun Kwak, Hyung-Sin Kim, Joonseok Lee Abstract: Camouflaged object detection (COD) aims to generate a fine-grained segmentation map of camouflaged objects hidden in their background. Due to the hidden nature of camouflaged objects, it is essential for the decoder to be tailored to effectively extract proper features of camouflaged objects and extra-carefully generate their complex boundaries. In this paper, we propose a novel architecture that\u2026 \u25bd More Camouflaged object detection (COD) aims to generate a fine-grained segmentation map of camouflaged objects hidden in their background. Due to the hidden nature of camouflaged objects, it is essential for the decoder to be tailored to effectively extract proper features of camouflaged objects and extra-carefully generate their complex boundaries. In this paper, we propose a novel architecture that augments the prevalent decoding strategy in COD with Enrich Decoder and Retouch Decoder, which help to generate a fine-grained segmentation map. Specifically, the Enrich Decoder amplifies the channels of features that are important for COD using channel-wise attention. Retouch Decoder further refines the segmentation maps by spatially attending to important pixels, such as the boundary regions. With extensive experiments, we demonstrate that ENTO shows superior performance using various encoders, with the two novel components playing their unique roles that are mutually complementary. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. Comments: Published in BMVC 2024, 13 pages, 7 figures (Appendix: 5 pages, 2 figures) Journal ref: British Machine Vision Conference (BMVC) 2024 arXiv:2503.13369 [pdf, other] Sightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions Authors: Wan Ju Kang, Eunki Kim, Na Min An, Sangryul Kim, Haemin Choi, Ki Hoon Kwak, James Thorne Abstract: Often, the needs and visual abilities differ between the annotator group and the end user group. Generating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain. Sighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costly, bias-prone, and somewhat lacking by BLV standards. In this study\u2026 \u25bd More Often, the needs and visual abilities differ between the annotator group and the end user group. Generating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain. Sighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costly, bias-prone, and somewhat lacking by BLV standards. In this study, we ask sighted individuals to assess -- rather than produce -- diagram descriptions generated by vision-language models (VLM) that have been guided with latent supervision via a multi-pass inference. The sighted assessments prove effective and useful to professional educators who are themselves BLV and teach visually impaired learners. We release Sightation, a collection of diagram description datasets spanning 5k diagrams and 137k samples for completion, preference, retrieval, question answering, and reasoning training purposes and demonstrate their fine-tuning potential in various downstream tasks. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. Comments: 37 pages, 10 figures, 21 tables arXiv:2503.13169 [pdf] Collaborative AI Enhances Image Understanding in Materials Science Authors: Ruoyan Avery Yin, Zhichu Ren, Zongyou Yin, Zhen Zhang, So Yeon Kim, Chia-Wei Hsu, Ju Li Abstract: The Copilot for Real-world Experimental Scientist (CRESt) system empowers researchers to control autonomous laboratories through conversational AI, providing a seamless interface for managing complex experimental workflows. We have enhanced CRESt by integrating a multi-agent collaboration mechanism that utilizes the complementary strengths of the ChatGPT and Gemini models for precise image analysi\u2026 \u25bd More The Copilot for Real-world Experimental Scientist (CRESt) system empowers researchers to control autonomous laboratories through conversational AI, providing a seamless interface for managing complex experimental workflows. We have enhanced CRESt by integrating a multi-agent collaboration mechanism that utilizes the complementary strengths of the ChatGPT and Gemini models for precise image analysis in materials science. This innovative approach significantly improves the accuracy of experimental outcomes by fostering structured debates between the AI models, which enhances decision-making processes in materials phase analysis. Additionally, to evaluate the generalizability of this approach, we tested it on a quantitative task of counting particles. Here, the collaboration between the AI models also led to improved results, demonstrating the versatility and robustness of this method. By harnessing this dual-AI framework, this approach stands as a pioneering method for enhancing experimental accuracy and efficiency in materials research, with applications extending beyond CRESt to broader scientific experimentation and analysis. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. Comments: 10 pages, 4 figures ACM Class: I.2.1; I.2.10 arXiv:2503.12836 [pdf, other] CompMarkGS: Robust Watermarking for Compression 3D Gaussian Splatting Authors: Sumin In, Youngdong Jang, Utae Jeong, MinHyuk Jang, Hyeongcheol Park, Eunbyung Park, Sangpil Kim Abstract: 3D Gaussian Splatting (3DGS) enables rapid differentiable rendering for 3D reconstruction and novel view synthesis, leading to its widespread commercial use. Consequently, copyright protection via watermarking has become critical. However, because 3DGS relies on millions of Gaussians, which require gigabytes of storage, efficient transfer and storage require compression. Existing 3DGS watermarking\u2026 \u25bd More 3D Gaussian Splatting (3DGS) enables rapid differentiable rendering for 3D reconstruction and novel view synthesis, leading to its widespread commercial use. Consequently, copyright protection via watermarking has become critical. However, because 3DGS relies on millions of Gaussians, which require gigabytes of storage, efficient transfer and storage require compression. Existing 3DGS watermarking methods are vulnerable to quantization-based compression, often resulting in the loss of the embedded watermark. To address this challenge, we propose a novel watermarking method that ensures watermark robustness after model compression while maintaining high rendering quality. In detail, we incorporate a quantization distortion layer that simulates compression during training, preserving the watermark under quantization-based compression. Also, we propose a learnable watermark embedding feature that embeds the watermark into the anchor feature, ensuring structural consistency and seamless integration into the 3D scene. Furthermore, we present a frequency-aware anchor growing mechanism to enhance image quality in high-frequency regions by effectively identifying Guassians within these regions. Experimental results confirm that our method preserves the watermark and maintains superior image quality under high compression, validating it as a promising approach for a secure 3DGS model. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. Comments: 23 pages, 17 figures arXiv:2503.12834 [pdf, other] PASTA: Part-Aware Sketch-to-3D Shape Generation with Text-Aligned Prior Authors: Seunggwan Lee, Hwanhee Jung, Byoungsoo Koh, Qixing Huang, Sangho Yoon, Sangpil Kim Abstract: A fundamental challenge in conditional 3D shape generation is to minimize the information loss and maximize the intention of user input. Existing approaches have predominantly focused on two types of isolated conditional signals, i.e., user sketches and text descriptions, each of which does not offer flexible control of the generated shape. In this paper, we introduce PASTA, the flexible approach\u2026 \u25bd More A fundamental challenge in conditional 3D shape generation is to minimize the information loss and maximize the intention of user input. Existing approaches have predominantly focused on two types of isolated conditional signals, i.e., user sketches and text descriptions, each of which does not offer flexible control of the generated shape. In this paper, we introduce PASTA, the flexible approach that seamlessly integrates a user sketch and a text description for 3D shape generation. The key idea is to use text embeddings from a vision-language model to enrich the semantic representation of sketches. Specifically, these text-derived priors specify the part components of the object, compensating for missing visual cues from ambiguous sketches. In addition, we introduce ISG-Net which employs two types of graph convolutional networks: IndivGCN, which processes fine-grained details, and PartGCN, which aggregates these details into parts and refines the structure of objects. Extensive experiments demonstrate that PASTA outperforms existing methods in part-level editing and achieves state-of-the-art results in sketch-to-3D shape generation. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. Comments: 19 pages, 18 figures arXiv:2503.12806 [pdf, other] AV-Surf: Surface-Enhanced Geometry-Aware Novel-View Acoustic Synthesis Authors: Hadam Baek, Hannie Shin, Jiyoung Seo, Chanwoo Kim, Saerom Kim, Hyeongbok Kim, Sangpil Kim Abstract: Accurately modeling sound propagation with complex real-world environments is essential for Novel View Acoustic Synthesis (NVAS). While previous studies have leveraged visual perception to estimate spatial acoustics, the combined use of surface normal and structural details from 3D representations in acoustic modeling has been underexplored. Given their direct impact on sound wave reflections and\u2026 \u25bd More Accurately modeling sound propagation with complex real-world environments is essential for Novel View Acoustic Synthesis (NVAS). While previous studies have leveraged visual perception to estimate spatial acoustics, the combined use of surface normal and structural details from 3D representations in acoustic modeling has been underexplored. Given their direct impact on sound wave reflections and propagation, surface normals should be jointly modeled with structural details to achieve accurate spatial acoustics. In this paper, we propose a surface-enhanced geometry-aware approach for NVAS to improve spatial acoustic modeling. To achieve this, we exploit geometric priors, such as image, depth map, surface normals, and point clouds obtained using a 3D Gaussian Splatting (3DGS) based framework. We introduce a dual cross-attention-based transformer integrating geometrical constraints into frequency query to understand the surroundings of the emitter. Additionally, we design a ConvNeXt-based spectral features processing network called Spectral Refinement Network (SRN) to synthesize realistic binaural audio. Experimental results on the RWAVS and SoundSpace datasets highlight the necessity of our approach, as it surpasses existing methods in novel view acoustic synthesis. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. arXiv:2503.12738 [pdf, other] Enhancing Circuit Trainability with Selective Gate Activation Strategy Authors: Jeihee Cho, Junyong Lee, Daniel Justice, Shiho Kim Abstract: Hybrid quantum-classical computing relies heavily on Variational Quantum Algorithms (VQAs) to tackle challenges in diverse fields like quantum chemistry and machine learning. However, VQAs face a critical limitation: the balance between circuit trainability and expressibility. Trainability, the ease of optimizing circuit parameters for problem-solving, is often hampered by the Barren Plateau, wher\u2026 \u25bd More Hybrid quantum-classical computing relies heavily on Variational Quantum Algorithms (VQAs) to tackle challenges in diverse fields like quantum chemistry and machine learning. However, VQAs face a critical limitation: the balance between circuit trainability and expressibility. Trainability, the ease of optimizing circuit parameters for problem-solving, is often hampered by the Barren Plateau, where gradients vanish and hinder optimization. On the other hand, increasing expressibility, the ability to represent a wide range of quantum states, often necessitates deeper circuits with more parameters, which in turn exacerbates trainability issues. In this work, we investigate selective gate activation strategies as a potential solution to these challenges within the context of Variational Quantum Eigensolvers (VQEs). We evaluate three different approaches: activating gates randomly without considering their type or parameter magnitude, activating gates randomly but limited to a single gate type, and activating gates based on the magnitude of their parameter values. Experiment results reveal that the Magnitude-based strategy surpasses other methods, achieving improved convergence. \u25b3 Less Submitted 16 March, 2025; originally announced March 2025. Comments: 5 pages, 4 figures arXiv:2503.12524 [pdf, other] EXAONE Deep: Reasoning Enhanced Language Models Authors: LG AI Research, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Yemuk Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Kijeong Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee , et al. (7 additional authors not shown) Abstract: We present EXAONE Deep series, which exhibits superior capabilities in various reasoning tasks, including math and coding benchmarks. We train our models mainly on the reasoning-specialized dataset that incorporates long streams of thought processes. Evaluation results show that our smaller models, EXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while the largest model, EXAO\u2026 \u25bd More We present EXAONE Deep series, which exhibits superior capabilities in various reasoning tasks, including math and coding benchmarks. We train our models mainly on the reasoning-specialized dataset that incorporates long streams of thought processes. Evaluation results show that our smaller models, EXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while the largest model, EXAONE Deep 32B, demonstrates competitive performance against leading open-weight models. All EXAONE Deep models are openly available for research purposes and can be downloaded from https://huggingface.co/LGAI-EXAONE \u25b3 Less Submitted 19 March, 2025; v1 submitted 16 March, 2025; originally announced March 2025. Comments: arXiv admin note: substantial text overlap with arXiv:2412.04862, arXiv:2408.03541 arXiv:2503.12358 [pdf, other] IPCGRL: Language-Instructed Reinforcement Learning for Procedural Level Generation Authors: In-Chang Baek, Sung-Hyun Kim, Seo-Young Lee, Dong-Hyeun Kim, Kyung-Joong Kim Abstract: Recent research has highlighted the significance of natural language in enhancing the controllability of generative models. While various efforts have been made to leverage natural language for content generation, research on deep reinforcement learning (DRL) agents utilizing text-based instructions for procedural content generation remains limited. In this paper, we propose IPCGRL, an instruction\u2026 \u25bd More Recent research has highlighted the significance of natural language in enhancing the controllability of generative models. While various efforts have been made to leverage natural language for content generation, research on deep reinforcement learning (DRL) agents utilizing text-based instructions for procedural content generation remains limited. In this paper, we propose IPCGRL, an instruction-based procedural content generation method via reinforcement learning, which incorporates a sentence embedding model. IPCGRL fine-tunes task-specific embedding representations to effectively compress game-level conditions. We evaluate IPCGRL in a two-dimensional level generation task and compare its performance with a general-purpose embedding method. The results indicate that IPCGRL achieves up to a 21.4% improvement in controllability and a 17.2% improvement in generalizability for unseen instructions. Furthermore, the proposed method extends the modality of conditional input, enabling a more flexible and expressive interaction framework for procedural content generation. \u25b3 Less Submitted 18 March, 2025; v1 submitted 16 March, 2025; originally announced March 2025. Comments: 9 pages, 9 figures, 3 tables arXiv:2503.11777 [pdf, other] Enhancing Resiliency of Sketch-based Security via LSB Sharing-based Dynamic Late Merging Authors: Seungsam Yang, Seyed Mohammad Mehdi Mirnajafizadeh, Sian Kim, Rhongho Jang, DaeHun Nyang Abstract: With the exponentially growing Internet traffic, sketch data structure with a probabilistic algorithm has been expected to be an alternative solution for non-compromised (non-selective) security monitoring. While facilitating counting within a confined memory space, the sketch's memory efficiency and accuracy were further pushed to their limit through finer-grained and dynamic control of constrain\u2026 \u25bd More With the exponentially growing Internet traffic, sketch data structure with a probabilistic algorithm has been expected to be an alternative solution for non-compromised (non-selective) security monitoring. While facilitating counting within a confined memory space, the sketch's memory efficiency and accuracy were further pushed to their limit through finer-grained and dynamic control of constrained memory space to adapt to the data stream's inherent skewness (i.e., Zipf distribution), namely small counters with extensions. In this paper, we unveil a vulnerable factor of the small counter design by introducing a new sketch-oriented attack, which threatens a stream of state-of-the-art sketches and their security applications. With the root cause analyses, we propose Siamese Counter with enhanced adversarial resiliency and verified feasibility with extensive experimental and theoretical analyses. Under a sketch pollution attack, Siamese Counter delivers 47% accurate results than a state-of-the-art scheme, and demonstrates up to 82% more accurate estimation under normal measurement scenarios. \u25b3 Less Submitted 14 March, 2025; originally announced March 2025. arXiv:2503.11737 [pdf, other] Multi-View Node Pruning for Accurate Graph Representation Authors: Jiseong Park, Hanjin Kim, Seojin Kim, Jueun Choi Abstract: Graph pooling, which compresses a whole graph into a smaller coarsened graph, is an essential component of graph representation learning. To efficiently compress a given graph, graph pooling methods often drop their nodes with attention-based scoring with the task loss. However, this often results in simply removing nodes with lower degrees without consideration of their feature-level relevance to\u2026 \u25bd More Graph pooling, which compresses a whole graph into a smaller coarsened graph, is an essential component of graph representation learning. To efficiently compress a given graph, graph pooling methods often drop their nodes with attention-based scoring with the task loss. However, this often results in simply removing nodes with lower degrees without consideration of their feature-level relevance to the given task. To fix this problem, we propose a Multi-View Pruning(MVP), a graph pruning method based on a multi-view framework and reconstruction loss. Given a graph, MVP first constructs multiple graphs for different views either by utilizing the predefined modalities or by randomly partitioning the input features, to consider the importance of each node in diverse perspectives. Then, it learns the score for each node by considering both the reconstruction and the task loss. MVP can be incorporated with any hierarchical pooling framework to score the nodes. We validate MVP on multiple benchmark datasets by coupling it with two graph pooling methods, and show that it significantly improves the performance of the base graph pooling method, outperforming all baselines. Further analysis shows that both the encoding of multiple views and the consideration of reconstruction loss are the key to the success of MVP, and that it indeed identifies nodes that are less important according to domain knowledge. \u25b3 Less Submitted 18 March, 2025; v1 submitted 14 March, 2025; originally announced March 2025. Comments: Jiseong Park and Hanjin Kim are co-first author for this work arXiv:2503.11660 [pdf, other] A 28 nm AI microcontroller with tightly coupled zero-standby power weight memory featuring standard logic compatible 4 Mb 4-bits/cell embedded flash technology Authors: Daewung Kim, Seong Hwan Jeon, Young Hee Jeon, Kyung-Bae Kwon, Jigon Kim, Yeounghun Choi, Hyunseung Cha, Kitae Kwon, Daesik Park, Jongseuk Lee, Sihwan Kim, Seung-Hwan Song Abstract: This study introduces a novel AI microcontroller optimized for cost-effective, battery-powered edge AI applications. Unlike traditional single bit/cell memory configurations, the proposed microcontroller integrates zero-standby power weight memory featuring standard logic compatible 4-bits/cell embedded flash technology tightly coupled to a Near-Memory Computing Unit. This architecture enables eff\u2026 \u25bd More This study introduces a novel AI microcontroller optimized for cost-effective, battery-powered edge AI applications. Unlike traditional single bit/cell memory configurations, the proposed microcontroller integrates zero-standby power weight memory featuring standard logic compatible 4-bits/cell embedded flash technology tightly coupled to a Near-Memory Computing Unit. This architecture enables efficient and low-power AI acceleration. Advanced state mapping and an overstress-free word line (WL) driver circuit extend verify levels, ensuring robust 16 state cell margin. A ping-pong buffer reduces internal data movement while supporting simultaneous multi-bit processing. The fabricated microcontroller demonstrated high reliability, maintaining accuracy after 160 hours of unpowered baking at 125$^\\circ$C. \u25b3 Less Submitted 12 February, 2025; originally announced March 2025. Comments: 6 pages, 8 figures, Accepted as a full paper by the 2025 EDGE AI FOUNDATION Austin arXiv:2503.11565 [pdf, other] Disentangled Object-Centric Image Representation for Robotic Manipulation Authors: David Emukpere, Romain Deffayet, Bingbing Wu, Romain Br\u00e9gier, Michael Niemaz, Jean-Luc Meunier, Denys Proux, Jean-Michel Renders, Seungsu Kim Abstract: Learning robotic manipulation skills from vision is a promising approach for developing robotics applications that can generalize broadly to real-world scenarios. As such, many approaches to enable this vision have been explored with fruitful results. Particularly, object-centric representation methods have been shown to provide better inductive biases for skill learning, leading to improved perfo\u2026 \u25bd More Learning robotic manipulation skills from vision is a promising approach for developing robotics applications that can generalize broadly to real-world scenarios. As such, many approaches to enable this vision have been explored with fruitful results. Particularly, object-centric representation methods have been shown to provide better inductive biases for skill learning, leading to improved performance and generalization. Nonetheless, we show that object-centric methods can struggle to learn simple manipulation skills in multi-object environments. Thus, we propose DOCIR, an object-centric framework that introduces a disentangled representation for objects of interest, obstacles, and robot embodiment. We show that this approach leads to state-of-the-art performance for learning pick and place skills from visual inputs in multi-object environments and generalizes at test time to changing objects of interest and distractors in the scene. Furthermore, we show its efficacy both in simulation and zero-shot transfer to the real world. \u25b3 Less Submitted 14 March, 2025; originally announced March 2025. arXiv:2503.11026 [pdf, other] MAVFlow: Preserving Paralinguistic Elements with Conditional Flow Matching for Zero-Shot AV2AV Multilingual Translation Authors: Sungwoo Cho, Jeongsoo Choi, Sungnyun Kim, Se-Young Yun Abstract: Despite recent advances in text-to-speech (TTS) models, audio-visual to audio-visual (AV2AV) translation still faces a critical challenge: maintaining speaker consistency between the original and translated vocal and facial features. To address this issue, we propose a conditional flow matching (CFM) zero-shot audio-visual renderer that utilizes strong dual guidance from both audio and visual moda\u2026 \u25bd More Despite recent advances in text-to-speech (TTS) models, audio-visual to audio-visual (AV2AV) translation still faces a critical challenge: maintaining speaker consistency between the original and translated vocal and facial features. To address this issue, we propose a conditional flow matching (CFM) zero-shot audio-visual renderer that utilizes strong dual guidance from both audio and visual modalities. By leveraging multi-modal guidance with CFM, our model robustly preserves speaker-specific characteristics and significantly enhances zero-shot AV2AV translation abilities. For the audio modality, we enhance the CFM process by integrating robust speaker embeddings with x-vectors, which serve to bolster speaker consistency. Additionally, we convey emotional nuances to the face rendering module. The guidance provided by both audio and visual cues remains independent of semantic or linguistic content, allowing our renderer to effectively handle zero-shot translation tasks for monolingual speakers in different languages. We empirically demonstrate that the inclusion of high-quality mel-spectrograms conditioned on facial information not only enhances the quality of the synthesized speech but also positively influences facial generation, leading to overall performance improvements. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. Comments: Preliminary work arXiv:2503.10349 [pdf, other] Autonomous Robotic Radio Source Localization via a Novel Gaussian Mixture Filtering Approach Authors: Sukkeun Kim, Sangwoo Moon, Ivan Petrunin, Hyo-Sang Shin, Shehryar Khattak Abstract: This study proposes a new Gaussian Mixture Filter (GMF) to improve the estimation performance for the autonomous robotic radio signal source search and localization problem in unknown environments. The proposed filter is first tested with a benchmark numerical problem to validate the performance with other state-of-practice approaches such as Particle Gaussian Mixture (PGM) filters and Particle Fi\u2026 \u25bd More This study proposes a new Gaussian Mixture Filter (GMF) to improve the estimation performance for the autonomous robotic radio signal source search and localization problem in unknown environments. The proposed filter is first tested with a benchmark numerical problem to validate the performance with other state-of-practice approaches such as Particle Gaussian Mixture (PGM) filters and Particle Filter (PF). Then the proposed approach is tested and compared against PF and PGM filters in real-world robotic field experiments to validate its impact for real-world robotic applications. The considered real-world scenarios have partial observability with the range-only measurement and uncertainty with the measurement model. The results show that the proposed filter can handle this partial observability effectively whilst showing improved performance compared to PF, reducing the computation requirements while demonstrating improved robustness over compared techniques. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. arXiv:2503.10310 [pdf, other] Capturing Semantic Flow of ML-based Systems Authors: Shin Yoo, Robert Feldt, Somin Kim, Naryeong Kim Abstract: ML-based systems are software systems that incorporates machine learning components such as Deep Neural Networks (DNNs) or Large Language Models (LLMs). While such systems enable advanced features such as high performance computer vision, natural language processing, and code generation, their internal behaviour remain largely opaque to traditional dynamic analysis such as testing: existing analys\u2026 \u25bd More ML-based systems are software systems that incorporates machine learning components such as Deep Neural Networks (DNNs) or Large Language Models (LLMs). While such systems enable advanced features such as high performance computer vision, natural language processing, and code generation, their internal behaviour remain largely opaque to traditional dynamic analysis such as testing: existing analysis typically concern only what is observable from the outside, such as input similarity or class label changes. We propose semantic flow, a concept designed to capture the internal behaviour of ML-based system and to provide a platform for traditional dynamic analysis techniques to be adapted to. Semantic flow combines the idea of control flow with internal states taken from executions of ML-based systems, such as activation values of a specific layer in a DNN, or embeddings of LLM responses at a specific inference step of LLM agents. The resulting representation, summarised as semantic flow graphs, can capture internal decisions that are not explicitly represented in the traditional control flow of ML-based systems. We propose the idea of semantic flow, introduce two examples using a DNN and an LLM agent, and finally sketch its properties and how it can be used to adapt existing dynamic analysis techniques for use in ML-based software systems. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. arXiv:2503.10003 [pdf, other] A New Benchmark for Few-Shot Class-Incremental Learning: Redefining the Upper Bound Authors: Shiwon Kim, Dongjun Hwang, Sungwon Woo, Rita Singh Abstract: Class-incremental learning (CIL) aims to continuously adapt to emerging classes while retaining knowledge of previously learned ones. Few-shot class-incremental learning (FSCIL) presents an even greater challenge which requires the model to learn incremental classes with only a limited number of samples. In conventional CIL, joint training is widely considered the upper bound, serving as both a be\u2026 \u25bd More Class-incremental learning (CIL) aims to continuously adapt to emerging classes while retaining knowledge of previously learned ones. Few-shot class-incremental learning (FSCIL) presents an even greater challenge which requires the model to learn incremental classes with only a limited number of samples. In conventional CIL, joint training is widely considered the upper bound, serving as both a benchmark and a methodological guide. However, we find that joint training fails to be a meaningful upper bound in FSCIL due to the inherent difficulty of inter-task class separation (ICS) caused by severe class imbalance. In this work, we introduce a new joint training benchmark tailored for FSCIL by integrating imbalance-aware techniques, effectively bridging the performance gap between base and incremental classes. Furthermore, we point out inconsistencies in the experimental setup and evaluation of existing FSCIL methods. To ensure fair comparisons between different FSCIL approaches and joint training, we standardize training conditions and propose a unified evaluation protocol that simultaneously considers the validation set and computational complexity. By establishing a reliable upper bound and a standardized evaluation framework for FSCIL, our work provides a clear benchmark and a practical foundation for future research. \u25b3 Less Submitted 12 March, 2025; originally announced March 2025. arXiv:2503.09572 [pdf, other] Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks Authors: Lutfi Eren Erdogan, Nicholas Lee, Sehoon Kim, Suhong Moon, Hiroki Furuta, Gopala Anumanchipalli, Kurt Keutzer, Amir Gholami Abstract: Large language models (LLMs) have shown remarkable advancements in enabling language agents to tackle simple tasks. However, applying them for complex, multi-step, long-horizon tasks remains a challenge. Recent work have found success by separating high-level planning from low-level execution, which enables the model to effectively balance high-level planning objectives and low-level execution det\u2026 \u25bd More Large language models (LLMs) have shown remarkable advancements in enabling language agents to tackle simple tasks. However, applying them for complex, multi-step, long-horizon tasks remains a challenge. Recent work have found success by separating high-level planning from low-level execution, which enables the model to effectively balance high-level planning objectives and low-level execution details. However, generating accurate plans remains difficult since LLMs are not inherently trained for this task. To address this, we propose Plan-and-Act, a novel framework that incorporates explicit planning into LLM-based agents and introduces a scalable method to enhance plan generation through a novel synthetic data generation method. Plan-and-Act consists of a Planner model which generates structured, high-level plans to achieve user goals, and an Executor model that translates these plans into environment-specific actions. To train the Planner effectively, we introduce a synthetic data generation method that annotates ground-truth trajectories with feasible plans, augmented with diverse and extensive examples to enhance generalization. We evaluate Plan-and-Act using web navigation as a representative long-horizon planning environment, demonstrating a state-of the-art 54% success rate on the WebArena-Lite benchmark. \u25b3 Less Submitted 12 March, 2025; originally announced March 2025. arXiv:2503.09477 [pdf, other] Neural reservoir control of a soft bio-hybrid arm Authors: Noel Naughton, Arman Tekinalp, Keshav Shivam, Seung Hung Kim, Volodymyr Kindratenko, Mattia Gazzola Abstract: A long-standing engineering problem, the control of soft robots is difficult because of their highly non-linear, heterogeneous, anisotropic, and distributed nature. Here, bridging engineering and biology, a neural reservoir is employed for the dynamic control of a bio-hybrid model arm made of multiple muscle-tendon groups enveloping an elastic spine. We show how the use of reservoirs facilitates s\u2026 \u25bd More A long-standing engineering problem, the control of soft robots is difficult because of their highly non-linear, heterogeneous, anisotropic, and distributed nature. Here, bridging engineering and biology, a neural reservoir is employed for the dynamic control of a bio-hybrid model arm made of multiple muscle-tendon groups enveloping an elastic spine. We show how the use of reservoirs facilitates simultaneous control and self-modeling across a set of challenging tasks, outperforming classic neural network approaches. Further, by implementing a spiking reservoir on neuromorphic hardware, energy efficiency is achieved, with nearly two-orders of magnitude improvement relative to standard CPUs, with implications for the on-board control of untethered, small-scale soft robots. \u25b3 Less Submitted 12 March, 2025; originally announced March 2025. Comments: 12 pages; 4 figures arXiv:2503.09097 [pdf, other] Self-Consistent Equation-guided Neural Networks for Censored Time-to-Event Data Authors: Sehwan Kim, Rui Wang, Wenbin Lu Abstract: In survival analysis, estimating the conditional survival function given predictors is often of interest. There is a growing trend in the development of deep learning methods for analyzing censored time-to-event data, especially when dealing with high-dimensional predictors that are complexly interrelated. Many existing deep learning approaches for estimating the conditional survival functions ext\u2026 \u25bd More In survival analysis, estimating the conditional survival function given predictors is often of interest. There is a growing trend in the development of deep learning methods for analyzing censored time-to-event data, especially when dealing with high-dimensional predictors that are complexly interrelated. Many existing deep learning approaches for estimating the conditional survival functions extend the Cox regression models by replacing the linear function of predictor effects by a shallow feed-forward neural network while maintaining the proportional hazards assumption. Their implementation can be computationally intensive due to the use of the full dataset at each iteration because the use of batch data may distort the at-risk set of the partial likelihood function. To overcome these limitations, we propose a novel deep learning approach to non-parametric estimation of the conditional survival functions using the generative adversarial networks leveraging self-consistent equations. The proposed method is model-free and does not require any parametric assumptions on the structure of the conditional survival function. We establish the convergence rate of our proposed estimator of the conditional survival function. In addition, we evaluate the performance of the proposed method through simulation studies and demonstrate its application on a real-world dataset. \u25b3 Less Submitted 12 March, 2025; originally announced March 2025. arXiv:2503.08737 [pdf, other] Representing 3D Shapes With 64 Latent Vectors for 3D Diffusion Models Authors: In Cho, Youngbeom Yoo, Subin Jeon, Seon Joo Kim Abstract: Constructing a compressed latent space through a variational autoencoder (VAE) is the key for efficient 3D diffusion models. This paper introduces COD-VAE, a VAE that encodes 3D shapes into a COmpact set of 1D latent vectors without sacrificing quality. COD-VAE introduces a two-stage autoencoder scheme to improve compression and decoding efficiency. First, our encoder block progressively compresse\u2026 \u25bd More Constructing a compressed latent space through a variational autoencoder (VAE) is the key for efficient 3D diffusion models. This paper introduces COD-VAE, a VAE that encodes 3D shapes into a COmpact set of 1D latent vectors without sacrificing quality. COD-VAE introduces a two-stage autoencoder scheme to improve compression and decoding efficiency. First, our encoder block progressively compresses point clouds into compact latent vectors via intermediate point patches. Second, our triplane-based decoder reconstructs dense triplanes from latent vectors instead of directly decoding neural fields, significantly reducing computational overhead of neural fields decoding. Finally, we propose uncertainty-guided token pruning, which allocates resources adaptively by skipping computations in simpler regions and improves the decoder efficiency. Experimental results demonstrate that COD-VAE achieves 16x compression compared to the baseline while maintaining quality. This enables 20.8x speedup in generation, highlighting that a large number of latent vectors is not a prerequisite for high-quality reconstruction and generation. \u25b3 Less Submitted 11 March, 2025; originally announced March 2025. arXiv:2503.08605 [pdf, other] Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled Sampling Authors: Subin Kim, Seoung Wug Oh, Jui-Hsien Wang, Joon-Young Lee, Jinwoo Shin Abstract: While recent advancements in text-to-video diffusion models enable high-quality short video generation from a single prompt, generating real-world long videos in a single pass remains challenging due to limited data and high computational costs. To address this, several works propose tuning-free approaches, i.e., extending existing models for long video generation, specifically using multiple prom\u2026 \u25bd More While recent advancements in text-to-video diffusion models enable high-quality short video generation from a single prompt, generating real-world long videos in a single pass remains challenging due to limited data and high computational costs. To address this, several works propose tuning-free approaches, i.e., extending existing models for long video generation, specifically using multiple prompts to allow for dynamic and controlled content changes. However, these methods primarily focus on ensuring smooth transitions between adjacent frames, often leading to content drift and a gradual loss of semantic coherence over longer sequences. To tackle such an issue, we propose Synchronized Coupled Sampling (SynCoS), a novel inference framework that synchronizes denoising paths across the entire video, ensuring long-range consistency across both adjacent and distant frames. Our approach combines two complementary sampling strategies: reverse and optimization-based sampling, which ensure seamless local transitions and enforce global coherence, respectively. However, directly alternating between these samplings misaligns denoising trajectories, disrupting prompt guidance and introducing unintended content changes as they operate independently. To resolve this, SynCoS synchronizes them through a grounded timestep and a fixed baseline noise, ensuring fully coupled sampling with aligned denoising paths. Extensive experiments show that SynCoS significantly improves multi-event long video generation, achieving smoother transitions and superior long-range coherence, outperforming previous approaches both quantitatively and qualitatively. \u25b3 Less Submitted 11 March, 2025; originally announced March 2025. Comments: Project page with visuals: https://syncos2025.github.io/ arXiv:2503.08136 [pdf, other] FlowDPS: Flow-Driven Posterior Sampling for Inverse Problems Authors: Jeongsol Kim, Bryan Sangwoo Kim, Jong Chul Ye Abstract: Flow matching is a recent state-of-the-art framework for generative modeling based on ordinary differential equations (ODEs). While closely related to diffusion models, it provides a more general perspective on generative modeling. Although inverse problem solving has been extensively explored using diffusion models, it has not been rigorously examined within the broader context of flow models. Th\u2026 \u25bd More Flow matching is a recent state-of-the-art framework for generative modeling based on ordinary differential equations (ODEs). While closely related to diffusion models, it provides a more general perspective on generative modeling. Although inverse problem solving has been extensively explored using diffusion models, it has not been rigorously examined within the broader context of flow models. Therefore, here we extend the diffusion inverse solvers (DIS) - which perform posterior sampling by combining a denoising diffusion prior with an likelihood gradient - into the flow framework. Specifically, by driving the flow-version of Tweedie's formula, we decompose the flow ODE into two components: one for clean image estimation and the other for noise estimation. By integrating the likelihood gradient and stochastic noise into each component, respectively, we demonstrate that posterior sampling for inverse problem solving can be effectively achieved using flows. Our proposed solver, Flow-Driven Posterior Sampling (FlowDPS), can also be seamlessly integrated into a latent flow model with a transformer architecture. Across four linear inverse problems, we confirm that FlowDPS outperforms state-of-the-art alternatives, all without requiring additional training. \u25b3 Less Submitted 11 March, 2025; originally announced March 2025. arXiv:2503.08037 [pdf, other] ObjectMover: Generative Object Movement with Video Prior Authors: Xin Yu, Tianyu Wang, Soo Ye Kim, Paul Guerrero, Xi Chen, Qing Liu, Zhe Lin, Xiaojuan Qi Abstract: Simple as it seems, moving an object to another location within an image is, in fact, a challenging image-editing task that requires re-harmonizing the lighting, adjusting the pose based on perspective, accurately filling occluded regions, and ensuring coherent synchronization of shadows and reflections while maintaining the object identity. In this paper, we present ObjectMover, a generative mode\u2026 \u25bd More Simple as it seems, moving an object to another location within an image is, in fact, a challenging image-editing task that requires re-harmonizing the lighting, adjusting the pose based on perspective, accurately filling occluded regions, and ensuring coherent synchronization of shadows and reflections while maintaining the object identity. In this paper, we present ObjectMover, a generative model that can perform object movement in highly challenging scenes. Our key insight is that we model this task as a sequence-to-sequence problem and fine-tune a video generation model to leverage its knowledge of consistent object generation across video frames. We show that with this approach, our model is able to adjust to complex real-world scenarios, handling extreme lighting harmonization and object effect movement. As large-scale data for object movement are unavailable, we construct a data generation pipeline using a modern game engine to synthesize high-quality data pairs. We further propose a multi-task learning strategy that enables training on real-world video data to improve the model generalization. Through extensive experiments, we demonstrate that ObjectMover achieves outstanding results and adapts well to real-world scenarios. \u25b3 Less Submitted 11 March, 2025; originally announced March 2025. Comments: CVPR 2025, Project Page: https://xinyu-andy.github.io/ObjMover arXiv:2503.08003 [pdf] How Can Video Generative AI Transform K-12 Education? Examining Teachers' Perspectives through TPACK and TAM Authors: Unggi Lee, Yeil Jeong, Seungha Kim, Yoorim Son, Gyuri Byun, Hyeoncheol Kim, Cheolil Lim Abstract: The rapid advancement of generative AI technology, particularly video generative AI (Video GenAI), has opened new possibilities for K-12 education by enabling the creation of dynamic, customized, and high-quality visual content. Despite its potential, there is limited research on how this emerging technology can be effectively integrated into educational practices. This study explores the perspect\u2026 \u25bd More The rapid advancement of generative AI technology, particularly video generative AI (Video GenAI), has opened new possibilities for K-12 education by enabling the creation of dynamic, customized, and high-quality visual content. Despite its potential, there is limited research on how this emerging technology can be effectively integrated into educational practices. This study explores the perspectives of leading K-12 teachers on the educational applications of Video GenAI, using the TPACK (Technological Pedagogical Content Knowledge) and TAM (Technology Acceptance Model) frameworks as analytical lenses. Through interviews and hands-on experimentation with video generation tools, the research identifies opportunities for enhancing teaching strategies, fostering student engagement, and supporting authentic task design. It also highlights challenges such as technical limitations, ethical considerations, and the need for institutional support. The findings provide actionable insights into how Video GenAI can transform teaching and learning, offering practical implications for policy, teacher training, and the future development of educational technology. \u25b3 Less Submitted 10 March, 2025; originally announced March 2025. arXiv:2503.07655 [pdf, other] GraphT5: Unified Molecular Graph-Language Modeling via Multi-Modal Cross-Token Attention Authors: Sangyeup Kim, Nayeon Kim, Yinhua Piao, Sun Kim Abstract: Molecular language modeling tasks such as molecule captioning have been recognized for their potential to further understand molecular properties that can aid drug discovery or material synthesis based on chemical reactions. Unlike the common use of molecule graphs in predicting molecular properties, most methods in molecular language modeling rely heavily on SMILES sequences. This preference is b\u2026 \u25bd More Molecular language modeling tasks such as molecule captioning have been recognized for their potential to further understand molecular properties that can aid drug discovery or material synthesis based on chemical reactions. Unlike the common use of molecule graphs in predicting molecular properties, most methods in molecular language modeling rely heavily on SMILES sequences. This preference is because the task involves generating a sequence of multiple tokens using transformer-based models. Therefore, a main challenge is determining how to integrate graph data, which contains structural and spatial information about molecules, with text data. In addition, simply using both 1D SMILES text and 2D graph as inputs without addressing how they align and represent the molecule structure in different modalities makes it challenging to fully utilize structural knowledge about molecules. To this end, we propose GraphT5, a multi-modal framework that integrates 1D SMILES text and 2D graph representations of molecules for molecular language modeling. Specifically, we introduce a novel cross-token attention module in GraphT5 to bridge the gap arising from the fundamental differences between the two modalities of molecule representations. Cross-token attention exploits implicit information between SMILES and graphs of molecules, resulting from their interactions at a fine-grained token level that benefits molecular language modeling. Extensive experiments including molecule captioning, IUPAC name prediction tasks, and case studies show that our GraphT5 outperforms the latest baseline approaches, which validates the effectiveness of our GraphT5 in sufficiently utilizing 1D SMILES text and 2D graph representations. \u25b3 Less Submitted 7 March, 2025; originally announced March 2025. arXiv:2503.07504 [pdf, other] PIPE Planner: Pathwise Information Gain with Map Predictions for Indoor Robot Exploration Authors: Seungjae Baek, Brady Moon, Seungchan Kim, Muqing Cao, Cherie Ho, Sebastian Scherer, Jeong hwan Jeon Abstract: Autonomous exploration in unknown environments requires estimating the information gain of an action to guide planning decisions. While prior approaches often compute information gain at discrete waypoints, pathwise integration offers a more comprehensive estimation but is often computationally challenging or infeasible and prone to overestimation. In this work, we propose the Pathwise Information\u2026 \u25bd More Autonomous exploration in unknown environments requires estimating the information gain of an action to guide planning decisions. While prior approaches often compute information gain at discrete waypoints, pathwise integration offers a more comprehensive estimation but is often computationally challenging or infeasible and prone to overestimation. In this work, we propose the Pathwise Information Gain with Map Prediction for Exploration (PIPE) planner, which integrates cumulative sensor coverage along planned trajectories while leveraging map prediction to mitigate overestimation. To enable efficient pathwise coverage computation, we introduce a method to efficiently calculate the expected observation mask along the planned path, significantly reducing computational overhead. We validate PIPE on real-world floorplan datasets, demonstrating its superior performance over state-of-the-art baselines. Our results highlight the benefits of integrating predictive mapping with pathwise information gain for efficient and informed exploration. \u25b3 Less Submitted 10 March, 2025; originally announced March 2025. Comments: 8 pages, 8 figures arXiv:2503.07279 [pdf, other] VizTrust: A Visual Analytics Tool for Capturing User Trust Dynamics in Human-AI Communication Authors: Xin Wang, Stephanie Tulk Jesso, Sadamori Kojaku, David M Neyens, Min Sun Kim Abstract: Trust plays a fundamental role in shaping the willingness of users to engage and collaborate with artificial intelligence (AI) systems. Yet, measuring user trust remains challenging due to its complex and dynamic nature. While traditional survey methods provide trust levels for long conversations, they fail to capture its dynamic evolution during ongoing interactions. Here, we present VizTrust, wh\u2026 \u25bd More Trust plays a fundamental role in shaping the willingness of users to engage and collaborate with artificial intelligence (AI) systems. Yet, measuring user trust remains challenging due to its complex and dynamic nature. While traditional survey methods provide trust levels for long conversations, they fail to capture its dynamic evolution during ongoing interactions. Here, we present VizTrust, which addresses this challenge by introducing a real-time visual analytics tool that leverages a multi-agent collaboration system to capture and analyze user trust dynamics in human-agent communication. Built on established human-computer trust scales-competence, integrity, benevolence, and predictability-, VizTrust enables stakeholders to observe trust formation as it happens, identify patterns in trust development, and pinpoint specific interaction elements that influence trust. Our tool offers actionable insights into human-agent trust formation and evolution in real time through a dashboard, supporting the design of adaptive conversational agents that responds effectively to user trust signals. \u25b3 Less Submitted 10 March, 2025; originally announced March 2025. Comments: Accepted by ACM CHI conference 2025 Journal ref: CHI Conference on Human Factors in Computing Systems (CHI EA 2025) arXiv:2503.07067 [pdf, other] DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs Authors: Jongwoo Ko, Tianyi Chen, Sungnyun Kim, Tianyu Ding, Luming Liang, Ilya Zharkov, Se-Young Yun Abstract: Despite the success of distillation in large language models (LLMs), most prior work applies identical loss functions to both teacher- and student-generated data. These strategies overlook the synergy between loss formulations and data types, leading to a suboptimal performance boost in student models. To address this, we propose DistiLLM-2, a contrastive approach that simultaneously increases the\u2026 \u25bd More Despite the success of distillation in large language models (LLMs), most prior work applies identical loss functions to both teacher- and student-generated data. These strategies overlook the synergy between loss formulations and data types, leading to a suboptimal performance boost in student models. To address this, we propose DistiLLM-2, a contrastive approach that simultaneously increases the likelihood of teacher responses and decreases that of student responses by harnessing this synergy. Our extensive experiments show that DistiLLM-2 not only builds high-performing student models across a wide range of tasks, including instruction-following and code generation, but also supports diverse applications, such as preference alignment and vision-language extensions. These findings highlight the potential of a contrastive approach to enhance the efficacy of LLM distillation by effectively aligning teacher and student models across varied data types. \u25b3 Less Submitted 10 March, 2025; originally announced March 2025. Comments: The code will be available soon at https://github.com/jongwooko/distillm-2 arXiv:2503.07061 [pdf, ps, other] Encoding Co-Lex Orders of Finite-State Automata in Linear Space Authors: Ruben Becker, Nicola Cotumaccio, Sung-Hwan Kim, Nicola Prezza, Carlo Tosoni Abstract: The Burrows-Wheeler transform (BWT) is a string transformation that enhances string indexing and compressibility. Cotumaccio and Prezza [SODA '21] extended this transformation to nondeterministic finite automata (NFAs) through co-lexicographic partial orders, i.e., by sorting the states of an NFA according to the co-lexicographic order of the strings reaching them. As the BWT of an NFA shares many\u2026 \u25bd More The Burrows-Wheeler transform (BWT) is a string transformation that enhances string indexing and compressibility. Cotumaccio and Prezza [SODA '21] extended this transformation to nondeterministic finite automata (NFAs) through co-lexicographic partial orders, i.e., by sorting the states of an NFA according to the co-lexicographic order of the strings reaching them. As the BWT of an NFA shares many properties with its original string variant, the transformation can be used to implement indices for locating specific patterns on the NFA itself. The efficiency of the resulting index is influenced by the width of the partial order on the states: the smaller the width, the faster the index. The most efficient index for arbitrary NFAs currently known in the literature is based on the coarsest forward-stable co-lex (CFS) order of Becker et al. [SPIRE '24]. In this paper, we prove that this CFS order can be encoded within linear space in the number of states in the automaton. The importance of this result stems from the fact that encoding such an order in linear space represents a big first step in the direction of building the index based on this order in near-linear time -- the biggest open research question in this context. The currently most efficient known algorithm for this task run in quadratic time in the number of transitions in the NFA and are thus infeasible to be run on very large graphs (e.g., pangenome graphs). At this point, a near-linear time algorithm is solely known for the simpler case of deterministic automata [Becker et al., ESA '23] and, in fact, this algorithmic result was enabled by a linear space encoding for deterministic automata [Kim et al., CPM '23]. \u25b3 Less Submitted 10 March, 2025; originally announced March 2025. Comments: Submitted to the 36th Annual Symposium on Combinatorial Pattern Matching (CPM 2025), 18 pages, 3 figures arXiv:2503.06963 [pdf, other] Multi-Behavior Recommender Systems: A Survey Authors: Kyungho Kim, Sunwoo Kim, Geon Lee, Jinhong Jung, Kijung Shin Abstract: Traditional recommender systems primarily rely on a single type of user-item interaction, such as item purchases or ratings, to predict user preferences. However, in real-world scenarios, users engage in a variety of behaviors, such as clicking on items or adding them to carts, offering richer insights into their interests. Multi-behavior recommender systems leverage these diverse interactions to\u2026 \u25bd More Traditional recommender systems primarily rely on a single type of user-item interaction, such as item purchases or ratings, to predict user preferences. However, in real-world scenarios, users engage in a variety of behaviors, such as clicking on items or adding them to carts, offering richer insights into their interests. Multi-behavior recommender systems leverage these diverse interactions to enhance recommendation quality, and research on this topic has grown rapidly in recent years. This survey provides a timely review of multi-behavior recommender systems, focusing on three key steps: (1) Data Modeling: representing multi-behaviors at the input level, (2) Encoding: transforming these inputs into vector representations (i.e., embeddings), and (3) Training: optimizing machine-learning models. We systematically categorize existing multi-behavior recommender systems based on the commonalities and differences in their approaches across the above steps. Additionally, we discuss promising future directions for advancing multi-behavior recommender systems. \u25b3 Less Submitted 10 March, 2025; originally announced March 2025. Comments: Accepted in the PAKDD 2025 Survey Track arXiv:2503.06832 [pdf, other] GUIDE-CoT: Goal-driven and User-Informed Dynamic Estimation for Pedestrian Trajectory using Chain-of-Thought Authors: Sungsik Kim, Janghyun Baek, Jinkyu Kim, Jaekoo Lee Abstract: While Large Language Models (LLMs) have recently shown impressive results in reasoning tasks, their application to pedestrian trajectory prediction remains challenging due to two key limitations: insufficient use of visual information and the difficulty of predicting entire trajectories. To address these challenges, we propose Goal-driven and User-Informed Dynamic Estimation for pedestrian traject\u2026 \u25bd More While Large Language Models (LLMs) have recently shown impressive results in reasoning tasks, their application to pedestrian trajectory prediction remains challenging due to two key limitations: insufficient use of visual information and the difficulty of predicting entire trajectories. To address these challenges, we propose Goal-driven and User-Informed Dynamic Estimation for pedestrian trajectory using Chain-of-Thought (GUIDE-CoT). Our approach integrates two innovative modules: (1) a goal-oriented visual prompt, which enhances goal prediction accuracy combining visual prompts with a pretrained visual encoder, and (2) a chain-of-thought (CoT) LLM for trajectory generation, which generates realistic trajectories toward the predicted goal. Moreover, our method introduces controllable trajectory generation, allowing for flexible and user-guided modifications to the predicted paths. Through extensive experiments on the ETH/UCY benchmark datasets, our method achieves state-of-the-art performance, delivering both high accuracy and greater adaptability in pedestrian trajectory prediction. Our code is publicly available at https://github.com/ai-kmu/GUIDE-CoT. \u25b3 Less Submitted 9 March, 2025; originally announced March 2025. Comments: 10 pages, 5 figures, will be published on The 24th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2025) MSC Class: 93C85 ACM Class: I.2.10 arXiv:2503.06238 [pdf, other] Image is All You Need: Towards Efficient and Effective Large Language Model-Based Recommender Systems Authors: Kibum Kim, Sein Kim, Hongseok Kang, Jiwan Kim, Heewoong Noh, Yeonjun In, Kanghoon Yoon, Jinoh Oh, Chanyoung Park Abstract: Large Language Models (LLMs) have recently emerged as a powerful backbone for recommender systems. Existing LLM-based recommender systems take two different approaches for representing items in natural language, i.e., Attribute-based Representation and Description-based Representation. In this work, we aim to address the trade-off between efficiency and effectiveness that these two approaches enco\u2026 \u25bd More Large Language Models (LLMs) have recently emerged as a powerful backbone for recommender systems. Existing LLM-based recommender systems take two different approaches for representing items in natural language, i.e., Attribute-based Representation and Description-based Representation. In this work, we aim to address the trade-off between efficiency and effectiveness that these two approaches encounter, when representing items consumed by users. Based on our interesting observation that there is a significant information overlap between images and descriptions associated with items, we propose a novel method, Image is all you need for LLM-based Recommender system (I-LLMRec). Our main idea is to leverage images as an alternative to lengthy textual descriptions for representing items, aiming at reducing token usage while preserving the rich semantic information of item descriptions. Through extensive experiments, we demonstrate that I-LLMRec outperforms existing methods in both efficiency and effectiveness by leveraging images. Moreover, a further appeal of I-LLMRec is its ability to reduce sensitivity to noise in descriptions, leading to more robust recommendations. \u25b3 Less Submitted 8 March, 2025; originally announced March 2025. arXiv:2503.06179 [pdf, other] ForestSplats: Deformable transient field for Gaussian Splatting in the Wild Authors: Wongi Park, Myeongseok Nam, Siwon Kim, Sangwoo Jo, Soomok Lee Abstract: Recently, 3D Gaussian Splatting (3D-GS) has emerged, showing real-time rendering speeds and high-quality results in static scenes. Although 3D-GS shows effectiveness in static scenes, their performance significantly degrades in real-world environments due to transient objects, lighting variations, and diverse levels of occlusion. To tackle this, existing methods estimate occluders or transient ele\u2026 \u25bd More Recently, 3D Gaussian Splatting (3D-GS) has emerged, showing real-time rendering speeds and high-quality results in static scenes. Although 3D-GS shows effectiveness in static scenes, their performance significantly degrades in real-world environments due to transient objects, lighting variations, and diverse levels of occlusion. To tackle this, existing methods estimate occluders or transient elements by leveraging pre-trained models or integrating additional transient field pipelines. However, these methods still suffer from two defects: 1) Using semantic features from the Vision Foundation model (VFM) causes additional computational costs. 2) The transient field requires significant memory to handle transient elements with per-view Gaussians and struggles to define clear boundaries for occluders, solely relying on photometric errors. To address these problems, we propose ForestSplats, a novel approach that leverages the deformable transient field and a superpixel-aware mask to efficiently represent transient elements in the 2D scene across unconstrained image collections and effectively decompose static scenes from transient distractors without VFM. We designed the transient field to be deformable, capturing per-view transient elements. Furthermore, we introduce a superpixel-aware mask that clearly defines the boundaries of occluders by considering photometric errors and superpixels. Additionally, we propose uncertainty-aware densification to avoid generating Gaussians within the boundaries of occluders during densification. Through extensive experiments across several benchmark datasets, we demonstrate that ForestSplats outperforms existing methods without VFM and shows significant memory efficiency in representing transient elements. \u25b3 Less Submitted 8 March, 2025; originally announced March 2025. arXiv:2503.04966 [pdf, other] Prediction of Frozen Region Growth in Kidney Cryoablation Intervention Using a 3D Flow-Matching Model Authors: Siyeop Yoon, Yujin Oh, Matthew Tivnan, Sifan Song, Pengfei Jin, Sekeun Kim, Hyun Jin Cho, Dufan Wu, Raul Uppot, Quanzheng Li Abstract: This study presents a 3D flow-matching model designed to predict the progression of the frozen region (iceball) during kidney cryoablation. Precise intraoperative guidance is critical in cryoablation to ensure complete tumor eradication while preserving adjacent healthy tissue. However, conventional methods, typically based on physics driven or diffusion based simulations, are computationally dema\u2026 \u25bd More This study presents a 3D flow-matching model designed to predict the progression of the frozen region (iceball) during kidney cryoablation. Precise intraoperative guidance is critical in cryoablation to ensure complete tumor eradication while preserving adjacent healthy tissue. However, conventional methods, typically based on physics driven or diffusion based simulations, are computationally demanding and often struggle to represent complex anatomical structures accurately. To address these limitations, our approach leverages intraoperative CT imaging to inform the model. The proposed 3D flow matching model is trained to learn a continuous deformation field that maps early-stage CT scans to future predictions. This transformation not only estimates the volumetric expansion of the iceball but also generates corresponding segmentation masks, effectively capturing spatial and morphological changes over time. Quantitative analysis highlights the model robustness, demonstrating strong agreement between predictions and ground-truth segmentations. The model achieves an Intersection over Union (IoU) score of 0.61 and a Dice coefficient of 0.75. By integrating real time CT imaging with advanced deep learning techniques, this approach has the potential to enhance intraoperative guidance in kidney cryoablation, improving procedural outcomes and advancing the field of minimally invasive surgery. \u25b3 Less Submitted 11 March, 2025; v1 submitted 6 March, 2025; originally announced March 2025. Comments: MICCAI 2025 submitted version (author list included) arXiv:2503.04856 [pdf, other] One-Shot is Enough: Consolidating Multi-Turn Attacks into Efficient Single-Turn Prompts for LLMs Authors: Junwoo Ha, Hyunjun Kim, Sangyoon Yu, Haon Park, Ashkan Yousefpour, Yuna Park, Suhyun Kim Abstract: Despite extensive safety enhancements in large language models (LLMs), multi-turn \"jailbreak\" conversations crafted by skilled human adversaries can still breach even the most sophisticated guardrails. However, these multi-turn attacks demand considerable manual effort, limiting their scalability. In this work, we introduce a novel approach called Multi-turn-to-Single-turn (M2S) that systematicall\u2026 \u25bd More Despite extensive safety enhancements in large language models (LLMs), multi-turn \"jailbreak\" conversations crafted by skilled human adversaries can still breach even the most sophisticated guardrails. However, these multi-turn attacks demand considerable manual effort, limiting their scalability. In this work, we introduce a novel approach called Multi-turn-to-Single-turn (M2S) that systematically converts multi-turn jailbreak prompts into single-turn attacks. Specifically, we propose three conversion strategies - Hyphenize, Numberize, and Pythonize - each preserving sequential context yet packaging it in a single query. Our experiments on the Multi-turn Human Jailbreak (MHJ) dataset show that M2S often increases or maintains high Attack Success Rates (ASRs) compared to original multi-turn conversations. Notably, using a StrongREJECT-based evaluation of harmfulness, M2S achieves up to 95.9% ASR on Mistral-7B and outperforms original multi-turn prompts by as much as 17.5% in absolute improvement on GPT-4o. Further analysis reveals that certain adversarial tactics, when consolidated into a single prompt, exploit structural formatting cues to evade standard policy checks. These findings underscore that single-turn attacks - despite being simpler and cheaper to conduct - can be just as potent, if not more, than their multi-turn counterparts. Our findings underscore the urgent need to reevaluate and reinforce LLM safety strategies, given how adversarial queries can be compacted into a single prompt while still retaining sufficient complexity to bypass existing safety measures. \u25b3 Less Submitted 6 March, 2025; originally announced March 2025. arXiv:2503.04780 [pdf, other] MV-CLAM: Multi-View Molecular Interpretation with Cross-Modal Projection via Language Model Authors: Sumin Ha, Jun Hyeong Kim, Yinhua Piao, Sun Kim Abstract: Human expertise in chemistry and biomedicine relies on contextual molecular understanding, a capability that large language models (LLMs) can extend through fine-grained alignment between molecular structures and text. Recent multimodal learning advances focus on cross-modal alignment, but existing molecule-text models ignore complementary information in different molecular views and rely on singl\u2026 \u25bd More Human expertise in chemistry and biomedicine relies on contextual molecular understanding, a capability that large language models (LLMs) can extend through fine-grained alignment between molecular structures and text. Recent multimodal learning advances focus on cross-modal alignment, but existing molecule-text models ignore complementary information in different molecular views and rely on single-view representations, limiting molecular understanding. Moreover, na\u00efve multi-view alignment strategies face two challenges: (1) separate aligned spaces with inconsistent mappings between molecule and text embeddings, and that (2) existing loss objectives fail to preserve complementary information for fine-grained alignment. This can limit the LLM's ability to fully understand the molecular properties. To address these issues, we propose MV-CLAM, a novel framework that aligns multi-view molecular representations into a unified textual space using a multi-query transformer (MQ-Former). Our approach ensures cross-view consistency while a token-level contrastive loss preserves diverse molecular features across textual queries. MV-CLAM enhances molecular reasoning, improving retrieval and captioning accuracy. The source code of MV-CLAM is available in https://github.com/sumin124/mv-clam.git. \u25b3 Less Submitted 23 February, 2025; originally announced March 2025. arXiv:2503.04675 [pdf, other] LLM-guided Plan and Retrieval: A Strategic Alignment for Interpretable User Satisfaction Estimation in Dialogue Authors: Sangyeop Kim, Sohhyung Park, Jaewon Jung, Jinseok Kim, Sungzoon Cho Abstract: Understanding user satisfaction with conversational systems, known as User Satisfaction Estimation (USE), is essential for assessing dialogue quality and enhancing user experiences. However, existing methods for USE face challenges due to limited understanding of underlying reasons for user dissatisfaction and the high costs of annotating user intentions. To address these challenges, we propose PR\u2026 \u25bd More Understanding user satisfaction with conversational systems, known as User Satisfaction Estimation (USE), is essential for assessing dialogue quality and enhancing user experiences. However, existing methods for USE face challenges due to limited understanding of underlying reasons for user dissatisfaction and the high costs of annotating user intentions. To address these challenges, we propose PRAISE (Plan and Retrieval Alignment for Interpretable Satisfaction Estimation), an interpretable framework for effective user satisfaction prediction. PRAISE operates through three key modules. The Strategy Planner develops strategies, which are natural language criteria for classifying user satisfaction. The Feature Retriever then incorporates knowledge on user satisfaction from Large Language Models (LLMs) and retrieves relevance features from utterances. Finally, the Score Analyzer evaluates strategy predictions and classifies user satisfaction. Experimental results demonstrate that PRAISE achieves state-of-the-art performance on three benchmarks for the USE task. Beyond its superior performance, PRAISE offers additional benefits. It enhances interpretability by providing instance-level explanations through effective alignment of utterances with strategies. Moreover, PRAISE operates more efficiently than existing approaches by eliminating the need for LLMs during the inference phase. \u25b3 Less Submitted 6 March, 2025; originally announced March 2025. Comments: Accepted by NAACL 2025 arXiv:2503.04141 [pdf, other] HEISIR: Hierarchical Expansion of Inverted Semantic Indexing for Training-free Retrieval of Conversational Data using LLMs Authors: Sangyeop Kim, Hangyeul Lee, Yohan Lee Abstract: The growth of conversational AI services has increased demand for effective information retrieval from dialogue data. However, existing methods often face challenges in capturing semantic intent or require extensive labeling and fine-tuning. This paper introduces HEISIR (Hierarchical Expansion of Inverted Semantic Indexing for Retrieval), a novel framework that enhances semantic understanding in c\u2026 \u25bd More The growth of conversational AI services has increased demand for effective information retrieval from dialogue data. However, existing methods often face challenges in capturing semantic intent or require extensive labeling and fine-tuning. This paper introduces HEISIR (Hierarchical Expansion of Inverted Semantic Indexing for Retrieval), a novel framework that enhances semantic understanding in conversational data retrieval through optimized data ingestion, eliminating the need for resource-intensive labeling or model adaptation. HEISIR implements a two-step process: (1) Hierarchical Triplets Formulation and (2) Adjunct Augmentation, creating semantic indices consisting of Subject-Verb-Object-Adjunct (SVOA) quadruplets. This structured representation effectively captures the underlying semantic information from dialogue content. HEISIR achieves high retrieval performance while maintaining low latency during the actual retrieval process. Our experimental results demonstrate that HEISIR outperforms fine-tuned models across various embedding types and language models. Beyond improving retrieval capabilities, HEISIR also offers opportunities for intent and topic analysis in conversational data, providing a versatile solution for dialogue systems. \u25b3 Less Submitted 6 March, 2025; originally announced March 2025. Comments: Accepted by NAACL 2025 (Findings) arXiv:2503.03995 [pdf, other] Subgraph Federated Learning for Local Generalization Authors: Sungwon Kim, Yoonho Lee, Yunhak Oh, Namkyeong Lee, Sukwon Yun, Junseok Lee, Sein Kim, Carl Yang, Chanyoung Park Abstract: Federated Learning (FL) on graphs enables collaborative model training to enhance performance without compromising the privacy of each client. However, existing methods often overlook the mutable nature of graph data, which frequently introduces new nodes and leads to shifts in label distribution. Since they focus solely on performing well on each client's local data, they are prone to overfitting\u2026 \u25bd More Federated Learning (FL) on graphs enables collaborative model training to enhance performance without compromising the privacy of each client. However, existing methods often overlook the mutable nature of graph data, which frequently introduces new nodes and leads to shifts in label distribution. Since they focus solely on performing well on each client's local data, they are prone to overfitting to their local distributions (i.e., local overfitting), which hinders their ability to generalize to unseen data with diverse label distributions. In contrast, our proposed method, FedLoG, effectively tackles this issue by mitigating local overfitting. Our model generates global synthetic data by condensing the reliable information from each class representation and its structural information across clients. Using these synthetic data as a training set, we alleviate the local overfitting problem by adaptively generalizing the absent knowledge within each local dataset. This enhances the generalization capabilities of local models, enabling them to handle unseen data effectively. Our model outperforms baselines in our proposed experimental settings, which are designed to measure generalization power to unseen data in practical scenarios. Our code is available at https://github.com/sung-won-kim/FedLoG \u25b3 Less Submitted 5 March, 2025; originally announced March 2025. Comments: ICLR 2025 (oral) arXiv:2503.03799 [pdf, other] DeepGrav: Anomalous Gravitational-Wave Detection Through Deep Latent Features Authors: Jianqi Yan, Alex P. Leung, Zhiyuan Pei, David C. Y. Hui, Sangin Kim Abstract: This work introduces a novel deep learning-based approach for gravitational wave anomaly detection, aiming to overcome the limitations of traditional matched filtering techniques in identifying unknown waveform gravitational wave signals. We introduce a modified convolutional neural network architecture inspired by ResNet that leverages residual blocks to extract high-dimensional features, effecti\u2026 \u25bd More This work introduces a novel deep learning-based approach for gravitational wave anomaly detection, aiming to overcome the limitations of traditional matched filtering techniques in identifying unknown waveform gravitational wave signals. We introduce a modified convolutional neural network architecture inspired by ResNet that leverages residual blocks to extract high-dimensional features, effectively capturing subtle differences between background noise and gravitational wave signals. This network architecture learns a high-dimensional projection while preserving discrepancies with the original input, facilitating precise identification of gravitational wave signals. In our experiments, we implement an innovative data augmentation strategy that generates new data by computing the arithmetic mean of multiple signal samples while retaining the key features of the original signals. In the NSF HDR A3D3: Detecting Anomalous Gravitational Wave Signals competition, it is honorable for us (group name: easonyan123) to get to the first place at the end with our model achieving a true negative rate (TNR) of 0.9708 during development/validation phase and 0.9832 on an unseen challenge dataset during final/testing phase, the highest among all competitors. These results demonstrate that our method not only achieves excellent generalization performance but also maintains robust adaptability in addressing the complex uncertainties inherent in gravitational wave anomaly detection. \u25b3 Less Submitted 15 March, 2025; v1 submitted 5 March, 2025; originally announced March 2025. Comments: 6 pages, 3 figures, A concise introduction to the winning solution for NSF HDR A3D3 GW challenge. Our training code is publicly available at https://github.com/yan123yan/HDR-anomaly-challenge-submission arXiv:2503.03139 [pdf, other] Convergence Analysis of Federated Learning Methods Using Backward Error Analysis Authors: Jinwoo Lim, Suhyun Kim, Soo-Mook Moon Abstract: Backward error analysis allows finding a modified loss function, which the parameter updates really follow under the influence of an optimization method. The additional loss terms included in this modified function is called implicit regularizer. In this paper, we attempt to find the implicit regularizer for various federated learning algorithms on non-IID data distribution, and explain why each m\u2026 \u25bd More Backward error analysis allows finding a modified loss function, which the parameter updates really follow under the influence of an optimization method. The additional loss terms included in this modified function is called implicit regularizer. In this paper, we attempt to find the implicit regularizer for various federated learning algorithms on non-IID data distribution, and explain why each method shows different convergence behavior. We first show that the implicit regularizer of FedAvg disperses the gradient of each client from the average gradient, thus increasing the gradient variance. We also empirically show that the implicit regularizer hampers its convergence. Similarly, we compute the implicit regularizers of FedSAM and SCAFFOLD, and explain why they converge better. While existing convergence analyses focus on pointing out the advantages of FedSAM and SCAFFOLD, our approach can explain their limitations in complex non-convex settings. In specific, we demonstrate that FedSAM can partially remove the bias in the first-order term of the implicit regularizer in FedAvg, whereas SCAFFOLD can fully eliminate the bias in the first-order term, but not in the second-order term. Consequently, the implicit regularizer can provide a useful insight on the convergence behavior of federated learning from a different theoretical perspective. \u25b3 Less Submitted 4 March, 2025; originally announced March 2025. Journal ref: AAAI 2025 arXiv:2503.02399 [pdf, other] VisAgent: Narrative-Preserving Story Visualization Framework Authors: Seungkwon Kim, GyuTae Park, Sangyeon Kim, Seung-Hun Nam Abstract: Story visualization is the transformation of narrative elements into image sequences. While existing research has primarily focused on visual contextual coherence, the deeper narrative essence of stories often remains overlooked. This limitation hinders the practical application of these approaches, as generated images frequently fail to capture the intended meaning and nuances of the narrative fu\u2026 \u25bd More Story visualization is the transformation of narrative elements into image sequences. While existing research has primarily focused on visual contextual coherence, the deeper narrative essence of stories often remains overlooked. This limitation hinders the practical application of these approaches, as generated images frequently fail to capture the intended meaning and nuances of the narrative fully. To address these challenges, we propose VisAgent, a training-free multi-agent framework designed to comprehend and visualize pivotal scenes within a given story. By considering story distillation, semantic consistency, and contextual coherence, VisAgent employs an agentic workflow. In this workflow, multiple specialized agents collaborate to: (i) refine layered prompts based on the narrative structure and (ii) seamlessly integrate \\gt{generated} elements, including refined prompts, scene elements, and subject placement, into the final image. The empirically validated effectiveness confirms the framework's suitability for practical story visualization applications. \u25b3 Less Submitted 4 March, 2025; originally announced March 2025. Comments: Accepted to ICASSP 2025. Equal contribution from first two authors arXiv:2503.02379 [pdf, other] Teaching Metric Distance to Autoregressive Multimodal Foundational Models Authors: Jiwan Chung, Saejin Kim, Yongrae Jo, Jaewoo Park, Dongjun Min, Youngjae Yu Abstract: As large language models expand beyond natural language to domains such as mathematics, multimodal understanding, and embodied agents, tokens increasingly reflect metric relationships rather than purely linguistic meaning. We introduce DIST2Loss, a distance-aware framework designed to train autoregressive discrete models by leveraging predefined distance relationships among output tokens. At its c\u2026 \u25bd More As large language models expand beyond natural language to domains such as mathematics, multimodal understanding, and embodied agents, tokens increasingly reflect metric relationships rather than purely linguistic meaning. We introduce DIST2Loss, a distance-aware framework designed to train autoregressive discrete models by leveraging predefined distance relationships among output tokens. At its core, DIST2Loss transforms continuous exponential family distributions derived from inherent distance metrics into discrete, categorical optimization targets compatible with the models' architectures. This approach enables the models to learn and preserve meaningful distance relationships during token generation while maintaining compatibility with existing architectures. Empirical evaluations show consistent performance gains in diverse multimodal applications, including visual grounding, robotic manipulation, generative reward modeling, and image generation using vector-quantized features. These improvements are pronounced in cases of limited training data, highlighting DIST2Loss's effectiveness in resource-constrained settings. \u25b3 Less Submitted 4 March, 2025; originally announced March 2025. arXiv:2503.02162 [pdf, other] X2CT-CLIP: Enable Multi-Abnormality Detection in Computed Tomography from Chest Radiography via Tri-Modal Contrastive Learning Authors: Jianzhong You, Yuan Gao, Sangwook Kim, Chris Mcintosh Abstract: Computed tomography (CT) is a key imaging modality for diagnosis, yet its clinical utility is marred by high radiation exposure and long turnaround times, restricting its use for larger-scale screening. Although chest radiography (CXR) is more accessible and safer, existing CXR foundation models focus primarily on detecting diseases that are readily visible on the CXR. Recently, works have explore\u2026 \u25bd More Computed tomography (CT) is a key imaging modality for diagnosis, yet its clinical utility is marred by high radiation exposure and long turnaround times, restricting its use for larger-scale screening. Although chest radiography (CXR) is more accessible and safer, existing CXR foundation models focus primarily on detecting diseases that are readily visible on the CXR. Recently, works have explored training disease classification models on simulated CXRs, but they remain limited to recognizing a single disease type from CT. CT foundation models have also emerged with significantly improved detection of pathologies in CT. However, the generalized application of CT-derived labels on CXR has remained illusive. In this study, we propose X2CT-CLIP, a tri-modal knowledge transfer learning framework that bridges the modality gap between CT and CXR while reducing the computational burden of model training. Our approach is the first work to enable multi-abnormality classification in CT, using CXR, by transferring knowledge from 3D CT volumes and associated radiology reports to a CXR encoder via a carefully designed tri-modal alignment mechanism in latent space. Extensive evaluations on three multi-label CT datasets demonstrate that our method outperforms state-of-the-art baselines in cross-modal retrieval, few-shot adaptation, and external validation. These results highlight the potential of CXR, enriched with knowledge derived from CT, as a viable efficient alternative for disease detection in resource-limited settings. \u25b3 Less Submitted 10 March, 2025; v1 submitted 3 March, 2025; originally announced March 2025. Comments: 11 pages, 1 figure, 5 tables arXiv:2503.02112 [pdf, other] Building Machine Learning Challenges for Anomaly Detection in Science Authors: Elizabeth G. Campolongo, Yuan-Tang Chou, Ekaterina Govorkova, Wahid Bhimji, Wei-Lun Chao, Chris Harris, Shih-Chieh Hsu, Hilmar Lapp, Mark S. Neubauer, Josephine Namayanja, Aneesh Subramanian, Philip Harris, Advaith Anand, David E. Carlyn, Subhankar Ghosh, Christopher Lawrence, Eric Moreno, Ryan Raikman, Jiaman Wu, Ziheng Zhang, Bayu Adhi, Mohammad Ahmadi Gharehtoragh, Sa\u00fal Alonso Monsalve, Marta Babicz, Furqan Baig , et al. (125 additional authors not shown) Abstract: Scientific discoveries are often made by finding a pattern or object that was not predicted by the known rules of science. Oftentimes, these anomalous events or objects that do not conform to the norms are an indication that the rules of science governing the data are incomplete, and something new needs to be present to explain these unexpected outliers. The challenge of finding anomalies can be c\u2026 \u25bd More Scientific discoveries are often made by finding a pattern or object that was not predicted by the known rules of science. Oftentimes, these anomalous events or objects that do not conform to the norms are an indication that the rules of science governing the data are incomplete, and something new needs to be present to explain these unexpected outliers. The challenge of finding anomalies can be confounding since it requires codifying a complete knowledge of the known scientific behaviors and then projecting these known behaviors on the data to look for deviations. When utilizing machine learning, this presents a particular challenge since we require that the model not only understands scientific data perfectly but also recognizes when the data is inconsistent and out of the scope of its trained behavior. In this paper, we present three datasets aimed at developing machine learning-based anomaly detection for disparate scientific domains covering astrophysics, genomics, and polar science. We present the different datasets along with a scheme to make machine learning challenges around the three datasets findable, accessible, interoperable, and reusable (FAIR). Furthermore, we present an approach that generalizes to future machine learning challenges, enabling the possibility of large, more compute-intensive challenges that can ultimately lead to scientific discovery. \u25b3 Less Submitted 3 March, 2025; originally announced March 2025. Comments: 18 pages 6 figures to be submitted to Nature Communications"
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Kim,+H",
    "title": "Showing 1\u201350 of 1,677 results for author: Kim, H",
    "author": "Authors:\nSejong Kim, \n      \n      Hyunseo Song, \n      \n      Hyunwoo Seo, \n      \n      Hyunjun Kim",
    "body": "arXiv:2503.15191 [pdf, other] Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems Authors: Sejong Kim, Hyunseo Song, Hyunwoo Seo, Hyunjun Kim Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising framework to mitigate hallucinations in Large Language Models (LLMs), yet its overall performance is dependent on the underlying retrieval system. In the finance domain, documents such as 10-K reports pose distinct challenges due to domain-specific vocabulary and multi-hierarchical tabular data. In this work, we introduce an efficient\u2026 \u25bd More Retrieval-Augmented Generation (RAG) has emerged as a promising framework to mitigate hallucinations in Large Language Models (LLMs), yet its overall performance is dependent on the underlying retrieval system. In the finance domain, documents such as 10-K reports pose distinct challenges due to domain-specific vocabulary and multi-hierarchical tabular data. In this work, we introduce an efficient, end-to-end RAG pipeline that enhances retrieval for financial documents through a three-phase approach: pre-retrieval, retrieval, and post-retrieval. In the pre-retrieval phase, various query and corpus preprocessing techniques are employed to enrich input data. During the retrieval phase, we fine-tuned state-of-the-art (SOTA) embedding models with domain-specific knowledge and implemented a hybrid retrieval strategy that combines dense and sparse representations. Finally, the post-retrieval phase leverages Direct Preference Optimization (DPO) training and document selection methods to further refine the results. Evaluations on seven financial question answering datasets-FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA, and MultiHiertt-demonstrate substantial improvements in retrieval performance, leading to more accurate and contextually appropriate generation. These findings highlight the critical role of tailored retrieval techniques in advancing the effectiveness of RAG systems for financial applications. A fully replicable pipeline is available on GitHub: https://github.com/seohyunwoo-0407/GAR. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. Comments: 15 pages, 3 figures, 11 tables. Accepted at ICLR 2025 Workshop on Advances in Financial AI. Code available at https://github.com/seohyunwoo-0407/GAR arXiv:2503.14035 [pdf, other] A Revisit to the Decoder for Camouflaged Object Detection Authors: Seung Woo Ko, Joopyo Hong, Suyoung Kim, Seungjai Bang, Sungzoon Cho, Nojun Kwak, Hyung-Sin Kim, Joonseok Lee Abstract: Camouflaged object detection (COD) aims to generate a fine-grained segmentation map of camouflaged objects hidden in their background. Due to the hidden nature of camouflaged objects, it is essential for the decoder to be tailored to effectively extract proper features of camouflaged objects and extra-carefully generate their complex boundaries. In this paper, we propose a novel architecture that\u2026 \u25bd More Camouflaged object detection (COD) aims to generate a fine-grained segmentation map of camouflaged objects hidden in their background. Due to the hidden nature of camouflaged objects, it is essential for the decoder to be tailored to effectively extract proper features of camouflaged objects and extra-carefully generate their complex boundaries. In this paper, we propose a novel architecture that augments the prevalent decoding strategy in COD with Enrich Decoder and Retouch Decoder, which help to generate a fine-grained segmentation map. Specifically, the Enrich Decoder amplifies the channels of features that are important for COD using channel-wise attention. Retouch Decoder further refines the segmentation maps by spatially attending to important pixels, such as the boundary regions. With extensive experiments, we demonstrate that ENTO shows superior performance using various encoders, with the two novel components playing their unique roles that are mutually complementary. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. Comments: Published in BMVC 2024, 13 pages, 7 figures (Appendix: 5 pages, 2 figures) Journal ref: British Machine Vision Conference (BMVC) 2024 arXiv:2503.12806 [pdf, other] AV-Surf: Surface-Enhanced Geometry-Aware Novel-View Acoustic Synthesis Authors: Hadam Baek, Hannie Shin, Jiyoung Seo, Chanwoo Kim, Saerom Kim, Hyeongbok Kim, Sangpil Kim Abstract: Accurately modeling sound propagation with complex real-world environments is essential for Novel View Acoustic Synthesis (NVAS). While previous studies have leveraged visual perception to estimate spatial acoustics, the combined use of surface normal and structural details from 3D representations in acoustic modeling has been underexplored. Given their direct impact on sound wave reflections and\u2026 \u25bd More Accurately modeling sound propagation with complex real-world environments is essential for Novel View Acoustic Synthesis (NVAS). While previous studies have leveraged visual perception to estimate spatial acoustics, the combined use of surface normal and structural details from 3D representations in acoustic modeling has been underexplored. Given their direct impact on sound wave reflections and propagation, surface normals should be jointly modeled with structural details to achieve accurate spatial acoustics. In this paper, we propose a surface-enhanced geometry-aware approach for NVAS to improve spatial acoustic modeling. To achieve this, we exploit geometric priors, such as image, depth map, surface normals, and point clouds obtained using a 3D Gaussian Splatting (3DGS) based framework. We introduce a dual cross-attention-based transformer integrating geometrical constraints into frequency query to understand the surroundings of the emitter. Additionally, we design a ConvNeXt-based spectral features processing network called Spectral Refinement Network (SRN) to synthesize realistic binaural audio. Experimental results on the RWAVS and SoundSpace datasets highlight the necessity of our approach, as it surpasses existing methods in novel view acoustic synthesis. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. arXiv:2503.12686 [pdf, other] Can LLMs Formally Reason as Abstract Interpreters for Program Analysis? Authors: Jacqueline L. Mitchell, Brian Hyeongseok Kim, Chenyu Zhou, Chao Wang Abstract: LLMs have demonstrated impressive capabilities in code generation and comprehension, but their potential in being able to perform program analysis in a formal, automatic manner remains under-explored. To that end, we systematically investigate whether LLMs can reason about programs using a program analysis framework called abstract interpretation. We prompt LLMs to follow two different strategies,\u2026 \u25bd More LLMs have demonstrated impressive capabilities in code generation and comprehension, but their potential in being able to perform program analysis in a formal, automatic manner remains under-explored. To that end, we systematically investigate whether LLMs can reason about programs using a program analysis framework called abstract interpretation. We prompt LLMs to follow two different strategies, denoted as Compositional and Fixed Point Equation, to formally reason in the style of abstract interpretation, which has never been done before to the best of our knowledge. We validate our approach using state-of-the-art LLMs on 22 challenging benchmark programs from the Software Verification Competition (SV-COMP) 2019 dataset, widely used in program analysis. Our results show that our strategies are able to elicit abstract interpretation-based reasoning in the tested models, but LLMs are susceptible to logical errors, especially while interpreting complex program structures, as well as general hallucinations. This highlights key areas for improvement in the formal reasoning capabilities of LLMs. \u25b3 Less Submitted 16 March, 2025; originally announced March 2025. arXiv:2503.12651 [pdf, other] VeriLA: A Human-Centered Evaluation Framework for Interpretable Verification of LLM Agent Failures Authors: Yoo Yeon Sung, Hannah Kim, Dan Zhang Abstract: AI practitioners increasingly use large language model (LLM) agents in compound AI systems to solve complex reasoning tasks, these agent executions often fail to meet human standards, leading to errors that compromise the system's overall performance. Addressing these failures through human intervention is challenging due to the agents' opaque reasoning processes, misalignment with human expectati\u2026 \u25bd More AI practitioners increasingly use large language model (LLM) agents in compound AI systems to solve complex reasoning tasks, these agent executions often fail to meet human standards, leading to errors that compromise the system's overall performance. Addressing these failures through human intervention is challenging due to the agents' opaque reasoning processes, misalignment with human expectations, the complexity of agent dependencies, and the high cost of manual inspection. This paper thus introduces a human-centered evaluation framework for Verifying LLM Agent failures (VeriLA), which systematically assesses agent failures to reduce human effort and make these agent failures interpretable to humans. The framework first defines clear expectations of each agent by curating human-designed agent criteria. Then, it develops a human-aligned agent verifier module, trained with human gold standards, to assess each agent's execution output. This approach enables granular evaluation of each agent's performance by revealing failures from a human standard, offering clear guidelines for revision, and reducing human cognitive load. Our case study results show that VeriLA is both interpretable and efficient in helping practitioners interact more effectively with the system. By upholding accountability in human-agent collaboration, VeriLA paves the way for more trustworthy and human-aligned compound AI systems. \u25b3 Less Submitted 16 March, 2025; originally announced March 2025. arXiv:2503.12524 [pdf, other] EXAONE Deep: Reasoning Enhanced Language Models Authors: LG AI Research, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Yemuk Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Kijeong Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee , et al. (7 additional authors not shown) Abstract: We present EXAONE Deep series, which exhibits superior capabilities in various reasoning tasks, including math and coding benchmarks. We train our models mainly on the reasoning-specialized dataset that incorporates long streams of thought processes. Evaluation results show that our smaller models, EXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while the largest model, EXAO\u2026 \u25bd More We present EXAONE Deep series, which exhibits superior capabilities in various reasoning tasks, including math and coding benchmarks. We train our models mainly on the reasoning-specialized dataset that incorporates long streams of thought processes. Evaluation results show that our smaller models, EXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while the largest model, EXAONE Deep 32B, demonstrates competitive performance against leading open-weight models. All EXAONE Deep models are openly available for research purposes and can be downloaded from https://huggingface.co/LGAI-EXAONE \u25b3 Less Submitted 19 March, 2025; v1 submitted 16 March, 2025; originally announced March 2025. Comments: arXiv admin note: substantial text overlap with arXiv:2412.04862, arXiv:2408.03541 arXiv:2503.11737 [pdf, other] Multi-View Node Pruning for Accurate Graph Representation Authors: Jiseong Park, Hanjin Kim, Seojin Kim, Jueun Choi Abstract: Graph pooling, which compresses a whole graph into a smaller coarsened graph, is an essential component of graph representation learning. To efficiently compress a given graph, graph pooling methods often drop their nodes with attention-based scoring with the task loss. However, this often results in simply removing nodes with lower degrees without consideration of their feature-level relevance to\u2026 \u25bd More Graph pooling, which compresses a whole graph into a smaller coarsened graph, is an essential component of graph representation learning. To efficiently compress a given graph, graph pooling methods often drop their nodes with attention-based scoring with the task loss. However, this often results in simply removing nodes with lower degrees without consideration of their feature-level relevance to the given task. To fix this problem, we propose a Multi-View Pruning(MVP), a graph pruning method based on a multi-view framework and reconstruction loss. Given a graph, MVP first constructs multiple graphs for different views either by utilizing the predefined modalities or by randomly partitioning the input features, to consider the importance of each node in diverse perspectives. Then, it learns the score for each node by considering both the reconstruction and the task loss. MVP can be incorporated with any hierarchical pooling framework to score the nodes. We validate MVP on multiple benchmark datasets by coupling it with two graph pooling methods, and show that it significantly improves the performance of the base graph pooling method, outperforming all baselines. Further analysis shows that both the encoding of multiple views and the consideration of reconstruction loss are the key to the success of MVP, and that it indeed identifies nodes that are less important according to domain knowledge. \u25b3 Less Submitted 18 March, 2025; v1 submitted 14 March, 2025; originally announced March 2025. Comments: Jiseong Park and Hanjin Kim are co-first author for this work arXiv:2503.11649 [pdf, other] Scalable Video Conferencing Using SDN Principles Authors: Oliver Michel, Satadal Sengupta, Hyojoon Kim, Ravi Netravali, Jennifer Rexford Abstract: Video-conferencing applications face an unwavering surge in traffic, stressing their underlying infrastructure in unprecedented ways. This paper rethinks the key building block for conferencing infrastructures -- selective forwarding units (SFUs). SFUs relay and adapt media streams between participants and, today, run in software on general-purpose servers. Our main insight, discerned from dissect\u2026 \u25bd More Video-conferencing applications face an unwavering surge in traffic, stressing their underlying infrastructure in unprecedented ways. This paper rethinks the key building block for conferencing infrastructures -- selective forwarding units (SFUs). SFUs relay and adapt media streams between participants and, today, run in software on general-purpose servers. Our main insight, discerned from dissecting the operation of production SFU servers, is that SFUs largely mimic traditional packet-processing operations such as dropping and forwarding. Guided by this, we present Scallop, an SDN-inspired SFU that decouples video-conferencing applications into a hardware-based data plane for latency-sensitive and frequent media operations, and a software control plane for the (infrequent) remaining tasks, such as analyzing feedback signals. Our Tofino-based implementation fully supports WebRTC and delivers 7-210 times improved scaling over a 32-core commodity server, while reaping performance improvements by cutting forwarding-induced latency by 26 times. \u25b3 Less Submitted 14 March, 2025; originally announced March 2025. arXiv:2503.10144 [pdf, other] Multiplicative Learning Authors: Han Kim, Hyungjoon Soh, Vipul Periwal, Junghyo Jo Abstract: Efficient training of artificial neural networks remains a key challenge in deep learning. Backpropagation (BP), the standard learning algorithm, relies on gradient descent and typically requires numerous iterations for convergence. In this study, we introduce Expectation Reflection (ER), a novel learning approach that updates weights multiplicatively based on the ratio of observed to predicted ou\u2026 \u25bd More Efficient training of artificial neural networks remains a key challenge in deep learning. Backpropagation (BP), the standard learning algorithm, relies on gradient descent and typically requires numerous iterations for convergence. In this study, we introduce Expectation Reflection (ER), a novel learning approach that updates weights multiplicatively based on the ratio of observed to predicted outputs. Unlike traditional methods, ER maintains consistency without requiring ad hoc loss functions or learning rate hyperparameters. We extend ER to multilayer networks and demonstrate its effectiveness in performing image classification tasks. Notably, ER achieves optimal weight updates in a single iteration. Additionally, we reinterpret ER as a modified form of gradient descent incorporating the inverse mapping of target propagation. These findings suggest that ER provides an efficient and scalable alternative for training neural networks. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. arXiv:2503.09477 [pdf, other] Neural reservoir control of a soft bio-hybrid arm Authors: Noel Naughton, Arman Tekinalp, Keshav Shivam, Seung Hung Kim, Volodymyr Kindratenko, Mattia Gazzola Abstract: A long-standing engineering problem, the control of soft robots is difficult because of their highly non-linear, heterogeneous, anisotropic, and distributed nature. Here, bridging engineering and biology, a neural reservoir is employed for the dynamic control of a bio-hybrid model arm made of multiple muscle-tendon groups enveloping an elastic spine. We show how the use of reservoirs facilitates s\u2026 \u25bd More A long-standing engineering problem, the control of soft robots is difficult because of their highly non-linear, heterogeneous, anisotropic, and distributed nature. Here, bridging engineering and biology, a neural reservoir is employed for the dynamic control of a bio-hybrid model arm made of multiple muscle-tendon groups enveloping an elastic spine. We show how the use of reservoirs facilitates simultaneous control and self-modeling across a set of challenging tasks, outperforming classic neural network approaches. Further, by implementing a spiking reservoir on neuromorphic hardware, energy efficiency is achieved, with nearly two-orders of magnitude improvement relative to standard CPUs, with implications for the on-board control of untethered, small-scale soft robots. \u25b3 Less Submitted 12 March, 2025; originally announced March 2025. Comments: 12 pages; 4 figures arXiv:2503.08214 [pdf, other] Safety-Ensured Control Framework for Robotic Endoscopic Task Automation Authors: Yitaek Kim, I\u00f1igo Iturrate, Christoffer Sloth, Hansoul Kim Abstract: There is growing interest in automating surgical tasks using robotic systems, such as endoscopy for treating gastrointestinal (GI) cancer. However, previous studies have primarily focused on detecting and analyzing objects or robots, with limited attention to ensuring safety, which is critical for clinical applications, where accidents can be caused by unsafe robot motions. In this study, we propo\u2026 \u25bd More There is growing interest in automating surgical tasks using robotic systems, such as endoscopy for treating gastrointestinal (GI) cancer. However, previous studies have primarily focused on detecting and analyzing objects or robots, with limited attention to ensuring safety, which is critical for clinical applications, where accidents can be caused by unsafe robot motions. In this study, we propose a new control framework that can formally ensure the safety of automating certain processes involved in endoscopic submucosal dissection (ESD), a representative endoscopic surgical method for the treatment of early GI cancer, by using an endoscopic robot. The proposed framework utilizes Control Barrier Functions (CBFs) to accurately identify the boundaries of individual tumors, even in close proximity within the GI tract, ensuring precise treatment and removal while preserving the surrounding normal tissue. Additionally, by adopting a model-free control scheme, safety assurance is made possible even in endoscopic robotic systems where dynamic modeling is challenging. We demonstrate the proposed framework in cases where the tumors to be removed are close to each other, showing that the safety constraints are enforced. We show that the model-free CBF-based controlled robot eliminates one tumor completely without damaging it, while not invading another nearby tumor. \u25b3 Less Submitted 11 March, 2025; originally announced March 2025. Comments: This paper is submitted to IEEE Access arXiv:2503.08003 [pdf] How Can Video Generative AI Transform K-12 Education? Examining Teachers' Perspectives through TPACK and TAM Authors: Unggi Lee, Yeil Jeong, Seungha Kim, Yoorim Son, Gyuri Byun, Hyeoncheol Kim, Cheolil Lim Abstract: The rapid advancement of generative AI technology, particularly video generative AI (Video GenAI), has opened new possibilities for K-12 education by enabling the creation of dynamic, customized, and high-quality visual content. Despite its potential, there is limited research on how this emerging technology can be effectively integrated into educational practices. This study explores the perspect\u2026 \u25bd More The rapid advancement of generative AI technology, particularly video generative AI (Video GenAI), has opened new possibilities for K-12 education by enabling the creation of dynamic, customized, and high-quality visual content. Despite its potential, there is limited research on how this emerging technology can be effectively integrated into educational practices. This study explores the perspectives of leading K-12 teachers on the educational applications of Video GenAI, using the TPACK (Technological Pedagogical Content Knowledge) and TAM (Technology Acceptance Model) frameworks as analytical lenses. Through interviews and hands-on experimentation with video generation tools, the research identifies opportunities for enhancing teaching strategies, fostering student engagement, and supporting authentic task design. It also highlights challenges such as technical limitations, ethical considerations, and the need for institutional support. The findings provide actionable insights into how Video GenAI can transform teaching and learning, offering practical implications for policy, teacher training, and the future development of educational technology. \u25b3 Less Submitted 10 March, 2025; originally announced March 2025. arXiv:2503.07660 [pdf, other] Research on Superalignment Should Advance Now with Parallel Optimization of Competence and Conformity Authors: HyunJin Kim, Xiaoyuan Yi, Jing Yao, Muhua Huang, JinYeong Bak, James Evans, Xing Xie Abstract: The recent leap in AI capabilities, driven by big generative models, has sparked the possibility of achieving Artificial General Intelligence (AGI) and further triggered discussions on Artificial Superintelligence (ASI), a system surpassing all humans across all domains. This gives rise to the critical research question of: If we realize ASI, how do we align it with human values, ensuring it benef\u2026 \u25bd More The recent leap in AI capabilities, driven by big generative models, has sparked the possibility of achieving Artificial General Intelligence (AGI) and further triggered discussions on Artificial Superintelligence (ASI), a system surpassing all humans across all domains. This gives rise to the critical research question of: If we realize ASI, how do we align it with human values, ensuring it benefits rather than harms human society, a.k.a., the Superalignment problem. Despite ASI being regarded by many as solely a hypothetical concept, in this paper, we argue that superalignment is achievable and research on it should advance immediately, through simultaneous and alternating optimization of task competence and value conformity. We posit that superalignment is not merely a safeguard for ASI but also necessary for its realization. To support this position, we first provide a formal definition of superalignment rooted in the gap between capability and capacity and elaborate on our argument. Then we review existing paradigms, explore their interconnections and limitations, and illustrate a potential path to superalignment centered on two fundamental principles. We hope this work sheds light on a practical approach for developing the value-aligned next-generation AI, garnering greater benefits and reducing potential harms for humanity. \u25b3 Less Submitted 7 March, 2025; originally announced March 2025. arXiv:2503.07447 [pdf, ps, other] A new density limit for unanimity in majority dynamics on random graphs Authors: Jeong Han Kim, BaoLinh Tran Abstract: Majority dynamics is a process on a simple, undirected graph $G$ with an initial Red/Blue color for every vertex of $G$. Each day, each vertex updates its color following the majority among its neighbors, using its previous color for tie-breaking. The dynamics achieves \\textit{unanimity} if every vertex has the same color after finitely many days, and such color is said to \\textit{win}. When\u2026 \u25bd More Majority dynamics is a process on a simple, undirected graph $G$ with an initial Red/Blue color for every vertex of $G$. Each day, each vertex updates its color following the majority among its neighbors, using its previous color for tie-breaking. The dynamics achieves \\textit{unanimity} if every vertex has the same color after finitely many days, and such color is said to \\textit{win}. When $G$ is a $G(n,p)$ random graph, L. Tran and Vu (2019) found a codition in terms of $p$ and the initial difference $2\u0394$ beteween the sizes of the Red and Blue camps, such that unanimity is achieved with probability arbitrarily close to 1. They showed that if $p\u0394^2 \\gg1 $, $p\u0394\\geq 100$, and $p\\geq (1+\\varepsilon) n^{-1}\\log n$ for a positive constant $\\varepsilon$, then unanimity occurs with probability $1 - o(1)$. If $p$ is not extremely small, namely $p > \\log^{-1/16} n $, then Sah and Sawhney (2022) showed that the condition $p\u0394^2 \\gg 1$ is sufficient. If $n^{-1}\\log^2 n \\ll p \\ll n^{-1/2}\\log^{1/4} n$, we show that $p^{3/2}\u0394\\gg n^{-1/2}\\log n$ is enough. Since this condition holds if $p\u0394\\geq 100$ for $p$ in this range, this is an improvement of Tran's and Vu's result. For the closely related problem of finding the optimal condition for $p$ to achieve unanimity when the initial coloring is chosen uniformly at random among all possible Red/Blue assignments, our result implies a new lower bound $p \\gg n^{-2/3}\\log^{2/3} n$, which improves upon the previous bound of $n^{-3/5}\\log n$ by Chakraborti, Kim, Lee and T. Tran (2021). \u25b3 Less Submitted 10 March, 2025; originally announced March 2025. Comments: 22 pages, 0 figures MSC Class: 05C80; 05C82; 05C85; 05C90 ACM Class: G.2.2; F.2.2 arXiv:2503.06986 [pdf, other] ConcreTizer: Model Inversion Attack via Occupancy Classification and Dispersion Control for 3D Point Cloud Restoration Authors: Youngseok Kim, Sunwook Hwang, Hyung-Sin Kim, Saewoong Bahk Abstract: The growing use of 3D point cloud data in autonomous vehicles (AVs) has raised serious privacy concerns, particularly due to the sensitive information that can be extracted from 3D data. While model inversion attacks have been widely studied in the context of 2D data, their application to 3D point clouds remains largely unexplored. To fill this gap, we present the first in-depth study of model inv\u2026 \u25bd More The growing use of 3D point cloud data in autonomous vehicles (AVs) has raised serious privacy concerns, particularly due to the sensitive information that can be extracted from 3D data. While model inversion attacks have been widely studied in the context of 2D data, their application to 3D point clouds remains largely unexplored. To fill this gap, we present the first in-depth study of model inversion attacks aimed at restoring 3D point cloud scenes. Our analysis reveals the unique challenges, the inherent sparsity of 3D point clouds and the ambiguity between empty and non-empty voxels after voxelization, which are further exacerbated by the dispersion of non-empty voxels across feature extractor layers. To address these challenges, we introduce ConcreTizer, a simple yet effective model inversion attack designed specifically for voxel-based 3D point cloud data. ConcreTizer incorporates Voxel Occupancy Classification to distinguish between empty and non-empty voxels and Dispersion-Controlled Supervision to mitigate non-empty voxel dispersion. Extensive experiments on widely used 3D feature extractors and benchmark datasets, such as KITTI and Waymo, demonstrate that ConcreTizer concretely restores the original 3D point cloud scene from disrupted 3D feature data. Our findings highlight both the vulnerability of 3D data to inversion attacks and the urgent need for robust defense strategies. \u25b3 Less Submitted 10 March, 2025; originally announced March 2025. arXiv:2503.04856 [pdf, other] One-Shot is Enough: Consolidating Multi-Turn Attacks into Efficient Single-Turn Prompts for LLMs Authors: Junwoo Ha, Hyunjun Kim, Sangyoon Yu, Haon Park, Ashkan Yousefpour, Yuna Park, Suhyun Kim Abstract: Despite extensive safety enhancements in large language models (LLMs), multi-turn \"jailbreak\" conversations crafted by skilled human adversaries can still breach even the most sophisticated guardrails. However, these multi-turn attacks demand considerable manual effort, limiting their scalability. In this work, we introduce a novel approach called Multi-turn-to-Single-turn (M2S) that systematicall\u2026 \u25bd More Despite extensive safety enhancements in large language models (LLMs), multi-turn \"jailbreak\" conversations crafted by skilled human adversaries can still breach even the most sophisticated guardrails. However, these multi-turn attacks demand considerable manual effort, limiting their scalability. In this work, we introduce a novel approach called Multi-turn-to-Single-turn (M2S) that systematically converts multi-turn jailbreak prompts into single-turn attacks. Specifically, we propose three conversion strategies - Hyphenize, Numberize, and Pythonize - each preserving sequential context yet packaging it in a single query. Our experiments on the Multi-turn Human Jailbreak (MHJ) dataset show that M2S often increases or maintains high Attack Success Rates (ASRs) compared to original multi-turn conversations. Notably, using a StrongREJECT-based evaluation of harmfulness, M2S achieves up to 95.9% ASR on Mistral-7B and outperforms original multi-turn prompts by as much as 17.5% in absolute improvement on GPT-4o. Further analysis reveals that certain adversarial tactics, when consolidated into a single prompt, exploit structural formatting cues to evade standard policy checks. These findings underscore that single-turn attacks - despite being simpler and cheaper to conduct - can be just as potent, if not more, than their multi-turn counterparts. Our findings underscore the urgent need to reevaluate and reinforce LLM safety strategies, given how adversarial queries can be compacted into a single prompt while still retaining sufficient complexity to bypass existing safety measures. \u25b3 Less Submitted 6 March, 2025; originally announced March 2025. arXiv:2503.04780 [pdf, other] MV-CLAM: Multi-View Molecular Interpretation with Cross-Modal Projection via Language Model Authors: Sumin Ha, Jun Hyeong Kim, Yinhua Piao, Sun Kim Abstract: Human expertise in chemistry and biomedicine relies on contextual molecular understanding, a capability that large language models (LLMs) can extend through fine-grained alignment between molecular structures and text. Recent multimodal learning advances focus on cross-modal alignment, but existing molecule-text models ignore complementary information in different molecular views and rely on singl\u2026 \u25bd More Human expertise in chemistry and biomedicine relies on contextual molecular understanding, a capability that large language models (LLMs) can extend through fine-grained alignment between molecular structures and text. Recent multimodal learning advances focus on cross-modal alignment, but existing molecule-text models ignore complementary information in different molecular views and rely on single-view representations, limiting molecular understanding. Moreover, na\u00efve multi-view alignment strategies face two challenges: (1) separate aligned spaces with inconsistent mappings between molecule and text embeddings, and that (2) existing loss objectives fail to preserve complementary information for fine-grained alignment. This can limit the LLM's ability to fully understand the molecular properties. To address these issues, we propose MV-CLAM, a novel framework that aligns multi-view molecular representations into a unified textual space using a multi-query transformer (MQ-Former). Our approach ensures cross-view consistency while a token-level contrastive loss preserves diverse molecular features across textual queries. MV-CLAM enhances molecular reasoning, improving retrieval and captioning accuracy. The source code of MV-CLAM is available in https://github.com/sumin124/mv-clam.git. \u25b3 Less Submitted 23 February, 2025; originally announced March 2025. arXiv:2503.04459 [pdf, other] Question-Aware Gaussian Experts for Audio-Visual Question Answering Authors: Hongyeob Kim, Inyoung Jung, Dayoon Suh, Youjia Zhang, Sangmin Lee, Sungeun Hong Abstract: Audio-Visual Question Answering (AVQA) requires not only question-based multimodal reasoning but also precise temporal grounding to capture subtle dynamics for accurate prediction. However, existing methods mainly use question information implicitly, limiting focus on question-specific details. Furthermore, most studies rely on uniform frame sampling, which can miss key question-relevant frames. A\u2026 \u25bd More Audio-Visual Question Answering (AVQA) requires not only question-based multimodal reasoning but also precise temporal grounding to capture subtle dynamics for accurate prediction. However, existing methods mainly use question information implicitly, limiting focus on question-specific details. Furthermore, most studies rely on uniform frame sampling, which can miss key question-relevant frames. Although recent Top-K frame selection methods aim to address this, their discrete nature still overlooks fine-grained temporal details. This paper proposes QA-TIGER, a novel framework that explicitly incorporates question information and models continuous temporal dynamics. Our key idea is to use Gaussian-based modeling to adaptively focus on both consecutive and non-consecutive frames based on the question, while explicitly injecting question information and applying progressive refinement. We leverage a Mixture of Experts (MoE) to flexibly implement multiple Gaussian models, activating temporal experts specifically tailored to the question. Extensive experiments on multiple AVQA benchmarks show that QA-TIGER consistently achieves state-of-the-art performance. Code is available at https://aim-skku.github.io/QA-TIGER/ \u25b3 Less Submitted 7 March, 2025; v1 submitted 6 March, 2025; originally announced March 2025. Comments: CVPR 2025. Code is available at https://github.com/AIM-SKKU/QA-TIGER arXiv:2503.04257 [pdf, other] How to Move Your Dragon: Text-to-Motion Synthesis for Large-Vocabulary Objects Authors: Wonkwang Lee, Jongwon Jeong, Taehong Moon, Hyeon-Jong Kim, Jaehyeon Kim, Gunhee Kim, Byeong-Uk Lee Abstract: Motion synthesis for diverse object categories holds great potential for 3D content creation but remains underexplored due to two key challenges: (1) the lack of comprehensive motion datasets that include a wide range of high-quality motions and annotations, and (2) the absence of methods capable of handling heterogeneous skeletal templates from diverse objects. To address these challenges, we con\u2026 \u25bd More Motion synthesis for diverse object categories holds great potential for 3D content creation but remains underexplored due to two key challenges: (1) the lack of comprehensive motion datasets that include a wide range of high-quality motions and annotations, and (2) the absence of methods capable of handling heterogeneous skeletal templates from diverse objects. To address these challenges, we contribute the following: First, we augment the Truebones Zoo dataset, a high-quality animal motion dataset covering over 70 species, by annotating it with detailed text descriptions, making it suitable for text-based motion synthesis. Second, we introduce rig augmentation techniques that generate diverse motion data while preserving consistent dynamics, enabling models to adapt to various skeletal configurations. Finally, we redesign existing motion diffusion models to dynamically adapt to arbitrary skeletal templates, enabling motion synthesis for a diverse range of objects with varying structures. Experiments show that our method learns to generate high-fidelity motions from textual descriptions for diverse and even unseen objects, setting a strong foundation for motion synthesis across diverse object categories and skeletal templates. Qualitative results are available on this link: t2m4lvo.github.io \u25b3 Less Submitted 6 March, 2025; originally announced March 2025. arXiv:2503.03796 [pdf, other] Human Implicit Preference-Based Policy Fine-tuning for Multi-Agent Reinforcement Learning in USV Swarm Authors: Hyeonjun Kim, Kanghoon Lee, Junho Park, Jiachen Li, Jinkyoo Park Abstract: Multi-Agent Reinforcement Learning (MARL) has shown promise in solving complex problems involving cooperation and competition among agents, such as an Unmanned Surface Vehicle (USV) swarm used in search and rescue, surveillance, and vessel protection. However, aligning system behavior with user preferences is challenging due to the difficulty of encoding expert intuition into reward functions. To\u2026 \u25bd More Multi-Agent Reinforcement Learning (MARL) has shown promise in solving complex problems involving cooperation and competition among agents, such as an Unmanned Surface Vehicle (USV) swarm used in search and rescue, surveillance, and vessel protection. However, aligning system behavior with user preferences is challenging due to the difficulty of encoding expert intuition into reward functions. To address the issue, we propose a Reinforcement Learning with Human Feedback (RLHF) approach for MARL that resolves credit-assignment challenges through an Agent-Level Feedback system categorizing feedback into intra-agent, inter-agent, and intra-team types. To overcome the challenges of direct human feedback, we employ a Large Language Model (LLM) evaluator to validate our approach using feedback scenarios such as region constraints, collision avoidance, and task allocation. Our method effectively refines USV swarm policies, addressing key challenges in multi-agent systems while maintaining fairness and performance consistency. \u25b3 Less Submitted 7 March, 2025; v1 submitted 5 March, 2025; originally announced March 2025. Comments: 7 pages, 4 figures arXiv:2503.03753 [pdf, other] Generative Diffusion Model-based Compression of MIMO CSI Authors: Heasung Kim, Taekyun Lee, Hyeji Kim, Gustavo De Veciana, Mohamed Amine Arfaoui, Asil Koc, Phil Pietraski, Guodong Zhang, John Kaewell Abstract: While neural lossy compression techniques have markedly advanced the efficiency of Channel State Information (CSI) compression and reconstruction for feedback in MIMO communications, efficient algorithms for more challenging and practical tasks-such as CSI compression for future channel prediction and reconstruction with relevant side information-remain underexplored, often resulting in suboptimal\u2026 \u25bd More While neural lossy compression techniques have markedly advanced the efficiency of Channel State Information (CSI) compression and reconstruction for feedback in MIMO communications, efficient algorithms for more challenging and practical tasks-such as CSI compression for future channel prediction and reconstruction with relevant side information-remain underexplored, often resulting in suboptimal performance when existing methods are extended to these scenarios. To that end, we propose a novel framework for compression with side information, featuring an encoding process with fixed-rate compression using a trainable codebook for codeword quantization, and a decoding procedure modeled as a backward diffusion process conditioned on both the codeword and the side information. Experimental results show that our method significantly outperforms existing CSI compression algorithms, often yielding over twofold performance improvement by achieving comparable distortion at less than half the data rate of competing methods in certain scenarios. These findings underscore the potential of diffusion-based compression for practical deployment in communication systems. \u25b3 Less Submitted 6 February, 2025; originally announced March 2025. Comments: 6 pages MSC Class: 68P30 ACM Class: I.2.0 arXiv:2503.02899 [pdf, other] OCL: Ordinal Contrastive Learning for Imputating Features with Progressive Labels Authors: Seunghun Baek, Jaeyoon Sim, Guorong Wu, Won Hwa Kim Abstract: Accurately discriminating progressive stages of Alzheimer's Disease (AD) is crucial for early diagnosis and prevention. It often involves multiple imaging modalities to understand the complex pathology of AD, however, acquiring a complete set of images is challenging due to high cost and burden for subjects. In the end, missing data become inevitable which lead to limited sample-size and decrease\u2026 \u25bd More Accurately discriminating progressive stages of Alzheimer's Disease (AD) is crucial for early diagnosis and prevention. It often involves multiple imaging modalities to understand the complex pathology of AD, however, acquiring a complete set of images is challenging due to high cost and burden for subjects. In the end, missing data become inevitable which lead to limited sample-size and decrease in precision in downstream analyses. To tackle this challenge, we introduce a holistic imaging feature imputation method that enables to leverage diverse imaging features while retaining all subjects. The proposed method comprises two networks: 1) An encoder to extract modality-independent embeddings and 2) A decoder to reconstruct the original measures conditioned on their imaging modalities. The encoder includes a novel {\\em ordinal contrastive loss}, which aligns samples in the embedding space according to the progression of AD. We also maximize modality-wise coherence of embeddings within each subject, in conjunction with domain adversarial training algorithms, to further enhance alignment between different imaging modalities. The proposed method promotes our holistic imaging feature imputation across various modalities in the shared embedding space. In the experiments, we show that our networks deliver favorable results for statistical analysis and classification against imputation baselines with Alzheimer's Disease Neuroimaging Initiative (ADNI) study. \u25b3 Less Submitted 3 March, 2025; originally announced March 2025. Comments: MICCAI 2024 (Provisional Accept) arXiv:2503.02898 [pdf, other] Modality-Agnostic Style Transfer for Holistic Feature Imputation Authors: Seunghun Baek, Jaeyoon Sim, Mustafa Dere, Minjeong Kim, Guorong Wu, Won Hwa Kim Abstract: Characterizing a preclinical stage of Alzheimer's Disease (AD) via single imaging is difficult as its early symptoms are quite subtle. Therefore, many neuroimaging studies are curated with various imaging modalities, e.g., MRI and PET, however, it is often challenging to acquire all of them from all subjects and missing data become inevitable. In this regards, in this paper, we propose a framework\u2026 \u25bd More Characterizing a preclinical stage of Alzheimer's Disease (AD) via single imaging is difficult as its early symptoms are quite subtle. Therefore, many neuroimaging studies are curated with various imaging modalities, e.g., MRI and PET, however, it is often challenging to acquire all of them from all subjects and missing data become inevitable. In this regards, in this paper, we propose a framework that generates unobserved imaging measures for specific subjects using their existing measures, thereby reducing the need for additional examinations. Our framework transfers modality-specific style while preserving AD-specific content. This is done by domain adversarial training that preserves modality-agnostic but AD-specific information, while a generative adversarial network adds an indistinguishable modality-specific style. Our proposed framework is evaluated on the Alzheimer's Disease Neuroimaging Initiative (ADNI) study and compared with other imputation methods in terms of generated data quality. Small average Cohen's $d$ $< 0.19$ between our generated measures and real ones suggests that the synthetic data are practically usable regardless of their modality type. \u25b3 Less Submitted 3 March, 2025; originally announced March 2025. Comments: ISBI 2024 (oral) arXiv:2503.02645 [pdf, other] A Generalized Theory of Mixup for Structure-Preserving Synthetic Data Authors: Chungpa Lee, Jongho Im, Joseph H. T. Kim Abstract: Mixup is a widely adopted data augmentation technique known for enhancing the generalization of machine learning models by interpolating between data points. Despite its success and popularity, limited attention has been given to understanding the statistical properties of the synthetic data it generates. In this paper, we delve into the theoretical underpinnings of mixup, specifically its effects\u2026 \u25bd More Mixup is a widely adopted data augmentation technique known for enhancing the generalization of machine learning models by interpolating between data points. Despite its success and popularity, limited attention has been given to understanding the statistical properties of the synthetic data it generates. In this paper, we delve into the theoretical underpinnings of mixup, specifically its effects on the statistical structure of synthesized data. We demonstrate that while mixup improves model performance, it can distort key statistical properties such as variance, potentially leading to unintended consequences in data synthesis. To address this, we propose a novel mixup method that incorporates a generalized and flexible weighting scheme, better preserving the original data's structure. Through theoretical developments, we provide conditions under which our proposed method maintains the (co)variance and distributional properties of the original dataset. Numerical experiments confirm that the new approach not only preserves the statistical characteristics of the original data but also sustains model performance across repeated synthesis, alleviating concerns of model collapse identified in previous research. \u25b3 Less Submitted 3 March, 2025; originally announced March 2025. Journal ref: Proceedings of the 28th International Conference on Artificial Intelligence and Statistics (AISTATS) 2025 arXiv:2503.02170 [pdf, other] Adaptive Camera Sensor for Vision Models Authors: Eunsu Baek, Sunghwan Han, Taesik Gong, Hyung-Sin Kim Abstract: Domain shift remains a persistent challenge in deep-learning-based computer vision, often requiring extensive model modifications or large labeled datasets to address. Inspired by human visual perception, which adjusts input quality through corrective lenses rather than over-training the brain, we propose Lens, a novel camera sensor control method that enhances model performance by capturing high-\u2026 \u25bd More Domain shift remains a persistent challenge in deep-learning-based computer vision, often requiring extensive model modifications or large labeled datasets to address. Inspired by human visual perception, which adjusts input quality through corrective lenses rather than over-training the brain, we propose Lens, a novel camera sensor control method that enhances model performance by capturing high-quality images from the model's perspective rather than relying on traditional human-centric sensor control. Lens is lightweight and adapts sensor parameters to specific models and scenes in real-time. At its core, Lens utilizes VisiT, a training-free, model-specific quality indicator that evaluates individual unlabeled samples at test time using confidence scores without additional adaptation costs. To validate Lens, we introduce ImageNet-ES Diverse, a new benchmark dataset capturing natural perturbations from varying sensor and lighting conditions. Extensive experiments on both ImageNet-ES and our new ImageNet-ES Diverse show that Lens significantly improves model accuracy across various baseline schemes for sensor control and model modification while maintaining low latency in image captures. Lens effectively compensates for large model size differences and integrates synergistically with model improvement techniques. Our code and dataset are available at github.com/Edw2n/Lens.git. \u25b3 Less Submitted 3 March, 2025; originally announced March 2025. Comments: The International Conference on Learning Representations (ICLR 2025) arXiv:2503.01232 [pdf, other] Learning Covariance-Based Multi-Scale Representation of Neuroimaging Measures for Alzheimer Classification Authors: Seunghun Baek, Injun Choi, Mustafa Dere, Minjeong Kim, Guorong Wu, Won Hwa Kim Abstract: Stacking excessive layers in DNN results in highly underdetermined system when training samples are limited, which is very common in medical applications. In this regard, we present a framework capable of deriving an efficient high-dimensional space with reasonable increase in model size. This is done by utilizing a transform (i.e., convolution) that leverages scale-space theory with covariance st\u2026 \u25bd More Stacking excessive layers in DNN results in highly underdetermined system when training samples are limited, which is very common in medical applications. In this regard, we present a framework capable of deriving an efficient high-dimensional space with reasonable increase in model size. This is done by utilizing a transform (i.e., convolution) that leverages scale-space theory with covariance structure. The overall model trains on this transform together with a downstream classifier (i.e., Fully Connected layer) to capture the optimal multi-scale representation of the original data which corresponds to task-specific components in a dual space. Experiments on neuroimaging measures from Alzheimer's Disease Neuroimaging Initiative (ADNI) study show that our model performs better and converges faster than conventional models even when the model size is significantly reduced. The trained model is made interpretable using gradient information over the multi-scale transform to delineate personalized AD-specific regions in the brain. \u25b3 Less Submitted 3 March, 2025; originally announced March 2025. Comments: ISBI 2023 arXiv:2503.00790 [pdf] Acoustic Anomaly Detection on UAM Propeller Defect with Acoustic dataset for Crack of drone Propeller (ADCP) Authors: Juho Lee, Donghyun Yoon, Gumoon Jeong, Hyeoncheol Kim Abstract: The imminent commercialization of UAM requires stable, AI-based maintenance systems to ensure safety for both passengers and pedestrians. This paper presents a methodology for non-destructively detecting cracks in UAM propellers using drone propeller sound datasets. Normal operating sounds were recorded, and abnormal sounds (categorized as ripped and broken) were differentiated by varying the micr\u2026 \u25bd More The imminent commercialization of UAM requires stable, AI-based maintenance systems to ensure safety for both passengers and pedestrians. This paper presents a methodology for non-destructively detecting cracks in UAM propellers using drone propeller sound datasets. Normal operating sounds were recorded, and abnormal sounds (categorized as ripped and broken) were differentiated by varying the microphone-propeller angle and throttle power. Our novel approach integrates FFT and STFT preprocessing techniques to capture both global frequency patterns and local time-frequency variations, thereby enhancing anomaly detection performance. The constructed Acoustic Dataset for Crack of Drone Propeller (ADCP) demonstrates the potential for detecting propeller cracks and lays the groundwork for future UAM maintenance applications. \u25b3 Less Submitted 2 March, 2025; originally announced March 2025. Comments: 25 pages arXiv:2503.00699 [pdf, other] Parameter Expanded Stochastic Gradient Markov Chain Monte Carlo Authors: Hyunsu Kim, Giung Nam, Chulhee Yun, Hongseok Yang, Juho Lee Abstract: Bayesian Neural Networks (BNNs) provide a promising framework for modeling predictive uncertainty and enhancing out-of-distribution robustness (OOD) by estimating the posterior distribution of network parameters. Stochastic Gradient Markov Chain Monte Carlo (SGMCMC) is one of the most powerful methods for scalable posterior sampling in BNNs, achieving efficiency by combining stochastic gradient de\u2026 \u25bd More Bayesian Neural Networks (BNNs) provide a promising framework for modeling predictive uncertainty and enhancing out-of-distribution robustness (OOD) by estimating the posterior distribution of network parameters. Stochastic Gradient Markov Chain Monte Carlo (SGMCMC) is one of the most powerful methods for scalable posterior sampling in BNNs, achieving efficiency by combining stochastic gradient descent with second-order Langevin dynamics. However, SGMCMC often suffers from limited sample diversity in practice, which affects uncertainty estimation and model performance. We propose a simple yet effective approach to enhance sample diversity in SGMCMC without the need for tempering or running multiple chains. Our approach reparameterizes the neural network by decomposing each of its weight matrices into a product of matrices, resulting in a sampling trajectory that better explores the target parameter space. This approach produces a more diverse set of samples, allowing faster mixing within the same computational budget. Notably, our sampler achieves these improvements without increasing the inference cost compared to the standard SGMCMC. Extensive experiments on image classification tasks, including OOD robustness, diversity, loss surface analyses, and a comparative study with Hamiltonian Monte Carlo, demonstrate the superiority of the proposed approach. \u25b3 Less Submitted 1 March, 2025; originally announced March 2025. Journal ref: ICLR 2025 arXiv:2503.00344 [pdf, other] Legged Robot State Estimation Using Invariant Neural-Augmented Kalman Filter with a Neural Compensator Authors: Seokju Lee, Hyun-Bin Kim, Kyung-Soo Kim Abstract: This paper presents an algorithm to improve state estimation for legged robots. Among existing model-based state estimation methods for legged robots, the contact-aided invariant extended Kalman filter defines the state on a Lie group to preserve invariance, thereby significantly accelerating convergence. It achieves more accurate state estimation by leveraging contact information as measurements\u2026 \u25bd More This paper presents an algorithm to improve state estimation for legged robots. Among existing model-based state estimation methods for legged robots, the contact-aided invariant extended Kalman filter defines the state on a Lie group to preserve invariance, thereby significantly accelerating convergence. It achieves more accurate state estimation by leveraging contact information as measurements for the update step. However, when the model exhibits strong nonlinearity, the estimation accuracy decreases. Such nonlinearities can cause initial errors to accumulate and lead to large drifts over time. To address this issue, we propose compensating for errors by augmenting the Kalman filter with an artificial neural network serving as a nonlinear function approximator. Furthermore, we design this neural network to respect the Lie group structure to ensure invariance, resulting in our proposed Invariant Neural-Augmented Kalman Filter (InNKF). The proposed algorithm offers improved state estimation performance by combining the strengths of model-based and learning-based approaches. Supplementary Video: https://youtu.be/k1ZVb6Xj8D8 \u25b3 Less Submitted 28 February, 2025; originally announced March 2025. Comments: 8 pages, 10 figures arXiv:2502.20727 [pdf, other] SPD: Sync-Point Drop for efficient tensor parallelism of Large Language Models Authors: Han-Byul Kim, Duc Hoang, Arnav Kundu, Mohammad Samragh, Minsik Cho Abstract: With the rapid expansion in the scale of large language models (LLMs), enabling efficient distributed inference across multiple computing units has become increasingly critical. However, communication overheads from popular distributed inference techniques such as Tensor Parallelism pose a significant challenge to achieve scalability and low latency. Therefore, we introduce a novel optimization te\u2026 \u25bd More With the rapid expansion in the scale of large language models (LLMs), enabling efficient distributed inference across multiple computing units has become increasingly critical. However, communication overheads from popular distributed inference techniques such as Tensor Parallelism pose a significant challenge to achieve scalability and low latency. Therefore, we introduce a novel optimization technique, Sync-Point Drop (SPD), to reduce communication overheads in tensor parallelism by selectively dropping synchronization on attention outputs. In detail, we first propose a block design that allows execution to proceed without communication through SPD. Second, we apply different SPD strategies to attention blocks based on their sensitivity to the model accuracy. The proposed methods effectively alleviate communication bottlenecks while minimizing accuracy degradation during LLM inference, offering a scalable solution for diverse distributed environments: SPD offered about 20% overall inference latency reduction with < 1% accuracy regression for LLaMA2-70B inference over 8 GPUs. \u25b3 Less Submitted 28 February, 2025; originally announced February 2025. Comments: Preprint arXiv:2502.20122 [pdf, other] Self-Training Elicits Concise Reasoning in Large Language Models Authors: Tergel Munkhbat, Namgyu Ho, Seo Hyun Kim, Yongjin Yang, Yujin Kim, Se-Young Yun Abstract: Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to utilize additional computation through intermediate tokens to solve complex tasks. However, we posit that typical reasoning traces contain many redundant tokens, incurring extraneous inference costs. Upon examination of the output distribution of current LLMs, we find evidence on their latent ability to reason more concis\u2026 \u25bd More Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to utilize additional computation through intermediate tokens to solve complex tasks. However, we posit that typical reasoning traces contain many redundant tokens, incurring extraneous inference costs. Upon examination of the output distribution of current LLMs, we find evidence on their latent ability to reason more concisely, relative to their default behavior. To elicit this capability, we propose simple fine-tuning methods which leverage self-generated concise reasoning paths obtained by best-of-N sampling and few-shot conditioning, in task-specific settings. Our combined method achieves a 30% reduction in output tokens on average, across five model families on GSM8K and MATH, while maintaining average accuracy. By exploiting the fundamental stochasticity and in-context learning capabilities of LLMs, our self-training approach robustly elicits concise reasoning on a wide range of models, including those with extensive post-training. Code is available at https://github.com/TergelMunkhbat/concise-reasoning \u25b3 Less Submitted 28 February, 2025; v1 submitted 27 February, 2025; originally announced February 2025. Comments: 23 pages, 10 figures, 18 tables arXiv:2502.19765 [pdf, other] EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion Language Models Authors: Che Hyun Lee, Heeseung Kim, Jiheum Yeom, Sungroh Yoon Abstract: We propose EdiText, a controllable text editing method that modify the reference text to desired attributes at various scales. We integrate an SDEdit-based editing technique that allows for broad adjustments in the degree of text editing. Additionally, we introduce a novel fine-level editing method based on self-conditioning, which allows subtle control of reference text. While being capable of ed\u2026 \u25bd More We propose EdiText, a controllable text editing method that modify the reference text to desired attributes at various scales. We integrate an SDEdit-based editing technique that allows for broad adjustments in the degree of text editing. Additionally, we introduce a novel fine-level editing method based on self-conditioning, which allows subtle control of reference text. While being capable of editing on its own, this fine-grained method, integrated with the SDEdit approach, enables EdiText to make precise adjustments within the desired range. EdiText demonstrates its controllability to robustly adjust reference text at broad range of levels across various tasks, including toxicity control and sentiment control. \u25b3 Less Submitted 27 February, 2025; originally announced February 2025. arXiv:2502.19759 [pdf, other] Does Your Voice Assistant Remember? Analyzing Conversational Context Recall and Utilization in Voice Interaction Models Authors: Heeseung Kim, Che Hyun Lee, Sangkwon Park, Jiheum Yeom, Nohil Park, Sangwon Yu, Sungroh Yoon Abstract: Recent advancements in multi-turn voice interaction models have improved user-model communication. However, while closed-source models effectively retain and recall past utterances, whether open-source models share this ability remains unexplored. To fill this gap, we systematically evaluate how well open-source interaction models utilize past utterances using ContextDialog, a benchmark we propose\u2026 \u25bd More Recent advancements in multi-turn voice interaction models have improved user-model communication. However, while closed-source models effectively retain and recall past utterances, whether open-source models share this ability remains unexplored. To fill this gap, we systematically evaluate how well open-source interaction models utilize past utterances using ContextDialog, a benchmark we proposed for this purpose. Our findings show that speech-based models have more difficulty than text-based ones, especially when recalling information conveyed in speech, and even with retrieval-augmented generation, models still struggle with questions about past utterances. These insights highlight key limitations in open-source models and suggest ways to improve memory retention and retrieval robustness. \u25b3 Less Submitted 26 February, 2025; originally announced February 2025. Comments: Work in Progress, Project Page: https://contextdialog.github.io/ arXiv:2502.19399 [pdf, other] DROID: Discrete-Time Simulation for Ring-Oscillator-Based Ising Design Authors: Abhimanyu Kumar, Ramprasath S., Chris H. Kim, Ulya R. Karpuzcu, Sachin S. Sapatnekar Abstract: Many combinatorial problems can be mapped to Ising machines, i.e., networks of coupled oscillators that settle to a minimum-energy ground state, from which the problem solution is inferred. This work proposes DROID, a novel event-driven method for simulating the evolution of a CMOS Ising machine to its ground state. The approach is accurate under general delay-phase relations that include the effe\u2026 \u25bd More Many combinatorial problems can be mapped to Ising machines, i.e., networks of coupled oscillators that settle to a minimum-energy ground state, from which the problem solution is inferred. This work proposes DROID, a novel event-driven method for simulating the evolution of a CMOS Ising machine to its ground state. The approach is accurate under general delay-phase relations that include the effects of the transistor nonlinearities and is computationally efficient. On a realistic-size all-to-all coupled ring oscillator array, DROID is nearly four orders of magnitude faster than a traditional HSPICE simulation in predicting the evolution of a coupled oscillator system and is demonstrated to attain a similar distribution of solutions as the hardware. \u25b3 Less Submitted 26 February, 2025; originally announced February 2025. arXiv:2502.18934 [pdf, other] Kanana: Compute-efficient Bilingual Language Models Authors: Kanana LLM Team, Yunju Bak, Hojin Lee, Minho Ryu, Jiyeon Ham, Seungjae Jung, Daniel Wontae Nam, Taegyeong Eo, Donghun Lee, Doohae Jung, Boseop Kim, Nayeon Kim, Jaesun Park, Hyunho Kim, Hyunwoong Ko, Changmin Lee, Kyoung-Woon On, Seulye Baeg, Junrae Cho, Sunghee Jung, Jieun Kang, EungGyun Kim, Eunhwa Kim, Byeongil Ko, Daniel Lee , et al. (4 additional authors not shown) Abstract: We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English. The computational cost of Kanana is significantly lower than that of state-of-the-art models of similar size. The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high quality dat\u2026 \u25bd More We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English. The computational cost of Kanana is significantly lower than that of state-of-the-art models of similar size. The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation. Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users. Lastly, the report elaborates on plausible approaches used for language model adaptation to specific scenarios, such as embedding, retrieval augmented generation, and function calling. The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models. \u25b3 Less Submitted 28 February, 2025; v1 submitted 26 February, 2025; originally announced February 2025. Comments: 40 pages, 15 figures arXiv:2502.18881 [pdf, other] Letters from Future Self: Augmenting the Letter-Exchange Exercise with LLM-based Agents to Enhance Young Adults' Career Exploration Authors: Hayeon Jeon, Suhwoo Yoon, Keyeun Lee, Seo Hyeong Kim, Esther Hehsun Kim, Seonghye Cho, Yena Ko, Soeun Yang, Laura Dabbish, John Zimmerman, Eun-mee Kim, Hajin Lim Abstract: Young adults often encounter challenges in career exploration. Self-guided interventions, such as the letter-exchange exercise, where participants envision and adopt the perspective of their future selves by exchanging letters with their envisioned future selves, can support career development. However, the broader adoption of such interventions may be limited without structured guidance. To addre\u2026 \u25bd More Young adults often encounter challenges in career exploration. Self-guided interventions, such as the letter-exchange exercise, where participants envision and adopt the perspective of their future selves by exchanging letters with their envisioned future selves, can support career development. However, the broader adoption of such interventions may be limited without structured guidance. To address this, we integrated Large Language Model (LLM)-based agents that simulate participants' future selves into the letter-exchange exercise and evaluated their effectiveness. A one-week experiment (N=36) compared three conditions: (1) participants manually writing replies to themselves from the perspective of their future selves (baseline), (2) future-self agents generating letters to participants, and (3) future-self agents engaging in chat conversations with participants. Results indicated that exchanging letters with future-self agents enhanced participants' engagement during the exercise, while overall benefits of the intervention on future orientation, career self-concept, and psychological support remained comparable across conditions. We discuss design implications for AI-augmented interventions for supporting young adults' career exploration. \u25b3 Less Submitted 26 February, 2025; originally announced February 2025. Comments: 21 pages, 9 figures, Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems arXiv:2502.18871 [pdf, ps, other] Inscanner: Dual-Phase Detection and Classification of Auxiliary Insulation Using YOLOv8 Models Authors: Youngtae Kim, Soonju Jeong, Sardar Arslan, Dhananjay Agnihotri, Yahya Ahmed, Ali Nawaz, Jinhee Song, Hyewon Kim Abstract: This study proposes a two-phase methodology for detecting and classifying auxiliary insulation in structural components. In the detection phase, a YOLOv8x model is trained on a dataset of complete structural blueprints, each annotated with bounding boxes indicating areas that should contain insulation. In the classification phase, these detected insulation patches are cropped and categorized into\u2026 \u25bd More This study proposes a two-phase methodology for detecting and classifying auxiliary insulation in structural components. In the detection phase, a YOLOv8x model is trained on a dataset of complete structural blueprints, each annotated with bounding boxes indicating areas that should contain insulation. In the classification phase, these detected insulation patches are cropped and categorized into two classes: present or missing. These are then used to train a YOLOv8x-CLS model that determines the presence or absence of auxiliary insulation. Preprocessing steps for both datasets included annotation, augmentation, and appropriate cropping of the insulation regions. The detection model achieved a mean average precision (mAP) score of 82%, while the classification model attained an accuracy of 98%. These findings demonstrate the effectiveness of the proposed approach in automating insulation detection and classification, providing a foundation for further advancements in this domain. \u25b3 Less Submitted 26 February, 2025; originally announced February 2025. arXiv:2502.18853 [pdf, other] Reimagining Personal Data: Unlocking the Potential of AI-Generated Images in Personal Data Meaning-Making Authors: Soobin Park, Hankyung Kim, Youn-kyung Lim Abstract: Image-generative AI provides new opportunities to transform personal data into alternative visual forms. In this paper, we illustrate the potential of AI-generated images in facilitating meaningful engagement with personal data. In a formative autobiographical design study, we explored the design and use of AI-generated images derived from personal data. Informed by this study, we designed a web-b\u2026 \u25bd More Image-generative AI provides new opportunities to transform personal data into alternative visual forms. In this paper, we illustrate the potential of AI-generated images in facilitating meaningful engagement with personal data. In a formative autobiographical design study, we explored the design and use of AI-generated images derived from personal data. Informed by this study, we designed a web-based application as a probe that represents personal data through generative images utilizing Open AI's GPT-4 model and DALL-E 3. We then conducted a 21-day diary study and interviews using the probe with 16 participants to investigate users' in-depth experiences with images generated by AI in everyday lives. Our findings reveal new qualities of experiences in users' engagement with data, highlighting how participants constructed personal meaning from their data through imagination and speculation on AI-generated images. We conclude by discussing the potential and concerns of leveraging image-generative AI for personal data meaning-making. \u25b3 Less Submitted 26 February, 2025; originally announced February 2025. Comments: 21 pages excluding reference and appendix. Accepted at ACM CHI 2025 ACM Class: H.5.0 arXiv:2502.18196 [pdf, ps, other] Machine Learning for Future Wireless Communications: Channel Prediction Perspectives Authors: Hwanjin Kim, Junil Choi, David J. Love Abstract: Precise channel state knowledge is crucial in future wireless communication systems, which drives the need for accurate channel prediction without additional pilot overhead. While machine-learning (ML) methods for channel prediction show potential, existing approaches have limitations in their capability to adapt to environmental changes due to their extensive training requirements. In this paper,\u2026 \u25bd More Precise channel state knowledge is crucial in future wireless communication systems, which drives the need for accurate channel prediction without additional pilot overhead. While machine-learning (ML) methods for channel prediction show potential, existing approaches have limitations in their capability to adapt to environmental changes due to their extensive training requirements. In this paper, we introduce the channel prediction approaches in terms of the temporal channel prediction and the environmental adaptation. Then, we elaborate on the use of the advanced ML-based channel prediction to resolve the issues in traditional ML methods. The numerical results show that the advanced ML-based channel prediction has comparable accuracy with much less training overhead compared to conventional prediction methods. Also, we examine the training process, dataset characteristics, and the impact of source tasks and pre-trained models on channel prediction approaches. Finally, we discuss open challenges and possible future research directions of ML-based channel prediction. \u25b3 Less Submitted 25 February, 2025; originally announced February 2025. Comments: 7 pages, 3 figures, 2 tables, submitted to IEEE Communications Magazine arXiv:2502.17481 [pdf, other] Toward Foundational Model for Sleep Analysis Using a Multimodal Hybrid Self-Supervised Learning Framework Authors: Cheol-Hui Lee, Hakseung Kim, Byung C. Yoon, Dong-Joo Kim Abstract: Sleep is essential for maintaining human health and quality of life. Analyzing physiological signals during sleep is critical in assessing sleep quality and diagnosing sleep disorders. However, manual diagnoses by clinicians are time-intensive and subjective. Despite advances in deep learning that have enhanced automation, these approaches remain heavily dependent on large-scale labeled datasets.\u2026 \u25bd More Sleep is essential for maintaining human health and quality of life. Analyzing physiological signals during sleep is critical in assessing sleep quality and diagnosing sleep disorders. However, manual diagnoses by clinicians are time-intensive and subjective. Despite advances in deep learning that have enhanced automation, these approaches remain heavily dependent on large-scale labeled datasets. This study introduces SynthSleepNet, a multimodal hybrid self-supervised learning framework designed for analyzing polysomnography (PSG) data. SynthSleepNet effectively integrates masked prediction and contrastive learning to leverage complementary features across multiple modalities, including electroencephalogram (EEG), electrooculography (EOG), electromyography (EMG), and electrocardiogram (ECG). This approach enables the model to learn highly expressive representations of PSG data. Furthermore, a temporal context module based on Mamba was developed to efficiently capture contextual information across signals. SynthSleepNet achieved superior performance compared to state-of-the-art methods across three downstream tasks: sleep-stage classification, apnea detection, and hypopnea detection, with accuracies of 89.89%, 99.75%, and 89.60%, respectively. The model demonstrated robust performance in a semi-supervised learning environment with limited labels, achieving accuracies of 87.98%, 99.37%, and 77.52% in the same tasks. These results underscore the potential of the model as a foundational tool for the comprehensive analysis of PSG data. SynthSleepNet demonstrates comprehensively superior performance across multiple downstream tasks compared to other methodologies, making it expected to set a new standard for sleep disorder monitoring and diagnostic systems. \u25b3 Less Submitted 28 February, 2025; v1 submitted 18 February, 2025; originally announced February 2025. Comments: 18 pages, 5 figures arXiv:2502.17470 [pdf, other] MC2SleepNet: Multi-modal Cross-masking with Contrastive Learning for Sleep Stage Classification Authors: Younghoon Na, Hyun Keun Ahn, Hyun-Kyung Lee, Yoongeol Lee, Seung Hun Oh, Hongkwon Kim, Jeong-Gun Lee Abstract: Sleep profoundly affects our health, and sleep deficiency or disorders can cause physical and mental problems. Despite significant findings from previous studies, challenges persist in optimizing deep learning models, especially in multi-modal learning for high-accuracy sleep stage classification. Our research introduces MC2SleepNet (Multi-modal Cross-masking with Contrastive learning for Sleep st\u2026 \u25bd More Sleep profoundly affects our health, and sleep deficiency or disorders can cause physical and mental problems. Despite significant findings from previous studies, challenges persist in optimizing deep learning models, especially in multi-modal learning for high-accuracy sleep stage classification. Our research introduces MC2SleepNet (Multi-modal Cross-masking with Contrastive learning for Sleep stage classification Network). It aims to facilitate the effective collaboration between Convolutional Neural Networks (CNNs) and Transformer architectures for multi-modal training with the help of contrastive learning and cross-masking. Raw single channel EEG signals and corresponding spectrogram data provide differently characterized modalities for multi-modal learning. Our MC2SleepNet has achieved state-of-the-art performance with an accuracy of both 84.6% on the SleepEDF-78 and 88.6% accuracy on the Sleep Heart Health Study (SHHS). These results demonstrate the effective generalization of our proposed network across both small and large datasets. \u25b3 Less Submitted 26 February, 2025; v1 submitted 13 February, 2025; originally announced February 2025. arXiv:2502.16843 [pdf, other] Online Friction Coefficient Identification for Legged Robots on Slippery Terrain Using Smoothed Contact Gradients Authors: Hajun Kim, Dongyun Kang, Min-Gyu Kim, Gijeong Kim, Hae-Won Park Abstract: This paper proposes an online friction coefficient identification framework for legged robots on slippery terrain. The approach formulates the optimization problem to minimize the sum of residuals between actual and predicted states parameterized by the friction coefficient in rigid body contact dynamics. Notably, the proposed framework leverages the analytic smoothed gradient of contact impulses,\u2026 \u25bd More This paper proposes an online friction coefficient identification framework for legged robots on slippery terrain. The approach formulates the optimization problem to minimize the sum of residuals between actual and predicted states parameterized by the friction coefficient in rigid body contact dynamics. Notably, the proposed framework leverages the analytic smoothed gradient of contact impulses, obtained by smoothing the complementarity condition of Coulomb friction, to solve the issue of non-informative gradients induced from the nonsmooth contact dynamics. Moreover, we introduce the rejection method to filter out data with high normal contact velocity following contact initiations during friction coefficient identification for legged robots. To validate the proposed framework, we conduct the experiments using a quadrupedal robot platform, KAIST HOUND, on slippery and nonslippery terrain. We observe that our framework achieves fast and consistent friction coefficient identification within various initial conditions. \u25b3 Less Submitted 24 February, 2025; originally announced February 2025. Comments: 8 pages, IEEE RA-L (2025) accepted Journal ref: IEEE Robotics and Automation Letters, April 2025, Volume 10, Issue 4, Pages: 3150-3157 arXiv:2502.16457 [pdf, other] Towards Fully-Automated Materials Discovery via Large-Scale Synthesis Dataset and Expert-Level LLM-as-a-Judge Authors: Heegyu Kim, Taeyang Jeon, Seungtaek Choi, Ji Hoon Hong, Dong Won Jeon, Ga-Yeon Baek, Gyeong-Won Kwak, Dong-Hee Lee, Jisu Bae, Chihoon Lee, Yunseo Kim, Seon-Jin Choi, Jin-Seong Park, Sung Beom Cho, Hyunsouk Cho Abstract: Materials synthesis is vital for innovations such as energy storage, catalysis, electronics, and biomedical devices. Yet, the process relies heavily on empirical, trial-and-error methods guided by expert intuition. Our work aims to support the materials science community by providing a practical, data-driven resource. We have curated a comprehensive dataset of 17K expert-verified synthesis recipes\u2026 \u25bd More Materials synthesis is vital for innovations such as energy storage, catalysis, electronics, and biomedical devices. Yet, the process relies heavily on empirical, trial-and-error methods guided by expert intuition. Our work aims to support the materials science community by providing a practical, data-driven resource. We have curated a comprehensive dataset of 17K expert-verified synthesis recipes from open-access literature, which forms the basis of our newly developed benchmark, AlchemyBench. AlchemyBench offers an end-to-end framework that supports research in large language models applied to synthesis prediction. It encompasses key tasks, including raw materials and equipment prediction, synthesis procedure generation, and characterization outcome forecasting. We propose an LLM-as-a-Judge framework that leverages large language models for automated evaluation, demonstrating strong statistical agreement with expert assessments. Overall, our contributions offer a supportive foundation for exploring the capabilities of LLMs in predicting and guiding materials synthesis, ultimately paving the way for more efficient experimental design and accelerated innovation in materials science. \u25b3 Less Submitted 19 March, 2025; v1 submitted 23 February, 2025; originally announced February 2025. Comments: under review arXiv:2502.13648 [pdf, other] Reliability Across Parametric and External Knowledge: Understanding Knowledge Handling in LLMs Authors: Youna Kim, Minjoon Choi, Sungmin Cho, Hyuhng Joon Kim, Sang-goo Lee, Taeuk Kim Abstract: Large Language Models (LLMs) enhance their problem-solving capability by leveraging both parametric and external knowledge. Beyond leveraging external knowledge to improve response accuracy, they require key capabilities for reliable knowledge-handling: resolving conflicts between knowledge sources, avoiding distraction from uninformative external knowledge, and abstaining when sufficient knowledg\u2026 \u25bd More Large Language Models (LLMs) enhance their problem-solving capability by leveraging both parametric and external knowledge. Beyond leveraging external knowledge to improve response accuracy, they require key capabilities for reliable knowledge-handling: resolving conflicts between knowledge sources, avoiding distraction from uninformative external knowledge, and abstaining when sufficient knowledge is unavailable. Prior studies have examined these scenarios in isolation or with limited scope. To systematically evaluate these capabilities, we introduce a comprehensive framework for analyzing knowledge-handling based on two key dimensions: the presence of parametric knowledge and the informativeness of external knowledge. Through analysis, we identify biases in knowledge utilization and examine how the ability to handle one scenario impacts performance in others. Furthermore, we demonstrate that training on data constructed based on the knowledge-handling scenarios improves LLMs' reliability in integrating and utilizing knowledge. \u25b3 Less Submitted 19 February, 2025; originally announced February 2025. Comments: under-review arXiv:2502.12602 [pdf, other] Learning-based Dynamic Robot-to-Human Handover Authors: Hyeonseong Kim, Chanwoo Kim, Matthew Pan, Kyungjae Lee, Sungjoon Choi Abstract: This paper presents a novel learning-based approach to dynamic robot-to-human handover, addressing the challenges of delivering objects to a moving receiver. We hypothesize that dynamic handover, where the robot adjusts to the receiver's movements, results in more efficient and comfortable interaction compared to static handover, where the receiver is assumed to be stationary. To validate this, we\u2026 \u25bd More This paper presents a novel learning-based approach to dynamic robot-to-human handover, addressing the challenges of delivering objects to a moving receiver. We hypothesize that dynamic handover, where the robot adjusts to the receiver's movements, results in more efficient and comfortable interaction compared to static handover, where the receiver is assumed to be stationary. To validate this, we developed a nonparametric method for generating continuous handover motion, conditioned on the receiver's movements, and trained the model using a dataset of 1,000 human-to-human handover demonstrations. We integrated preference learning for improved handover effectiveness and applied impedance control to ensure user safety and adaptiveness. The approach was evaluated in both simulation and real-world settings, with user studies demonstrating that dynamic handover significantly reduces handover time and improves user comfort compared to static methods. Videos and demonstrations of our approach are available at https://zerotohero7886.github.io/dyn-r2h-handover . \u25b3 Less Submitted 18 February, 2025; originally announced February 2025. Comments: Accepted to ICRA 2025. For associated videos, see https://zerotohero7886.github.io/dyn-r2h-handover arXiv:2502.11881 [pdf, other] Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models Authors: Hyunwoo Kim, Melanie Sclar, Tan Zhi-Xuan, Lance Ying, Sydney Levine, Yang Liu, Joshua B. Tenenbaum, Yejin Choi Abstract: Existing LLM reasoning methods have shown impressive capabilities across various tasks, such as solving math and coding problems. However, applying these methods to scenarios without ground-truth answers or rule-based verification methods - such as tracking the mental states of an agent - remains challenging. Inspired by the sequential Monte Carlo algorithm, we introduce thought-tracing, an infere\u2026 \u25bd More Existing LLM reasoning methods have shown impressive capabilities across various tasks, such as solving math and coding problems. However, applying these methods to scenarios without ground-truth answers or rule-based verification methods - such as tracking the mental states of an agent - remains challenging. Inspired by the sequential Monte Carlo algorithm, we introduce thought-tracing, an inference-time reasoning algorithm designed to trace the mental states of specific agents by generating hypotheses and weighting them based on observations without relying on ground-truth solutions to questions in datasets. Our algorithm is modeled after the Bayesian theory-of-mind framework, using LLMs to approximate probabilistic inference over agents' evolving mental states based on their perceptions and actions. We evaluate thought-tracing on diverse theory-of-mind benchmarks, demonstrating significant performance improvements compared to baseline LLMs. Our experiments also reveal interesting behaviors of the recent reasoning models - e.g., o1 and R1 - on theory-of-mind, highlighting the difference of social reasoning compared to other domains. \u25b3 Less Submitted 17 February, 2025; originally announced February 2025. arXiv:2502.11763 [pdf, other] Lightweight Deepfake Detection Based on Multi-Feature Fusion Authors: Siddiqui Muhammad Yasir, Hyun Kim Abstract: Deepfake technology utilizes deep learning based face manipulation techniques to seamlessly replace faces in videos creating highly realistic but artificially generated content. Although this technology has beneficial applications in media and entertainment misuse of its capabilities may lead to serious risks including identity theft cyberbullying and false information. The integration of DL with\u2026 \u25bd More Deepfake technology utilizes deep learning based face manipulation techniques to seamlessly replace faces in videos creating highly realistic but artificially generated content. Although this technology has beneficial applications in media and entertainment misuse of its capabilities may lead to serious risks including identity theft cyberbullying and false information. The integration of DL with visual cognition has resulted in important technological improvements particularly in addressing privacy risks caused by artificially generated deepfake images on digital media platforms. In this study we propose an efficient and lightweight method for detecting deepfake images and videos making it suitable for devices with limited computational resources. In order to reduce the computational burden usually associated with DL models our method integrates machine learning classifiers in combination with keyframing approaches and texture analysis. Moreover the features extracted with a histogram of oriented gradients (HOG) local binary pattern (LBP) and KAZE bands were integrated to evaluate using random forest extreme gradient boosting extra trees and support vector classifier algorithms. Our findings show a feature-level fusion of HOG LBP and KAZE features improves accuracy to 92% and 96% on FaceForensics++ and Celeb-DFv2 respectively. \u25b3 Less Submitted 17 February, 2025; originally announced February 2025. Journal ref: Yasir, S.M.; Kim, H. Lightweight Deepfake Detection Based on Multi-Feature Fusion. Appl. Sci. 2025, 15, 1954 arXiv:2502.10460 [pdf, other] SenDaL: An Effective and Efficient Calibration Framework of Low-Cost Sensors for Daily Life Authors: Seokho Ahn, Hyungjin Kim, Euijong Lee, Young-Duk Seo Abstract: The collection of accurate and noise-free data is a crucial part of Internet of Things (IoT)-controlled environments. However, the data collected from various sensors in daily life often suffer from inaccuracies. Additionally, IoT-controlled devices with low-cost sensors lack sufficient hardware resources to employ conventional deep-learning models. To overcome this limitation, we propose sensors\u2026 \u25bd More The collection of accurate and noise-free data is a crucial part of Internet of Things (IoT)-controlled environments. However, the data collected from various sensors in daily life often suffer from inaccuracies. Additionally, IoT-controlled devices with low-cost sensors lack sufficient hardware resources to employ conventional deep-learning models. To overcome this limitation, we propose sensors for daily life (SenDaL), the first framework that utilizes neural networks for calibrating low cost sensors. SenDaL introduces novel training and inference processes that enable it to achieve accuracy comparable to deep learning models while simultaneously preserving latency and energy consumption similar to linear models. SenDaL is first trained in a bottom-up manner, making decisions based on calibration results from both linear and deep learning models. Once both models are trained, SenDaL makes independent decisions through a top-down inference process, ensuring accuracy and inference speed. Furthermore, SenDaL can select the optimal deep learning model according to the resources of the IoT devices because it is compatible with various deep learning models, such as long short-term memory-based and Transformer-based models. We have verified that SenDaL outperforms existing deep learning models in terms of accuracy, latency, and energy efficiency through experiments conducted in different IoT environments and real-life scenarios. \u25b3 Less Submitted 12 February, 2025; originally announced February 2025. Comments: Accepted by IEEE IoTJ arXiv:2502.10433 [pdf, other] Neural Genetic Search in Discrete Spaces Authors: Hyeonah Kim, Sanghyeok Choi, Jiwoo Son, Jinkyoo Park, Changhyun Kwon Abstract: Effective search methods are crucial for improving the performance of deep generative models at test time. In this paper, we introduce a novel test-time search method, Neural Genetic Search (NGS), which incorporates the evolutionary mechanism of genetic algorithms into the generation procedure of deep models. The core idea behind NGS is its crossover, which is defined as parent-conditioned generat\u2026 \u25bd More Effective search methods are crucial for improving the performance of deep generative models at test time. In this paper, we introduce a novel test-time search method, Neural Genetic Search (NGS), which incorporates the evolutionary mechanism of genetic algorithms into the generation procedure of deep models. The core idea behind NGS is its crossover, which is defined as parent-conditioned generation using trained generative models. This approach offers a versatile and easy-to-implement search algorithm for deep generative models. We demonstrate the effectiveness and flexibility of NGS through experiments across three distinct domains: routing problems, adversarial prompt generation for language models, and molecular design. \u25b3 Less Submitted 8 February, 2025; originally announced February 2025. Comments: 19 pages arXiv:2502.09993 [pdf, other] Navigating Label Ambiguity for Facial Expression Recognition in the Wild Authors: JunGyu Lee, Yeji Choi, Haksub Kim, Ig-Jae Kim, Gi Pyo Nam Abstract: Facial expression recognition (FER) remains a challenging task due to label ambiguity caused by the subjective nature of facial expressions and noisy samples. Additionally, class imbalance, which is common in real-world datasets, further complicates FER. Although many studies have shown impressive improvements, they typically address only one of these issues, leading to suboptimal results. To tack\u2026 \u25bd More Facial expression recognition (FER) remains a challenging task due to label ambiguity caused by the subjective nature of facial expressions and noisy samples. Additionally, class imbalance, which is common in real-world datasets, further complicates FER. Although many studies have shown impressive improvements, they typically address only one of these issues, leading to suboptimal results. To tackle both challenges simultaneously, we propose a novel framework called Navigating Label Ambiguity (NLA), which is robust under real-world conditions. The motivation behind NLA is that dynamically estimating and emphasizing ambiguous samples at each iteration helps mitigate noise and class imbalance by reducing the model's bias toward majority classes. To achieve this, NLA consists of two main components: Noise-aware Adaptive Weighting (NAW) and consistency regularization. Specifically, NAW adaptively assigns higher importance to ambiguous samples and lower importance to noisy ones, based on the correlation between the intermediate prediction scores for the ground truth and the nearest negative. Moreover, we incorporate a regularization term to ensure consistent latent distributions. Consequently, NLA enables the model to progressively focus on more challenging ambiguous samples, which primarily belong to the minority class, in the later stages of training. Extensive experiments demonstrate that NLA outperforms existing methods in both overall and mean accuracy, confirming its robustness against noise and class imbalance. To the best of our knowledge, this is the first framework to address both problems simultaneously. \u25b3 Less Submitted 14 February, 2025; originally announced February 2025. Comments: Accepted by AAAI2025"
  },
  {
    "url": "https://arxiv.org/format/2503.14887",
    "title": "arXiv.org >\n2503.14887 > format",
    "author": "Unknown",
    "body": "Help | Advanced Search Note: Many of the formats above are served gzipped (Content-Encoding: x-gzip). Your browser may silently uncompress after downloading so the files you see saved may appear uncompressed. Please report any problems to arXiv technical support and include the paper identifier. arXiv Operational Status Get status notifications via email or slack"
  },
  {
    "url": "https://arxiv.org/abs/2503.14887",
    "title": "Computer Science > Information Retrieval",
    "author": "Authors:Hang Li, Xiao Wang, Bevan Koopman, Guido Zuccon",
    "body": "Help | Advanced Search arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. arXiv Operational Status Get status notifications via email or slack"
  },
  {
    "url": "https://arxiv.org/html/2503.14887v1",
    "title": "Pseudo-Relevance Feedback Can Improve Zero-Shot LLM-Based Dense Retrieval",
    "author": "Pseudo-Relevance Feedback Can Improve Zero-Shot LLM-Based Dense Retrieval\n\n\nHang Li\n\n0000-0002-5317-7227\nThe University of QueenslandBrisbaneAustralia\n\nhang.li@uq.edu.au\n\n,\u00a0\nXiao Wang\n\n0000-0002-5151-2773\nUniversity of International Business and EconomicsBeijingChina\n\nxiao.wang@uibe.edu.cn\n\n,\u00a0\nBevan Koopman\n\n0000-0001-5577-3391\nCSIROBrisbaneAustralia\n\nbevan.koopman@csiro.au\n\n\u00a0and\u00a0\nGuido Zuccon\n\n0000-0003-0271-5563\nThe University of QueenslandBrisbaneAustralia\n\ng.zuccon@uq.edu.au\n\n\n(2025)\n\nAbstract.\nPseudo-relevance feedback (PRF) refines queries by leveraging initially retrieved documents to improve retrieval effectiveness.\nIn this paper, we investigate how large language models (LLMs) can facilitate PRF for zero-shot LLM-based dense retrieval, extending the recently proposed PromptReps method.\nSpecifically, our approach uses LLMs to extract salient passage features\u2014such as keywords and summaries\u2014from top-ranked documents, which are then integrated into PromptReps to produce enhanced query representations.\nExperiments on passage retrieval benchmarks demonstrate that incorporating PRF significantly boosts retrieval performance.\nNotably, smaller rankers with PRF can match the effectiveness of larger rankers without PRF, highlighting PRF\u2019s potential to improve LLM-driven search while maintaining an efficient balance between effectiveness and resource usage.\n\n\nPseudo Relevance Feedback, Zero-shot Dense Retrieval\n\n\u2020\u2020copyright: acmlicensed\u2020\u2020journalyear: 2025\u2020\u2020doi: XXXXXXX.XXXXXXX\u2020\u2020conference: Make sure to enter the correct\nconference title from your rights confirmation email; June 03\u201305,\n2025; Woodstock, NY\u2020\u2020isbn: 978-1-4503-XXXX-X/2025/06\u2020\u2020ccs: Information systems\u00a0Query representation\n\n\n1. Introduction\n\nIn dense retrieval, queries and documents are encoded into embeddings using language modelling backbones typically fine-tuned with contrastive learning; retrieval then occurs by computing the similarity between query and document embeddings\u00a0(Yates et\u00a0al., 2021; Macdonald et\u00a0al., 2021). Recent advances in language modelling have been motivated the replacement of encoder-only backbones like BERT with larger decoder-only backbones (generative LLMs) to form dense representations\u00a0(BehnamGhader et\u00a0al., 2024; Ma et\u00a0al., 2023; Zhuang et\u00a0al., 2024), allowing to leverage richer contextual information and enhancing dense retrieval generalization. Of particular interest for this paper is PromptReps\u00a0(Zhuang et\u00a0al., 2024), an LLM-based approach for dense retrieval. PromptReps is unique in that it does not require contrastive learning, producing effective representations for dense retrieval in a zero-shot manner111Another uniqueness of PromptReps compared to other methods in the field is that it can contemporary output both dense and sparse representations, enabling effective hybrid retrieval. In this paper we ignore PromptReps\u2019 sparse representation capabilities and solely focus on the dense representations it produces..\n\n\nIn this work, we introduce a novel pseudo-relevance feedback (PRF) approach, PromptPRF, for enhancing LLM-based dense retrieval, and we investigate this approach within the framework of PromptReps. Common PRF techniques for dense retrieval use encoder-only language modelling backbones to either encode feedback documents individually and then combine their representations\u00a0(Li et\u00a0al., 2022b; Wang et\u00a0al., 2023b), or encode input strings that have been formed by padding the query tokens with the feedback document tokens\u00a0(Yu et\u00a0al., 2021; Li et\u00a0al., 2022c). Diverging from these previous approaches, we devise prompts that guide the LLM to generate various types of passage features (as text tokens) for each PRF document. The extracted features, along with the rank information from the ranked list, are then used to create a new dense representation of the query, exploiting an adapted prompt within the PromptReps framework. The new dense representation is then used for the second round of retrieval.\n\n\nOur PromptPRF method presents several advantages. It is simple to implement and requires no additional training, thus fitting well within the PromptReps zero-shot paradigm. Experimental results demonstrate that PromptPRF significantly outperforms baselines on TREC DL\u00a0(Craswell et\u00a0al., 2020, 2021) passage ranking tasks. Importantly, we found that, when paired with PromptPRF, smaller dense retrievers (i.e. with less number of parameters) can become as effective as larger dense retrievers, shifting the need for scaling up retriever model size to scaling up the LLMs used for feature extraction. This has practical implications in that retrievers are run online (i.e. at query time) and thus directly affect query latency and the hardware requirements of systems in production, while the LLM feature generation of PromptPRF can be run offline, thus not affecting query latency and providing more opportunities for better computation distribution and management.\n\n\n\n\n2. Methodology\n\nOur approach integrates PRF into LLM-based dense retrieval within the PromptReps framework\u00a0(Zhuang et\u00a0al., 2024). The methodology consists of four key components: (1) initial retrieval, (2) feature extraction, (3) query refinement, and (4) second-stage retrieval.\n\n\nInitial Retrieval.\nWe employ the dense retrieval component of PromptReps\u00a0(Zhuang et\u00a0al., 2024), which has a bi-encoder structure consisting of EDsubscriptE\ud835\udc37\\text{E}_{D}E start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT and EQsubscriptE\ud835\udc44\\text{E}_{Q}E start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT and operates in a zero-shot manner without additional training. Given a query q\ud835\udc5eqitalic_q PromptReps generates a dense representation \ud835\udc2a\ud835\udc2a\\mathbf{q}bold_q by prompting an LLM-based EQsubscriptE\ud835\udc44\\text{E}_{Q}E start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT to summarize the query into a single word and extract its hidden representation.\nRelevance is estimated using the cosine similarity: s\u2062(q,p)=\ud835\udc2a\u22c5\ud835\udc29|\ud835\udc2a|\u2062|\ud835\udc29|\ud835\udc60\ud835\udc5e\ud835\udc5d\u22c5\ud835\udc2a\ud835\udc29\ud835\udc2a\ud835\udc29s(q,p)=\\frac{\\mathbf{q}\\cdot\\mathbf{p}}{|\\mathbf{q}||\\mathbf{p}|}italic_s ( italic_q , italic_p ) = divide start_ARG bold_q \u22c5 bold_p end_ARG start_ARG | bold_q | | bold_p | end_ARG,\nwhere \ud835\udc29\ud835\udc29\\mathbf{p}bold_p is the dense representation of a passage p\ud835\udc5dpitalic_p. After applying the similarity function among each passage in the collection in response to a query, we obtain a ranked list ranked according to the relevance score s\u2062(q,pj)\ud835\udc60\ud835\udc5esubscript\ud835\udc5d\ud835\udc57s(q,p_{j})italic_s ( italic_q , italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) in descending order: R={p1,p2,\u2026,pn}\ud835\udc45subscript\ud835\udc5d1subscript\ud835\udc5d2\u2026subscript\ud835\udc5d\ud835\udc5bR=\\{{p_{1},p_{2},...,p_{n}}\\}italic_R = { italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }, each pjsubscript\ud835\udc5d\ud835\udc57p_{j}italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT consists of the text: t\u2062e\u2062x\u2062t\u2062(pj)\ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61subscript\ud835\udc5d\ud835\udc57text(p_{j})italic_t italic_e italic_x italic_t ( italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) and rank: j\ud835\udc57jitalic_j.\n\n\nFeature Extraction.\nBased on the pseudo-relevance feedback assumption, we only obtain the top k\ud835\udc58kitalic_k passage and make the PRF passage set \ud835\udcabk=[(text(p1),1],[text(p2),2],\u2026,[text(pk],k]\\mathcal{P}_{k}=[(text(p_{1}),1],[text(p_{2}),2],...,[text(p_{k}],k]caligraphic_P start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = [ ( italic_t italic_e italic_x italic_t ( italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , 1 ] , [ italic_t italic_e italic_x italic_t ( italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , 2 ] , \u2026 , [ italic_t italic_e italic_x italic_t ( italic_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ] , italic_k ].\nTraditional PRF methods expand the query using the original text information. However, a PRF passage is typically long and contains noisy information\u00a0(Li et\u00a0al., 2022a); this could mislead the PromptReps model. To enhance PRF, we generate passage-level features from the top-ranked results of the initial retrieval. To generate features we prompt an LLM backbone with an instruction and the passage.\n\n\nLet pjsubscript\ud835\udc5d\ud835\udc57p_{j}italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT denote the PRF passage at rank j\ud835\udc57jitalic_j, and let T=t1,t2,\u2026,tn\ud835\udc47subscript\ud835\udc611subscript\ud835\udc612\u2026subscript\ud835\udc61\ud835\udc5bT={t_{1},t_{2},...,t_{n}}italic_T = italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT represent the set of the feature types to be generated by LLMs. Following\u00a0(Mackie et\u00a0al., 2023a), the feature types extracted are T\ud835\udc47Titalic_T ={{\\{{keywords, entities, summaries, essays, keywords-COT222COT is short for Chain-of-Thoughts., entities-COT, news articles, facts, query keywords, and documents}}\\}}. For each feature type t\u2208T\ud835\udc61\ud835\udc47t\\in Titalic_t \u2208 italic_T, let Itsubscript\ud835\udc3c\ud835\udc61I_{t}italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT denote the instruction or prompt template designed to guide the LLM in generating feature t\ud835\udc61titalic_t. The feature generation for the feature at rank j, i.e. ft(j)superscriptsubscript\ud835\udc53\ud835\udc61\ud835\udc57f_{t}^{(j)}italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT can be described as ft(j)=LLM\u2061(t\u2062e\u2062x\u2062t\u2062(pj),It)superscriptsubscript\ud835\udc53\ud835\udc61\ud835\udc57LLM\ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61subscript\ud835\udc5d\ud835\udc57subscript\ud835\udc3c\ud835\udc61f_{t}^{(j)}=\\operatorname{LLM}\\left(text(p_{j}),I_{t}\\right)italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT = roman_LLM ( italic_t italic_e italic_x italic_t ( italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) , italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ).\nThe feature collection for the PRF passage pjsubscript\ud835\udc5d\ud835\udc57p_{j}italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is obtained as:\n\n\n(1)\n\n\u2131j=\u22c3t\u2208T{ft(j)}={ft(j)\u2223t\u2208T}.subscript\u2131\ud835\udc57subscript\ud835\udc61\ud835\udc47superscriptsubscript\ud835\udc53\ud835\udc61\ud835\udc57conditional-setsuperscriptsubscript\ud835\udc53\ud835\udc61\ud835\udc57\ud835\udc61\ud835\udc47\\mathcal{F}_{j}=\\bigcup_{t\\in T}\\left\\{f_{t}^{(j)}\\right\\}=\\left\\{f_{t}^{(j)}%\n\\mid t\\in T\\right\\}.caligraphic_F start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = \u22c3 start_POSTSUBSCRIPT italic_t \u2208 italic_T end_POSTSUBSCRIPT { italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT } = { italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT \u2223 italic_t \u2208 italic_T } .\n\n\n\n\n\nWe provide the prompts in our GitHub repository (see Sec.\u00a03).\n\n\nFeature extraction can be performed offline as it does not depend on the query, thus it does not contribute to extra query latency.\n\n\nQuery Refinement with PRF.\nWe modify the PromptReps query encoding by incorporating the extracted passage features. Instead of encoding the original query alone, we include top-ranked passages (or their extracted features) within the prompt. This allows for enhanced contextualization of the query representation. The refined query representation \ud835\udc2a\u2032superscript\ud835\udc2a\u2032\\mathbf{q}^{\\prime}bold_q start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT is obtained as:\n\n\n(2)\n\n\ud835\udc2a\u2032=EQ(Template(q)\u2295(\u2a01j=1k\u2a01f\u2208\u2131j``Rankj:\u03b2ift(j)\")),\\mathbf{q}^{\\prime}=\\text{E}_{Q}\\left({\\operatorname{Template}(q)\\oplus\\left(%\n\\bigoplus_{j=1}^{k}\\bigoplus_{f\\in\\mathcal{F}_{j}}``\\operatorname{Rank}j:\\beta%\n_{i}f_{t}^{(j)}\"\\right)}\\right),bold_q start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = E start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT ( roman_Template ( italic_q ) \u2295 ( \u2a01 start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT \u2a01 start_POSTSUBSCRIPT italic_f \u2208 caligraphic_F start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT ` ` roman_Rank italic_j : italic_\u03b2 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT \" ) ) ,\n\n\n\nwhere \u03b2isubscript\ud835\udefd\ud835\udc56\\beta_{i}italic_\u03b2 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is a hyperparameter that controls the contribution of each passage\u2019s extracted feature and Template\u2062(q)Template\ud835\udc5e\\text{Template}(q)Template ( italic_q ) refers to the query-specific prompt template used in this work, and reported in the github repository. Along with features, we also include information about the rank of the passage from the first round of retrieval (Rank\u2061jRank\ud835\udc57\\operatorname{Rank}jroman_Rank italic_j). In this paper we do not experiment with \u03b2isubscript\ud835\udefd\ud835\udc56\\beta_{i}italic_\u03b2 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (thus, \u03b2i=1subscript\ud835\udefd\ud835\udc561\\beta_{i}=1italic_\u03b2 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1), nor we study how different feature types could be combined; we plan to study these aspects in the future.\n\n\nSecond Retrieval.\nThe refined query representation \ud835\udc2a\u2032superscript\ud835\udc2a\u2032\\mathbf{q}^{\\prime}bold_q start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT is then used in a second-stage retrieval, replacing \ud835\udc2a\ud835\udc2a\\mathbf{q}bold_q within the PromptReps prompt. The new relevance score s\u2062(q\u2032,p)\ud835\udc60superscript\ud835\udc5e\u2032\ud835\udc5ds(q^{\\prime},p)italic_s ( italic_q start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_p ) can then again be computed as the similarity between the query dense representation and the passage dense representation.\n\n\n\n2.1. Relationship with Other Approaches\n\nOur PromptPRF method shares similarities with recent methods in the literature.\nThe GenPRF method directly generates expansion terms using the FlanT5 model\u00a0(Wang et\u00a0al., 2023a); it is similar in that like PromptPRF it uses an LLM to generate features, but in the case of GenPRF the type of features are limited to expansion terms. GRF\u00a0(Mackie et\u00a0al., 2023b) and Query2doc\u00a0(Wang et\u00a0al., 2023c) are query expansion methods that enrich queries using LLM-generated text.\nQuery2doc does this by generating pseudo-documents given a query; in PromptPRF we also include a feature that generates a document, but the input is another document.\nGRF is the most similar work to ours. We share the same features and adapt their prompts; however PromptPRF refines the query using both the extracted passage features as well as the rank structure of the PRF document list.\n\n\n\n\n\n\n\n\n3. Experimental Setup\n\nDatasets: We study PromptPRF on TREC DL\u201919\u00a0(Craswell et\u00a0al., 2020) and \u201920\u00a0(Craswell et\u00a0al., 2021) benchmarks.\n\n\nFigure 1. nDCG@10 of PromptPRF when used in combination with a Llama3.2 3B based dense retriever (top) and Llama3 8B dense retriever (bottom). Results are obtained on DL19 (left) and DL20 (right) and are displayed with respect to different feature extractors (x axis); the marker size represents the size of the feature extractor model (# parameters).\n\n\nEvaluation Metrics: To measure retrieval effectiveness, we use nDCG@10 to assess ranking quality and Recall@1000 to evaluate retrieval coverage, providing insights into both precision-oriented and recall-oriented performance aspects.\n\n\nRankers: We use Llama 3 8B Instruct and Llama 3.2 3B Instruct within the PromptReps framework as base rankers without PRF. These models represent a larger and a smaller LLM, allowing us to investigate the effect of model scaling.\nWe use the same backbones for encoding query representations obtained with PromptPRF.\n\n\nFeatures Extractors: We use four LLMs for feature extraction: Llama 3 8B Instruct333https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\u00a0(Grattafiori and et. al., 2024), Llama 3.2 3B Instruct444https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\u00a0(Grattafiori and et. al., 2024), Llama 3.3 70B Instruct555https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct\u00a0(Grattafiori and et. al., 2024), and DeepSeek R1 70B666https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B\u00a0(DeepSeek-AI and et. al., 2025).\nThese models vary in parameter size, allowing us to understand how PRF feature quality scales with model size.\n\n\nBaselines: Along with PromptReps without PRF, we include a PRF baseline where the feedback passage(s) text is simply concatenated to the text of the query and then encoded using PromptReps, thus forming a naive PRF method that resembles PromptPRF but does not use feature extractions.\n\n\nHardware & Code: Experiments are conducted on a server equipped with two H100 GPUs. Code and prompts are made available at https://anonymous.4open.science/r/PromptPRF-BE44.\n\n\n\n\n4. Result Analysis\n\nIn this section, we investigate the effectiveness of PromptPRF (Sec.\u00a04.1), the impact of the feature extractor (Sec.\u00a04.2), and the impact of feature types and PDF depth on PromptPRF (Sec.\u00a04.3).\n\n\nTable 1. Results for PromptPRF; hyper-parameters optimized for nDCG@10. The best results are in Bold. The symbols \u2020\u2020\\dagger\u2020 and \u2021\u2021\\ddagger\u2021 represent statistically significant (p<0.05\ud835\udc5d0.05p<0.05italic_p < 0.05) to NoPRF and PassPRF, respectively.\n\n\n\n\n\nTREC DL19 (43 queries)\nTREC DL20 (54 queries)\n\n\n\nModel\nFeature Extractor\nFeature\nPRF Depth\nnDCG@10\nRecall@1000\nFeature Extractor\nFeature\nPRF Depth\nnDCG@10\nRecall@1000\n\n\n\n\n\n\nLlama3.2 3B\n\n\n\nNoPRF\n\u2013\n\u2013\n\u2013\n0.3695\n0.6032\n\u2013\n\u2013\n\u2013\n0.3334\n0.6085\n\n\nPassPRF\n\u2013\n\u2013\n1\n0.4255\u2020\n\n0.5535\n\u2013\n\u2013\n10\n0.3902\u2020\n\n0.5927\n\n\nOurPRF\nLlama3.3 70B\nEntities-COT\n5\n0.5013\u2020\u2021superscript0.5013\u2020absent\u2021\\textbf{0.5013}^{\\dagger\\ddagger}0.5013 start_POSTSUPERSCRIPT \u2020 \u2021 end_POSTSUPERSCRIPT\n0.6252\nLlama3.2 3B\nFacts\n10\n0.4350\u2020\u2021superscript0.4350\u2020absent\u2021\\textbf{0.4350}^{\\dagger\\ddagger}0.4350 start_POSTSUPERSCRIPT \u2020 \u2021 end_POSTSUPERSCRIPT\n0.6400\n\n\n\n\n\n\nLlama3 8B\n\n\n\nNoPRF\n\u2013\n\u2013\n\u2013\n0.5062\n0.7301\n\u2013\n\u2013\n\u2013\n0.4381\n0.6839\n\n\nPassPRF\n\u2013\n\u2013\n10\n0.5330\n0.7247\n\u2013\n\u2013\n20\n0.4632\n0.6335\n\n\nOurPRF\nLlama3.2 3B\nEssay\n1\n0.5560\u2020superscript0.5560\u2020\\textbf{0.5560}^{\\dagger}0.5560 start_POSTSUPERSCRIPT \u2020 end_POSTSUPERSCRIPT\n0.7293\nLlama3.2 3B\nKeywords-COT\n20\n0.5038\u2020\u2021superscript0.5038\u2020absent\u2021\\textbf{0.5038}^{\\dagger\\ddagger}0.5038 start_POSTSUPERSCRIPT \u2020 \u2021 end_POSTSUPERSCRIPT\n0.6698\n\n\n\n\n\n\n4.1. PromptPRF Effectiveness\n\nTable\u00a01 presents the effectiveness of PromptPRF for a large (Llama3 8B) and a small (Llama3.2 3B) LLM-based dense retriever (DR). The results show that incorporating PRF with targeted feature extraction enables smaller models to achieve retrieval effectiveness comparable to that of larger LLM-based DRs without PRF.\n\n\nFor Llama3.2 3B, which serves as a smaller DR, applying PRF leads to a significant improvement in nDCG@10. On DL19, adopting Llama3.3 70B as the feature extractor with Entities-COT at PRF depth 5 increases nDCG@10 from 0.3695 (no PRF) to 0.5013, approaching the baseline Llama3 8B (no PRF, nDCG@10 = 0.5062). Similarly, on DL20, using Llama3.2 3B with Facts features boosts nDCG@10 close to that of the baseline Llama3 8B (no PRF).\n\n\nFor Llama3 8B, applying PromptPRF consistently outperforms the no PRF and passage PRF (PassPRF) baselines. On DL19, employing Llama3.2 3B as a feature extractor with Essay at PRF depth 1 achieves an nDCG@10 of 0.5560, which is higher than the no PRF and PassPRF baselines. Similarly, on DL20, using Keywords-COT at PRF depth 20 achieves 0.5038 nDCG@10, also significantly surpassing the no PRF and PassPRF baselines with only a small LLM (Llama3.2 3B) as the feature extractor.\n\n\nThe results suggest that smaller LLM-based DRs, when enhanced with PromptPRF, can reach effectiveness levels comparable to significantly larger models without PRF, offering a practical and efficient alternative to relying solely on larger LLM-based retrievers. \n\n\n\n\n4.2. Impact of Feature Extractor\n\nTo investigate the impact of the feature extraction model, we freeze PRF depth and feature type according to the best DL19 results (c.f. Table\u00a01: DR = Llama3.2 3B, PRF depth = 5, entities-cot; DR = Llama3 8B, PRF depth = 1, essay feature). Results are reported in Fig.\u00a01.\n\n\nEffectiveness. The results highlight a relationship between feature extractor size and PRF effectiveness.\nThe size of feature extractors and dense retriever models jointly affect retrieval effectiveness, where the impact of larger feature extractors is more pronounced for smaller LLM-based dense retrievers and larger dense retrievers achieve better effectiveness with smaller feature extractors.\n\n\nMore specifically, for the DR based on Llama3.2-3B, larger feature extractors (e.g., Llama3.3-70B) significantly enhance retrieval effectiveness, especially in the Entities-COT PRF with PRF depth 5, where they consistently yield higher nDCG@10. The improvement shows that larger extractors provide richer contextual representations, compensating for the weaker initial query representations produced by the smaller dense retriever.\n\n\nIn contrast, when the DR is based on Llama3-8B, smaller feature extractors (e.g., Llama3.2-3B) provide substantial improvements in retrieval effectiveness. However, scaling up to larger feature extractors (e.g., 70B models) leads to diminishing returns. These results suggest that larger dense retrievers have sufficient internal representation capacity and semantic understanding to be effective with smaller feature extractors, beyond which additional scaling of the extractor does not significantly improve effectiveness.\n\n\nEfficiency. From an efficiency standpoint, the results demonstrate that scaling feature extractors is critical for enhancing PRF effectiveness in smaller DRs. However, these benefits diminish for larger DRs. As feature extraction is conducted offline, the primary consideration shifts from the extraction cost to the computational overhead incurred during the integration of expanded features into the on-the-fly PRF process. The DR based on Llama3-8B achieves strong effectiveness even when paired with smaller feature extractors, and while the use of larger extractors provides some incremental improvements, these gains do not justify the associated increase in extraction costs. In contrast, the smaller DR based on Llama3.2-3B requires larger feature extractors to obtain high PRF effectiveness. This suggests that feature extraction scaling is a necessary step for maximizing effectiveness in lightweight retrievers.\n\n\nOverall, these findings indicate that PRF optimization should aim to strike a balance between retrieval effectiveness and (offline) computational costs, employing larger feature extractors for smaller retrievers while relying on cheaper extractors for larger retrievers without compromising retrieval quality.\n\n\n\n\n4.3. Impact of Feature Types and PRF Depth\n\nFigure 2. Impact of features and PRF depth across dense retrievers of different size.\n\n\nFigure 2 studies the impact of varying PRF depths and feature types777Certain features are incompatible with greater PRF depths due to their extensive content exceeding the model\u2019s input length constraints. for the two DRs used in our experiments. For the DR based on Llama3.2 3B (plots 1 and 3 in Fig. 2), we freeze the feature extraction model to be Llama3.3 70B aligning with the top effectiveness for Llama3.2 3B (Table 1). We also applied the same settings to the DR based on Llama3 8B (plots 2 and 4 in Fig. 2). The results reveal that both the choice of features and the depth of feedback retrieval play a crucial role in PRF effectiveness, with distinct trends emerging across different configurations.\n\n\nA general trend observed across both models is that moderate PRF depths, particularly in the range of 3 to 10, contribute to a consistent improvement in retrieval effectiveness. However, at deeper depths (e.g., PRF depth of 50), effectiveness degradation becomes evident, especially for certain feature types such as Queries-Keyword and News-Article in Llama3 8B.\nThis suggests that while PRF is beneficial in augmenting query representations, an overly broad expansion may introduce excessive noise, diluting the relevance of retrieved passages. These findings align with previous studies on retrieval sensitivity to top-k expansion, reinforcing the necessity of controlled retrieval augmentation.\n\n\nWhen examining different feature types, we find that the structured entity- and keyword-based features, such as Entities-COT and Keywords-COT, consistently enhance nDCG@10 across various PRF depths, highlighting the significance of structured contextual enrichment for improved query comprehension. In contrast, features based on factual summarization, such as Facts and Summary maintain stable effectiveness, indicating that concise, fact-oriented context aids in query augmentation without introducing ambiguity. Document-based PRF, however, shows diminishing returns at higher PRF depths, suggesting that using full passages can introduce excessive noise and lead to query drift. Similarly, features relying on query-keyword and news-article exhibit high variability, with effectiveness deteriorating at larger PRF depths. Overall, structured information improves query representation, but overly broad or loosely related context can reduce retrieval precision.\n\n\nComparing the two dense retrievers, Llama3 8B consistently outperforms Llama3.2 3B across all configurations, both with and without PRF. This is expected, as Llama3 8B is inherently a stronger retrieval model, and its superior effectiveness with PRF is more likely due to its higher base retrieval document list. While both models benefit from PRF, Llama3.2 3B exhibits more pronounced improvements at lower PRF depths (1\u20135), whereas Llama3 8B maintains effectiveness across moderate depths (10\u201320). This suggests that smaller models may require more conservative query expansion strategies, while stronger base retrievers can incorporate expanded context more effectively at a broader range of PRF depths. Additionally, the impact of specific features varies between models, as evidenced by essay-based PRF yielding stronger improvements for Llama3.2 3B but having a lesser effect on Llama3 8B.\n\n\nIn summary, we find that structured features like Entities-COT and Keywords-COT can improve retrieval effectiveness, while broad expansions through raw passages or loosely related contexts can cause query drift. PRF strategies should be tailored to the underlying retrieval model, with weaker models (e.g., Llama3.2 3B) requiring conservative expansion, while stronger models (e.g., Llama3 8B) can handle deeper augmentations.\nThese findings underscore the importance of tuning PRF depth and feature selection for optimal retrieval effectiveness.\n\n\n\n\n\n5. Cost Analysis and Efficiency-Effectiveness Trade-offs\n\nEfficient retrieval in real-world applications requires a careful balance between effectiveness and computational cost. In this section, we analyze the infrastructure requirements, query latency, and the trade-offs between retrieval effectiveness and resource efficiency.\n\n\n\n5.1. Infrastructure Requirements\n\nWe evaluate the minimum computational resources needed to deploy the different LLMs used in our study. Table\u00a02 presents the disk space, RAM, and GPU requirements for the LLMs used in our experiments as the feature extractors and dense retrievers.\n\n\nTable 2. Minimal hardware requirements for different models used in retrieval experiments.\n\n\nModel\nDisk Space\nRAM Requirement\nMinimum GPU VRAM\n\n\nLlama3.2 3B Instruct\n\n\u223csimilar-to\\sim\u223c6 GB\n8 GB\n12 GB (RTX 3060, RTX 4070, RTX A2000, RTX A4000)\n\n\nLlama3 8B Instruct\n\n\u223csimilar-to\\sim\u223c16 GB\n16 GB\n20 GB (RTX 3080 Ti, RTX 4000 ADA)\n\n\nLlama3.3 70B Instruct\n\n\u223csimilar-to\\sim\u223c40 GB\n48 GB\n161 GB (2\u00d7H100 or 2\u00d7A100)\n\n\nDeepSeek R1 70B\n\n\u223csimilar-to\\sim\u223c43 GB\n128 GB\n160 GB (2\u00d7H100 or 2\u00d7A100)\n\n\n\n\nThese numbers highlight that while smaller models like Llama3.2 3B Instruct can operate on consumer-grade GPUs, larger models require high-end hardware with multiple H100 or A100 GPUs. The RAM overhead of the 70B models further increases deployment complexity, making them significantly more costly.\n\n\n\n\n5.2. Query Latency and Computational Efficiency\n\nTable\u00a03 summarizes the query encoding latency for the two base LLMs used in our retrieval experiments.\n\n\nTable 3. Query encoding latency for different models.\n\n\nModel\nQuery Latency (1\u00d7 H100, Batch Size 64)\n\n\nLlama3.2 3B Instruct\n0.0194s/query\n\n\nLlama3 8B Instruct\n0.0529s/query\n\n\n\n\nFrom these results, Llama3.2 3B is approximately 2.7\u00d7 faster per query than Llama3 8B, making it significantly more efficient for real-time retrieval applications. Given that PRF feature extraction is performed offline, the online latency of query encoding is the primary factor that affecting real-time retrieval performance.\n\n\n\n\n5.3. Effectiveness vs. Efficiency Trade-offs\n\nThe key finding from our experiments is that smaller dense retrievers with PRF can achieve effectiveness comparable to larger models without PRF, while significantly reducing computational cost. This trade-off is evident in the following observations:\n\n\n\n\n\u2022\n\nLlama3.2 3B with PRF approaches Llama3 8B without PRF, achieving similar nDCG@10 scores while reducing query latency by approximately 63%.\n\n\n\n\u2022\n\nFor feature extraction, larger models (e.g., Llama3.3 70B) provide marginal effectiveness gains when paired with Llama3 8B, but require substantially higher inference costs.\n\n\n\n\u2022\n\nSmaller feature extractors (e.g., Llama3.2 3B) are often sufficient for PRF, particularly for larger retrievers, reducing overhead without compromising performance.\n\n\n\n\n\nThese observations suggest that optimizing PRF enables cost-effective retrieval without requiring large LLMs. By strategically selecting feature extractors and balancing PRF depth, retrieval effectiveness can be maximized while minimizing inference cost.\n\n\n\n\n5.4. Practical Implications\n\nFor real-world deployment:\n\n\n\n\n\u2022\n\nSmaller models (Llama3.2 3B) should be prioritized for efficiency-critical applications such as conversational search or mobile deployments.\n\n\n\n\u2022\n\nLarger models (Llama3 8B and above) are selected only if the marginal gain in retrieval quality offsets the increased computational costs.\n\n\n\n\u2022\n\nPRF should be tuned based on hardware constraints, leveraging offline processing for feature extraction to minimize online latency.\n\n\n\n\n\nThese insights highlight that efficient PRF strategies can mitigate the need for large-scale dense retrievers, using a smaller dense retriever with PRF can achieve comparable effectiveness to larger dense retrievers, making advanced retrieval techniques more accessible and scalable.\n\n\n\n\n\n6. Discussion and Conclusion\n\nThis study demonstrates that integrating pseudo-relevance feedback into LLM-based dense retrieval significantly improves retrieval effectiveness, particularly for smaller retrieval models. By leveraging LLM-derived passage features for query expansion, PromptPRF enables smaller dense retrievers (Llama3.2-3B) to achieve performance comparable to significantly larger retrievers without PRF (Llama3-8B). This is unlike findings in related literature, where it is shown LLMs exhibit a power-law scaling with respect to the number of parameters (and the amount of training data)\u00a0(Kaplan et\u00a0al., 2020; Hoffmann et\u00a0al., 2022), a scaling law that has also been observed for dense retrieval\u00a0(Fang et\u00a0al., 2024). The fact that using PromptPRF enables PromptReps-based dense retrieval to break this law is advantageous in that it helps to reduce retrieval costs at query time. The ability to run in production a smaller model also comes with the added advantage that less capable hardware (e.g. GPUs with smaller memory) is required.\n\n\nOur analysis also reveals that smaller dense retrievers benefit significantly from feature extractors based on larger LLM backbones, as these smaller rankers can rely on externally enriched representations to improve query reformulation. In contrast, larger dense retrievers (e.g., based on Llama3-8B) benefit relatively less from increasing extractor size. This suggests that the effectiveness of PRF depends on both the retrieval model and the feature extractor, rather than simply scaling either component.\n\n\nWe also examine the relationship between PRF depth and feature types, finding that moderate PRF depths (e.g., 3 to 10) provide the most consistent retrieval improvements, while deeper expansion can introduce noise and query drift; a result that is not surprising in light of previous work. Additionally, structured features such as Entities-Cot and Keywords-Cot yield more stable improvements than unstructured expansions like full passages or news articles. This is good news because these structured features typically produce less output tokens than the unstructured expansions, resulting in lower feature extraction and PRF utilization costs.\n\n\nThe promising results obtained by PromptPRF open up a number of future directions, which could not be explored in this paper. These include investigating (i) the impact of the method\u2019s parameters such as \u03b2isubscript\ud835\udefd\ud835\udc56\\beta_{i}italic_\u03b2 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, which controls the contribution of the features of the individual passage pisubscript\ud835\udc5d\ud835\udc56p_{i}italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT on a per-rank basis, (ii) the impact of the original passage rank information within PromptPRF\u2019s prompt, and in particular whether the removal of this information leads to decreased effectiveness, and (iii) the combination of multiple features to represent the PRF signal.\n\n\n\nReferences\n\n\n(1)\n\n\n\n\nBajaj et\u00a0al. (2018)\n\nPayal Bajaj, Daniel\nCampos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu,\nRangan Majumder, Andrew McNamara,\nBhaskar Mitra, Tri Nguyen,\nMir Rosenberg, Xia Song,\nAlina Stoica, Saurabh Tiwary, and\nTong Wang. 2018.\n\n\nMS MARCO: A Human Generated MAchine Reading\nCOmprehension Dataset.\n\n\n\n\narXiv:1611.09268\u00a0[cs.CL]\n\nhttps://arxiv.org/abs/1611.09268\n\n\n\nBehnamGhader et\u00a0al. (2024)\n\nParishad BehnamGhader,\nVaibhav Adlakha, Marius Mosbach,\nDzmitry Bahdanau, Nicolas Chapados, and\nSiva Reddy. 2024.\n\n\nLLM2Vec: Large Language Models Are Secretly\nPowerful Text Encoders. In First Conference on\nLanguage Modeling.\n\n\n\nhttps://openreview.net/forum?id=IW1PR7vEBf\n\n\n\nCraswell et\u00a0al. (2020)\n\nNick Craswell, Bhaskar\nMitra, Emine Yilmaz, Daniel Campos,\nand Ellen\u00a0M Voorhees. 2020.\n\n\nOverview of the TREC 2019 Deep Learning Track. In\nText REtrieval Conference, TREC.\n\n\n\n\n\n\nCraswell et\u00a0al. (2021)\n\nNick Craswell, Bhaskar\nMitra, Emine Yilmaz, Daniel Campos,\nand Ellen\u00a0M Voorhees. 2021.\n\n\nOverview of the TREC 2020 Deep Learning Track. In\nText REtrieval Conference, TREC.\n\n\n\n\n\n\nDeepSeek-AI and et. al. (2025)\n\nDeepSeek-AI and et.\nal. 2025.\n\n\nDeepSeek-R1: Incentivizing Reasoning Capability in\nLLMs via Reinforcement Learning.",
    "body": "Pseudo-relevance feedback (PRF) refines queries by leveraging initially retrieved documents to improve retrieval effectiveness. In this paper, we investigate how large language models (LLMs) can facilitate PRF for zero-shot LLM-based dense retrieval, extending the recently proposed PromptReps method. Specifically, our approach uses LLMs to extract salient passage features\u2014such as keywords and summaries\u2014from top-ranked documents, which are then integrated into PromptReps to produce enhanced query representations. Experiments on passage retrieval benchmarks demonstrate that incorporating PRF significantly boosts retrieval performance. Notably, smaller rankers with PRF can match the effectiveness of larger rankers without PRF, highlighting PRF\u2019s potential to improve LLM-driven search while maintaining an efficient balance between effectiveness and resource usage. In dense retrieval, queries and documents are encoded into embeddings using language modelling backbones typically fine-tuned with contrastive learning; retrieval then occurs by computing the similarity between query and document embeddings (Yates et al., 2021; Macdonald et al., 2021). Recent advances in language modelling have been motivated the replacement of encoder-only backbones like BERT with larger decoder-only backbones (generative LLMs) to form dense representations (BehnamGhader et al., 2024; Ma et al., 2023; Zhuang et al., 2024), allowing to leverage richer contextual information and enhancing dense retrieval generalization. Of particular interest for this paper is PromptReps (Zhuang et al., 2024), an LLM-based approach for dense retrieval. PromptReps is unique in that it does not require contrastive learning, producing effective representations for dense retrieval in a zero-shot manner111Another uniqueness of PromptReps compared to other methods in the field is that it can contemporary output both dense and sparse representations, enabling effective hybrid retrieval. In this paper we ignore PromptReps\u2019 sparse representation capabilities and solely focus on the dense representations it produces.. In this work, we introduce a novel pseudo-relevance feedback (PRF) approach, PromptPRF, for enhancing LLM-based dense retrieval, and we investigate this approach within the framework of PromptReps. Common PRF techniques for dense retrieval use encoder-only language modelling backbones to either encode feedback documents individually and then combine their representations (Li et al., 2022b; Wang et al., 2023b), or encode input strings that have been formed by padding the query tokens with the feedback document tokens (Yu et al., 2021; Li et al., 2022c). Diverging from these previous approaches, we devise prompts that guide the LLM to generate various types of passage features (as text tokens) for each PRF document. The extracted features, along with the rank information from the ranked list, are then used to create a new dense representation of the query, exploiting an adapted prompt within the PromptReps framework. The new dense representation is then used for the second round of retrieval. Our PromptPRF method presents several advantages. It is simple to implement and requires no additional training, thus fitting well within the PromptReps zero-shot paradigm. Experimental results demonstrate that PromptPRF significantly outperforms baselines on TREC DL (Craswell et al., 2020, 2021) passage ranking tasks. Importantly, we found that, when paired with PromptPRF, smaller dense retrievers (i.e. with less number of parameters) can become as effective as larger dense retrievers, shifting the need for scaling up retriever model size to scaling up the LLMs used for feature extraction. This has practical implications in that retrievers are run online (i.e. at query time) and thus directly affect query latency and the hardware requirements of systems in production, while the LLM feature generation of PromptPRF can be run offline, thus not affecting query latency and providing more opportunities for better computation distribution and management. Our approach integrates PRF into LLM-based dense retrieval within the PromptReps framework (Zhuang et al., 2024). The methodology consists of four key components: (1) initial retrieval, (2) feature extraction, (3) query refinement, and (4) second-stage retrieval. Initial Retrieval. We employ the dense retrieval component of PromptReps (Zhuang et al., 2024), which has a bi-encoder structure consisting of EDsubscriptE\ud835\udc37\\text{E}_{D}E start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT and EQsubscriptE\ud835\udc44\\text{E}_{Q}E start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT and operates in a zero-shot manner without additional training. Given a query q\ud835\udc5eqitalic_q PromptReps generates a dense representation \ud835\udc2a\ud835\udc2a\\mathbf{q}bold_q by prompting an LLM-based EQsubscriptE\ud835\udc44\\text{E}_{Q}E start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT to summarize the query into a single word and extract its hidden representation. Relevance is estimated using the cosine similarity: s\u2062(q,p)=\ud835\udc2a\u22c5\ud835\udc29|\ud835\udc2a|\u2062|\ud835\udc29|\ud835\udc60\ud835\udc5e\ud835\udc5d\u22c5\ud835\udc2a\ud835\udc29\ud835\udc2a\ud835\udc29s(q,p)=\\frac{\\mathbf{q}\\cdot\\mathbf{p}}{|\\mathbf{q}||\\mathbf{p}|}italic_s ( italic_q , italic_p ) = divide start_ARG bold_q \u22c5 bold_p end_ARG start_ARG | bold_q | | bold_p | end_ARG, where \ud835\udc29\ud835\udc29\\mathbf{p}bold_p is the dense representation of a passage p\ud835\udc5dpitalic_p. After applying the similarity function among each passage in the collection in response to a query, we obtain a ranked list ranked according to the relevance score s\u2062(q,pj)\ud835\udc60\ud835\udc5esubscript\ud835\udc5d\ud835\udc57s(q,p_{j})italic_s ( italic_q , italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) in descending order: R={p1,p2,\u2026,pn}\ud835\udc45subscript\ud835\udc5d1subscript\ud835\udc5d2\u2026subscript\ud835\udc5d\ud835\udc5bR=\\{{p_{1},p_{2},...,p_{n}}\\}italic_R = { italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }, each pjsubscript\ud835\udc5d\ud835\udc57p_{j}italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT consists of the text: t\u2062e\u2062x\u2062t\u2062(pj)\ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61subscript\ud835\udc5d\ud835\udc57text(p_{j})italic_t italic_e italic_x italic_t ( italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) and rank: j\ud835\udc57jitalic_j. Feature Extraction. Based on the pseudo-relevance feedback assumption, we only obtain the top k\ud835\udc58kitalic_k passage and make the PRF passage set \ud835\udcabk=[(text(p1),1],[text(p2),2],\u2026,[text(pk],k]\\mathcal{P}_{k}=[(text(p_{1}),1],[text(p_{2}),2],...,[text(p_{k}],k]caligraphic_P start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = [ ( italic_t italic_e italic_x italic_t ( italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , 1 ] , [ italic_t italic_e italic_x italic_t ( italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , 2 ] , \u2026 , [ italic_t italic_e italic_x italic_t ( italic_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ] , italic_k ]. Traditional PRF methods expand the query using the original text information. However, a PRF passage is typically long and contains noisy information (Li et al., 2022a); this could mislead the PromptReps model. To enhance PRF, we generate passage-level features from the top-ranked results of the initial retrieval. To generate features we prompt an LLM backbone with an instruction and the passage. Let pjsubscript\ud835\udc5d\ud835\udc57p_{j}italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT denote the PRF passage at rank j\ud835\udc57jitalic_j, and let T=t1,t2,\u2026,tn\ud835\udc47subscript\ud835\udc611subscript\ud835\udc612\u2026subscript\ud835\udc61\ud835\udc5bT={t_{1},t_{2},...,t_{n}}italic_T = italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 , italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT represent the set of the feature types to be generated by LLMs. Following (Mackie et al., 2023a), the feature types extracted are T\ud835\udc47Titalic_T ={{\\{{keywords, entities, summaries, essays, keywords-COT222COT is short for Chain-of-Thoughts., entities-COT, news articles, facts, query keywords, and documents}}\\}}. For each feature type t\u2208T\ud835\udc61\ud835\udc47t\\in Titalic_t \u2208 italic_T, let Itsubscript\ud835\udc3c\ud835\udc61I_{t}italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT denote the instruction or prompt template designed to guide the LLM in generating feature t\ud835\udc61titalic_t. The feature generation for the feature at rank j, i.e. ft(j)superscriptsubscript\ud835\udc53\ud835\udc61\ud835\udc57f_{t}^{(j)}italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT can be described as ft(j)=LLM\u2061(t\u2062e\u2062x\u2062t\u2062(pj),It)superscriptsubscript\ud835\udc53\ud835\udc61\ud835\udc57LLM\ud835\udc61\ud835\udc52\ud835\udc65\ud835\udc61subscript\ud835\udc5d\ud835\udc57subscript\ud835\udc3c\ud835\udc61f_{t}^{(j)}=\\operatorname{LLM}\\left(text(p_{j}),I_{t}\\right)italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT = roman_LLM ( italic_t italic_e italic_x italic_t ( italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) , italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ). The feature collection for the PRF passage pjsubscript\ud835\udc5d\ud835\udc57p_{j}italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is obtained as: We provide the prompts in our GitHub repository (see Sec. 3). Feature extraction can be performed offline as it does not depend on the query, thus it does not contribute to extra query latency. Query Refinement with PRF. We modify the PromptReps query encoding by incorporating the extracted passage features. Instead of encoding the original query alone, we include top-ranked passages (or their extracted features) within the prompt. This allows for enhanced contextualization of the query representation. The refined query representation \ud835\udc2a\u2032superscript\ud835\udc2a\u2032\\mathbf{q}^{\\prime}bold_q start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT is obtained as: where \u03b2isubscript\ud835\udefd\ud835\udc56\\beta_{i}italic_\u03b2 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is a hyperparameter that controls the contribution of each passage\u2019s extracted feature and Template\u2062(q)Template\ud835\udc5e\\text{Template}(q)Template ( italic_q ) refers to the query-specific prompt template used in this work, and reported in the github repository. Along with features, we also include information about the rank of the passage from the first round of retrieval (Rank\u2061jRank\ud835\udc57\\operatorname{Rank}jroman_Rank italic_j). In this paper we do not experiment with \u03b2isubscript\ud835\udefd\ud835\udc56\\beta_{i}italic_\u03b2 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (thus, \u03b2i=1subscript\ud835\udefd\ud835\udc561\\beta_{i}=1italic_\u03b2 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1), nor we study how different feature types could be combined; we plan to study these aspects in the future. Second Retrieval. The refined query representation \ud835\udc2a\u2032superscript\ud835\udc2a\u2032\\mathbf{q}^{\\prime}bold_q start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT is then used in a second-stage retrieval, replacing \ud835\udc2a\ud835\udc2a\\mathbf{q}bold_q within the PromptReps prompt. The new relevance score s\u2062(q\u2032,p)\ud835\udc60superscript\ud835\udc5e\u2032\ud835\udc5ds(q^{\\prime},p)italic_s ( italic_q start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_p ) can then again be computed as the similarity between the query dense representation and the passage dense representation. Our PromptPRF method shares similarities with recent methods in the literature. The GenPRF method directly generates expansion terms using the FlanT5 model (Wang et al., 2023a); it is similar in that like PromptPRF it uses an LLM to generate features, but in the case of GenPRF the type of features are limited to expansion terms. GRF (Mackie et al., 2023b) and Query2doc (Wang et al., 2023c) are query expansion methods that enrich queries using LLM-generated text. Query2doc does this by generating pseudo-documents given a query; in PromptPRF we also include a feature that generates a document, but the input is another document. GRF is the most similar work to ours. We share the same features and adapt their prompts; however PromptPRF refines the query using both the extracted passage features as well as the rank structure of the PRF document list. Datasets: We study PromptPRF on TREC DL\u201919 (Craswell et al., 2020) and \u201920 (Craswell et al., 2021) benchmarks. Evaluation Metrics: To measure retrieval effectiveness, we use nDCG@10 to assess ranking quality and Recall@1000 to evaluate retrieval coverage, providing insights into both precision-oriented and recall-oriented performance aspects. Rankers: We use Llama 3 8B Instruct and Llama 3.2 3B Instruct within the PromptReps framework as base rankers without PRF. These models represent a larger and a smaller LLM, allowing us to investigate the effect of model scaling. We use the same backbones for encoding query representations obtained with PromptPRF. Features Extractors: We use four LLMs for feature extraction: Llama 3 8B Instruct333https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct (Grattafiori and et. al., 2024), Llama 3.2 3B Instruct444https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct (Grattafiori and et. al., 2024), Llama 3.3 70B Instruct555https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct (Grattafiori and et. al., 2024), and DeepSeek R1 70B666https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B (DeepSeek-AI and et. al., 2025). These models vary in parameter size, allowing us to understand how PRF feature quality scales with model size. Baselines: Along with PromptReps without PRF, we include a PRF baseline where the feedback passage(s) text is simply concatenated to the text of the query and then encoded using PromptReps, thus forming a naive PRF method that resembles PromptPRF but does not use feature extractions. Hardware & Code: Experiments are conducted on a server equipped with two H100 GPUs. Code and prompts are made available at https://anonymous.4open.science/r/PromptPRF-BE44. In this section, we investigate the effectiveness of PromptPRF (Sec. 4.1), the impact of the feature extractor (Sec. 4.2), and the impact of feature types and PDF depth on PromptPRF (Sec. 4.3). Table 1 presents the effectiveness of PromptPRF for a large (Llama3 8B) and a small (Llama3.2 3B) LLM-based dense retriever (DR). The results show that incorporating PRF with targeted feature extraction enables smaller models to achieve retrieval effectiveness comparable to that of larger LLM-based DRs without PRF. For Llama3.2 3B, which serves as a smaller DR, applying PRF leads to a significant improvement in nDCG@10. On DL19, adopting Llama3.3 70B as the feature extractor with Entities-COT at PRF depth 5 increases nDCG@10 from 0.3695 (no PRF) to 0.5013, approaching the baseline Llama3 8B (no PRF, nDCG@10 = 0.5062). Similarly, on DL20, using Llama3.2 3B with Facts features boosts nDCG@10 close to that of the baseline Llama3 8B (no PRF). For Llama3 8B, applying PromptPRF consistently outperforms the no PRF and passage PRF (PassPRF) baselines. On DL19, employing Llama3.2 3B as a feature extractor with Essay at PRF depth 1 achieves an nDCG@10 of 0.5560, which is higher than the no PRF and PassPRF baselines. Similarly, on DL20, using Keywords-COT at PRF depth 20 achieves 0.5038 nDCG@10, also significantly surpassing the no PRF and PassPRF baselines with only a small LLM (Llama3.2 3B) as the feature extractor. The results suggest that smaller LLM-based DRs, when enhanced with PromptPRF, can reach effectiveness levels comparable to significantly larger models without PRF, offering a practical and efficient alternative to relying solely on larger LLM-based retrievers. To investigate the impact of the feature extraction model, we freeze PRF depth and feature type according to the best DL19 results (c.f. Table 1: DR = Llama3.2 3B, PRF depth = 5, entities-cot; DR = Llama3 8B, PRF depth = 1, essay feature). Results are reported in Fig. 1. Effectiveness. The results highlight a relationship between feature extractor size and PRF effectiveness. The size of feature extractors and dense retriever models jointly affect retrieval effectiveness, where the impact of larger feature extractors is more pronounced for smaller LLM-based dense retrievers and larger dense retrievers achieve better effectiveness with smaller feature extractors. More specifically, for the DR based on Llama3.2-3B, larger feature extractors (e.g., Llama3.3-70B) significantly enhance retrieval effectiveness, especially in the Entities-COT PRF with PRF depth 5, where they consistently yield higher nDCG@10. The improvement shows that larger extractors provide richer contextual representations, compensating for the weaker initial query representations produced by the smaller dense retriever. In contrast, when the DR is based on Llama3-8B, smaller feature extractors (e.g., Llama3.2-3B) provide substantial improvements in retrieval effectiveness. However, scaling up to larger feature extractors (e.g., 70B models) leads to diminishing returns. These results suggest that larger dense retrievers have sufficient internal representation capacity and semantic understanding to be effective with smaller feature extractors, beyond which additional scaling of the extractor does not significantly improve effectiveness. Efficiency. From an efficiency standpoint, the results demonstrate that scaling feature extractors is critical for enhancing PRF effectiveness in smaller DRs. However, these benefits diminish for larger DRs. As feature extraction is conducted offline, the primary consideration shifts from the extraction cost to the computational overhead incurred during the integration of expanded features into the on-the-fly PRF process. The DR based on Llama3-8B achieves strong effectiveness even when paired with smaller feature extractors, and while the use of larger extractors provides some incremental improvements, these gains do not justify the associated increase in extraction costs. In contrast, the smaller DR based on Llama3.2-3B requires larger feature extractors to obtain high PRF effectiveness. This suggests that feature extraction scaling is a necessary step for maximizing effectiveness in lightweight retrievers. Overall, these findings indicate that PRF optimization should aim to strike a balance between retrieval effectiveness and (offline) computational costs, employing larger feature extractors for smaller retrievers while relying on cheaper extractors for larger retrievers without compromising retrieval quality. Figure 2 studies the impact of varying PRF depths and feature types777Certain features are incompatible with greater PRF depths due to their extensive content exceeding the model\u2019s input length constraints. for the two DRs used in our experiments. For the DR based on Llama3.2 3B (plots 1 and 3 in Fig. 2), we freeze the feature extraction model to be Llama3.3 70B aligning with the top effectiveness for Llama3.2 3B (Table 1). We also applied the same settings to the DR based on Llama3 8B (plots 2 and 4 in Fig. 2). The results reveal that both the choice of features and the depth of feedback retrieval play a crucial role in PRF effectiveness, with distinct trends emerging across different configurations. A general trend observed across both models is that moderate PRF depths, particularly in the range of 3 to 10, contribute to a consistent improvement in retrieval effectiveness. However, at deeper depths (e.g., PRF depth of 50), effectiveness degradation becomes evident, especially for certain feature types such as Queries-Keyword and News-Article in Llama3 8B. This suggests that while PRF is beneficial in augmenting query representations, an overly broad expansion may introduce excessive noise, diluting the relevance of retrieved passages. These findings align with previous studies on retrieval sensitivity to top-k expansion, reinforcing the necessity of controlled retrieval augmentation. When examining different feature types, we find that the structured entity- and keyword-based features, such as Entities-COT and Keywords-COT, consistently enhance nDCG@10 across various PRF depths, highlighting the significance of structured contextual enrichment for improved query comprehension. In contrast, features based on factual summarization, such as Facts and Summary maintain stable effectiveness, indicating that concise, fact-oriented context aids in query augmentation without introducing ambiguity. Document-based PRF, however, shows diminishing returns at higher PRF depths, suggesting that using full passages can introduce excessive noise and lead to query drift. Similarly, features relying on query-keyword and news-article exhibit high variability, with effectiveness deteriorating at larger PRF depths. Overall, structured information improves query representation, but overly broad or loosely related context can reduce retrieval precision. Comparing the two dense retrievers, Llama3 8B consistently outperforms Llama3.2 3B across all configurations, both with and without PRF. This is expected, as Llama3 8B is inherently a stronger retrieval model, and its superior effectiveness with PRF is more likely due to its higher base retrieval document list. While both models benefit from PRF, Llama3.2 3B exhibits more pronounced improvements at lower PRF depths (1\u20135), whereas Llama3 8B maintains effectiveness across moderate depths (10\u201320). This suggests that smaller models may require more conservative query expansion strategies, while stronger base retrievers can incorporate expanded context more effectively at a broader range of PRF depths. Additionally, the impact of specific features varies between models, as evidenced by essay-based PRF yielding stronger improvements for Llama3.2 3B but having a lesser effect on Llama3 8B. In summary, we find that structured features like Entities-COT and Keywords-COT can improve retrieval effectiveness, while broad expansions through raw passages or loosely related contexts can cause query drift. PRF strategies should be tailored to the underlying retrieval model, with weaker models (e.g., Llama3.2 3B) requiring conservative expansion, while stronger models (e.g., Llama3 8B) can handle deeper augmentations. These findings underscore the importance of tuning PRF depth and feature selection for optimal retrieval effectiveness. Efficient retrieval in real-world applications requires a careful balance between effectiveness and computational cost. In this section, we analyze the infrastructure requirements, query latency, and the trade-offs between retrieval effectiveness and resource efficiency. We evaluate the minimum computational resources needed to deploy the different LLMs used in our study. Table 2 presents the disk space, RAM, and GPU requirements for the LLMs used in our experiments as the feature extractors and dense retrievers. These numbers highlight that while smaller models like Llama3.2 3B Instruct can operate on consumer-grade GPUs, larger models require high-end hardware with multiple H100 or A100 GPUs. The RAM overhead of the 70B models further increases deployment complexity, making them significantly more costly. Table 3 summarizes the query encoding latency for the two base LLMs used in our retrieval experiments. From these results, Llama3.2 3B is approximately 2.7\u00d7 faster per query than Llama3 8B, making it significantly more efficient for real-time retrieval applications. Given that PRF feature extraction is performed offline, the online latency of query encoding is the primary factor that affecting real-time retrieval performance. The key finding from our experiments is that smaller dense retrievers with PRF can achieve effectiveness comparable to larger models without PRF, while significantly reducing computational cost. This trade-off is evident in the following observations: Llama3.2 3B with PRF approaches Llama3 8B without PRF, achieving similar nDCG@10 scores while reducing query latency by approximately 63%. For feature extraction, larger models (e.g., Llama3.3 70B) provide marginal effectiveness gains when paired with Llama3 8B, but require substantially higher inference costs. Smaller feature extractors (e.g., Llama3.2 3B) are often sufficient for PRF, particularly for larger retrievers, reducing overhead without compromising performance. These observations suggest that optimizing PRF enables cost-effective retrieval without requiring large LLMs. By strategically selecting feature extractors and balancing PRF depth, retrieval effectiveness can be maximized while minimizing inference cost. For real-world deployment: Smaller models (Llama3.2 3B) should be prioritized for efficiency-critical applications such as conversational search or mobile deployments. Larger models (Llama3 8B and above) are selected only if the marginal gain in retrieval quality offsets the increased computational costs. PRF should be tuned based on hardware constraints, leveraging offline processing for feature extraction to minimize online latency. These insights highlight that efficient PRF strategies can mitigate the need for large-scale dense retrievers, using a smaller dense retriever with PRF can achieve comparable effectiveness to larger dense retrievers, making advanced retrieval techniques more accessible and scalable. This study demonstrates that integrating pseudo-relevance feedback into LLM-based dense retrieval significantly improves retrieval effectiveness, particularly for smaller retrieval models. By leveraging LLM-derived passage features for query expansion, PromptPRF enables smaller dense retrievers (Llama3.2-3B) to achieve performance comparable to significantly larger retrievers without PRF (Llama3-8B). This is unlike findings in related literature, where it is shown LLMs exhibit a power-law scaling with respect to the number of parameters (and the amount of training data) (Kaplan et al., 2020; Hoffmann et al., 2022), a scaling law that has also been observed for dense retrieval (Fang et al., 2024). The fact that using PromptPRF enables PromptReps-based dense retrieval to break this law is advantageous in that it helps to reduce retrieval costs at query time. The ability to run in production a smaller model also comes with the added advantage that less capable hardware (e.g. GPUs with smaller memory) is required. Our analysis also reveals that smaller dense retrievers benefit significantly from feature extractors based on larger LLM backbones, as these smaller rankers can rely on externally enriched representations to improve query reformulation. In contrast, larger dense retrievers (e.g., based on Llama3-8B) benefit relatively less from increasing extractor size. This suggests that the effectiveness of PRF depends on both the retrieval model and the feature extractor, rather than simply scaling either component. We also examine the relationship between PRF depth and feature types, finding that moderate PRF depths (e.g., 3 to 10) provide the most consistent retrieval improvements, while deeper expansion can introduce noise and query drift; a result that is not surprising in light of previous work. Additionally, structured features such as Entities-Cot and Keywords-Cot yield more stable improvements than unstructured expansions like full passages or news articles. This is good news because these structured features typically produce less output tokens than the unstructured expansions, resulting in lower feature extraction and PRF utilization costs. The promising results obtained by PromptPRF open up a number of future directions, which could not be explored in this paper. These include investigating (i) the impact of the method\u2019s parameters such as \u03b2isubscript\ud835\udefd\ud835\udc56\\beta_{i}italic_\u03b2 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, which controls the contribution of the features of the individual passage pisubscript\ud835\udc5d\ud835\udc56p_{i}italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT on a per-rank basis, (ii) the impact of the original passage rank information within PromptPRF\u2019s prompt, and in particular whether the removal of this information leads to decreased effectiveness, and (iii) the combination of multiple features to represent the PRF signal."
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Li,+H",
    "title": "Showing 1\u201350 of 5,055 results for author: Li, H",
    "author": "Authors:\nJunyi Ao, \n      \n      Dekun Chen, \n      \n      Xiaohai Tian, \n      \n      Wenjie Feng, \n      \n      Jun Zhang, \n      \n      Lu Lu, \n      \n      Yuxuan Wang, \n      \n      Haizhou Li, \n      \n      Zhizheng Wu",
    "body": "arXiv:2503.15338 [pdf, other] Solla: Towards a Speech-Oriented LLM That Hears Acoustic Context Authors: Junyi Ao, Dekun Chen, Xiaohai Tian, Wenjie Feng, Jun Zhang, Lu Lu, Yuxuan Wang, Haizhou Li, Zhizheng Wu Abstract: Large Language Models (LLMs) have recently shown remarkable ability to process not only text but also multimodal inputs such as speech and audio. However, most existing models primarily focus on analyzing input signals using text instructions, overlooking scenarios in which speech instructions and audio are mixed and serve as inputs to the model. To address these challenges, we introduce Solla, a\u2026 \u25bd More Large Language Models (LLMs) have recently shown remarkable ability to process not only text but also multimodal inputs such as speech and audio. However, most existing models primarily focus on analyzing input signals using text instructions, overlooking scenarios in which speech instructions and audio are mixed and serve as inputs to the model. To address these challenges, we introduce Solla, a novel framework designed to understand speech-based questions and hear the acoustic context concurrently. Solla incorporates an audio tagging module to effectively identify and represent audio events, as well as an ASR-assisted prediction method to improve comprehension of spoken content. To rigorously evaluate Solla and other publicly available models, we propose a new benchmark dataset called SA-Eval, which includes three tasks: audio event classification, audio captioning, and audio question answering. SA-Eval has diverse speech instruction with various speaking styles, encompassing two difficulty levels, easy and hard, to capture the range of real-world acoustic conditions. Experimental results show that Solla performs on par with or outperforms baseline models on both the easy and hard test sets, underscoring its effectiveness in jointly understanding speech and audio. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. arXiv:2503.15275 [pdf, other] Challenges and Trends in Egocentric Vision: A Survey Authors: Xiang Li, Heqian Qiu, Lanxiao Wang, Hanwen Zhang, Chenghao Qi, Linfeng Han, Huiyu Xiong, Hongliang Li Abstract: With the rapid development of artificial intelligence technologies and wearable devices, egocentric vision understanding has emerged as a new and challenging research direction, gradually attracting widespread attention from both academia and industry. Egocentric vision captures visual and multimodal data through cameras or sensors worn on the human body, offering a unique perspective that simulat\u2026 \u25bd More With the rapid development of artificial intelligence technologies and wearable devices, egocentric vision understanding has emerged as a new and challenging research direction, gradually attracting widespread attention from both academia and industry. Egocentric vision captures visual and multimodal data through cameras or sensors worn on the human body, offering a unique perspective that simulates human visual experiences. This paper provides a comprehensive survey of the research on egocentric vision understanding, systematically analyzing the components of egocentric scenes and categorizing the tasks into four main areas: subject understanding, object understanding, environment understanding, and hybrid understanding. We explore in detail the sub-tasks within each category. We also summarize the main challenges and trends currently existing in the field. Furthermore, this paper presents an overview of high-quality egocentric vision datasets, offering valuable resources for future research. By summarizing the latest advancements, we anticipate the broad applications of egocentric vision technologies in fields such as augmented reality, virtual reality, and embodied intelligence, and propose future research directions based on the latest developments in the field. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. arXiv:2503.15167 [pdf, other] Volumetric Reconstruction From Partial Views for Task-Oriented Grasping Authors: Fujian Yan, Hui Li, Hongsheng He Abstract: Object affordance and volumetric information are essential in devising effective grasping strategies under task-specific constraints. This paper presents an approach for inferring suitable grasping strategies from limited partial views of an object. To achieve this, a recurrent generative adversarial network (R-GAN) was proposed by incorporating a recurrent generator with long short-term memory (L\u2026 \u25bd More Object affordance and volumetric information are essential in devising effective grasping strategies under task-specific constraints. This paper presents an approach for inferring suitable grasping strategies from limited partial views of an object. To achieve this, a recurrent generative adversarial network (R-GAN) was proposed by incorporating a recurrent generator with long short-term memory (LSTM) units for it to process a variable number of depth scans. To determine object affordances, the AffordPose knowledge dataset is utilized as prior knowledge. Affordance retrieving is defined by the volume similarity measured via Chamfer Distance and action similarities. A Proximal Policy Optimization (PPO) reinforcement learning model is further implemented to refine the retrieved grasp strategies for task-oriented grasping. The retrieved grasp strategies were evaluated on a dual-arm mobile manipulation robot with an overall grasping accuracy of 89% for four tasks: lift, handle grasp, wrap grasp, and press. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. arXiv:2503.15044 [pdf, other] SPADE: Systematic Prompt Framework for Automated Dialogue Expansion in Machine-Generated Text Detection Authors: Haoyi Li, Angela Yifei Yuan, Soyeon Caren Han, Christopher Leckie Abstract: The increasing capability of large language models (LLMs) to generate synthetic content has heightened concerns about their misuse, driving the development of Machine-Generated Text (MGT) detection models. However, these detectors face significant challenges due to the lack of systematically generated, high-quality datasets for training. To address this issue, we propose five novel data augmentati\u2026 \u25bd More The increasing capability of large language models (LLMs) to generate synthetic content has heightened concerns about their misuse, driving the development of Machine-Generated Text (MGT) detection models. However, these detectors face significant challenges due to the lack of systematically generated, high-quality datasets for training. To address this issue, we propose five novel data augmentation frameworks for synthetic user dialogue generation through a structured prompting approach, reducing the costs associated with traditional data collection methods. Our proposed method yields 14 new dialogue datasets, which we benchmark against seven MGT detection models. The results demonstrate improved generalization performance when utilizing a mixed dataset produced by our proposed augmentation framework. Furthermore, considering that real-world agents lack knowledge of future opponent utterances, we simulate online dialogue detection and examine the relationship between chat history length and detection accuracy. We also benchmark online detection performance with limited chat history on our frameworks. Our open-source datasets can be downloaded from https://github.com/AngieYYF/SPADE-customer-service-dialogue. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. Comments: 9 pages arXiv:2503.15024 [pdf, other] Forensics-Bench: A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models Authors: Jin Wang, Chenghui Lv, Xian Li, Shichao Dong, Huadong Li, kelu Yao, Chao Li, Wenqi Shao, Ping Luo Abstract: Recently, the rapid development of AIGC has significantly boosted the diversities of fake media spread in the Internet, posing unprecedented threats to social security, politics, law, and etc. To detect the ever-increasingly diverse malicious fake media in the new era of AIGC, recent studies have proposed to exploit Large Vision Language Models (LVLMs) to design robust forgery detectors due to the\u2026 \u25bd More Recently, the rapid development of AIGC has significantly boosted the diversities of fake media spread in the Internet, posing unprecedented threats to social security, politics, law, and etc. To detect the ever-increasingly diverse malicious fake media in the new era of AIGC, recent studies have proposed to exploit Large Vision Language Models (LVLMs) to design robust forgery detectors due to their impressive performance on a wide range of multimodal tasks. However, it still lacks a comprehensive benchmark designed to comprehensively assess LVLMs' discerning capabilities on forgery media. To fill this gap, we present Forensics-Bench, a new forgery detection evaluation benchmark suite to assess LVLMs across massive forgery detection tasks, requiring comprehensive recognition, location and reasoning capabilities on diverse forgeries. Forensics-Bench comprises 63,292 meticulously curated multi-choice visual questions, covering 112 unique forgery detection types from 5 perspectives: forgery semantics, forgery modalities, forgery tasks, forgery types and forgery models. We conduct thorough evaluations on 22 open-sourced LVLMs and 3 proprietary models GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet, highlighting the significant challenges of comprehensive forgery detection posed by Forensics-Bench. We anticipate that Forensics-Bench will motivate the community to advance the frontier of LVLMs, striving for all-around forgery detectors in the era of AIGC. The deliverables will be updated at https://Forensics-Bench.github.io/. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. Comments: 31 pages, 19 figures arXiv:2503.14887 [pdf, other] Pseudo-Relevance Feedback Can Improve Zero-Shot LLM-Based Dense Retrieval Authors: Hang Li, Xiao Wang, Bevan Koopman, Guido Zuccon Abstract: Pseudo-relevance feedback (PRF) refines queries by leveraging initially retrieved documents to improve retrieval effectiveness. In this paper, we investigate how large language models (LLMs) can facilitate PRF for zero-shot LLM-based dense retrieval, extending the recently proposed PromptReps method. Specifically, our approach uses LLMs to extract salient passage features-such as keywords and summ\u2026 \u25bd More Pseudo-relevance feedback (PRF) refines queries by leveraging initially retrieved documents to improve retrieval effectiveness. In this paper, we investigate how large language models (LLMs) can facilitate PRF for zero-shot LLM-based dense retrieval, extending the recently proposed PromptReps method. Specifically, our approach uses LLMs to extract salient passage features-such as keywords and summaries-from top-ranked documents, which are then integrated into PromptReps to produce enhanced query representations. Experiments on passage retrieval benchmarks demonstrate that incorporating PRF significantly boosts retrieval performance. Notably, smaller rankers with PRF can match the effectiveness of larger rankers without PRF, highlighting PRF's potential to improve LLM-driven search while maintaining an efficient balance between effectiveness and resource usage. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. arXiv:2503.14863 [pdf, other] Temporal-Consistent Video Restoration with Pre-trained Diffusion Models Authors: Hengkang Wang, Yang Liu, Huidong Liu, Chien-Chih Wang, Yanhui Guo, Hongdong Li, Bryan Wang, Ju Sun Abstract: Video restoration (VR) aims to recover high-quality videos from degraded ones. Although recent zero-shot VR methods using pre-trained diffusion models (DMs) show good promise, they suffer from approximation errors during reverse diffusion and insufficient temporal consistency. Moreover, dealing with 3D video data, VR is inherently computationally intensive. In this paper, we advocate viewing the r\u2026 \u25bd More Video restoration (VR) aims to recover high-quality videos from degraded ones. Although recent zero-shot VR methods using pre-trained diffusion models (DMs) show good promise, they suffer from approximation errors during reverse diffusion and insufficient temporal consistency. Moreover, dealing with 3D video data, VR is inherently computationally intensive. In this paper, we advocate viewing the reverse process in DMs as a function and present a novel Maximum a Posterior (MAP) framework that directly parameterizes video frames in the seed space of DMs, eliminating approximation errors. We also introduce strategies to promote bilevel temporal consistency: semantic consistency by leveraging clustering structures in the seed space, and pixel-level consistency by progressive warping with optical flow refinements. Extensive experiments on multiple virtual reality tasks demonstrate superior visual quality and temporal consistency achieved by our method compared to the state-of-the-art. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. arXiv:2503.14647 [pdf, other] Towards More Economical Context-Augmented LLM Generation by Reusing Stored KV Cache Authors: Hanchen Li, Yuhan Liu, Yihua Cheng, Kuntai Du, Junchen Jiang Abstract: Across large language model (LLM) applications, we observe an emerging trend for reusing KV caches to save the prefill delays of processing repeated input texts in different LLM inputs. This has led to a broad design space, including colocating stored KV caches with (or close to) GPUs to various KV cache compression. However, a key question remains unanswered: can these delay reductions also be ec\u2026 \u25bd More Across large language model (LLM) applications, we observe an emerging trend for reusing KV caches to save the prefill delays of processing repeated input texts in different LLM inputs. This has led to a broad design space, including colocating stored KV caches with (or close to) GPUs to various KV cache compression. However, a key question remains unanswered: can these delay reductions also be economically favorable? Specifically, we ask whether a developer can use public cloud services to store precomputed KV caches and reuse them to save delay without incurring more costs in terms of compute, storage, and network. To answer this question, we propose an validated analytical model for the cloud cost (in compute, storage, and network) of storing and reusing KV caches based on various workload parameters, such as reuse frequency, generated text lengths, model sizes, etc. Preliminary results show that KV cache reusing is able to save both delay and cloud cost across a range of workloads with long context. And we call more efforts on building more economical context augmented LLM by KV cache reusing. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. arXiv:2503.14535 [pdf, other] Interpretable Unsupervised Joint Denoising and Enhancement for Real-World low-light Scenarios Authors: Huaqiu Li, Xiaowan Hu, Haoqian Wang Abstract: Real-world low-light images often suffer from complex degradations such as local overexposure, low brightness, noise, and uneven illumination. Supervised methods tend to overfit to specific scenarios, while unsupervised methods, though better at generalization, struggle to model these degradations due to the lack of reference images. To address this issue, we propose an interpretable, zero-referen\u2026 \u25bd More Real-world low-light images often suffer from complex degradations such as local overexposure, low brightness, noise, and uneven illumination. Supervised methods tend to overfit to specific scenarios, while unsupervised methods, though better at generalization, struggle to model these degradations due to the lack of reference images. To address this issue, we propose an interpretable, zero-reference joint denoising and low-light enhancement framework tailored for real-world scenarios. Our method derives a training strategy based on paired sub-images with varying illumination and noise levels, grounded in physical imaging principles and retinex theory. Additionally, we leverage the Discrete Cosine Transform (DCT) to perform frequency domain decomposition in the sRGB space, and introduce an implicit-guided hybrid representation strategy that effectively separates intricate compounded degradations. In the backbone network design, we develop retinal decomposition network guided by implicit degradation representation mechanisms. Extensive experiments demonstrate the superiority of our method. Code will be available at https://github.com/huaqlili/unsupervised-light-enhance-ICLR2025. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. arXiv:2503.14229 [pdf, other] HA-VLN: A Benchmark for Human-Aware Navigation in Discrete-Continuous Environments with Dynamic Multi-Human Interactions, Real-World Validation, and an Open Leaderboard Authors: Yifei Dong, Fengyi Wu, Qi He, Heng Li, Minghan Li, Zebang Cheng, Yuxuan Zhou, Jingdong Sun, Qi Dai, Zhi-Qi Cheng, Alexander G Hauptmann Abstract: Vision-and-Language Navigation (VLN) systems often focus on either discrete (panoramic) or continuous (free-motion) paradigms alone, overlooking the complexities of human-populated, dynamic environments. We introduce a unified Human-Aware VLN (HA-VLN) benchmark that merges these paradigms under explicit social-awareness constraints. Our contributions include: 1. A standardized task definition that\u2026 \u25bd More Vision-and-Language Navigation (VLN) systems often focus on either discrete (panoramic) or continuous (free-motion) paradigms alone, overlooking the complexities of human-populated, dynamic environments. We introduce a unified Human-Aware VLN (HA-VLN) benchmark that merges these paradigms under explicit social-awareness constraints. Our contributions include: 1. A standardized task definition that balances discrete-continuous navigation with personal-space requirements; 2. An enhanced human motion dataset (HAPS 2.0) and upgraded simulators capturing realistic multi-human interactions, outdoor contexts, and refined motion-language alignment; 3. Extensive benchmarking on 16,844 human-centric instructions, revealing how multi-human dynamics and partial observability pose substantial challenges for leading VLN agents; 4. Real-world robot tests validating sim-to-real transfer in crowded indoor spaces; and 5. A public leaderboard supporting transparent comparisons across discrete and continuous tasks. Empirical results show improved navigation success and fewer collisions when social context is integrated, underscoring the need for human-centric design. By releasing all datasets, simulators, agent code, and evaluation tools, we aim to advance safer, more capable, and socially responsible VLN research. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. Comments: 27 pages, website: https://ha-vln-project.vercel.app/ arXiv:2503.14012 [pdf, other] LEGNet: Lightweight Edge-Gaussian Driven Network for Low-Quality Remote Sensing Image Object Detection Authors: Wei Lu, Si-Bao Chen, Hui-Dong Li, Qing-Ling Shu, Chris H. Q. Ding, Jin Tang, Bin Luo Abstract: Remote sensing object detection (RSOD) faces formidable challenges in complex visual environments. Aerial and satellite images inherently suffer from limitations such as low spatial resolution, sensor noise, blurred objects, low-light degradation, and partial occlusions. These degradation factors collectively compromise the feature discriminability in detection models, resulting in three key issue\u2026 \u25bd More Remote sensing object detection (RSOD) faces formidable challenges in complex visual environments. Aerial and satellite images inherently suffer from limitations such as low spatial resolution, sensor noise, blurred objects, low-light degradation, and partial occlusions. These degradation factors collectively compromise the feature discriminability in detection models, resulting in three key issues: (1) reduced contrast that hampers foreground-background separation, (2) structural discontinuities in edge representations, and (3) ambiguous feature responses caused by variations in illumination. These collectively weaken model robustness and deployment feasibility. To address these challenges, we propose LEGNet, a lightweight network that incorporates a novel edge-Gaussian aggregation (EGA) module specifically designed for low-quality remote sensing images. Our key innovation lies in the synergistic integration of Scharr operator-based edge priors with uncertainty-aware Gaussian modeling: (a) The orientation-aware Scharr filters preserve high-frequency edge details with rotational invariance; (b) The uncertainty-aware Gaussian layers probabilistically refine low-confidence features through variance estimation. This design enables precision enhancement while maintaining architectural simplicity. Comprehensive evaluations across four RSOD benchmarks (DOTA-v1.0, v1.5, DIOR-R, FAIR1M-v1.0) and a UAV-view dataset (VisDrone2019) demonstrate significant improvements. LEGNet achieves state-of-the-art performance across five benchmark datasets while ensuring computational efficiency, making it well-suited for deployment on resource-constrained edge devices in real-world remote sensing applications. The code is available at https://github.com/lwCVer/LEGNet. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. Comments: 12 pages, 5 figures. Remote Sensing Image Object Detection arXiv:2503.13980 [pdf, other] Empowering LLMs in Decision Games through Algorithmic Data Synthesis Authors: Haolin Wang, Xueyan Li, Yazhe Niu, Shuai Hu, Hongsheng Li Abstract: Large Language Models (LLMs) have exhibited impressive capabilities across numerous domains, yet they often struggle with complex reasoning and decision-making tasks. Decision-making games, which inherently require multifaceted reasoning logic, serve as ideal sandboxes for evaluating and enhancing the reasoning abilities of LLMs. In this work, we first explore whether LLMs can master complex decis\u2026 \u25bd More Large Language Models (LLMs) have exhibited impressive capabilities across numerous domains, yet they often struggle with complex reasoning and decision-making tasks. Decision-making games, which inherently require multifaceted reasoning logic, serve as ideal sandboxes for evaluating and enhancing the reasoning abilities of LLMs. In this work, we first explore whether LLMs can master complex decision-making games through targeted post-training. To this end, we design data synthesis strategies and curate extensive offline datasets from two classic games, Doudizhu and Go. We further develop a suite of techniques to effectively incorporate this data into LLM training, resulting in two novel agents: Mastermind-Dou and Mastermind-Go. Our experimental results demonstrate that these Mastermind LLMs achieve competitive performance in their respective games. Additionally, we explore whether integrating decision-making data can enhance the general reasoning abilities of LLMs. Our findings suggest that such post-training improves certain aspects of reasoning, providing valuable insights for optimizing LLM data collection and synthesis strategies. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. arXiv:2503.13940 [pdf, other] Multi-Modal Self-Supervised Semantic Communication Authors: Hang Zhao, Hongru Li, Dongfang Xu, Shenghui Song, Khaled B. Letaief Abstract: Semantic communication is emerging as a promising paradigm that focuses on the extraction and transmission of semantic meanings using deep learning techniques. While current research primarily addresses the reduction of semantic communication overhead, it often overlooks the training phase, which can incur significant communication costs in dynamic wireless environments. To address this challenge,\u2026 \u25bd More Semantic communication is emerging as a promising paradigm that focuses on the extraction and transmission of semantic meanings using deep learning techniques. While current research primarily addresses the reduction of semantic communication overhead, it often overlooks the training phase, which can incur significant communication costs in dynamic wireless environments. To address this challenge, we propose a multi-modal semantic communication system that leverages multi-modal self-supervised learning to enhance task-agnostic feature extraction. The proposed approach employs self-supervised learning during the pre-training phase to extract task-agnostic semantic features, followed by supervised fine-tuning for downstream tasks. This dual-phase strategy effectively captures both modality-invariant and modality-specific features while minimizing training-related communication overhead. Experimental results on the NYU Depth V2 dataset demonstrate that the proposed method significantly reduces training-related communication overhead while maintaining or exceeding the performance of existing supervised learning approaches. The findings underscore the advantages of multi-modal self-supervised learning in semantic communication, paving the way for more efficient and scalable edge inference systems. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. arXiv:2503.13784 [pdf, other] SwarmUpdate: Hierarchical Software Updates and Deep Learning Model Patching for Heterogeneous UAV Swarms Authors: Lin Geng, Hao Li, Sidney Givigi, Bram Adams Abstract: Heterogeneous unmanned aerial vehicle (UAV) swarms consist of dozens to hundreds of drones with different roles and varying hardware and software requirements collaborating towards a shared mission. While traditional approaches for synchronized software updates assume swarms to be unstructured and homogeneous, the heterogeneous nature of modern swarms and the emerging need of drones to update thei\u2026 \u25bd More Heterogeneous unmanned aerial vehicle (UAV) swarms consist of dozens to hundreds of drones with different roles and varying hardware and software requirements collaborating towards a shared mission. While traditional approaches for synchronized software updates assume swarms to be unstructured and homogeneous, the heterogeneous nature of modern swarms and the emerging need of drones to update their deep learning (perception) models with new objectives or data as a mission unfolds, has made efficient software update methods crucial for swarms to adapt to dynamic environments. To address these challenges, we introduce the SwarmUpdate framework for software updates in heterogeneous UAV swarms, composed of two key components: SwarmSync and SwarmModelPatch. SwarmSync is a hierarchical software update synchronization strategy to distribute a software update to the right subset of drones within a swarm, while SwarmModelPatch is a deep learning model patching method that reduces the size of a (deep learning model) update by only allowing some layers of the model to be updated (freezing the other layers). In this paper, we systematically evaluate the performance of SwarmSync through large-scale simulations in the ARGoS swarm simulator, comparing SwarmSync to auction-based (SOUL) and gossip-based rebroadcasting (Gossip) baselines, and SwarmModelPatch to a non-incremental model patching strategy. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. arXiv:2503.13617 [pdf, other] Let Synthetic Data Shine: Domain Reassembly and Soft-Fusion for Single Domain Generalization Authors: Hao Li, Yubin Xiao, Ke Liang, Mengzhu Wang, Long Lan, Kenli Li, Xinwang Liu Abstract: Single Domain Generalization (SDG) aims to train models with consistent performance across diverse scenarios using data from a single source. While using latent diffusion models (LDMs) show promise in augmenting limited source data, we demonstrate that directly using synthetic data can be detrimental due to significant feature distribution discrepancies between synthetic and real target domains, l\u2026 \u25bd More Single Domain Generalization (SDG) aims to train models with consistent performance across diverse scenarios using data from a single source. While using latent diffusion models (LDMs) show promise in augmenting limited source data, we demonstrate that directly using synthetic data can be detrimental due to significant feature distribution discrepancies between synthetic and real target domains, leading to performance degradation. To address this issue, we propose Discriminative Domain Reassembly and Soft-Fusion (DRSF), a training framework leveraging synthetic data to improve model generalization. We employ LDMs to produce diverse pseudo-target domain samples and introduce two key modules to handle distribution bias. First, Discriminative Feature Decoupling and Reassembly (DFDR) module uses entropy-guided attention to recalibrate channel-level features, suppressing synthetic noise while preserving semantic consistency. Second, Multi-pseudo-domain Soft Fusion (MDSF) module uses adversarial training with latent-space feature interpolation, creating continuous feature transitions between domains. Extensive SDG experiments on object detection and semantic segmentation tasks demonstrate that DRSF achieves substantial performance gains with only marginal computational overhead. Notably, DRSF's plug-and-play architecture enables seamless integration with unsupervised domain adaptation paradigms, underscoring its broad applicability in addressing diverse and real-world domain challenges. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. Comments: 26 pages, 10 figures arXiv:2503.13582 [pdf, ps, other] Spectrally-Corrected and Regularized QDA Classifier for Spiked Covariance Model Authors: Wenya Luo, Hua Li, Zhidong Bai, Zhijun Liu Abstract: Quadratic discriminant analysis (QDA) is a widely used method for classification problems, particularly preferable over Linear Discriminant Analysis (LDA) for heterogeneous data. However, QDA loses its effectiveness in high-dimensional settings, where the data dimension and sample size tend to infinity. To address this issue, we propose a novel QDA method utilizing spectral correction and regulari\u2026 \u25bd More Quadratic discriminant analysis (QDA) is a widely used method for classification problems, particularly preferable over Linear Discriminant Analysis (LDA) for heterogeneous data. However, QDA loses its effectiveness in high-dimensional settings, where the data dimension and sample size tend to infinity. To address this issue, we propose a novel QDA method utilizing spectral correction and regularization techniques, termed SR-QDA. The regularization parameters in our method are selected by maximizing the Fisher-discriminant ratio. We compare SR-QDA with QDA, regularized quadratic discriminant analysis (R-QDA), and several other competitors. The results indicate that SR-QDA performs exceptionally well, especially in moderate and high-dimensional situations. Empirical experiments across diverse datasets further support this conclusion. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. arXiv:2503.13492 [pdf, other] Event-Driven Implementation of a Physical Reservoir Computing Framework for superficial EMG-based Gesture Recognition Authors: Yuqi Ding, Elisa Donati, Haobo Li, Hadi Heidari Abstract: Wearable health devices have a strong demand in real-time biomedical signal processing. However traditional methods often require data transmission to centralized processing unit with substantial computational resources after collecting it from edge devices. Neuromorphic computing is an emerging field that seeks to design specialized hardware for computing systems inspired by the structure, functi\u2026 \u25bd More Wearable health devices have a strong demand in real-time biomedical signal processing. However traditional methods often require data transmission to centralized processing unit with substantial computational resources after collecting it from edge devices. Neuromorphic computing is an emerging field that seeks to design specialized hardware for computing systems inspired by the structure, function, and dynamics of the human brain, offering significant advantages in latency and power consumption. This paper explores a novel neuromorphic implementation approach for gesture recognition by extracting spatiotemporal spiking information from surface electromyography (sEMG) data in an event-driven manner. At the same time, the network was designed by implementing a simple-structured and hardware-friendly Physical Reservoir Computing (PRC) framework called Rotating Neuron Reservoir (RNR) within the domain of Spiking neural network (SNN). The spiking RNR (sRNR) is promising to pipeline an innovative solution to compact embedded wearable systems, enabling low-latency, real-time processing directly at the sensor level. The proposed system was validated by an open-access large-scale sEMG database and achieved an average classification accuracy of 74.6\\% and 80.3\\% using a classical machine learning classifier and a delta learning rule algorithm respectively. While the delta learning rule could be fully spiking and implementable on neuromorphic chips, the proposed gesture recognition system demonstrates the potential for near-sensor low-latency processing. \u25b3 Less Submitted 10 March, 2025; originally announced March 2025. Comments: 11 pages, 9 figures, journal arXiv:2503.13443 [pdf, other] DPC: Dual-Prompt Collaboration for Tuning Vision-Language Models Authors: Haoyang Li, Liang Wang, Chao Wang, Jing Jiang, Yan Peng, Guodong Long Abstract: The Base-New Trade-off (BNT) problem universally exists during the optimization of CLIP-based prompt tuning, where continuous fine-tuning on base (target) classes leads to a simultaneous decrease of generalization ability on new (unseen) classes. Existing approaches attempt to regulate the prompt tuning process to balance BNT by appending constraints. However, imposed on the same target prompt, th\u2026 \u25bd More The Base-New Trade-off (BNT) problem universally exists during the optimization of CLIP-based prompt tuning, where continuous fine-tuning on base (target) classes leads to a simultaneous decrease of generalization ability on new (unseen) classes. Existing approaches attempt to regulate the prompt tuning process to balance BNT by appending constraints. However, imposed on the same target prompt, these constraints fail to fully avert the mutual exclusivity between the optimization directions for base and new. As a novel solution to this challenge, we propose the plug-and-play Dual-Prompt Collaboration (DPC) framework, the first that decoupling the optimization processes of base and new tasks at the prompt level. Specifically, we clone a learnable parallel prompt based on the backbone prompt, and introduce a variable Weighting-Decoupling framework to independently control the optimization directions of dual prompts specific to base or new tasks, thus avoiding the conflict in generalization. Meanwhile, we propose a Dynamic Hard Negative Optimizer, utilizing dual prompts to construct a more challenging optimization task on base classes for enhancement. For interpretability, we prove the feature channel invariance of the prompt vector during the optimization process, providing theoretical support for the Weighting-Decoupling of DPC. Extensive experiments on multiple backbones demonstrate that DPC can significantly improve base performance without introducing any external knowledge beyond the base classes, while maintaining generalization to new classes. Code is available at: https://github.com/JREion/DPC. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. Comments: Accepted by the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2025 (CVPR 2025) arXiv:2503.13434 [pdf, other] BlobCtrl: A Unified and Flexible Framework for Element-level Image Generation and Editing Authors: Yaowei Li, Lingen Li, Zhaoyang Zhang, Xiaoyu Li, Guangzhi Wang, Hongxiang Li, Xiaodong Cun, Ying Shan, Yuexian Zou Abstract: Element-level visual manipulation is essential in digital content creation, but current diffusion-based methods lack the precision and flexibility of traditional tools. In this work, we introduce BlobCtrl, a framework that unifies element-level generation and editing using a probabilistic blob-based representation. By employing blobs as visual primitives, our approach effectively decouples and rep\u2026 \u25bd More Element-level visual manipulation is essential in digital content creation, but current diffusion-based methods lack the precision and flexibility of traditional tools. In this work, we introduce BlobCtrl, a framework that unifies element-level generation and editing using a probabilistic blob-based representation. By employing blobs as visual primitives, our approach effectively decouples and represents spatial location, semantic content, and identity information, enabling precise element-level manipulation. Our key contributions include: 1) a dual-branch diffusion architecture with hierarchical feature fusion for seamless foreground-background integration; 2) a self-supervised training paradigm with tailored data augmentation and score functions; and 3) controllable dropout strategies to balance fidelity and diversity. To support further research, we introduce BlobData for large-scale training and BlobBench for systematic evaluation. Experiments show that BlobCtrl excels in various element-level manipulation tasks while maintaining computational efficiency, offering a practical solution for precise and flexible visual content creation. Project page: https://liyaowei-stu.github.io/project/BlobCtrl/ \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. Comments: Project Webpage: https://liyaowei-stu.github.io/project/BlobCtrl/ arXiv:2503.13038 [pdf, other] Overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) Task Authors: Junjie Chen, Haitao Li, Zhumin Chu, Yiqun Liu, Qingyao Ai Abstract: In this paper, we provide an overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) task. As large language models (LLMs) grow popular in both academia and industry, how to effectively evaluate the capacity of LLMs becomes an increasingly critical but still challenging issue. Existing methods can be divided into two types: manual evaluation, which is expensive, and automatic evaluation, wh\u2026 \u25bd More In this paper, we provide an overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) task. As large language models (LLMs) grow popular in both academia and industry, how to effectively evaluate the capacity of LLMs becomes an increasingly critical but still challenging issue. Existing methods can be divided into two types: manual evaluation, which is expensive, and automatic evaluation, which faces many limitations including task format (the majority belong to multiple-choice questions) and evaluation criteria (occupied by reference-based metrics). To advance the innovation of automatic evaluation, we propose the AEOLLM task which focuses on generative tasks and encourages reference-free methods. Besides, we set up diverse subtasks such as dialogue generation, text expansion, summary generation and non-factoid question answering to comprehensively test different methods. This year, we received 48 runs from 4 teams in total. This paper will describe the background of the task, the data set, the evaluation measures and the evaluation results, respectively. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. arXiv:2503.12786 [pdf, other] Privacy-Preserving Biometric Verification with Handwritten Random Digit String Authors: Peirong Zhang, Yuliang Liu, Songxuan Lai, Hongliang Li, Lianwen Jin Abstract: Handwriting verification has stood as a steadfast identity authentication method for decades. However, this technique risks potential privacy breaches due to the inclusion of personal information in handwritten biometrics such as signatures. To address this concern, we propose using the Random Digit String (RDS) for privacy-preserving handwriting verification. This approach allows users to authent\u2026 \u25bd More Handwriting verification has stood as a steadfast identity authentication method for decades. However, this technique risks potential privacy breaches due to the inclusion of personal information in handwritten biometrics such as signatures. To address this concern, we propose using the Random Digit String (RDS) for privacy-preserving handwriting verification. This approach allows users to authenticate themselves by writing an arbitrary digit sequence, effectively ensuring privacy protection. To evaluate the effectiveness of RDS, we construct a new HRDS4BV dataset composed of online naturally handwritten RDS. Unlike conventional handwriting, RDS encompasses unconstrained and variable content, posing significant challenges for modeling consistent personal writing style. To surmount this, we propose the Pattern Attentive VErification Network (PAVENet), along with a Discriminative Pattern Mining (DPM) module. DPM adaptively enhances the recognition of consistent and discriminative writing patterns, thus refining handwriting style representation. Through comprehensive evaluations, we scrutinize the applicability of online RDS verification and showcase a pronounced outperformance of our model over existing methods. Furthermore, we discover a noteworthy forgery phenomenon that deviates from prior findings and discuss its positive impact in countering malicious impostor attacks. Substantially, our work underscores the feasibility of privacy-preserving biometric verification and propels the prospects of its broader acceptance and application. \u25b3 Less Submitted 16 March, 2025; originally announced March 2025. arXiv:2503.12698 [pdf, other] A Continual Learning-driven Model for Accurate and Generalizable Segmentation of Clinically Comprehensive and Fine-grained Whole-body Anatomies in CT Authors: Dazhou Guo, Zhanghexuan Ji, Yanzhou Su, Dandan Zheng, Heng Guo, Puyang Wang, Ke Yan, Yirui Wang, Qinji Yu, Zi Li, Minfeng Xu, Jianfeng Zhang, Haoshen Li, Jia Ge, Tsung-Ying Ho, Bing-Shen Huang, Tashan Ai, Kuaile Zhao, Na Shen, Qifeng Wang, Yun Bian, Tingyu Wu, Peng Du, Hua Zhang, Feng-Ming Kong , et al. (9 additional authors not shown) Abstract: Precision medicine in the quantitative management of chronic diseases and oncology would be greatly improved if the Computed Tomography (CT) scan of any patient could be segmented, parsed and analyzed in a precise and detailed way. However, there is no such fully annotated CT dataset with all anatomies delineated for training because of the exceptionally high manual cost, the need for specialized\u2026 \u25bd More Precision medicine in the quantitative management of chronic diseases and oncology would be greatly improved if the Computed Tomography (CT) scan of any patient could be segmented, parsed and analyzed in a precise and detailed way. However, there is no such fully annotated CT dataset with all anatomies delineated for training because of the exceptionally high manual cost, the need for specialized clinical expertise, and the time required to finish the task. To this end, we proposed a novel continual learning-driven CT model that can segment complete anatomies presented using dozens of previously partially labeled datasets, dynamically expanding its capacity to segment new ones without compromising previously learned organ knowledge. Existing multi-dataset approaches are not able to dynamically segment new anatomies without catastrophic forgetting and would encounter optimization difficulty or infeasibility when segmenting hundreds of anatomies across the whole range of body regions. Our single unified CT segmentation model, CL-Net, can highly accurately segment a clinically comprehensive set of 235 fine-grained whole-body anatomies. Composed of a universal encoder, multiple optimized and pruned decoders, CL-Net is developed using 13,952 CT scans from 20 public and 16 private high-quality partially labeled CT datasets of various vendors, different contrast phases, and pathologies. Extensive evaluation demonstrates that CL-Net consistently outperforms the upper limit of an ensemble of 36 specialist nnUNets trained per dataset with the complexity of 5% model size and significantly surpasses the segmentation accuracy of recent leading Segment Anything-style medical image foundation models by large margins. Our continual learning-driven CL-Net model would lay a solid foundation to facilitate many downstream tasks of oncology and chronic diseases using the most widely adopted CT imaging. \u25b3 Less Submitted 16 March, 2025; originally announced March 2025. arXiv:2503.12689 [pdf, other] MagicID: Hybrid Preference Optimization for ID-Consistent and Dynamic-Preserved Video Customization Authors: Hengjia Li, Lifan Jiang, Xi Xiao, Tianyang Wang, Hongwei Yi, Boxi Wu, Deng Cai Abstract: Video identity customization seeks to produce high-fidelity videos that maintain consistent identity and exhibit significant dynamics based on users' reference images. However, existing approaches face two key challenges: identity degradation over extended video length and reduced dynamics during training, primarily due to their reliance on traditional self-reconstruction training with static imag\u2026 \u25bd More Video identity customization seeks to produce high-fidelity videos that maintain consistent identity and exhibit significant dynamics based on users' reference images. However, existing approaches face two key challenges: identity degradation over extended video length and reduced dynamics during training, primarily due to their reliance on traditional self-reconstruction training with static images. To address these issues, we introduce $\\textbf{MagicID}$, a novel framework designed to directly promote the generation of identity-consistent and dynamically rich videos tailored to user preferences. Specifically, we propose constructing pairwise preference video data with explicit identity and dynamic rewards for preference learning, instead of sticking to the traditional self-reconstruction. To address the constraints of customized preference data, we introduce a hybrid sampling strategy. This approach first prioritizes identity preservation by leveraging static videos derived from reference images, then enhances dynamic motion quality in the generated videos using a Frontier-based sampling method. By utilizing these hybrid preference pairs, we optimize the model to align with the reward differences between pairs of customized preferences. Extensive experiments show that MagicID successfully achieves consistent identity and natural dynamics, surpassing existing methods across various metrics. \u25b3 Less Submitted 16 March, 2025; originally announced March 2025. arXiv:2503.12589 [pdf, other] Context-Aware Two-Step Training Scheme for Domain Invariant Speech Separation Authors: Wupeng Wang, Zexu Pan, Jingru Lin, Shuai Wang, Haizhou Li Abstract: Speech separation seeks to isolate individual speech signals from a multi-talk speech mixture. Despite much progress, a system well-trained on synthetic data often experiences performance degradation on out-of-domain data, such as real-world speech mixtures. To address this, we introduce a novel context-aware, two-stage training scheme for speech separation models. In this training scheme, the con\u2026 \u25bd More Speech separation seeks to isolate individual speech signals from a multi-talk speech mixture. Despite much progress, a system well-trained on synthetic data often experiences performance degradation on out-of-domain data, such as real-world speech mixtures. To address this, we introduce a novel context-aware, two-stage training scheme for speech separation models. In this training scheme, the conventional end-to-end architecture is replaced with a framework that contains a context extractor and a segregator. The two modules are trained step by step to simulate the speech separation process of an auditory system. We evaluate the proposed training scheme through cross-domain experiments on both synthetic and real-world speech mixtures, and demonstrate that our new scheme effectively boosts separation quality across different domains without adaptation, as measured by signal quality metrics and word error rate (WER). Additionally, an ablation study on the real test set highlights that the context information, including phoneme and word representations from pretrained SSL models, serves as effective domain invariant training targets for separation models. \u25b3 Less Submitted 16 March, 2025; originally announced March 2025. arXiv:2503.12552 [pdf, other] MTGS: Multi-Traversal Gaussian Splatting Authors: Tianyu Li, Yihang Qiu, Zhenhua Wu, Carl Lindstr\u00f6m, Peng Su, Matthias Nie\u00dfner, Hongyang Li Abstract: Multi-traversal data, commonly collected through daily commutes or by self-driving fleets, provides multiple viewpoints for scene reconstruction within a road block. This data offers significant potential for high-quality novel view synthesis, which is crucial for applications such as autonomous vehicle simulators. However, inherent challenges in multi-traversal data often result in suboptimal rec\u2026 \u25bd More Multi-traversal data, commonly collected through daily commutes or by self-driving fleets, provides multiple viewpoints for scene reconstruction within a road block. This data offers significant potential for high-quality novel view synthesis, which is crucial for applications such as autonomous vehicle simulators. However, inherent challenges in multi-traversal data often result in suboptimal reconstruction quality, including variations in appearance and the presence of dynamic objects. To address these issues, we propose Multi-Traversal Gaussian Splatting (MTGS), a novel approach that reconstructs high-quality driving scenes from arbitrarily collected multi-traversal data by modeling a shared static geometry while separately handling dynamic elements and appearance variations. Our method employs a multi-traversal dynamic scene graph with a shared static node and traversal-specific dynamic nodes, complemented by color correction nodes with learnable spherical harmonics coefficient residuals. This approach enables high-fidelity novel view synthesis and provides flexibility to navigate any viewpoint. We conduct extensive experiments on a large-scale driving dataset, nuPlan, with multi-traversal data. Our results demonstrate that MTGS improves LPIPS by 23.5% and geometry accuracy by 46.3% compared to single-traversal baselines. The code and data would be available to the public. \u25b3 Less Submitted 16 March, 2025; originally announced March 2025. arXiv:2503.12485 [pdf, other] Cross-Modal Consistency Learning for Sign Language Recognition Authors: Kepeng Wu, Zecheng Li, Weichao Zhao, Hezhen Hu, Wengang Zhou, Houqiang Li Abstract: Pre-training has been proven to be effective in boosting the performance of Isolated Sign Language Recognition (ISLR). Existing pre-training methods solely focus on the compact pose data, which eliminate background perturbation but inevitably suffer from insufficient semantic cues compared to raw RGB videos. Nevertheless, direct representation learning only from RGB videos remains challenging due\u2026 \u25bd More Pre-training has been proven to be effective in boosting the performance of Isolated Sign Language Recognition (ISLR). Existing pre-training methods solely focus on the compact pose data, which eliminate background perturbation but inevitably suffer from insufficient semantic cues compared to raw RGB videos. Nevertheless, direct representation learning only from RGB videos remains challenging due to the presence of sign-independent visual features. To address this dilemma, we propose a Cross-modal Consistency Learning framework (CCL-SLR), which leverages the cross-modal consistency from both RGB and pose modalities based on self-supervised pre-training. First, CCL-SLR employs contrastive learning for instance discrimination within and across modalities. Through the single-modal and cross-modal contrastive learning, CCL-SLR gradually aligns the feature spaces of RGB and pose modalities, thereby extracting consistent sign representations. Second, we further introduce Motion-Preserving Masking (MPM) and Semantic Positive Mining (SPM) techniques to improve cross-modal consistency from the perspective of data augmentation and sample similarity, respectively. Extensive experiments on four ISLR benchmarks show that CCL-SLR achieves impressive performance, demonstrating its effectiveness. The code will be released to the public. \u25b3 Less Submitted 16 March, 2025; originally announced March 2025. arXiv:2503.12450 [pdf, other] LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching Authors: Feihong Yan, Qingyan Wei, Jiayi Tang, Jiajun Li, Yulin Wang, Xuming Hu, Huiqi Li, Linfeng Zhang Abstract: Masked Autoregressive (MAR) models have emerged as a promising approach in image generation, expected to surpass traditional autoregressive models in computational efficiency by leveraging the capability of parallel decoding. However, their dependence on bidirectional self-attention inherently conflicts with conventional KV caching mechanisms, creating unexpected computational bottlenecks that und\u2026 \u25bd More Masked Autoregressive (MAR) models have emerged as a promising approach in image generation, expected to surpass traditional autoregressive models in computational efficiency by leveraging the capability of parallel decoding. However, their dependence on bidirectional self-attention inherently conflicts with conventional KV caching mechanisms, creating unexpected computational bottlenecks that undermine their expected efficiency. To address this problem, this paper studies the caching mechanism for MAR by leveraging two types of redundancy: Token Redundancy indicates that a large portion of tokens have very similar representations in the adjacent decoding steps, which allows us to first cache them in previous steps and then reuse them in the later steps. Condition Redundancy indicates that the difference between conditional and unconditional output in classifier-free guidance exhibits very similar values in adjacent steps. Based on these two redundancies, we propose LazyMAR, which introduces two caching mechanisms to handle them one by one. LazyMAR is training-free and plug-and-play for all MAR models. Experimental results demonstrate that our method achieves 2.83 times acceleration with almost no drop in generation quality. Our codes will be released in https://github.com/feihongyan1/LazyMAR. \u25b3 Less Submitted 16 March, 2025; originally announced March 2025. Comments: 10 pages, 6 figures arXiv:2503.12369 [pdf, other] L2COcc: Lightweight Camera-Centric Semantic Scene Completion via Distillation of LiDAR Model Authors: Ruoyu Wang, Yukai Ma, Yi Yao, Sheng Tao, Haoang Li, Zongzhi Zhu, Yong Liu, Xingxing Zuo Abstract: Semantic Scene Completion (SSC) constitutes a pivotal element in autonomous driving perception systems, tasked with inferring the 3D semantic occupancy of a scene from sensory data. To improve accuracy, prior research has implemented various computationally demanding and memory-intensive 3D operations, imposing significant computational requirements on the platform during training and testing. Thi\u2026 \u25bd More Semantic Scene Completion (SSC) constitutes a pivotal element in autonomous driving perception systems, tasked with inferring the 3D semantic occupancy of a scene from sensory data. To improve accuracy, prior research has implemented various computationally demanding and memory-intensive 3D operations, imposing significant computational requirements on the platform during training and testing. This paper proposes L2COcc, a lightweight camera-centric SSC framework that also accommodates LiDAR inputs. With our proposed efficient voxel transformer (EVT) and cross-modal knowledge modules, including feature similarity distillation (FSD), TPV distillation (TPVD) and prediction alignment distillation (PAD), our method substantially reduce computational burden while maintaining high accuracy. The experimental evaluations demonstrate that our proposed method surpasses the current state-of-the-art vision-based SSC methods regarding accuracy on both the SemanticKITTI and SSCBench-KITTI-360 benchmarks, respectively. Additionally, our method is more lightweight, exhibiting a reduction in both memory consumption and inference time by over 23% compared to the current state-of-the-arts method. Code is available at our project page:https://studyingfufu.github.io/L2COcc/. \u25b3 Less Submitted 16 March, 2025; originally announced March 2025. arXiv:2503.12167 [pdf, other] PLM: Efficient Peripheral Language Models Hardware-Co-Designed for Ubiquitous Computing Authors: Cheng Deng, Luoyang Sun, Jiwen Jiang, Yongcheng Zeng, Xinjian Wu, Wenxin Zhao, Qingfa Xiao, Jiachuan Wang, Haoyang Li, Lei Chen, Lionel M. Ni, Haifeng Zhang, Jun Wang Abstract: While scaling laws have been continuously validated in large language models (LLMs) with increasing model parameters, the inherent tension between the inference demands of LLMs and the limited resources of edge devices poses a critical challenge to the development of edge intelligence. Recently, numerous small language models have emerged, aiming to distill the capabilities of LLMs into smaller fo\u2026 \u25bd More While scaling laws have been continuously validated in large language models (LLMs) with increasing model parameters, the inherent tension between the inference demands of LLMs and the limited resources of edge devices poses a critical challenge to the development of edge intelligence. Recently, numerous small language models have emerged, aiming to distill the capabilities of LLMs into smaller footprints. However, these models often retain the fundamental architectural principles of their larger counterparts, still imposing considerable strain on the storage and bandwidth capacities of edge devices. In this paper, we introduce the PLM, a Peripheral Language Model, developed through a co-design process that jointly optimizes model architecture and edge system constraints. The PLM utilizes a Multi-head Latent Attention mechanism and employs the squared ReLU activation function to encourage sparsity, thereby reducing peak memory footprint during inference. During training, we collect and reorganize open-source datasets, implement a multi-phase training strategy, and empirically investigate the Warmup-Stable-Decay-Constant (WSDC) learning rate scheduler. Additionally, we incorporate Reinforcement Learning from Human Feedback (RLHF) by adopting the ARIES preference learning approach. Following a two-phase SFT process, this method yields performance gains of 2% in general tasks, 9% in the GSM8K task, and 11% in coding tasks. In addition to its novel architecture, evaluation results demonstrate that PLM outperforms existing small language models trained on publicly available data while maintaining the lowest number of activated parameters. Furthermore, deployment across various edge devices, including consumer-grade GPUs, mobile phones, and Raspberry Pis, validates PLM's suitability for peripheral applications. The PLM series models are publicly available at https://github.com/plm-team/PLM. \u25b3 Less Submitted 19 March, 2025; v1 submitted 15 March, 2025; originally announced March 2025. ACM Class: I.2.7 arXiv:2503.11999 [pdf, other] Diffusion Dynamics Models with Generative State Estimation for Cloth Manipulation Authors: Tongxuan Tian, Haoyang Li, Bo Ai, Xiaodi Yuan, Zhiao Huang, Hao Su Abstract: Manipulating deformable objects like cloth is challenging due to their complex dynamics, near-infinite degrees of freedom, and frequent self-occlusions, which complicate state estimation and dynamics modeling. Prior work has struggled with robust cloth state estimation, while dynamics models, primarily based on Graph Neural Networks (GNNs), are limited by their locality. Inspired by recent advance\u2026 \u25bd More Manipulating deformable objects like cloth is challenging due to their complex dynamics, near-infinite degrees of freedom, and frequent self-occlusions, which complicate state estimation and dynamics modeling. Prior work has struggled with robust cloth state estimation, while dynamics models, primarily based on Graph Neural Networks (GNNs), are limited by their locality. Inspired by recent advances in generative models, we hypothesize that these expressive models can effectively capture intricate cloth configurations and deformation patterns from data. Building on this insight, we propose a diffusion-based generative approach for both perception and dynamics modeling. Specifically, we formulate state estimation as reconstructing the full cloth state from sparse RGB-D observations conditioned on a canonical cloth mesh and dynamics modeling as predicting future states given the current state and robot actions. Leveraging a transformer-based diffusion model, our method achieves high-fidelity state reconstruction while reducing long-horizon dynamics prediction errors by an order of magnitude compared to GNN-based approaches. Integrated with model-predictive control (MPC), our framework successfully executes cloth folding on a real robotic system, demonstrating the potential of generative models for manipulation tasks with partial observability and complex dynamics. \u25b3 Less Submitted 15 March, 2025; originally announced March 2025. arXiv:2503.11650 [pdf, other] Centaur: Robust End-to-End Autonomous Driving with Test-Time Training Authors: Chonghao Sima, Kashyap Chitta, Zhiding Yu, Shiyi Lan, Ping Luo, Andreas Geiger, Hongyang Li, Jose M. Alvarez Abstract: How can we rely on an end-to-end autonomous vehicle's complex decision-making system during deployment? One common solution is to have a ``fallback layer'' that checks the planned trajectory for rule violations and replaces it with a pre-defined safe action if necessary. Another approach involves adjusting the planner's decisions to minimize a pre-defined ``cost function'' using additional system\u2026 \u25bd More How can we rely on an end-to-end autonomous vehicle's complex decision-making system during deployment? One common solution is to have a ``fallback layer'' that checks the planned trajectory for rule violations and replaces it with a pre-defined safe action if necessary. Another approach involves adjusting the planner's decisions to minimize a pre-defined ``cost function'' using additional system predictions such as road layouts and detected obstacles. However, these pre-programmed rules or cost functions cannot learn and improve with new training data, often resulting in overly conservative behaviors. In this work, we propose Centaur (Cluster Entropy for Test-time trAining using Uncertainty) which updates a planner's behavior via test-time training, without relying on hand-engineered rules or cost functions. Instead, we measure and minimize the uncertainty in the planner's decisions. For this, we develop a novel uncertainty measure, called Cluster Entropy, which is simple, interpretable, and compatible with state-of-the-art planning algorithms. Using data collected at prior test-time time-steps, we perform an update to the model's parameters using a gradient that minimizes the Cluster Entropy. With only this sole gradient update prior to inference, Centaur exhibits significant improvements, ranking first on the navtest leaderboard with notable gains in safety-critical metrics such as time to collision. To provide detailed insights on a per-scenario basis, we also introduce navsafe, a challenging new benchmark, which highlights previously undiscovered failure modes of driving models. \u25b3 Less Submitted 14 March, 2025; originally announced March 2025. arXiv:2503.11646 [pdf, other] Adversarial Data Collection: Human-Collaborative Perturbations for Efficient and Robust Robotic Imitation Learning Authors: Siyuan Huang, Yue Liao, Siyuan Feng, Shu Jiang, Si Liu, Hongsheng Li, Maoqing Yao, Guanghui Ren Abstract: The pursuit of data efficiency, where quality outweighs quantity, has emerged as a cornerstone in robotic manipulation, especially given the high costs associated with real-world data collection. We propose that maximizing the informational density of individual demonstrations can dramatically reduce reliance on large-scale datasets while improving task performance. To this end, we introduce Adver\u2026 \u25bd More The pursuit of data efficiency, where quality outweighs quantity, has emerged as a cornerstone in robotic manipulation, especially given the high costs associated with real-world data collection. We propose that maximizing the informational density of individual demonstrations can dramatically reduce reliance on large-scale datasets while improving task performance. To this end, we introduce Adversarial Data Collection, a Human-in-the-Loop (HiL) framework that redefines robotic data acquisition through real-time, bidirectional human-environment interactions. Unlike conventional pipelines that passively record static demonstrations, ADC adopts a collaborative perturbation paradigm: during a single episode, an adversarial operator dynamically alters object states, environmental conditions, and linguistic commands, while the tele-operator adaptively adjusts actions to overcome these evolving challenges. This process compresses diverse failure-recovery behaviors, compositional task variations, and environmental perturbations into minimal demonstrations. Our experiments demonstrate that ADC-trained models achieve superior compositional generalization to unseen task instructions, enhanced robustness to perceptual perturbations, and emergent error recovery capabilities. Strikingly, models trained with merely 20% of the demonstration volume collected through ADC significantly outperform traditional approaches using full datasets. These advances bridge the gap between data-centric learning paradigms and practical robotic deployment, demonstrating that strategic data acquisition, not merely post-hoc processing, is critical for scalable, real-world robot learning. Additionally, we are curating a large-scale ADC-Robotics dataset comprising real-world manipulation tasks with adversarial perturbations. This benchmark will be open-sourced to facilitate advancements in robotic imitation learning. \u25b3 Less Submitted 14 March, 2025; originally announced March 2025. Comments: More information can be found on our project page:https://sites.google.com/view/adc-robot arXiv:2503.10743 [pdf, other] Spatial-Temporal Graph Diffusion Policy with Kinematic Modeling for Bimanual Robotic Manipulation Authors: Qi Lv, Hao Li, Xiang Deng, Rui Shao, Yinchuan Li, Jianye Hao, Longxiang Gao, Michael Yu Wang, Liqiang Nie Abstract: Despite the significant success of imitation learning in robotic manipulation, its application to bimanual tasks remains highly challenging. Existing approaches mainly learn a policy to predict a distant next-best end-effector pose (NBP) and then compute the corresponding joint rotation angles for motion using inverse kinematics. However, they suffer from two important issues: (1) rarely consideri\u2026 \u25bd More Despite the significant success of imitation learning in robotic manipulation, its application to bimanual tasks remains highly challenging. Existing approaches mainly learn a policy to predict a distant next-best end-effector pose (NBP) and then compute the corresponding joint rotation angles for motion using inverse kinematics. However, they suffer from two important issues: (1) rarely considering the physical robotic structure, which may cause self-collisions or interferences, and (2) overlooking the kinematics constraint, which may result in the predicted poses not conforming to the actual limitations of the robot joints. In this paper, we propose Kinematics enhanced Spatial-TemporAl gRaph Diffuser (KStar Diffuser). Specifically, (1) to incorporate the physical robot structure information into action prediction, KStar Diffuser maintains a dynamic spatial-temporal graph according to the physical bimanual joint motions at continuous timesteps. This dynamic graph serves as the robot-structure condition for denoising the actions; (2) to make the NBP learning objective consistent with kinematics, we introduce the differentiable kinematics to provide the reference for optimizing KStar Diffuser. This module regularizes the policy to predict more reliable and kinematics-aware next end-effector poses. Experimental results show that our method effectively leverages the physical structural information and generates kinematics-aware actions in both simulation and real-world \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. Comments: Accepted by CVPR 2025 arXiv:2503.10742 [pdf, other] Keyframe-oriented Vision Token Pruning: Enhancing Efficiency of Large Vision Language Models on Long-Form Video Processing Authors: Yudong Liu, Jingwei Sun, Yueqian Lin, Jingyang Zhang, Ming Yin, Qinsi Wang, Jianyi Zhang, Hai Li, Yiran Chen Abstract: Vision language models (VLMs) demonstrate strong capabilities in jointly processing visual and textual data. However, they often incur substantial computational overhead due to redundant visual information, particularly in long-form video scenarios. Existing approaches predominantly focus on either vision token pruning, which may overlook spatio-temporal dependencies, or keyframe selection, which\u2026 \u25bd More Vision language models (VLMs) demonstrate strong capabilities in jointly processing visual and textual data. However, they often incur substantial computational overhead due to redundant visual information, particularly in long-form video scenarios. Existing approaches predominantly focus on either vision token pruning, which may overlook spatio-temporal dependencies, or keyframe selection, which identifies informative frames but discards others, thus disrupting contextual continuity. In this work, we propose KVTP (Keyframe-oriented Vision Token Pruning), a novel framework that overcomes the drawbacks of token pruning and keyframe selection. By adaptively assigning pruning rates based on frame relevance to the query, KVTP effectively retains essential contextual information while significantly reducing redundant computation. To thoroughly evaluate the long-form video understanding capacities of VLMs, we curated and reorganized subsets from VideoMME, EgoSchema, and NextQA into a unified benchmark named SparseKV-QA that highlights real-world scenarios with sparse but crucial events. Our experiments with VLMs of various scales show that KVTP can reduce token usage by 80% without compromising spatiotemporal and contextual consistency, significantly cutting computation while maintaining the performance. These results demonstrate our approach's effectiveness in efficient long-video processing, facilitating more scalable VLM deployment. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. arXiv:2503.10639 [pdf, other] GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing Authors: Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Xihui Liu, Hongsheng Li Abstract: Current image generation and editing methods primarily process textual prompts as direct inputs without reasoning about visual composition and explicit operations. We present Generation Chain-of-Thought (GoT), a novel paradigm that enables generation and editing through an explicit language reasoning process before outputting images. This approach transforms conventional text-to-image generation a\u2026 \u25bd More Current image generation and editing methods primarily process textual prompts as direct inputs without reasoning about visual composition and explicit operations. We present Generation Chain-of-Thought (GoT), a novel paradigm that enables generation and editing through an explicit language reasoning process before outputting images. This approach transforms conventional text-to-image generation and editing into a reasoning-guided framework that analyzes semantic relationships and spatial arrangements. We define the formulation of GoT and construct large-scale GoT datasets containing over 9M samples with detailed reasoning chains capturing semantic-spatial relationships. To leverage the advantages of GoT, we implement a unified framework that integrates Qwen2.5-VL for reasoning chain generation with an end-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance Module. Experiments show our GoT framework achieves excellent performance on both generation and editing tasks, with significant improvements over baselines. Additionally, our approach enables interactive visual generation, allowing users to explicitly modify reasoning steps for precise image adjustments. GoT pioneers a new direction for reasoning-driven visual generation and editing, producing images that better align with human intent. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/rongyaofang/GoT. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. Comments: Dataset and models are released in https://github.com/rongyaofang/GoT arXiv:2503.10592 [pdf, other] CameraCtrl II: Dynamic Scene Exploration via Camera-controlled Video Diffusion Models Authors: Hao He, Ceyuan Yang, Shanchuan Lin, Yinghao Xu, Meng Wei, Liangke Gui, Qi Zhao, Gordon Wetzstein, Lu Jiang, Hongsheng Li Abstract: This paper introduces CameraCtrl II, a framework that enables large-scale dynamic scene exploration through a camera-controlled video diffusion model. Previous camera-conditioned video generative models suffer from diminished video dynamics and limited range of viewpoints when generating videos with large camera movement. We take an approach that progressively expands the generation of dynamic sce\u2026 \u25bd More This paper introduces CameraCtrl II, a framework that enables large-scale dynamic scene exploration through a camera-controlled video diffusion model. Previous camera-conditioned video generative models suffer from diminished video dynamics and limited range of viewpoints when generating videos with large camera movement. We take an approach that progressively expands the generation of dynamic scenes -- first enhancing dynamic content within individual video clip, then extending this capability to create seamless explorations across broad viewpoint ranges. Specifically, we construct a dataset featuring a large degree of dynamics with camera parameter annotations for training while designing a lightweight camera injection module and training scheme to preserve dynamics of the pretrained models. Building on these improved single-clip techniques, we enable extended scene exploration by allowing users to iteratively specify camera trajectories for generating coherent video sequences. Experiments across diverse scenarios demonstrate that CameraCtrl Ii enables camera-controlled dynamic scene synthesis with substantially wider spatial exploration than previous approaches. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. Comments: Project page: https://hehao13.github.io/Projects-CameraCtrl-II/ arXiv:2503.10568 [pdf, other] Autoregressive Image Generation with Randomized Parallel Decoding Authors: Haopeng Li, Jinyue Yang, Guoqi Li, Huan Wang Abstract: We introduce ARPG, a novel visual autoregressive model that enables randomized parallel generation, addressing the inherent limitations of conventional raster-order approaches, which hinder inference efficiency and zero-shot generalization due to their sequential, predefined token generation order. Our key insight is that effective random-order modeling necessitates explicit guidance for determini\u2026 \u25bd More We introduce ARPG, a novel visual autoregressive model that enables randomized parallel generation, addressing the inherent limitations of conventional raster-order approaches, which hinder inference efficiency and zero-shot generalization due to their sequential, predefined token generation order. Our key insight is that effective random-order modeling necessitates explicit guidance for determining the position of the next predicted token. To this end, we propose a novel guided decoding framework that decouples positional guidance from content representation, encoding them separately as queries and key-value pairs. By directly incorporating this guidance into the causal attention mechanism, our approach enables fully random-order training and generation, eliminating the need for bidirectional attention. Consequently, ARPG readily generalizes to zero-shot tasks such as image inpainting, outpainting, and resolution expansion. Furthermore, it supports parallel inference by concurrently processing multiple queries using a shared KV cache. On the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only 64 sampling steps, achieving over a 20-fold increase in throughput while reducing memory consumption by over 75% compared to representative recent autoregressive models at a similar scale. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. arXiv:2503.10525 [pdf, other] Efficient Precoding in XL-MIMO-AFDM System Authors: Jun Zhu, Yin Xu, Dazhi He, Haoyang Li, Yunfeng Guan, Wenjun Zhang, Tianyao Ma, Haozhi Yuan Abstract: This paper explores the potential of affine frequency division multiplexing (AFDM) to mitigate the multiuser interference (MUI) problem by employing time-domain precoding in extremely-large-scale multiple-input multiple-output (XL-MIMO) systems. In XL-MIMO systems, user mobility significantly improves network capacity and transmission quality. Meanwhile, the robustness of AFDM to Doppler shift is\u2026 \u25bd More This paper explores the potential of affine frequency division multiplexing (AFDM) to mitigate the multiuser interference (MUI) problem by employing time-domain precoding in extremely-large-scale multiple-input multiple-output (XL-MIMO) systems. In XL-MIMO systems, user mobility significantly improves network capacity and transmission quality. Meanwhile, the robustness of AFDM to Doppler shift is enhanced in user mobility scenarios, which further improves the system performance. However, the multicarrier nature of AFDM has attracted much attention, and it leads to a significant increase in precoding complexity. However, the serious problem is that the multicarrier use of AFDM leads to a sharp increase in precoding complexity. Therefore, we employ efficient precoding randomized Kaczmarz (rKA) to reduce the complexity overhead. Through simulation analysis, we compare the performance of XL-MIMO-AFDM and XL-MIMO orthogonal frequency division multiplexing (XL-MIMO-OFDM) in mobile scenarios, and the results show that our proposed AFDM-based XL-MIMO precoding design can be more efficient. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. Comments: 6 arXiv:2503.10497 [pdf, ps, other] MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation Authors: Weihao Xuan, Rui Yang, Heli Qi, Qingcheng Zeng, Yunze Xiao, Yun Xing, Junjue Wang, Huitao Li, Xin Li, Kunyu Yu, Nan Liu, Qingyu Chen, Douglas Teodoro, Edison Marrese-Taylor, Shijian Lu, Yusuke Iwasawa, Yutaka Matsuo, Irene Li Abstract: Traditional benchmarks struggle to evaluate increasingly sophisticated language models in multilingual and culturally diverse contexts. To address this gap, we introduce MMLU-ProX, a comprehensive multilingual benchmark covering 13 typologically diverse languages with approximately 11,829 questions per language. Building on the challenging reasoning-focused design of MMLU-Pro, our framework employ\u2026 \u25bd More Traditional benchmarks struggle to evaluate increasingly sophisticated language models in multilingual and culturally diverse contexts. To address this gap, we introduce MMLU-ProX, a comprehensive multilingual benchmark covering 13 typologically diverse languages with approximately 11,829 questions per language. Building on the challenging reasoning-focused design of MMLU-Pro, our framework employs a semi-automatic translation process: translations generated by state-of-the-art large language models (LLMs) are rigorously evaluated by expert annotators to ensure conceptual accuracy, terminological consistency, and cultural relevance. We comprehensively evaluate 25 state-of-the-art LLMs using 5-shot chain-of-thought (CoT) and zero-shot prompting strategies, analyzing their performance across linguistic and cultural boundaries. Our experiments reveal consistent performance degradation from high-resource languages to lower-resource ones, with the best models achieving over 70% accuracy on English but dropping to around 40% for languages like Swahili, highlighting persistent gaps in multilingual capabilities despite recent advances. MMLU-ProX is an ongoing project; we are expanding our benchmark by incorporating additional languages and evaluating more language models to provide a more comprehensive assessment of multilingual capabilities. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. arXiv:2503.10322 [pdf, other] Towards Fast, Memory-based and Data-Efficient Vision-Language Policy Authors: Haoxuan Li, Sixu Yan, Yuhan Li, Xinggang Wang Abstract: Vision Language Models (VLMs) pretrained on Internet-scale vision-language data have demonstrated the potential to transfer their knowledge to robotic learning. However, the existing paradigm encounters three critical challenges: (1) expensive inference cost resulting from large-scale model parameters, (2) frequent domain shifts caused by mismatched data modalities, and (3) limited capacity to han\u2026 \u25bd More Vision Language Models (VLMs) pretrained on Internet-scale vision-language data have demonstrated the potential to transfer their knowledge to robotic learning. However, the existing paradigm encounters three critical challenges: (1) expensive inference cost resulting from large-scale model parameters, (2) frequent domain shifts caused by mismatched data modalities, and (3) limited capacity to handle past or future experiences. In this work, we propose LiteVLP, a lightweight, memory-based, and general-purpose vision-language policy generation model. LiteVLP is built upon a pre-trained 1B-parameter VLM and fine-tuned on a tiny-scale and conversation-style robotic dataset. Through extensive experiments, we demonstrate that LiteVLP outperforms state-of-the-art vision-language policy on VIMA-Bench, with minimal training time. Furthermore, LiteVLP exhibits superior inference speed while maintaining exceptional high accuracy. In long-horizon manipulation tasks, LiteVLP also shows remarkable memory ability, outperforming the best-performing baseline model by 18.8%. These results highlight LiteVLP as a promising model to integrating the intelligence of VLMs into robotic learning. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. Comments: 11 pages, 7 figures, 6 tables arXiv:2503.10076 [pdf, other] VMBench: A Benchmark for Perception-Aligned Video Motion Generation Authors: Xinran Ling, Chen Zhu, Meiqi Wu, Hangyu Li, Xiaokun Feng, Cundian Yang, Aiming Hao, Jiashu Zhu, Jiahong Wu, Xiangxiang Chu Abstract: Video generation has advanced rapidly, improving evaluation methods, yet assessing video's motion remains a major challenge. Specifically, there are two key issues: 1) current motion metrics do not fully align with human perceptions; 2) the existing motion prompts are limited. Based on these findings, we introduce VMBench--a comprehensive Video Motion Benchmark that has perception-aligned motion m\u2026 \u25bd More Video generation has advanced rapidly, improving evaluation methods, yet assessing video's motion remains a major challenge. Specifically, there are two key issues: 1) current motion metrics do not fully align with human perceptions; 2) the existing motion prompts are limited. Based on these findings, we introduce VMBench--a comprehensive Video Motion Benchmark that has perception-aligned motion metrics and features the most diverse types of motion. VMBench has several appealing properties: 1) Perception-Driven Motion Evaluation Metrics, we identify five dimensions based on human perception in motion video assessment and develop fine-grained evaluation metrics, providing deeper insights into models' strengths and weaknesses in motion quality. 2) Meta-Guided Motion Prompt Generation, a structured method that extracts meta-information, generates diverse motion prompts with LLMs, and refines them through human-AI validation, resulting in a multi-level prompt library covering six key dynamic scene dimensions. 3) Human-Aligned Validation Mechanism, we provide human preference annotations to validate our benchmarks, with our metrics achieving an average 35.3% improvement in Spearman's correlation over baseline methods. This is the first time that the quality of motion in videos has been evaluated from the perspective of human perception alignment. Additionally, we will soon release VMBench at https://github.com/GD-AIGC/VMBench, setting a new standard for evaluating and advancing motion generation models. \u25b3 Less Submitted 16 March, 2025; v1 submitted 13 March, 2025; originally announced March 2025. arXiv:2503.09445 [pdf, other] Astrea: A MOE-based Visual Understanding Model with Progressive Alignment Authors: Xiaoda Yang, JunYu Lu, Hongshun Qiu, Sijing Li, Hao Li, Shengpeng Ji, Xudong Tang, Jiayang Xu, Jiaqi Duan, Ziyue Jiang, Cong Lin, Sihang Cai, Zejian Xie, Zhuoyang Song, Songxin Zhang Abstract: Vision-Language Models (VLMs) based on Mixture-of-Experts (MoE) architectures have emerged as a pivotal paradigm in multimodal understanding, offering a powerful framework for integrating visual and linguistic information. However, the increasing complexity and diversity of tasks present significant challenges in coordinating load balancing across heterogeneous visual experts, where optimizing one\u2026 \u25bd More Vision-Language Models (VLMs) based on Mixture-of-Experts (MoE) architectures have emerged as a pivotal paradigm in multimodal understanding, offering a powerful framework for integrating visual and linguistic information. However, the increasing complexity and diversity of tasks present significant challenges in coordinating load balancing across heterogeneous visual experts, where optimizing one specialist's performance often compromises others' capabilities. To address task heterogeneity and expert load imbalance, we propose Astrea, a novel multi-expert collaborative VLM architecture based on progressive pre-alignment. Astrea introduces three key innovations: 1) A heterogeneous expert coordination mechanism that integrates four specialized models (detection, segmentation, classification, captioning) into a comprehensive expert matrix covering essential visual comprehension elements; 2) A dynamic knowledge fusion strategy featuring progressive pre-alignment to harmonize experts within the VLM latent space through contrastive learning, complemented by probabilistically activated stochastic residual connections to preserve knowledge continuity; 3) An enhanced optimization framework utilizing momentum contrastive learning for long-range dependency modeling and adaptive weight allocators for real-time expert contribution calibration. Extensive evaluations across 12 benchmark tasks spanning VQA, image captioning, and cross-modal retrieval demonstrate Astrea's superiority over state-of-the-art models, achieving an average performance gain of +4.7\\%. This study provides the first empirical demonstration that progressive pre-alignment strategies enable VLMs to overcome task heterogeneity limitations, establishing new methodological foundations for developing general-purpose multimodal agents. \u25b3 Less Submitted 12 March, 2025; originally announced March 2025. arXiv:2503.09358 [pdf, other] RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image Reports Authors: Jiushen Cai, Weihang Zhang, Hanruo Liu, Ningli Wang, Huiqi Li Abstract: Standardization of clinical reports is crucial for improving the quality of healthcare and facilitating data integration. The lack of unified standards, including format, terminology, and style, is a great challenge in clinical fundus diagnostic reports, which increases the difficulty for large language models (LLMs) to understand the data. To address this, we construct a bilingual standard termin\u2026 \u25bd More Standardization of clinical reports is crucial for improving the quality of healthcare and facilitating data integration. The lack of unified standards, including format, terminology, and style, is a great challenge in clinical fundus diagnostic reports, which increases the difficulty for large language models (LLMs) to understand the data. To address this, we construct a bilingual standard terminology, containing fundus clinical terms and commonly used descriptions in clinical diagnosis. Then, we establish two models, RetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, fine-tuned on an augmented dataset simulating clinical scenarios, demonstrates powerful standardization behaviors. However, it encounters a challenge of limitation to cover a wider range of diseases. To further enhance standardization performance, we build RetSTA-7B, which integrates a substantial amount of standardized data generated by RetSTA-7B-Zero along with corresponding English data, covering diverse complex clinical scenarios and achieving report-level standardization for the first time. Experimental results demonstrate that RetSTA-7B outperforms other compared LLMs in bilingual standardization task, which validates its superior performance and generalizability. The checkpoints are available at https://github.com/AB-Story/RetSTA-7B. \u25b3 Less Submitted 12 March, 2025; originally announced March 2025. arXiv:2503.09355 [pdf, other] GIGP: A Global Information Interacting and Geometric Priors Focusing Framework for Semi-supervised Medical Image Segmentation Authors: Lianyuan Yu, Xiuzhen Guo, Ji Shi, Hongxiao Wang, Hongwei Li Abstract: Semi-supervised learning enhances medical image segmentation by leveraging unlabeled data, reducing reliance on extensive labeled datasets. On the one hand, the distribution discrepancy between limited labeled data and abundant unlabeled data can hinder model generalization. Most existing methods rely on local similarity matching, which may introduce bias. In contrast, Mamba effectively models glo\u2026 \u25bd More Semi-supervised learning enhances medical image segmentation by leveraging unlabeled data, reducing reliance on extensive labeled datasets. On the one hand, the distribution discrepancy between limited labeled data and abundant unlabeled data can hinder model generalization. Most existing methods rely on local similarity matching, which may introduce bias. In contrast, Mamba effectively models global context with linear complexity, learning more comprehensive data representations. On the other hand, medical images usually exhibit consistent anatomical structures defined by geometric features. Most existing methods fail to fully utilize global geometric priors, such as volumes, moments etc. In this work, we introduce a global information interaction and geometric priors focus framework (GIGP). Firstly, we present a Global Information Interaction Mamba module to reduce distribution discrepancy between labeled and unlabeled data. Secondly, we propose a Geometric Moment Attention Mechanism to extract richer global geometric features. Finally, we propose Global Geometric Perturbation Consistency to simulate organ dynamics and geometric variations, enhancing the ability of the model to learn generalized features. The superior performance on the NIH Pancreas and Left Atrium datasets demonstrates the effectiveness of our approach. \u25b3 Less Submitted 12 March, 2025; originally announced March 2025. arXiv:2503.09279 [pdf, other] Cockatiel: Ensembling Synthetic and Human Preferenced Training for Detailed Video Caption Authors: Luozheng Qin, Zhiyu Tan, Mengping Yang, Xiaomeng Yang, Hao Li Abstract: Video Detailed Captioning (VDC) is a crucial task for vision-language bridging, enabling fine-grained descriptions of complex video content. In this paper, we first comprehensively benchmark current state-of-the-art approaches and systematically identified two critical limitations: biased capability towards specific captioning aspect and misalignment with human preferences. To address these defici\u2026 \u25bd More Video Detailed Captioning (VDC) is a crucial task for vision-language bridging, enabling fine-grained descriptions of complex video content. In this paper, we first comprehensively benchmark current state-of-the-art approaches and systematically identified two critical limitations: biased capability towards specific captioning aspect and misalignment with human preferences. To address these deficiencies, we propose Cockatiel, a novel three-stage training pipeline that ensembles synthetic and human-aligned training for improving VDC performance. In the first stage, we derive a scorer from a meticulously annotated dataset to select synthetic captions high-performing on certain fine-grained video-caption alignment and human-preferred while disregarding others. Then, we train Cockatiel-13B, using this curated dataset to infuse it with assembled model strengths and human preferences. Finally, we further distill Cockatiel-8B from Cockatiel-13B for the ease of usage. Extensive quantitative and qualitative experiments reflect the effectiveness of our method, as we not only set new state-of-the-art performance on VDCSCORE in a dimension-balanced way but also surpass leading alternatives on human preference by a large margin as depicted by the human evaluation results. \u25b3 Less Submitted 12 March, 2025; originally announced March 2025. Comments: For more details, please refer to our project page: https://sais-fuxi.github.io/projects/cockatiel/ arXiv:2503.09002 [pdf, other] KNighter: Transforming Static Analysis with LLM-Synthesized Checkers Authors: Chenyuan Yang, Zijie Zhao, Zichen Xie, Haoyu Li, Lingming Zhang Abstract: Static analysis is a powerful technique for bug detection in critical systems like operating system kernels. However, designing and implementing static analyzers is challenging, time-consuming, and typically limited to predefined bug patterns. While large language models (LLMs) have shown promise for static analysis, directly applying them to scan large codebases remains impractical due to computa\u2026 \u25bd More Static analysis is a powerful technique for bug detection in critical systems like operating system kernels. However, designing and implementing static analyzers is challenging, time-consuming, and typically limited to predefined bug patterns. While large language models (LLMs) have shown promise for static analysis, directly applying them to scan large codebases remains impractical due to computational constraints and contextual limitations. We present KNighter, the first approach that unlocks practical LLM-based static analysis by automatically synthesizing static analyzers from historical bug patterns. Rather than using LLMs to directly analyze massive codebases, our key insight is leveraging LLMs to generate specialized static analyzers guided by historical patch knowledge. KNighter implements this vision through a multi-stage synthesis pipeline that validates checker correctness against original patches and employs an automated refinement process to iteratively reduce false positives. Our evaluation on the Linux kernel demonstrates that KNighter generates high-precision checkers capable of detecting diverse bug patterns overlooked by existing human-written analyzers. To date, KNighter-synthesized checkers have discovered 70 new bugs/vulnerabilities in the Linux kernel, with 56 confirmed and 41 already fixed. 11 of these findings have been assigned CVE numbers. This work establishes an entirely new paradigm for scalable, reliable, and traceable LLM-based static analysis for real-world systems via checker synthesis. \u25b3 Less Submitted 11 March, 2025; originally announced March 2025. arXiv:2503.08931 [pdf, other] ARCHED: A Human-Centered Framework for Transparent, Responsible, and Collaborative AI-Assisted Instructional Design Authors: Hongming Li, Yizirui Fang, Shan Zhang, Seiyon M. Lee, Yiming Wang, Mark Trexler, Anthony F. Botelho Abstract: Integrating Large Language Models (LLMs) in educational technology presents unprecedented opportunities to improve instructional design (ID), yet existing approaches often prioritize automation over pedagogical rigor and human agency. This paper introduces ARCHED (AI for Responsible, Collaborative, Human-centered Education Instructional Design), a structured multi-stage framework that ensures huma\u2026 \u25bd More Integrating Large Language Models (LLMs) in educational technology presents unprecedented opportunities to improve instructional design (ID), yet existing approaches often prioritize automation over pedagogical rigor and human agency. This paper introduces ARCHED (AI for Responsible, Collaborative, Human-centered Education Instructional Design), a structured multi-stage framework that ensures human educators remain central in the design process while leveraging AI capabilities. Unlike traditional AI-generated instructional materials that lack transparency, ARCHED employs a cascaded workflow aligned with Bloom's taxonomy. The framework integrates specialized AI agents - one generating diverse pedagogical options and another evaluating alignment with learning objectives - while maintaining educators as primary decision-makers. This approach addresses key limitations in current AI-assisted instructional design, ensuring transparency, pedagogical foundation, and meaningful human agency. Empirical evaluations demonstrate that ARCHED enhances instructional design quality while preserving educator oversight, marking a step forward in responsible AI integration in education. \u25b3 Less Submitted 11 March, 2025; originally announced March 2025. Comments: Accepted to the iRAISE Workshop at AAAI 2025. To be published in PMLR Volume 273 ACM Class: K.3.1; I.2.6 arXiv:2503.08906 [pdf, other] Prompt-OT: An Optimal Transport Regularization Paradigm for Knowledge Preservation in Vision-Language Model Adaptation Authors: Xiwen Chen, Wenhui Zhu, Peijie Qiu, Hao Wang, Huayu Li, Haiyu Wu, Aristeidis Sotiras, Yalin Wang, Abolfazl Razi Abstract: Vision-language models (VLMs) such as CLIP demonstrate strong performance but struggle when adapted to downstream tasks. Prompt learning has emerged as an efficient and effective strategy to adapt VLMs while preserving their pre-trained knowledge. However, existing methods still lead to overfitting and degrade zero-shot generalization. To address this challenge, we propose an optimal transport (OT\u2026 \u25bd More Vision-language models (VLMs) such as CLIP demonstrate strong performance but struggle when adapted to downstream tasks. Prompt learning has emerged as an efficient and effective strategy to adapt VLMs while preserving their pre-trained knowledge. However, existing methods still lead to overfitting and degrade zero-shot generalization. To address this challenge, we propose an optimal transport (OT)-guided prompt learning framework that mitigates forgetting by preserving the structural consistency of feature distributions between pre-trained and fine-tuned models. Unlike conventional point-wise constraints, OT naturally captures cross-instance relationships and expands the feasible parameter space for prompt tuning, allowing a better trade-off between adaptation and generalization. Our approach enforces joint constraints on both vision and text representations, ensuring a holistic feature alignment. Extensive experiments on benchmark datasets demonstrate that our simple yet effective method can outperform existing prompt learning strategies in base-to-novel generalization, cross-dataset evaluation, and domain generalization without additional augmentation or ensemble techniques. The code is available at https://github.com/ChongQingNoSubway/Prompt-OT \u25b3 Less Submitted 11 March, 2025; originally announced March 2025. arXiv:2503.08253 [pdf, other] SARA: Structural and Adversarial Representation Alignment for Training-efficient Diffusion Models Authors: Hesen Chen, Junyan Wang, Zhiyu Tan, Hao Li Abstract: Modern diffusion models encounter a fundamental trade-off between training efficiency and generation quality. While existing representation alignment methods, such as REPA, accelerate convergence through patch-wise alignment, they often fail to capture structural relationships within visual representations and ensure global distribution consistency between pretrained encoders and denoising network\u2026 \u25bd More Modern diffusion models encounter a fundamental trade-off between training efficiency and generation quality. While existing representation alignment methods, such as REPA, accelerate convergence through patch-wise alignment, they often fail to capture structural relationships within visual representations and ensure global distribution consistency between pretrained encoders and denoising networks. To address these limitations, we introduce SARA, a hierarchical alignment framework that enforces multi-level representation constraints: (1) patch-wise alignment to preserve local semantic details, (2) autocorrelation matrix alignment to maintain structural consistency within representations, and (3) adversarial distribution alignment to mitigate global representation discrepancies. Unlike previous approaches, SARA explicitly models both intra-representation correlations via self-similarity matrices and inter-distribution coherence via adversarial alignment, enabling comprehensive alignment across local and global scales. Experiments on ImageNet-256 show that SARA achieves an FID of 1.36 while converging twice as fast as REPA, surpassing recent state-of-the-art image generation methods. This work establishes a systematic paradigm for optimizing diffusion training through hierarchical representation alignment. \u25b3 Less Submitted 11 March, 2025; originally announced March 2025. Comments: Technical report arXiv:2503.08217 [pdf, other] S3R-GS: Streamlining the Pipeline for Large-Scale Street Scene Reconstruction Authors: Guangting Zheng, Jiajun Deng, Xiaomeng Chu, Yu Yuan, Houqiang Li, Yanyong Zhang Abstract: Recently, 3D Gaussian Splatting (3DGS) has reshaped the field of photorealistic 3D reconstruction, achieving impressive rendering quality and speed. However, when applied to large-scale street scenes, existing methods suffer from rapidly escalating per-viewpoint reconstruction costs as scene size increases, leading to significant computational overhead. After revisiting the conventional pipeline,\u2026 \u25bd More Recently, 3D Gaussian Splatting (3DGS) has reshaped the field of photorealistic 3D reconstruction, achieving impressive rendering quality and speed. However, when applied to large-scale street scenes, existing methods suffer from rapidly escalating per-viewpoint reconstruction costs as scene size increases, leading to significant computational overhead. After revisiting the conventional pipeline, we identify three key factors accounting for this issue: unnecessary local-to-global transformations, excessive 3D-to-2D projections, and inefficient rendering of distant content. To address these challenges, we propose S3R-GS, a 3DGS framework that Streamlines the pipeline for large-scale Street Scene Reconstruction, effectively mitigating these limitations. Moreover, most existing street 3DGS methods rely on ground-truth 3D bounding boxes to separate dynamic and static components, but 3D bounding boxes are difficult to obtain, limiting real-world applicability. To address this, we propose an alternative solution with 2D boxes, which are easier to annotate or can be predicted by off-the-shelf vision foundation models. Such designs together make S3R-GS readily adapt to large, in-the-wild scenarios. Extensive experiments demonstrate that S3R-GS enhances rendering quality and significantly accelerates reconstruction. Remarkably, when applied to videos from the challenging Argoverse2 dataset, it achieves state-of-the-art PSNR and SSIM, reducing reconstruction time to below 50%--and even 20%--of competing methods. \u25b3 Less Submitted 11 March, 2025; originally announced March 2025."
  },
  {
    "url": "https://arxiv.org/abs/2503.14802",
    "title": "Computer Science > Information Retrieval",
    "author": "Authors:Md Shahir Zaoad, Niamat Zawad, Priyanka Ranade, Richard Krogman, Latifur Khan, James Holt",
    "body": "Help | Advanced Search arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. arXiv Operational Status Get status notifications via email or slack"
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Koopman,+B",
    "title": "Showing 1\u201335 of 35 results for author: Koopman, B",
    "author": "Authors:\nHang Li, \n      \n      Xiao Wang, \n      \n      Bevan Koopman, \n      \n      Guido Zuccon",
    "body": "arXiv:2503.14887 [pdf, other] Pseudo-Relevance Feedback Can Improve Zero-Shot LLM-Based Dense Retrieval Authors: Hang Li, Xiao Wang, Bevan Koopman, Guido Zuccon Abstract: Pseudo-relevance feedback (PRF) refines queries by leveraging initially retrieved documents to improve retrieval effectiveness. In this paper, we investigate how large language models (LLMs) can facilitate PRF for zero-shot LLM-based dense retrieval, extending the recently proposed PromptReps method. Specifically, our approach uses LLMs to extract salient passage features-such as keywords and summ\u2026 \u25bd More Pseudo-relevance feedback (PRF) refines queries by leveraging initially retrieved documents to improve retrieval effectiveness. In this paper, we investigate how large language models (LLMs) can facilitate PRF for zero-shot LLM-based dense retrieval, extending the recently proposed PromptReps method. Specifically, our approach uses LLMs to extract salient passage features-such as keywords and summaries-from top-ranked documents, which are then integrated into PromptReps to produce enhanced query representations. Experiments on passage retrieval benchmarks demonstrate that incorporating PRF significantly boosts retrieval performance. Notably, smaller rankers with PRF can match the effectiveness of larger rankers without PRF, highlighting PRF's potential to improve LLM-driven search while maintaining an efficient balance between effectiveness and resource usage. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. arXiv:2503.06034 [pdf, other] Rank-R1: Enhancing Reasoning in LLM-based Document Rerankers via Reinforcement Learning Authors: Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin, Guido Zuccon Abstract: In this paper, we introduce Rank-R1, a novel LLM-based reranker that performs reasoning over both the user query and candidate documents before performing the ranking task. Existing document reranking methods based on large language models (LLMs) typically rely on prompting or fine-tuning LLMs to order or label candidate documents according to their relevance to a query. For Rank-R1, we use a rein\u2026 \u25bd More In this paper, we introduce Rank-R1, a novel LLM-based reranker that performs reasoning over both the user query and candidate documents before performing the ranking task. Existing document reranking methods based on large language models (LLMs) typically rely on prompting or fine-tuning LLMs to order or label candidate documents according to their relevance to a query. For Rank-R1, we use a reinforcement learning algorithm along with only a small set of relevance labels (without any reasoning supervision) to enhance the reasoning ability of LLM-based rerankers. Our hypothesis is that adding reasoning capabilities to the rerankers can improve their relevance assessement and ranking capabilities. Our experiments on the TREC DL and BRIGHT datasets show that Rank-R1 is highly effective, especially for complex queries. In particular, we find that Rank-R1 achieves effectiveness on in-domain datasets at par with that of supervised fine-tuning methods, but utilizing only 18\\% of the training data used by the fine-tuning methods. We also find that the model largely outperforms zero-shot and supervised fine-tuning when applied to out-of-domain datasets featuring complex queries, especially when a 14B-size model is used. Finally, we qualitatively observe that Rank-R1's reasoning process improves the explainability of the ranking results, opening new opportunities for search engine results presentation and fruition. \u25b3 Less Submitted 7 March, 2025; originally announced March 2025. arXiv:2501.16902 [pdf, other] Document Screenshot Retrievers are Vulnerable to Pixel Poisoning Attacks Authors: Shengyao Zhuang, Ekaterina Khramtsova, Xueguang Ma, Bevan Koopman, Jimmy Lin, Guido Zuccon Abstract: Recent advancements in dense retrieval have introduced vision-language model (VLM)-based retrievers, such as DSE and ColPali, which leverage document screenshots embedded as vectors to enable effective search and offer a simplified pipeline over traditional text-only methods. In this study, we propose three pixel poisoning attack methods designed to compromise VLM-based retrievers and evaluate the\u2026 \u25bd More Recent advancements in dense retrieval have introduced vision-language model (VLM)-based retrievers, such as DSE and ColPali, which leverage document screenshots embedded as vectors to enable effective search and offer a simplified pipeline over traditional text-only methods. In this study, we propose three pixel poisoning attack methods designed to compromise VLM-based retrievers and evaluate their effectiveness under various attack settings and parameter configurations. Our empirical results demonstrate that injecting even a single adversarial screenshot into the retrieval corpus can significantly disrupt search results, poisoning the top-10 retrieved documents for 41.9% of queries in the case of DSE and 26.4% for ColPali. These vulnerability rates notably exceed those observed with equivalent attacks on text-only retrievers. Moreover, when targeting a small set of known queries, the attack success rate raises, achieving complete success in certain cases. By exposing the vulnerabilities inherent in vision-language models, this work highlights the potential risks associated with their deployment. \u25b3 Less Submitted 28 January, 2025; originally announced January 2025. arXiv:2412.14457 [pdf, other] VISA: Retrieval Augmented Generation with Visual Source Attribution Authors: Xueguang Ma, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Wenhu Chen, Jimmy Lin Abstract: Generation with source attribution is important for enhancing the verifiability of retrieval-augmented generation (RAG) systems. However, existing approaches in RAG primarily link generated content to document-level references, making it challenging for users to locate evidence among multiple content-rich retrieved documents. To address this challenge, we propose Retrieval-Augmented Generation wit\u2026 \u25bd More Generation with source attribution is important for enhancing the verifiability of retrieval-augmented generation (RAG) systems. However, existing approaches in RAG primarily link generated content to document-level references, making it challenging for users to locate evidence among multiple content-rich retrieved documents. To address this challenge, we propose Retrieval-Augmented Generation with Visual Source Attribution (VISA), a novel approach that combines answer generation with visual source attribution. Leveraging large vision-language models (VLMs), VISA identifies the evidence and highlights the exact regions that support the generated answers with bounding boxes in the retrieved document screenshots. To evaluate its effectiveness, we curated two datasets: Wiki-VISA, based on crawled Wikipedia webpage screenshots, and Paper-VISA, derived from PubLayNet and tailored to the medical domain. Experimental results demonstrate the effectiveness of VISA for visual source attribution on documents' original look, as well as highlighting the challenges for improvement. Code, data, and model checkpoints will be released. \u25b3 Less Submitted 18 December, 2024; originally announced December 2024. arXiv:2411.17299 [pdf, other] 2D Matryoshka Training for Information Retrieval Authors: Shuai Wang, Shengyao Zhuang, Bevan Koopman, Guido Zuccon Abstract: 2D Matryoshka Training is an advanced embedding representation training approach designed to train an encoder model simultaneously across various layer-dimension setups. This method has demonstrated higher effectiveness in Semantic Text Similarity (STS) tasks over traditional training approaches when using sub-layers for embeddings. Despite its success, discrepancies exist between two published im\u2026 \u25bd More 2D Matryoshka Training is an advanced embedding representation training approach designed to train an encoder model simultaneously across various layer-dimension setups. This method has demonstrated higher effectiveness in Semantic Text Similarity (STS) tasks over traditional training approaches when using sub-layers for embeddings. Despite its success, discrepancies exist between two published implementations, leading to varied comparative results with baseline models. In this reproducibility study, we implement and evaluate both versions of 2D Matryoshka Training on STS tasks and extend our analysis to retrieval tasks. Our findings indicate that while both versions achieve higher effectiveness than traditional Matryoshka training on sub-dimensions, and traditional full-sized model training approaches, they do not outperform models trained separately on specific sub-layer and sub-dimension setups. Moreover, these results generalize well to retrieval tasks, both in supervised (MSMARCO) and zero-shot (BEIR) settings. Further explorations of different loss computations reveals more suitable implementations for retrieval tasks, such as incorporating full-dimension loss and training on a broader range of target dimensions. Conversely, some intuitive approaches, such as fixing document encoders to full model outputs, do not yield improvements. Our reproduction code is available at https://github.com/ielab/2DMSE-Reproduce. \u25b3 Less Submitted 26 November, 2024; originally announced November 2024. arXiv:2410.13230 [pdf, other] Starbucks: Improved Training for 2D Matryoshka Embeddings Authors: Shengyao Zhuang, Shuai Wang, Bevan Koopman, Guido Zuccon Abstract: Effective approaches that can scale embedding model depth (i.e. layers) and embedding size allow for the creation of models that are highly scalable across different computational resources and task requirements. While the recently proposed 2D Matryoshka training approach can efficiently produce a single embedding model such that its sub-layers and sub-dimensions can measure text similarity, its e\u2026 \u25bd More Effective approaches that can scale embedding model depth (i.e. layers) and embedding size allow for the creation of models that are highly scalable across different computational resources and task requirements. While the recently proposed 2D Matryoshka training approach can efficiently produce a single embedding model such that its sub-layers and sub-dimensions can measure text similarity, its effectiveness is significantly worse than if smaller models were trained separately. To address this issue, we propose Starbucks, a new training strategy for Matryoshka-like embedding models, which encompasses both the fine-tuning and pre-training phases. For the fine-tuning phase, we discover that, rather than sampling a random sub-layer and sub-dimensions for each training steps, providing a fixed list of layer-dimension pairs, from small size to large sizes, and computing the loss across all pairs significantly improves the effectiveness of 2D Matryoshka embedding models, bringing them on par with their separately trained counterparts. To further enhance performance, we introduce a new pre-training strategy, which applies masked autoencoder language modelling to sub-layers and sub-dimensions during pre-training, resulting in a stronger backbone for subsequent fine-tuning of the embedding model. Experimental results on both semantic text similarity and retrieval benchmarks demonstrate that the proposed pre-training and fine-tuning strategies significantly improved the effectiveness over 2D Matryoshka models, enabling Starbucks models to perform more efficiently and effectively than separately trained models. \u25b3 Less Submitted 18 October, 2024; v1 submitted 17 October, 2024; originally announced October 2024. arXiv:2410.06628 [pdf, other] Does Vec2Text Pose a New Corpus Poisoning Threat? Authors: Shengyao Zhuang, Bevan Koopman, Guido Zuccon Abstract: The emergence of Vec2Text -- a method for text embedding inversion -- has raised serious privacy concerns for dense retrieval systems which use text embeddings. This threat comes from the ability for an attacker with access to embeddings to reconstruct the original text. In this paper, we take a new look at Vec2Text and investigate how much of a threat it poses to the different attacks of corpus p\u2026 \u25bd More The emergence of Vec2Text -- a method for text embedding inversion -- has raised serious privacy concerns for dense retrieval systems which use text embeddings. This threat comes from the ability for an attacker with access to embeddings to reconstruct the original text. In this paper, we take a new look at Vec2Text and investigate how much of a threat it poses to the different attacks of corpus poisoning, whereby an attacker injects adversarial passages into a retrieval corpus with the intention of misleading dense retrievers. Theoretically, Vec2Text is far more dangerous than previous attack methods because it does not need access to the embedding model's weights and it can efficiently generate many adversarial passages. We show that under certain conditions, corpus poisoning with Vec2Text can pose a serious threat to dense retriever system integrity and user experience by injecting adversarial passaged into top ranked positions. Code and data are made available at https://github.com/ielab/vec2text-corpus-poisoning \u25b3 Less Submitted 9 October, 2024; originally announced October 2024. Comments: arXiv admin note: substantial text overlap with arXiv:2402.12784 arXiv:2408.03500 [pdf, other] e-Health CSIRO at RRG24: Entropy-Augmented Self-Critical Sequence Training for Radiology Report Generation Authors: Aaron Nicolson, Jinghui Liu, Jason Dowling, Anthony Nguyen, Bevan Koopman Abstract: The Shared Task on Large-Scale Radiology Report Generation (RRG24) aims to expedite the development of assistive systems for interpreting and reporting on chest X-ray (CXR) images. This task challenges participants to develop models that generate the findings and impression sections of radiology reports from CXRs from a patient's study, using five different datasets. This paper outlines the e-Heal\u2026 \u25bd More The Shared Task on Large-Scale Radiology Report Generation (RRG24) aims to expedite the development of assistive systems for interpreting and reporting on chest X-ray (CXR) images. This task challenges participants to develop models that generate the findings and impression sections of radiology reports from CXRs from a patient's study, using five different datasets. This paper outlines the e-Health CSIRO team's approach, which achieved multiple first-place finishes in RRG24. The core novelty of our approach lies in the addition of entropy regularisation to self-critical sequence training, to maintain a higher entropy in the token distribution. This prevents overfitting to common phrases and ensures a broader exploration of the vocabulary during training, essential for handling the diversity of the radiology reports in the RRG24 datasets. Our model is available on Hugging Face https://huggingface.co/aehrc/cxrmate-rrg24. \u25b3 Less Submitted 6 August, 2024; originally announced August 2024. arXiv:2407.02723 [pdf, other] e-Health CSIRO at \"Discharge Me!\" 2024: Generating Discharge Summary Sections with Fine-tuned Language Models Authors: Jinghui Liu, Aaron Nicolson, Jason Dowling, Bevan Koopman, Anthony Nguyen Abstract: Clinical documentation is an important aspect of clinicians' daily work and often demands a significant amount of time. The BioNLP 2024 Shared Task on Streamlining Discharge Documentation (Discharge Me!) aims to alleviate this documentation burden by automatically generating discharge summary sections, including brief hospital course and discharge instruction, which are often time-consuming to syn\u2026 \u25bd More Clinical documentation is an important aspect of clinicians' daily work and often demands a significant amount of time. The BioNLP 2024 Shared Task on Streamlining Discharge Documentation (Discharge Me!) aims to alleviate this documentation burden by automatically generating discharge summary sections, including brief hospital course and discharge instruction, which are often time-consuming to synthesize and write manually. We approach the generation task by fine-tuning multiple open-sourced language models (LMs), including both decoder-only and encoder-decoder LMs, with various configurations on input context. We also examine different setups for decoding algorithms, model ensembling or merging, and model specialization. Our results show that conditioning on the content of discharge summary prior to the target sections is effective for the generation task. Furthermore, we find that smaller encoder-decoder LMs can work as well or even slightly better than larger decoder based LMs fine-tuned through LoRA. The model checkpoints from our team (aehrc) are openly available. \u25b3 Less Submitted 2 July, 2024; originally announced July 2024. Comments: BioNLP @ ACL 2024 arXiv:2407.00635 [pdf, other] Dense Retrieval with Continuous Explicit Feedback for Systematic Review Screening Prioritisation Authors: Xinyu Mao, Shengyao Zhuang, Bevan Koopman, Guido Zuccon Abstract: The goal of screening prioritisation in systematic reviews is to identify relevant documents with high recall and rank them in early positions for review. This saves reviewing effort if paired with a stopping criterion, and speeds up review completion if performed alongside downstream tasks. Recent studies have shown that neural models have good potential on this task, but their time-consuming fin\u2026 \u25bd More The goal of screening prioritisation in systematic reviews is to identify relevant documents with high recall and rank them in early positions for review. This saves reviewing effort if paired with a stopping criterion, and speeds up review completion if performed alongside downstream tasks. Recent studies have shown that neural models have good potential on this task, but their time-consuming fine-tuning and inference discourage their widespread use for screening prioritisation. In this paper, we propose an alternative approach that still relies on neural models, but leverages dense representations and relevance feedback to enhance screening prioritisation, without the need for costly model fine-tuning and inference. This method exploits continuous relevance feedback from reviewers during document screening to efficiently update the dense query representation, which is then applied to rank the remaining documents to be screened. We evaluate this approach across the CLEF TAR datasets for this task. Results suggest that the investigated dense query-driven approach is more efficient than directly using neural models and shows promising effectiveness compared to previous methods developed on the considered datasets. Our code is available at https://github.com/ielab/dense-screening-feedback. \u25b3 Less Submitted 17 July, 2024; v1 submitted 30 June, 2024; originally announced July 2024. Comments: Accepted at SIGIR 2024;typos corrected arXiv:2406.13181 [pdf, other] The Impact of Auxiliary Patient Data on Automated Chest X-Ray Report Generation and How to Incorporate It Authors: Aaron Nicolson, Shengyao Zhuang, Jason Dowling, Bevan Koopman Abstract: This study investigates the integration of diverse patient data sources into multimodal language models for automated chest X-ray (CXR) report generation. Traditionally, CXR report generation relies solely on CXR images and limited radiology data, overlooking valuable information from patient health records, particularly from emergency departments. Utilising the MIMIC-CXR and MIMIC-IV-ED datasets,\u2026 \u25bd More This study investigates the integration of diverse patient data sources into multimodal language models for automated chest X-ray (CXR) report generation. Traditionally, CXR report generation relies solely on CXR images and limited radiology data, overlooking valuable information from patient health records, particularly from emergency departments. Utilising the MIMIC-CXR and MIMIC-IV-ED datasets, we incorporate detailed patient information such as vital signs, medicines, and clinical history to enhance diagnostic accuracy. We introduce a novel approach to transform these heterogeneous data sources into embeddings that prompt a multimodal language model; this significantly enhances the diagnostic accuracy of generated radiology reports. Our comprehensive evaluation demonstrates the benefits of using a broader set of patient data, underscoring the potential for enhanced diagnostic capabilities and better patient outcomes through the integration of multimodal data in CXR report generation. \u25b3 Less Submitted 15 December, 2024; v1 submitted 18 June, 2024; originally announced June 2024. arXiv:2405.07920 [pdf, other] A Systematic Investigation of Distilling Large Language Models into Cross-Encoders for Passage Re-ranking Authors: Ferdinand Schlatt, Maik Fr\u00f6be, Harrisen Scells, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Benno Stein, Martin Potthast, Matthias Hagen Abstract: Cross-encoders distilled from large language models (LLMs) are often more effective re-rankers than cross-encoders fine-tuned on manually labeled data. However, the distilled models usually do not reach their teacher LLM's effectiveness. To investigate whether best practices for fine-tuning cross-encoders on manually labeled data (e.g., hard-negative sampling, deep sampling, and listwise loss func\u2026 \u25bd More Cross-encoders distilled from large language models (LLMs) are often more effective re-rankers than cross-encoders fine-tuned on manually labeled data. However, the distilled models usually do not reach their teacher LLM's effectiveness. To investigate whether best practices for fine-tuning cross-encoders on manually labeled data (e.g., hard-negative sampling, deep sampling, and listwise loss functions) can help to improve LLM ranker distillation, we construct and release a new distillation dataset: Rank-DistiLLM. In our experiments, cross-encoders trained on Rank-DistiLLM reach the effectiveness of LLMs while being orders of magnitude more efficient. Our code and data is available at https://github.com/webis-de/msmarco-llm-distillation. \u25b3 Less Submitted 16 June, 2024; v1 submitted 13 May, 2024; originally announced May 2024. arXiv:2404.18424 [pdf, other] PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval Authors: Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin, Guido Zuccon Abstract: Utilizing large language models (LLMs) for zero-shot document ranking is done in one of two ways: (1) prompt-based re-ranking methods, which require no further training but are only feasible for re-ranking a handful of candidate documents due to computational costs; and (2) unsupervised contrastive trained dense retrieval methods, which can retrieve relevant documents from the entire corpus but re\u2026 \u25bd More Utilizing large language models (LLMs) for zero-shot document ranking is done in one of two ways: (1) prompt-based re-ranking methods, which require no further training but are only feasible for re-ranking a handful of candidate documents due to computational costs; and (2) unsupervised contrastive trained dense retrieval methods, which can retrieve relevant documents from the entire corpus but require a large amount of paired text data for contrastive training. In this paper, we propose PromptReps, which combines the advantages of both categories: no need for training and the ability to retrieve from the whole corpus. Our method only requires prompts to guide an LLM to generate query and document representations for effective document retrieval. Specifically, we prompt the LLMs to represent a given text using a single word, and then use the last token's hidden states and the corresponding logits associated with the prediction of the next token to construct a hybrid document retrieval system. The retrieval system harnesses both dense text embedding and sparse bag-of-words representations given by the LLM. Our experimental evaluation on the MSMARCO, TREC deep learning and BEIR zero-shot document retrieval datasets illustrates that this simple prompt-based LLM retrieval method can achieve a similar or higher retrieval effectiveness than state-of-the-art LLM embedding methods that are trained with large amounts of unsupervised data, especially when using a larger LLM. \u25b3 Less Submitted 18 October, 2024; v1 submitted 29 April, 2024; originally announced April 2024. Comments: EMNLP2024 main arXiv:2404.06912 [pdf, other] Set-Encoder: Permutation-Invariant Inter-Passage Attention for Listwise Passage Re-Ranking with Cross-Encoders Authors: Ferdinand Schlatt, Maik Fr\u00f6be, Harrisen Scells, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Benno Stein, Martin Potthast, Matthias Hagen Abstract: Existing cross-encoder re-rankers can be categorized as pointwise, pairwise, or listwise models. Pair- and listwise models allow passage interactions, which usually makes them more effective than pointwise models but also less efficient and less robust to input order permutations. To enable efficient permutation-invariant passage interactions during re-ranking, we propose a new cross-encoder archi\u2026 \u25bd More Existing cross-encoder re-rankers can be categorized as pointwise, pairwise, or listwise models. Pair- and listwise models allow passage interactions, which usually makes them more effective than pointwise models but also less efficient and less robust to input order permutations. To enable efficient permutation-invariant passage interactions during re-ranking, we propose a new cross-encoder architecture with inter-passage attention: the Set-Encoder. In Cranfield-style experiments on TREC Deep Learning and TIREx, the Set-Encoder is as effective as state-of-the-art listwise models while improving efficiency and robustness to input permutations. Interestingly, a pointwise model is similarly effective, but when additionally requiring the models to consider novelty, the Set-Encoder is more effective than its pointwise counterpart and retains its advantageous properties compared to other listwise models. Our code and models are publicly available at https://github.com/webis-de/set-encoder. \u25b3 Less Submitted 16 June, 2024; v1 submitted 10 April, 2024; originally announced April 2024. arXiv:2402.12784 [pdf, other] Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval Systems Authors: Shengyao Zhuang, Bevan Koopman, Xiaoran Chu, Guido Zuccon Abstract: The emergence of Vec2Text -- a method for text embedding inversion -- has raised serious privacy concerns for dense retrieval systems which use text embeddings, such as those offered by OpenAI and Cohere. This threat comes from the ability for a malicious attacker with access to embeddings to reconstruct the original text. In this paper, we investigate various factors related to embedding models t\u2026 \u25bd More The emergence of Vec2Text -- a method for text embedding inversion -- has raised serious privacy concerns for dense retrieval systems which use text embeddings, such as those offered by OpenAI and Cohere. This threat comes from the ability for a malicious attacker with access to embeddings to reconstruct the original text. In this paper, we investigate various factors related to embedding models that may impact text recoverability via Vec2Text. We explore factors such as distance metrics, pooling functions, bottleneck pre-training, training with noise addition, embedding quantization, and embedding dimensions, which were not considered in the original Vec2Text paper. Through a comprehensive analysis of these factors, our objective is to gain a deeper understanding of the key elements that affect the trade-offs between the text recoverability and retrieval effectiveness of dense retrieval systems, offering insights for practitioners designing privacy-aware dense retrieval systems. We also propose a simple embedding transformation fix that guarantees equal ranking effectiveness while mitigating the recoverability risk. Overall, this study reveals that Vec2Text could pose a threat to current dense retrieval systems, but there are some effective methods to patch such systems. \u25b3 Less Submitted 24 July, 2024; v1 submitted 20 February, 2024; originally announced February 2024. arXiv:2401.17645 [pdf, other] ReSLLM: Large Language Models are Strong Resource Selectors for Federated Search Authors: Shuai Wang, Shengyao Zhuang, Bevan Koopman, Guido Zuccon Abstract: Federated search, which involves integrating results from multiple independent search engines, will become increasingly pivotal in the context of Retrieval-Augmented Generation pipelines empowering LLM-based applications such as chatbots. These systems often distribute queries among various search engines, ranging from specialized (e.g., PubMed) to general (e.g., Google), based on the nature of us\u2026 \u25bd More Federated search, which involves integrating results from multiple independent search engines, will become increasingly pivotal in the context of Retrieval-Augmented Generation pipelines empowering LLM-based applications such as chatbots. These systems often distribute queries among various search engines, ranging from specialized (e.g., PubMed) to general (e.g., Google), based on the nature of user utterances. A critical aspect of federated search is resource selection - the selection of appropriate resources prior to issuing the query to ensure high-quality and rapid responses, and contain costs associated with calling the external search engines. However, current SOTA resource selection methodologies primarily rely on feature-based learning approaches. These methods often involve the labour intensive and expensive creation of training labels for each resource. In contrast, LLMs have exhibited strong effectiveness as zero-shot methods across NLP and IR tasks. We hypothesise that in the context of federated search LLMs can assess the relevance of resources without the need for extensive predefined labels or features. In this paper, we propose ReSLLM. Our ReSLLM method exploits LLMs to drive the selection of resources in federated search in a zero-shot setting. In addition, we devise an unsupervised fine tuning protocol, the Synthetic Label Augmentation Tuning (SLAT), where the relevance of previously logged queries and snippets from resources is predicted using an off-the-shelf LLM and then in turn used to fine-tune ReSLLM with respect to resource selection. Our empirical evaluation and analysis details the factors influencing the effectiveness of LLMs in this context. The results showcase the merits of ReSLLM for resource selection: not only competitive effectiveness in the zero-shot setting, but also obtaining large when fine-tuned using SLAT-protocol. \u25b3 Less Submitted 31 January, 2024; originally announced January 2024. arXiv:2401.13509 [pdf, other] TPRF: A Transformer-based Pseudo-Relevance Feedback Model for Efficient and Effective Retrieval Authors: Hang Li, Chuting Yu, Ahmed Mourad, Bevan Koopman, Guido Zuccon Abstract: This paper considers Pseudo-Relevance Feedback (PRF) methods for dense retrievers in a resource constrained environment such as that of cheap cloud instances or embedded systems (e.g., smartphones and smartwatches), where memory and CPU are limited and GPUs are not present. For this, we propose a transformer-based PRF method (TPRF), which has a much smaller memory footprint and faster inference ti\u2026 \u25bd More This paper considers Pseudo-Relevance Feedback (PRF) methods for dense retrievers in a resource constrained environment such as that of cheap cloud instances or embedded systems (e.g., smartphones and smartwatches), where memory and CPU are limited and GPUs are not present. For this, we propose a transformer-based PRF method (TPRF), which has a much smaller memory footprint and faster inference time compared to other deep language models that employ PRF mechanisms, with a marginal effectiveness loss. TPRF learns how to effectively combine the relevance feedback signals from dense passage representations. Specifically, TPRF provides a mechanism for modelling relationships and weights between the query and the relevance feedback signals. The method is agnostic to the specific dense representation used and thus can be generally applied to any dense retriever. \u25b3 Less Submitted 6 December, 2024; v1 submitted 24 January, 2024; originally announced January 2024. arXiv:2401.08104 [pdf, other] A Reproducibility Study of Goldilocks: Just-Right Tuning of BERT for TAR Authors: Xinyu Mao, Bevan Koopman, Guido Zuccon Abstract: Screening documents is a tedious and time-consuming aspect of high-recall retrieval tasks, such as compiling a systematic literature review, where the goal is to identify all relevant documents for a topic. To help streamline this process, many Technology-Assisted Review (TAR) methods leverage active learning techniques to reduce the number of documents requiring review. BERT-based models have sho\u2026 \u25bd More Screening documents is a tedious and time-consuming aspect of high-recall retrieval tasks, such as compiling a systematic literature review, where the goal is to identify all relevant documents for a topic. To help streamline this process, many Technology-Assisted Review (TAR) methods leverage active learning techniques to reduce the number of documents requiring review. BERT-based models have shown high effectiveness in text classification, leading to interest in their potential use in TAR workflows. In this paper, we investigate recent work that examined the impact of further pre-training epochs on the effectiveness and efficiency of a BERT-based active learning pipeline. We first report that we could replicate the original experiments on two specific TAR datasets, confirming some of the findings: importantly, that further pre-training is critical to high effectiveness, but requires attention in terms of selecting the correct training epoch. We then investigate the generalisability of the pipeline on a different TAR task, that of medical systematic reviews. In this context, we show that there is no need for further pre-training if a domain-specific BERT backbone is used within the active learning pipeline. This finding provides practical implications for using the studied active learning pipeline within domain-specific TAR tasks. \u25b3 Less Submitted 15 January, 2024; originally announced January 2024. Comments: Accepted at ECIR 2024 (reproducibility) arXiv:2401.06320 [pdf, other] Zero-shot Generative Large Language Models for Systematic Review Screening Automation Authors: Shuai Wang, Harrisen Scells, Shengyao Zhuang, Martin Potthast, Bevan Koopman, Guido Zuccon Abstract: Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models~(LLMs\u2026 \u25bd More Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models~(LLMs) for automatic screening. We evaluate the effectiveness of eight different LLMs and investigate a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Our comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders LLMs practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches. \u25b3 Less Submitted 31 January, 2024; v1 submitted 11 January, 2024; originally announced January 2024. Comments: Accepted to ECIR2024 full paper (findings) arXiv:2401.01566 [pdf, other] Team IELAB at TREC Clinical Trial Track 2023: Enhancing Clinical Trial Retrieval with Neural Rankers and Large Language Models Authors: Shengyao Zhuang, Bevan Koopman, Guido Zuccon Abstract: We describe team ielab from CSIRO and The University of Queensland's approach to the 2023 TREC Clinical Trials Track. Our approach was to use neural rankers but to utilise Large Language Models to overcome the issue of lack of training data for such rankers. Specifically, we employ ChatGPT to generate relevant patient descriptions for randomly selected clinical trials from the corpus. This synthet\u2026 \u25bd More We describe team ielab from CSIRO and The University of Queensland's approach to the 2023 TREC Clinical Trials Track. Our approach was to use neural rankers but to utilise Large Language Models to overcome the issue of lack of training data for such rankers. Specifically, we employ ChatGPT to generate relevant patient descriptions for randomly selected clinical trials from the corpus. This synthetic dataset, combined with human-annotated training data from previous years, is used to train both dense and sparse retrievers based on PubmedBERT. Additionally, a cross-encoder re-ranker is integrated into the system. To further enhance the effectiveness of our approach, we prompting GPT-4 as a TREC annotator to provide judgments on our run files. These judgments are subsequently employed to re-rank the results. This architecture tightly integrates strong PubmedBERT-based rankers with the aid of SOTA Large Language Models, demonstrating a new approach to clinical trial retrieval. \u25b3 Less Submitted 3 January, 2024; originally announced January 2024. Comments: TREC Notebook arXiv:2310.13243 [pdf, other] Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking Authors: Shengyao Zhuang, Bing Liu, Bevan Koopman, Guido Zuccon Abstract: In the field of information retrieval, Query Likelihood Models (QLMs) rank documents based on the probability of generating the query given the content of a document. Recently, advanced large language models (LLMs) have emerged as effective QLMs, showcasing promising ranking capabilities. This paper focuses on investigating the genuine zero-shot ranking effectiveness of recent LLMs, which are sole\u2026 \u25bd More In the field of information retrieval, Query Likelihood Models (QLMs) rank documents based on the probability of generating the query given the content of a document. Recently, advanced large language models (LLMs) have emerged as effective QLMs, showcasing promising ranking capabilities. This paper focuses on investigating the genuine zero-shot ranking effectiveness of recent LLMs, which are solely pre-trained on unstructured text data without supervised instruction fine-tuning. Our findings reveal the robust zero-shot ranking ability of such LLMs, highlighting that additional instruction fine-tuning may hinder effectiveness unless a question generation task is present in the fine-tuning dataset. Furthermore, we introduce a novel state-of-the-art ranking system that integrates LLM-based QLMs with a hybrid zero-shot retriever, demonstrating exceptional effectiveness in both zero-shot and few-shot scenarios. We make our codebase publicly available at https://github.com/ielab/llm-qlm. \u25b3 Less Submitted 19 October, 2023; originally announced October 2023. Comments: 5 pages arXiv:2310.09497 [pdf, other] A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models Authors: Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, Guido Zuccon Abstract: We propose a novel zero-shot document ranking approach based on Large Language Models (LLMs): the Setwise prompting approach. Our approach complements existing prompting approaches for LLM-based zero-shot ranking: Pointwise, Pairwise, and Listwise. Through the first-of-its-kind comparative evaluation within a consistent experimental framework and considering factors like model size, token consumpt\u2026 \u25bd More We propose a novel zero-shot document ranking approach based on Large Language Models (LLMs): the Setwise prompting approach. Our approach complements existing prompting approaches for LLM-based zero-shot ranking: Pointwise, Pairwise, and Listwise. Through the first-of-its-kind comparative evaluation within a consistent experimental framework and considering factors like model size, token consumption, latency, among others, we show that existing approaches are inherently characterised by trade-offs between effectiveness and efficiency. We find that while Pointwise approaches score high on efficiency, they suffer from poor effectiveness. Conversely, Pairwise approaches demonstrate superior effectiveness but incur high computational overhead. Our Setwise approach, instead, reduces the number of LLM inferences and the amount of prompt token consumption during the ranking procedure, compared to previous methods. This significantly improves the efficiency of LLM-based zero-shot ranking, while also retaining high zero-shot ranking effectiveness. We make our code and results publicly available at \\url{https://github.com/ielab/llm-rankers}. \u25b3 Less Submitted 30 May, 2024; v1 submitted 14 October, 2023; originally announced October 2023. Comments: SIGIR2024 full paper arXiv:2309.09401 [pdf, other] ChatGPT Hallucinates when Attributing Answers Authors: Guido Zuccon, Bevan Koopman, Razia Shaik Abstract: Can ChatGPT provide evidence to support its answers? Does the evidence it suggests actually exist and does it really support its answer? We investigate these questions using a collection of domain-specific knowledge-based questions, specifically prompting ChatGPT to provide both an answer and supporting evidence in the form of references to external sources. We also investigate how different promp\u2026 \u25bd More Can ChatGPT provide evidence to support its answers? Does the evidence it suggests actually exist and does it really support its answer? We investigate these questions using a collection of domain-specific knowledge-based questions, specifically prompting ChatGPT to provide both an answer and supporting evidence in the form of references to external sources. We also investigate how different prompts impact answers and evidence. We find that ChatGPT provides correct or partially correct answers in about half of the cases (50.6% of the times), but its suggested references only exist 14% of the times. We further provide insights on the generated references that reveal common traits among the references that ChatGPT generates, and show how even if a reference provided by the model does exist, this reference often does not support the claims ChatGPT attributes to it. Our findings are important because (1) they are the first systematic analysis of the references created by ChatGPT in its answers; (2) they suggest that the model may leverage good quality information in producing correct answers, but is unable to attribute real evidence to support its answers. Prompts, raw result files and manual analysis are made publicly available. \u25b3 Less Submitted 17 September, 2023; originally announced September 2023. arXiv:2309.05238 [pdf, other] Generating Natural Language Queries for More Effective Systematic Review Screening Prioritisation Authors: Shuai Wang, Harrisen Scells, Martin Potthast, Bevan Koopman, Guido Zuccon Abstract: Screening prioritisation in medical systematic reviews aims to rank the set of documents retrieved by complex Boolean queries. Prioritising the most important documents ensures that subsequent review steps can be carried out more efficiently and effectively. The current state of the art uses the final title of the review as a query to rank the documents using BERT-based neural rankers. However, th\u2026 \u25bd More Screening prioritisation in medical systematic reviews aims to rank the set of documents retrieved by complex Boolean queries. Prioritising the most important documents ensures that subsequent review steps can be carried out more efficiently and effectively. The current state of the art uses the final title of the review as a query to rank the documents using BERT-based neural rankers. However, the final title is only formulated at the end of the review process, which makes this approach impractical as it relies on ex post facto information. At the time of screening, only a rough working title is available, with which the BERT-based ranker performs significantly worse than with the final title. In this paper, we explore alternative sources of queries for prioritising screening, such as the Boolean query used to retrieve the documents to be screened and queries generated by instruction-based generative large-scale language models such as ChatGPT and Alpaca. Our best approach is not only viable based on the information available at the time of screening, but also has similar effectiveness to the final title. \u25b3 Less Submitted 23 November, 2023; v1 submitted 11 September, 2023; originally announced September 2023. Comments: Preprints for Accepted paper in SIGIR-AP-2023, note that this is updated from ACM published paper. The working title was wrong in the ACM-published version due to a bug in data preprocessing; however, this does not have any influence on the final conclusion/observation made from the paper arXiv:2307.09758 [pdf, other] Longitudinal Data and a Semantic Similarity Reward for Chest X-Ray Report Generation Authors: Aaron Nicolson, Jason Dowling, Bevan Koopman Abstract: Radiologists face high burnout rates, partially due to the increasing volume of Chest X-rays (CXRs) requiring interpretation and reporting. Automated CXR report generation holds promise for reducing this burden and improving patient care. While current models show potential, their diagnostic accuracy is limited. Our proposed CXR report generator integrates elements of the radiologist workflow and\u2026 \u25bd More Radiologists face high burnout rates, partially due to the increasing volume of Chest X-rays (CXRs) requiring interpretation and reporting. Automated CXR report generation holds promise for reducing this burden and improving patient care. While current models show potential, their diagnostic accuracy is limited. Our proposed CXR report generator integrates elements of the radiologist workflow and introduces a novel reward for reinforcement learning. Our approach leverages longitudinal data from a patient's prior CXR study and effectively handles cases where no prior study exist, thus mirroring the radiologist's workflow. In contrast, existing models typically lack this flexibility, often requiring prior studies for the model to function optimally. Our approach also incorporates all CXRs from a patient's study and distinguishes between report sections through section embeddings. Our reward for reinforcement learning leverages CXR-BERT, which forces our model to learn the clinical semantics of radiology reporting. We conduct experiments on publicly available datasets -- MIMIC-CXR and Open-i IU X-ray -- with metrics shown to more closely correlate with radiologists' assessment of reporting. Results from our study demonstrate that the proposed model generates reports that are more aligned with radiologists' reports than state-of-the-art models, such as those utilising large language models, reinforcement learning, and multi-task learning. The proposed model improves the diagnostic accuracy of CXR report generation, which could one day reduce radiologists' workload and enhance patient care. Our Hugging Face checkpoint (https://huggingface.co/aehrc/cxrmate) and code (https://github.com/aehrc/cxrmate) are publicly available. \u25b3 Less Submitted 18 June, 2024; v1 submitted 19 July, 2023; originally announced July 2023. arXiv:2302.13793 [pdf, other] Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness Authors: Guido Zuccon, Bevan Koopman Abstract: Generative pre-trained language models (GPLMs) like ChatGPT encode in the model's parameters knowledge the models observe during the pre-training phase. This knowledge is then used at inference to address the task specified by the user in their prompt. For example, for the question-answering task, the GPLMs leverage the knowledge and linguistic patterns learned at training to produce an answer to\u2026 \u25bd More Generative pre-trained language models (GPLMs) like ChatGPT encode in the model's parameters knowledge the models observe during the pre-training phase. This knowledge is then used at inference to address the task specified by the user in their prompt. For example, for the question-answering task, the GPLMs leverage the knowledge and linguistic patterns learned at training to produce an answer to a user question. Aside from the knowledge encoded in the model itself, answers produced by GPLMs can also leverage knowledge provided in the prompts. For example, a GPLM can be integrated into a retrieve-then-generate paradigm where a search engine is used to retrieve documents relevant to the question; the content of the documents is then transferred to the GPLM via the prompt. In this paper we study the differences in answer correctness generated by ChatGPT when leveraging the model's knowledge alone vs. in combination with the prompt knowledge. We study this in the context of consumers seeking health advice from the model. Aside from measuring the effectiveness of ChatGPT in this context, we show that the knowledge passed in the prompt can overturn the knowledge encoded in the model and this is, in our experiments, to the detriment of answer correctness. This work has important implications for the development of more robust and transparent question-answering systems based on generative pre-trained language models. \u25b3 Less Submitted 23 February, 2023; originally announced February 2023. arXiv:2302.03495 [pdf, other] Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search? Authors: Shuai Wang, Harrisen Scells, Bevan Koopman, Guido Zuccon Abstract: Systematic reviews are comprehensive reviews of the literature for a highly focused research question. These reviews are often treated as the highest form of evidence in evidence-based medicine, and are the key strategy to answer research questions in the medical field. To create a high-quality systematic review, complex Boolean queries are often constructed to retrieve studies for the review topi\u2026 \u25bd More Systematic reviews are comprehensive reviews of the literature for a highly focused research question. These reviews are often treated as the highest form of evidence in evidence-based medicine, and are the key strategy to answer research questions in the medical field. To create a high-quality systematic review, complex Boolean queries are often constructed to retrieve studies for the review topic. However, it often takes a long time for systematic review researchers to construct a high quality systematic review Boolean query, and often the resulting queries are far from effective. Poor queries may lead to biased or invalid reviews, because they missed to retrieve key evidence, or to extensive increase in review costs, because they retrieved too many irrelevant studies. Recent advances in Transformer-based generative models have shown great potential to effectively follow instructions from users and generate answers based on the instructions being made. In this paper, we investigate the effectiveness of the latest of such models, ChatGPT, in generating effective Boolean queries for systematic review literature search. Through a number of extensive experiments on standard test collections for the task, we find that ChatGPT is capable of generating queries that lead to high search precision, although trading-off this for recall. Overall, our study demonstrates the potential of ChatGPT in generating effective Boolean queries for systematic review literature search. The ability of ChatGPT to follow complex instructions and generate queries with high precision makes it a valuable tool for researchers conducting systematic reviews, particularly for rapid reviews where time is a constraint and often trading-off higher precision for lower recall is acceptable. \u25b3 Less Submitted 9 February, 2023; v1 submitted 2 February, 2023; originally announced February 2023. arXiv:2212.10762 [pdf, other] AgAsk: An Agent to Help Answer Farmer's Questions From Scientific Documents Authors: Bevan Koopman, Ahmed Mourad, Hang Li, Anton van der Vegt, Shengyao Zhuang, Simon Gibson, Yash Dang, David Lawrence, Guido Zuccon Abstract: Decisions in agriculture are increasingly data-driven; however, valuable agricultural knowledge is often locked away in free-text reports, manuals and journal articles. Specialised search systems are needed that can mine agricultural information to provide relevant answers to users' questions. This paper presents AgAsk -- an agent able to answer natural language agriculture questions by mining sci\u2026 \u25bd More Decisions in agriculture are increasingly data-driven; however, valuable agricultural knowledge is often locked away in free-text reports, manuals and journal articles. Specialised search systems are needed that can mine agricultural information to provide relevant answers to users' questions. This paper presents AgAsk -- an agent able to answer natural language agriculture questions by mining scientific documents. We carefully survey and analyse farmers' information needs. On the basis of these needs we release an information retrieval test collection comprising real questions, a large collection of scientific documents split in passages, and ground truth relevance assessments indicating which passages are relevant to each question. We implement and evaluate a number of information retrieval models to answer farmers questions, including two state-of-the-art neural ranking models. We show that neural rankers are highly effective at matching passages to questions in this context. Finally, we propose a deployment architecture for AgAsk that includes a client based on the Telegram messaging platform and retrieval model deployed on commodity hardware. The test collection we provide is intended to stimulate more research in methods to match natural language to answers in scientific documents. While the retrieval models were evaluated in the agriculture domain, they are generalisable and of interest to others working on similar problems. The test collection is available at: \\url{https://github.com/ielab/agvaluate}. \u25b3 Less Submitted 20 December, 2022; originally announced December 2022. Comments: 17 pages, submitted to IJDL arXiv:2212.09017 [pdf, other] Neural Rankers for Effective Screening Prioritisation in Medical Systematic Review Literature Search Authors: Shuai Wang, Harrisen Scells, Bevan Koopman, Guido Zuccon Abstract: Medical systematic reviews typically require assessing all the documents retrieved by a search. The reason is two-fold: the task aims for ``total recall''; and documents retrieved using Boolean search are an unordered set, and thus it is unclear how an assessor could examine only a subset. Screening prioritisation is the process of ranking the (unordered) set of retrieved documents, allowing asses\u2026 \u25bd More Medical systematic reviews typically require assessing all the documents retrieved by a search. The reason is two-fold: the task aims for ``total recall''; and documents retrieved using Boolean search are an unordered set, and thus it is unclear how an assessor could examine only a subset. Screening prioritisation is the process of ranking the (unordered) set of retrieved documents, allowing assessors to begin the downstream processes of the systematic review creation earlier, leading to earlier completion of the review, or even avoiding screening documents ranked least relevant. Screening prioritisation requires highly effective ranking methods. Pre-trained language models are state-of-the-art on many IR tasks but have yet to be applied to systematic review screening prioritisation. In this paper, we apply several pre-trained language models to the systematic review document ranking task, both directly and fine-tuned. An empirical analysis compares how effective neural methods compare to traditional methods for this task. We also investigate different types of document representations for neural methods and their impact on ranking performance. Our results show that BERT-based rankers outperform the current state-of-the-art screening prioritisation methods. However, BERT rankers and existing methods can actually be complementary, and thus, further improvements may be achieved if used in conjunction. \u25b3 Less Submitted 18 December, 2022; originally announced December 2022. arXiv:2209.08687 [pdf, other] Automated MeSH Term Suggestion for Effective Query Formulation in Systematic Reviews Literature Search Authors: Shuai Wang, Harrisen Scells, Bevan Koopman, Guido Zuccon Abstract: High-quality medical systematic reviews require comprehensive literature searches to ensure the recommendations and outcomes are sufficiently reliable. Indeed, searching for relevant medical literature is a key phase in constructing systematic reviews and often involves domain (medical researchers) and search (information specialists) experts in developing the search queries. Queries in this conte\u2026 \u25bd More High-quality medical systematic reviews require comprehensive literature searches to ensure the recommendations and outcomes are sufficiently reliable. Indeed, searching for relevant medical literature is a key phase in constructing systematic reviews and often involves domain (medical researchers) and search (information specialists) experts in developing the search queries. Queries in this context are highly complex, based on Boolean logic, include free-text terms and index terms from standardised terminologies (e.g., the Medical Subject Headings (MeSH) thesaurus), and are difficult and time-consuming to build. The use of MeSH terms, in particular, has been shown to improve the quality of the search results. However, identifying the correct MeSH terms to include in a query is difficult: information experts are often unfamiliar with the MeSH database and unsure about the appropriateness of MeSH terms for a query. Naturally, the full value of the MeSH terminology is often not fully exploited. This article investigates methods to suggest MeSH terms based on an initial Boolean query that includes only free-text terms. In this context, we devise lexical and pre-trained language models based methods. These methods promise to automatically identify highly effective MeSH terms for inclusion in a systematic review query. Our study contributes an empirical evaluation of several MeSH term suggestion methods. We further contribute an extensive analysis of MeSH term suggestions for each method and how these suggestions impact the effectiveness of Boolean queries. \u25b3 Less Submitted 18 September, 2022; originally announced September 2022. Comments: This paper is currently in submission with Intelligent Systems with Applications Journal Technology-Assisted Review Systems Special issue and is under peer review. arXiv admin note: text overlap with arXiv:2112.00277 arXiv:2205.05888 [pdf, ps, other] How does Feedback Signal Quality Impact Effectiveness of Pseudo Relevance Feedback for Passage Retrieval? Authors: Hang Li, Ahmed Mourad, Bevan Koopman, Guido Zuccon Abstract: Pseudo-Relevance Feedback (PRF) assumes that the top results retrieved by a first-stage ranker are relevant to the original query and uses them to improve the query representation for a second round of retrieval. This assumption however is often not correct: some or even all of the feedback documents may be irrelevant. Indeed, the effectiveness of PRF methods may well depend on the quality of the\u2026 \u25bd More Pseudo-Relevance Feedback (PRF) assumes that the top results retrieved by a first-stage ranker are relevant to the original query and uses them to improve the query representation for a second round of retrieval. This assumption however is often not correct: some or even all of the feedback documents may be irrelevant. Indeed, the effectiveness of PRF methods may well depend on the quality of the feedback signal and thus on the effectiveness of the first-stage ranker. This aspect however has received little attention before. In this paper we control the quality of the feedback signal and measure its impact on a range of PRF methods, including traditional bag-of-words methods (Rocchio), and dense vector-based methods (learnt and not learnt). Our results show the important role the quality of the feedback signal plays on the effectiveness of PRF methods. Importantly, and surprisingly, our analysis reveals that not all PRF methods are the same when dealing with feedback signals of varying quality. These findings are critical to gain a better understanding of the PRF methods and of which and when they should be used, depending on the feedback signal quality, and set the basis for future research in this area. \u25b3 Less Submitted 12 May, 2022; originally announced May 2022. Comments: Accepted at SIGIR 2022 arXiv:2204.03096 [pdf, other] From Little Things Big Things Grow: A Collection with Seed Studies for Medical Systematic Review Literature Search Authors: Shuai Wang, Harrisen Scells, Justin Clark, Bevan Koopman, Guido Zuccon Abstract: Medical systematic review query formulation is a highly complex task done by trained information specialists. Complexity comes from the reliance on lengthy Boolean queries, which express a detailed research question. To aid query formulation, information specialists use a set of exemplar documents, called `seed studies', prior to query formulation. Seed studies help verify the effectiveness of a q\u2026 \u25bd More Medical systematic review query formulation is a highly complex task done by trained information specialists. Complexity comes from the reliance on lengthy Boolean queries, which express a detailed research question. To aid query formulation, information specialists use a set of exemplar documents, called `seed studies', prior to query formulation. Seed studies help verify the effectiveness of a query prior to the full assessment of retrieved studies. Beyond this use of seeds, specific IR methods can exploit seed studies for guiding both automatic query formulation and new retrieval models. One major limitation of work to date is that these methods exploit `pseudo seed studies' through retrospective use of included studies (i.e., relevance assessments). However, we show pseudo seed studies are not representative of real seed studies used by information specialists. Hence, we provide a test collection with real world seed studies used to assist with the formulation of queries. To support our collection, we provide an analysis, previously not possible, on how seed studies impact retrieval and perform several experiments using seed-study based methods to compare the effectiveness of using seed studies versus pseudo seed studies. We make our test collection and the results of all of our experiments and analysis available at http://github.com/ielab/sysrev-seed-collection \u25b3 Less Submitted 24 April, 2022; v1 submitted 6 April, 2022; originally announced April 2022. Comments: Accepted and To be appeared in SIGIR 2022 proceeding arXiv:2201.09405 [pdf, other] Improving Chest X-Ray Report Generation by Leveraging Warm Starting Authors: Aaron Nicolson, Jason Dowling, Bevan Koopman Abstract: Automatically generating a report from a patient's Chest X-Rays (CXRs) is a promising solution to reducing clinical workload and improving patient care. However, current CXR report generators -- which are predominantly encoder-to-decoder models -- lack the diagnostic accuracy to be deployed in a clinical setting. To improve CXR report generation, we investigate warm starting the encoder and decode\u2026 \u25bd More Automatically generating a report from a patient's Chest X-Rays (CXRs) is a promising solution to reducing clinical workload and improving patient care. However, current CXR report generators -- which are predominantly encoder-to-decoder models -- lack the diagnostic accuracy to be deployed in a clinical setting. To improve CXR report generation, we investigate warm starting the encoder and decoder with recent open-source computer vision and natural language processing checkpoints, such as the Vision Transformer (ViT) and PubMedBERT. To this end, each checkpoint is evaluated on the MIMIC-CXR and IU X-Ray datasets. Our experimental investigation demonstrates that the Convolutional vision Transformer (CvT) ImageNet-21K and the Distilled Generative Pre-trained Transformer 2 (DistilGPT2) checkpoints are best for warm starting the encoder and decoder, respectively. Compared to the state-of-the-art ($\\mathcal{M}^2$ Transformer Progressive), CvT2DistilGPT2 attained an improvement of 8.3\\% for CE F-1, 1.8\\% for BLEU-4, 1.6\\% for ROUGE-L, and 1.0\\% for METEOR. The reports generated by CvT2DistilGPT2 have a higher similarity to radiologist reports than previous approaches. This indicates that leveraging warm starting improves CXR report generation. Code and checkpoints for CvT2DistilGPT2 are available at https://github.com/aehrc/cvt2distilgpt2. \u25b3 Less Submitted 12 July, 2023; v1 submitted 23 January, 2022; originally announced January 2022. arXiv:2201.00118 [pdf, ps, other] Semantic Search for Large Scale Clinical Ontologies Authors: Duy-Hoa Ngo, Madonna Kemp, Donna Truran, Bevan Koopman, Alejandro Metke-Jimenez Abstract: Finding concepts in large clinical ontologies can be challenging when queries use different vocabularies. A search algorithm that overcomes this problem is useful in applications such as concept normalisation and ontology matching, where concepts can be referred to in different ways, using different synonyms. In this paper, we present a deep learning based approach to build a semantic search syste\u2026 \u25bd More Finding concepts in large clinical ontologies can be challenging when queries use different vocabularies. A search algorithm that overcomes this problem is useful in applications such as concept normalisation and ontology matching, where concepts can be referred to in different ways, using different synonyms. In this paper, we present a deep learning based approach to build a semantic search system for large clinical ontologies. We propose a Triplet-BERT model and a method that generates training data directly from the ontologies. The model is evaluated using five real benchmark data sets and the results show that our approach achieves high results on both free text to concept and concept to concept searching tasks, and outperforms all baseline methods. \u25b3 Less Submitted 1 January, 2022; originally announced January 2022. arXiv:2108.11044 [pdf, other] Pseudo Relevance Feedback with Deep Language Models and Dense Retrievers: Successes and Pitfalls Authors: Hang Li, Ahmed Mourad, Shengyao Zhuang, Bevan Koopman, Guido Zuccon Abstract: Pseudo Relevance Feedback (PRF) is known to improve the effectiveness of bag-of-words retrievers. At the same time, deep language models have been shown to outperform traditional bag-of-words rerankers. However, it is unclear how to integrate PRF directly with emergent deep language models. In this article, we address this gap by investigating methods for integrating PRF signals into rerankers and\u2026 \u25bd More Pseudo Relevance Feedback (PRF) is known to improve the effectiveness of bag-of-words retrievers. At the same time, deep language models have been shown to outperform traditional bag-of-words rerankers. However, it is unclear how to integrate PRF directly with emergent deep language models. In this article, we address this gap by investigating methods for integrating PRF signals into rerankers and dense retrievers based on deep language models. We consider text-based and vector-based PRF approaches, and investigate different ways of combining and scoring relevance signals. An extensive empirical evaluation was conducted across four different datasets and two task settings (retrieval and ranking). Text-based PRF results show that the use of PRF had a mixed effect on deep rerankers across different datasets. We found that the best effectiveness was achieved when (i) directly concatenating each PRF passage with the query, searching with the new set of queries, and then aggregating the scores; (ii) using Borda to aggregate scores from PRF runs. Vector-based PRF results show that the use of PRF enhanced the effectiveness of deep rerankers and dense retrievers over several evaluation metrics. We found that higher effectiveness was achieved when (i) the query retains either the majority or the same weight within the PRF mechanism, and (ii) a shallower PRF signal (i.e., a smaller number of top-ranked passages) was employed, rather than a deeper signal. Our vector-based PRF method is computationally efficient; thus this represents a general PRF method others can use with deep rerankers and dense retrievers. \u25b3 Less Submitted 30 June, 2022; v1 submitted 25 August, 2021; originally announced August 2021. Comments: Accepted to the Journal of ACM Transactions on Information Systems (TOIS)"
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Wang,+X",
    "title": "Showing 1\u201350 of 7,400 results for author: Wang, X",
    "author": "Authors:\nZineng Tang, \n      \n      Long Lian, \n      \n      Seun Eisape, \n      \n      XuDong Wang, \n      \n      Roei Herzig, \n      \n      Adam Yala, \n      \n      Alane Suhr, \n      \n      Trevor Darrell, \n      \n      David M. Chan",
    "body": "arXiv:2503.15485 [pdf, other] TULIP: Towards Unified Language-Image Pretraining Authors: Zineng Tang, Long Lian, Seun Eisape, XuDong Wang, Roei Herzig, Adam Yala, Alane Suhr, Trevor Darrell, David M. Chan Abstract: Despite the recent success of image-text contrastive models like CLIP and SigLIP, these models often struggle with vision-centric tasks that demand high-fidelity image understanding, such as counting, depth estimation, and fine-grained object recognition. These models, by performing language alignment, tend to prioritize high-level semantics over visual understanding, weakening their image underst\u2026 \u25bd More Despite the recent success of image-text contrastive models like CLIP and SigLIP, these models often struggle with vision-centric tasks that demand high-fidelity image understanding, such as counting, depth estimation, and fine-grained object recognition. These models, by performing language alignment, tend to prioritize high-level semantics over visual understanding, weakening their image understanding. On the other hand, vision-focused models are great at processing visual information but struggle to understand language, limiting their flexibility for language-driven tasks. In this work, we introduce TULIP, an open-source, drop-in replacement for existing CLIP-like models. Our method leverages generative data augmentation, enhanced image-image and text-text contrastive learning, and image/text reconstruction regularization to learn fine-grained visual features while preserving global semantic alignment. Our approach, scaling to over 1B parameters, outperforms existing state-of-the-art (SOTA) models across multiple benchmarks, establishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to a $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot classification, and improving vision-language models, achieving over $3\\times$ higher scores than SigLIP on MMVP. Our code/checkpoints are available at https://tulip-berkeley.github.io \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. arXiv:2503.15457 [pdf, other] Di$\\mathtt{[M]}$O: Distilling Masked Diffusion Models into One-step Generator Authors: Yuanzhi Zhu, Xi Wang, St\u00e9phane Lathuili\u00e8re, Vicky Kalogeiton Abstract: Masked Diffusion Models (MDMs) have emerged as a powerful generative modeling technique. Despite their remarkable results, they typically suffer from slow inference with several steps. In this paper, we propose Di$\\mathtt{[M]}$O, a novel approach that distills masked diffusion models into a one-step generator. Di$\\mathtt{[M]}$O addresses two key challenges: (1) the intractability of using intermed\u2026 \u25bd More Masked Diffusion Models (MDMs) have emerged as a powerful generative modeling technique. Despite their remarkable results, they typically suffer from slow inference with several steps. In this paper, we propose Di$\\mathtt{[M]}$O, a novel approach that distills masked diffusion models into a one-step generator. Di$\\mathtt{[M]}$O addresses two key challenges: (1) the intractability of using intermediate-step information for one-step generation, which we solve through token-level distribution matching that optimizes model output logits by an 'on-policy framework' with the help of an auxiliary model; and (2) the lack of entropy in the initial distribution, which we address through a token initialization strategy that injects randomness while maintaining similarity to teacher training distribution. We show Di$\\mathtt{[M]}$O's effectiveness on both class-conditional and text-conditional image generation, impressively achieving performance competitive to multi-step teacher outputs while drastically reducing inference time. To our knowledge, we are the first to successfully achieve one-step distillation of masked diffusion models and the first to apply discrete distillation to text-to-image generation, opening new paths for efficient generative modeling. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. arXiv:2503.15277 [pdf, other] What Makes a Good TODO Comment? Authors: Haoye Wang, Zhipeng Gao, Tingting Bi, John Grundy, Xinyu Wang, Minghui Wu, Xiaohu Yang Abstract: Software development is a collaborative process that involves various interactions among individuals and teams. TODO comments in source code play a critical role in managing and coordinating diverse tasks during this process. However, this study finds that a large proportion of open-source project TODO comments are left unresolved or take a long time to be resolved. About 46.7\\% of TODO comments i\u2026 \u25bd More Software development is a collaborative process that involves various interactions among individuals and teams. TODO comments in source code play a critical role in managing and coordinating diverse tasks during this process. However, this study finds that a large proportion of open-source project TODO comments are left unresolved or take a long time to be resolved. About 46.7\\% of TODO comments in open-source repositories are of low-quality (e.g., TODOs that are ambiguous, lack information, or are useless to developers). This highlights the need for better TODO practices. In this study, we investigate four aspects regarding the quality of TODO comments in open-source projects: (1) the prevalence of low-quality TODO comments; (2) the key characteristics of high-quality TODO comments; (3) how are TODO comments of different quality managed in practice; and (4) the feasibility of automatically assessing TODO comment quality. Examining 2,863 TODO comments from Top100 GitHub Java repositories, we propose criteria to identify high-quality TODO comments and provide insights into their optimal composition. We discuss the lifecycle of TODO comments with varying quality. we construct deep learning-based methods that show promising performance in identifying the quality of TODO comments, potentially enhancing development efficiency and code quality. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. arXiv:2503.15029 [pdf, other] DRoPE: Directional Rotary Position Embedding for Efficient Agent Interaction Modeling Authors: Jianbo Zhao, Taiyu Ban, Zhihao Liu, Hangning Zhou, Xiyang Wang, Qibin Zhou, Hailong Qin, Mu Yang, Lei Liu, Bin Li Abstract: Accurate and efficient modeling of agent interactions is essential for trajectory generation, the core of autonomous driving systems. Existing methods, scene-centric, agent-centric, and query-centric frameworks, each present distinct advantages and drawbacks, creating an impossible triangle among accuracy, computational time, and memory efficiency. To break this limitation, we propose Directional\u2026 \u25bd More Accurate and efficient modeling of agent interactions is essential for trajectory generation, the core of autonomous driving systems. Existing methods, scene-centric, agent-centric, and query-centric frameworks, each present distinct advantages and drawbacks, creating an impossible triangle among accuracy, computational time, and memory efficiency. To break this limitation, we propose Directional Rotary Position Embedding (DRoPE), a novel adaptation of Rotary Position Embedding (RoPE), originally developed in natural language processing. Unlike traditional relative position embedding (RPE), which introduces significant space complexity, RoPE efficiently encodes relative positions without explicitly increasing complexity but faces inherent limitations in handling angular information due to periodicity. DRoPE overcomes this limitation by introducing a uniform identity scalar into RoPE's 2D rotary transformation, aligning rotation angles with realistic agent headings to naturally encode relative angular information. We theoretically analyze DRoPE's correctness and efficiency, demonstrating its capability to simultaneously optimize trajectory generation accuracy, time complexity, and space complexity. Empirical evaluations compared with various state-of-the-art trajectory generation models, confirm DRoPE's good performance and significantly reduced space complexity, indicating both theoretical soundness and practical effectiveness. The video documentation is available at https://drope-traj.github.io/. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. arXiv:2503.14908 [pdf, other] POSTA: A Go-to Framework for Customized Artistic Poster Generation Authors: Haoyu Chen, Xiaojie Xu, Wenbo Li, Jingjing Ren, Tian Ye, Songhua Liu, Ying-Cong Chen, Lei Zhu, Xinchao Wang Abstract: Poster design is a critical medium for visual communication. Prior work has explored automatic poster design using deep learning techniques, but these approaches lack text accuracy, user customization, and aesthetic appeal, limiting their applicability in artistic domains such as movies and exhibitions, where both clear content delivery and visual impact are essential. To address these limitations\u2026 \u25bd More Poster design is a critical medium for visual communication. Prior work has explored automatic poster design using deep learning techniques, but these approaches lack text accuracy, user customization, and aesthetic appeal, limiting their applicability in artistic domains such as movies and exhibitions, where both clear content delivery and visual impact are essential. To address these limitations, we present POSTA: a modular framework powered by diffusion models and multimodal large language models (MLLMs) for customized artistic poster generation. The framework consists of three modules. Background Diffusion creates a themed background based on user input. Design MLLM then generates layout and typography elements that align with and complement the background style. Finally, to enhance the poster's aesthetic appeal, ArtText Diffusion applies additional stylization to key text elements. The final result is a visually cohesive and appealing poster, with a fully modular process that allows for complete customization. To train our models, we develop the PosterArt dataset, comprising high-quality artistic posters annotated with layout, typography, and pixel-level stylized text segmentation. Our comprehensive experimental analysis demonstrates POSTA's exceptional controllability and design diversity, outperforming existing models in both text accuracy and aesthetic quality. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. Comments: Accepted to CVPR 2025 arXiv:2503.14887 [pdf, other] Pseudo-Relevance Feedback Can Improve Zero-Shot LLM-Based Dense Retrieval Authors: Hang Li, Xiao Wang, Bevan Koopman, Guido Zuccon Abstract: Pseudo-relevance feedback (PRF) refines queries by leveraging initially retrieved documents to improve retrieval effectiveness. In this paper, we investigate how large language models (LLMs) can facilitate PRF for zero-shot LLM-based dense retrieval, extending the recently proposed PromptReps method. Specifically, our approach uses LLMs to extract salient passage features-such as keywords and summ\u2026 \u25bd More Pseudo-relevance feedback (PRF) refines queries by leveraging initially retrieved documents to improve retrieval effectiveness. In this paper, we investigate how large language models (LLMs) can facilitate PRF for zero-shot LLM-based dense retrieval, extending the recently proposed PromptReps method. Specifically, our approach uses LLMs to extract salient passage features-such as keywords and summaries-from top-ranked documents, which are then integrated into PromptReps to produce enhanced query representations. Experiments on passage retrieval benchmarks demonstrate that incorporating PRF significantly boosts retrieval performance. Notably, smaller rankers with PRF can match the effectiveness of larger rankers without PRF, highlighting PRF's potential to improve LLM-driven search while maintaining an efficient balance between effectiveness and resource usage. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. arXiv:2503.14512 [pdf] Machine learning algorithms to predict stroke in China based on causal inference of time series analysis Authors: Qizhi Zheng, Ayang Zhao, Xinzhu Wang, Yanhong Bai, Zikun Wang, Xiuying Wang, Xianzhang Zeng, Guanghui Dong Abstract: Participants: This study employed a combination of Vector Autoregression (VAR) model and Graph Neural Networks (GNN) to systematically construct dynamic causal inference. Multiple classic classification algorithms were compared, including Random Forest, Logistic Regression, XGBoost, Support Vector Machine (SVM), K-Nearest Neighbor (KNN), Gradient Boosting, and Multi Layer Perceptron (MLP). The SMO\u2026 \u25bd More Participants: This study employed a combination of Vector Autoregression (VAR) model and Graph Neural Networks (GNN) to systematically construct dynamic causal inference. Multiple classic classification algorithms were compared, including Random Forest, Logistic Regression, XGBoost, Support Vector Machine (SVM), K-Nearest Neighbor (KNN), Gradient Boosting, and Multi Layer Perceptron (MLP). The SMOTE algorithm was used to undersample a small number of samples and employed Stratified K-fold Cross Validation. Results: This study included a total of 11,789 participants, including 6,334 females (53.73%) and 5,455 males (46.27%), with an average age of 65 years. Introduction of dynamic causal inference features has significantly improved the performance of almost all models. The area under the ROC curve of each model ranged from 0.78 to 0.83, indicating significant difference (P < 0.01). Among all the models, the Gradient Boosting model demonstrated the highest performance and stability. Model explanation and feature importance analysis generated model interpretation that illustrated significant contributors associated with risks of stroke. Conclusions and Relevance: This study proposes a stroke risk prediction method that combines dynamic causal inference with machine learning models, significantly improving prediction accuracy and revealing key health factors that affect stroke. The research results indicate that dynamic causal inference features have important value in predicting stroke risk, especially in capturing the impact of changes in health status over time on stroke risk. By further optimizing the model and introducing more variables, this study provides theoretical basis and practical guidance for future stroke prevention and intervention strategies. \u25b3 Less Submitted 10 March, 2025; originally announced March 2025. Comments: 17 pages arXiv:2503.14487 [pdf, other] DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers Authors: Minglei Shi, Ziyang Yuan, Haotian Yang, Xintao Wang, Mingwu Zheng, Xin Tao, Wenliang Zhao, Wenzhao Zheng, Jie Zhou, Jiwen Lu, Pengfei Wan, Di Zhang, Kun Gai Abstract: Diffusion models have demonstrated remarkable success in various image generation tasks, but their performance is often limited by the uniform processing of inputs across varying conditions and noise levels. To address this limitation, we propose a novel approach that leverages the inherent heterogeneity of the diffusion process. Our method, DiffMoE, introduces a batch-level global token pool that\u2026 \u25bd More Diffusion models have demonstrated remarkable success in various image generation tasks, but their performance is often limited by the uniform processing of inputs across varying conditions and noise levels. To address this limitation, we propose a novel approach that leverages the inherent heterogeneity of the diffusion process. Our method, DiffMoE, introduces a batch-level global token pool that enables experts to access global token distributions during training, promoting specialized expert behavior. To unleash the full potential of the diffusion process, DiffMoE incorporates a capacity predictor that dynamically allocates computational resources based on noise levels and sample complexity. Through comprehensive evaluation, DiffMoE achieves state-of-the-art performance among diffusion models on ImageNet benchmark, substantially outperforming both dense architectures with 3x activated parameters and existing MoE approaches while maintaining 1x activated parameters. The effectiveness of our approach extends beyond class-conditional generation to more challenging tasks such as text-to-image generation, demonstrating its broad applicability across different diffusion model applications. Project Page: https://shiml20.github.io/DiffMoE/ \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. Comments: Project Page: https://shiml20.github.io/DiffMoE/ arXiv:2503.14247 [pdf] GeoFlow-SLAM: A Robust Tightly-Coupled RGBD-Inertial Fusion SLAM for Dynamic Legged Robotics Authors: Tingyang Xiao, Xiaolin Zhou, Liu Liu, Wei Sui, Wei Feng, Jiaxiong Qiu, Xinjie Wang, Zhizhong Su Abstract: This paper presents GeoFlow-SLAM, a robust and effective Tightly-Coupled RGBD-inertial SLAM for legged robots operating in highly dynamic environments.By integrating geometric consistency, legged odometry constraints, and dual-stream optical flow (GeoFlow), our method addresses three critical challenges:feature matching and pose initialization failures during fast locomotion and visual feature sca\u2026 \u25bd More This paper presents GeoFlow-SLAM, a robust and effective Tightly-Coupled RGBD-inertial SLAM for legged robots operating in highly dynamic environments.By integrating geometric consistency, legged odometry constraints, and dual-stream optical flow (GeoFlow), our method addresses three critical challenges:feature matching and pose initialization failures during fast locomotion and visual feature scarcity in texture-less scenes.Specifically, in rapid motion scenarios, feature matching is notably enhanced by leveraging dual-stream optical flow, which combines prior map points and poses. Additionally, we propose a robust pose initialization method for fast locomotion and IMU error in legged robots, integrating IMU/Legged odometry, inter-frame Perspective-n-Point (PnP), and Generalized Iterative Closest Point (GICP). Furthermore, a novel optimization framework that tightly couples depth-to-map and GICP geometric constraints is first introduced to improve the robustness and accuracy in long-duration, visually texture-less environments. The proposed algorithms achieve state-of-the-art (SOTA) on collected legged robots and open-source datasets. To further promote research and development, the open-source datasets and code will be made publicly available at https://github.com/NSN-Hello/GeoFlow-SLAM \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. Comments: 8 pages arXiv:2503.14110 [pdf, other] A Comprehensive Survey on Cross-Domain Recommendation: Taxonomy, Progress, and Prospects Authors: Hao Zhang, Mingyue Cheng, Qi Liu, Junzhe Jiang, Xianquan Wang, Rujiao Zhang, Chenyi Lei, Enhong Chen Abstract: Recommender systems (RS) have become crucial tools for information filtering in various real world scenarios. And cross domain recommendation (CDR) has been widely explored in recent years in order to provide better recommendation results in the target domain with the help of other domains. The CDR technology has developed rapidly, yet there is a lack of a comprehensive survey summarizing recent w\u2026 \u25bd More Recommender systems (RS) have become crucial tools for information filtering in various real world scenarios. And cross domain recommendation (CDR) has been widely explored in recent years in order to provide better recommendation results in the target domain with the help of other domains. The CDR technology has developed rapidly, yet there is a lack of a comprehensive survey summarizing recent works. Therefore, in this paper, we will summarize the progress and prospects based on the main procedure of CDR, including Cross Domain Relevance, Cross Domain Interaction, Cross Domain Representation Enhancement and Model Optimization. To help researchers better understand and engage in this field, we also organize the applications and resources, and highlight several current important challenges and future directions of CDR. More details of the survey articles are available at https://github.com/USTCAGI/Awesome-Cross-Domain Recommendation-Papers-and-Resources. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. arXiv:2503.13903 [pdf, other] TGBFormer: Transformer-GraphFormer Blender Network for Video Object Detection Authors: Qiang Qi, Xiao Wang Abstract: Video object detection has made significant progress in recent years thanks to convolutional neural networks (CNNs) and vision transformers (ViTs). Typically, CNNs excel at capturing local features but struggle to model global representations. Conversely, ViTs are adept at capturing long-range global features but face challenges in representing local feature details. Off-the-shelf video object det\u2026 \u25bd More Video object detection has made significant progress in recent years thanks to convolutional neural networks (CNNs) and vision transformers (ViTs). Typically, CNNs excel at capturing local features but struggle to model global representations. Conversely, ViTs are adept at capturing long-range global features but face challenges in representing local feature details. Off-the-shelf video object detection methods solely rely on CNNs or ViTs to conduct feature aggregation, which hampers their capability to simultaneously leverage global and local information, thereby resulting in limited detection performance. In this paper, we propose a Transformer-GraphFormer Blender Network (TGBFormer) for video object detection, with three key technical improvements to fully exploit the advantages of transformers and graph convolutional networks while compensating for their limitations. First, we develop a spatial-temporal transformer module to aggregate global contextual information, constituting global representations with long-range feature dependencies. Second, we introduce a spatial-temporal GraphFormer module that utilizes local spatial and temporal relationships to aggregate features, generating new local representations that are complementary to the transformer outputs. Third, we design a global-local feature blender module to adaptively couple transformer-based global representations and GraphFormer-based local representations. Extensive experiments demonstrate that our TGBFormer establishes new state-of-the-art results on the ImageNet VID dataset. Particularly, our TGBFormer achieves 86.5% mAP while running at around 41.0 FPS on a single Tesla A100 GPU. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. Comments: Accepted by AAAI2025 arXiv:2503.13441 [pdf, other] Humanoid Policy ~ Human Policy Authors: Ri-Zhao Qiu, Shiqi Yang, Xuxin Cheng, Chaitanya Chawla, Jialong Li, Tairan He, Ge Yan, Lars Paulsen, Ge Yang, Sha Yi, Guanya Shi, Xiaolong Wang Abstract: Training manipulation policies for humanoid robots with diverse data enhances their robustness and generalization across tasks and platforms. However, learning solely from robot demonstrations is labor-intensive, requiring expensive tele-operated data collection which is difficult to scale. This paper investigates a more scalable data source, egocentric human demonstrations, to serve as cross-embo\u2026 \u25bd More Training manipulation policies for humanoid robots with diverse data enhances their robustness and generalization across tasks and platforms. However, learning solely from robot demonstrations is labor-intensive, requiring expensive tele-operated data collection which is difficult to scale. This paper investigates a more scalable data source, egocentric human demonstrations, to serve as cross-embodiment training data for robot learning. We mitigate the embodiment gap between humanoids and humans from both the data and modeling perspectives. We collect an egocentric task-oriented dataset (PH2D) that is directly aligned with humanoid manipulation demonstrations. We then train a human-humanoid behavior policy, which we term Human Action Transformer (HAT). The state-action space of HAT is unified for both humans and humanoid robots and can be differentiably retargeted to robot actions. Co-trained with smaller-scale robot data, HAT directly models humanoid robots and humans as different embodiments without additional supervision. We show that human data improves both generalization and robustness of HAT with significantly better data collection efficiency. Code and data: https://human-as-robot.github.io/ \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. Comments: Code and data: https://human-as-robot.github.io/ arXiv:2503.13440 [pdf, other] MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling Authors: Yingyue Li, Bencheng Liao, Wenyu Liu, Xinggang Wang Abstract: With the advancement of RNN models with linear complexity, the quadratic complexity challenge of transformers has the potential to be overcome. Notably, the emerging Mamba-2 has demonstrated competitive performance, bridging the gap between RNN models and transformers. However, due to sequential processing and vanishing gradients, RNN models struggle to capture long-range dependencies, limiting co\u2026 \u25bd More With the advancement of RNN models with linear complexity, the quadratic complexity challenge of transformers has the potential to be overcome. Notably, the emerging Mamba-2 has demonstrated competitive performance, bridging the gap between RNN models and transformers. However, due to sequential processing and vanishing gradients, RNN models struggle to capture long-range dependencies, limiting contextual understanding. This results in slow convergence, high resource demands, and poor performance on downstream understanding and complex reasoning tasks. In this work, we present a hybrid model MaTVLM by substituting a portion of the transformer decoder layers in a pre-trained VLM with Mamba-2 layers. Leveraging the inherent relationship between attention and Mamba-2, we initialize Mamba-2 with corresponding attention weights to accelerate convergence. Subsequently, we employ a single-stage distillation process, using the pre-trained VLM as the teacher model to transfer knowledge to the MaTVLM, further enhancing convergence speed and performance. Furthermore, we investigate the impact of differential distillation loss within our training framework. We evaluate the MaTVLM on multiple benchmarks, demonstrating competitive performance against the teacher model and existing VLMs while surpassing both Mamba-based VLMs and models of comparable parameter scales. Remarkably, the MaTVLM achieves up to 3.6x faster inference than the teacher model while reducing GPU memory consumption by 27.5%, all without compromising performance. Code and models are released at http://github.com/hustvl/MaTVLM. \u25b3 Less Submitted 18 March, 2025; v1 submitted 17 March, 2025; originally announced March 2025. Comments: Code and model are available at http://github.com/hustvl/MaTVLM arXiv:2503.13356 [pdf, other] Agents Play Thousands of 3D Video Games Authors: Zhongwen Xu, Xianliang Wang, Siyi Li, Tao Yu, Liang Wang, Qiang Fu, Wei Yang Abstract: We present PORTAL, a novel framework for developing artificial intelligence agents capable of playing thousands of 3D video games through language-guided policy generation. By transforming decision-making problems into language modeling tasks, our approach leverages large language models (LLMs) to generate behavior trees represented in domain-specific language (DSL). This method eliminates the com\u2026 \u25bd More We present PORTAL, a novel framework for developing artificial intelligence agents capable of playing thousands of 3D video games through language-guided policy generation. By transforming decision-making problems into language modeling tasks, our approach leverages large language models (LLMs) to generate behavior trees represented in domain-specific language (DSL). This method eliminates the computational burden associated with traditional reinforcement learning approaches while preserving strategic depth and rapid adaptability. Our framework introduces a hybrid policy structure that combines rule-based nodes with neural network components, enabling both high-level strategic reasoning and precise low-level control. A dual-feedback mechanism incorporating quantitative game metrics and vision-language model analysis facilitates iterative policy improvement at both tactical and strategic levels. The resulting policies are instantaneously deployable, human-interpretable, and capable of generalizing across diverse gaming environments. Experimental results demonstrate PORTAL's effectiveness across thousands of first-person shooter (FPS) games, showcasing significant improvements in development efficiency, policy generalization, and behavior diversity compared to traditional approaches. PORTAL represents a significant advancement in game AI development, offering a practical solution for creating sophisticated agents that can operate across thousands of commercial video games with minimal development overhead. Experiment results on the 3D video games are best viewed on https://zhongwen.one/projects/portal . \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. arXiv:2503.13340 [pdf, other] LearnMate: Enhancing Online Education with LLM-Powered Personalized Learning Plans and Support Authors: Xinyu Jessica Wang, Christine Lee, Bilge Mutlu Abstract: With the increasing prevalence of online learning, adapting education to diverse learner needs remains a persistent challenge. Recent advancements in artificial intelligence (AI), particularly large language models (LLMs), promise powerful tools and capabilities to enhance personalized learning in online educational environments. In this work, we explore how LLMs can improve personalized learning\u2026 \u25bd More With the increasing prevalence of online learning, adapting education to diverse learner needs remains a persistent challenge. Recent advancements in artificial intelligence (AI), particularly large language models (LLMs), promise powerful tools and capabilities to enhance personalized learning in online educational environments. In this work, we explore how LLMs can improve personalized learning experiences by catering to individual user needs toward enhancing the overall quality of online education. We designed personalization guidelines based on the growing literature on personalized learning to ground LLMs in generating tailored learning plans. To operationalize these guidelines, we implemented LearnMate, an LLM-based system that generates personalized learning plans and provides users with real-time learning support. We discuss the implications and future directions of this work, aiming to move beyond the traditional one-size-fits-all approach by integrating LLM-based personalized support into online learning environments. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. Comments: In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA '25), April 26-May 1, 2025, Yokohama, Japan arXiv:2503.13160 [pdf, other] Language-guided Open-world Video Anomaly Detection Authors: Zihao Liu, Xiaoyu Wu, Jianqin Wu, Xuxu Wang, Linlin Yang Abstract: Video anomaly detection models aim to detect anomalies that deviate from what is expected. In open-world scenarios, the expected events may change as requirements change. For example, not wearing a mask is considered abnormal during a flu outbreak but normal otherwise. However, existing methods assume that the definition of anomalies is invariable, and thus are not applicable to the open world. To\u2026 \u25bd More Video anomaly detection models aim to detect anomalies that deviate from what is expected. In open-world scenarios, the expected events may change as requirements change. For example, not wearing a mask is considered abnormal during a flu outbreak but normal otherwise. However, existing methods assume that the definition of anomalies is invariable, and thus are not applicable to the open world. To address this, we propose a novel open-world VAD paradigm with variable definitions, allowing guided detection through user-provided natural language at inference time. This paradigm necessitates establishing a robust mapping from video and textual definition to anomaly score. Therefore, we propose LaGoVAD (Language-guided Open-world VAD), a model that dynamically adapts anomaly definitions through two regularization strategies: diversifying the relative durations of anomalies via dynamic video synthesis, and enhancing feature robustness through contrastive learning with negative mining. Training such adaptable models requires diverse anomaly definitions, but existing datasets typically provide given labels without semantic descriptions. To bridge this gap, we collect PreVAD (Pre-training Video Anomaly Dataset), the largest and most diverse video anomaly dataset to date, featuring 35,279 annotated videos with multi-level category labels and descriptions that explicitly define anomalies. Zero-shot experiments on seven datasets demonstrate SOTA performance. Data and code will be released. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. arXiv:2503.13086 [pdf, other] Gaussian On-the-Fly Splatting: A Progressive Framework for Robust Near Real-Time 3DGS Optimization Authors: Yiwei Xu, Yifei Yu, Wentian Gan, Tengfei Wang, Zongqian Zhan, Hao Cheng, Xin Wang Abstract: 3D Gaussian Splatting (3DGS) achieves high-fidelity rendering with fast real-time performance, but existing methods rely on offline training after full Structure-from-Motion (SfM) processing. In contrast, this work introduces On-the-Fly GS, a progressive framework enabling near real-time 3DGS optimization during image capture. As each image arrives, its pose and sparse points are updated via on-th\u2026 \u25bd More 3D Gaussian Splatting (3DGS) achieves high-fidelity rendering with fast real-time performance, but existing methods rely on offline training after full Structure-from-Motion (SfM) processing. In contrast, this work introduces On-the-Fly GS, a progressive framework enabling near real-time 3DGS optimization during image capture. As each image arrives, its pose and sparse points are updated via on-the-fly SfM, and newly optimized Gaussians are immediately integrated into the 3DGS field. We propose a progressive local optimization strategy to prioritize new images and their neighbors by their corresponding overlapping relationship, allowing the new image and its overlapping images to get more training. To further stabilize training across old and new images, an adaptive learning rate schedule balances the iterations and the learning rate. Moreover, to maintain overall quality of the 3DGS field, an efficient global optimization scheme prevents overfitting to the newly added images. Experiments on multiple benchmark datasets show that our On-the-Fly GS reduces training time significantly, optimizing each new image in seconds with minimal rendering loss, offering the first practical step toward rapid, progressive 3DGS reconstruction. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. arXiv:2503.13073 [pdf, other] DehazeMamba: SAR-guided Optical Remote Sensing Image Dehazing with Adaptive State Space Model Authors: Zhicheng Zhao, Jinquan Yan, Chenglong Li, Xiao Wang, Jin Tang Abstract: Optical remote sensing image dehazing presents significant challenges due to its extensive spatial scale and highly non-uniform haze distribution, which traditional single-image dehazing methods struggle to address effectively. While Synthetic Aperture Radar (SAR) imagery offers inherently haze-free reference information for large-scale scenes, existing SAR-guided dehazing approaches face two crit\u2026 \u25bd More Optical remote sensing image dehazing presents significant challenges due to its extensive spatial scale and highly non-uniform haze distribution, which traditional single-image dehazing methods struggle to address effectively. While Synthetic Aperture Radar (SAR) imagery offers inherently haze-free reference information for large-scale scenes, existing SAR-guided dehazing approaches face two critical limitations: the integration of SAR information often diminishes the quality of haze-free regions, and the instability of feature quality further exacerbates cross-modal domain shift. To overcome these challenges, we introduce DehazeMamba, a novel SAR-guided dehazing network built on a progressive haze decoupling fusion strategy. Our approach incorporates two key innovations: a Haze Perception and Decoupling Module (HPDM) that dynamically identifies haze-affected regions through optical-SAR difference analysis, and a Progressive Fusion Module (PFM) that mitigates domain shift through a two-stage fusion process based on feature quality assessment. To facilitate research in this domain, we present MRSHaze, a large-scale benchmark dataset comprising 8,000 pairs of temporally synchronized, precisely geo-registered SAR-optical images with high resolution and diverse haze conditions. Extensive experiments demonstrate that DehazeMamba significantly outperforms state-of-the-art methods, achieving a 0.73 dB improvement in PSNR and substantial enhancements in downstream tasks such as semantic segmentation. The dataset is available at https://github.com/mmic-lcl/Datasets-and-benchmark-code. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. arXiv:2503.13048 [pdf, other] Robot Skin with Touch and Bend Sensing using Electrical Impedance Tomography Authors: Haofeng Chen, Bin Li, Bedrich Himmel, Xiaojie Wang, Matej Hoffmann Abstract: Flexible electronic skins that simultaneously sense touch and bend are desired in several application areas, such as to cover articulated robot structures. This paper introduces a flexible tactile sensor based on Electrical Impedance Tomography (EIT), capable of simultaneously detecting and measuring contact forces and flexion of the sensor. The sensor integrates a magnetic hydrogel composite and\u2026 \u25bd More Flexible electronic skins that simultaneously sense touch and bend are desired in several application areas, such as to cover articulated robot structures. This paper introduces a flexible tactile sensor based on Electrical Impedance Tomography (EIT), capable of simultaneously detecting and measuring contact forces and flexion of the sensor. The sensor integrates a magnetic hydrogel composite and utilizes EIT to reconstruct internal conductivity distributions. Real-time estimation is achieved through the one-step Gauss-Newton method, which dynamically updates reference voltages to accommodate sensor deformation. A convolutional neural network is employed to classify interactions, distinguishing between touch, bending, and idle states using pre-reconstructed images. Experimental results demonstrate an average touch localization error of 5.4 mm (SD 2.2 mm) and average bending angle estimation errors of 1.9$^\\circ$ (SD 1.6$^\\circ$). The proposed adaptive reference method effectively distinguishes between single- and multi-touch scenarios while compensating for deformation effects. This makes the sensor a promising solution for multimodal sensing in robotics and human-robot collaboration. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. arXiv:2503.12619 [pdf, other] Rubikon: Intelligent Tutoring for Rubik's Cube Learning Through AR-enabled Physical Task Reconfiguration Authors: Muzhe Wu, Haocheng Ren, Gregory Croisdale, Anhong Guo, Xu Wang Abstract: Learning to solve a Rubik's Cube requires the learners to repeatedly practice a skill component, e.g., identifying a misplaced square and putting it back. However, for 3D physical tasks such as this, generating sufficient repeated practice opportunities for learners can be challenging, in part because it is difficult for novices to reconfigure the physical object to specific states. We propose Rub\u2026 \u25bd More Learning to solve a Rubik's Cube requires the learners to repeatedly practice a skill component, e.g., identifying a misplaced square and putting it back. However, for 3D physical tasks such as this, generating sufficient repeated practice opportunities for learners can be challenging, in part because it is difficult for novices to reconfigure the physical object to specific states. We propose Rubikon, an intelligent tutoring system for learning to solve the Rubik's Cube. Rubikon reduces the necessity for repeated manual configurations of the Rubik's Cube without compromising the tactile experience of handling a physical cube. The foundational design of Rubikon is an AR setup, where learners manipulate a physical cube while seeing an AR-rendered cube on a display. Rubikon automatically generates configurations of the Rubik's Cube to target learners' weaknesses and help them exercise diverse knowledge components. In a between-subjects experiment, we showed that Rubikon learners scored 25% higher on a post-test compared to baselines. \u25b3 Less Submitted 16 March, 2025; originally announced March 2025. arXiv:2503.12576 [pdf, other] RaSA: Rank-Sharing Low-Rank Adaptation Authors: Zhiwei He, Zhaopeng Tu, Xing Wang, Xingyu Chen, Zhijie Wang, Jiahao Xu, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Rui Wang Abstract: Low-rank adaptation (LoRA) has been prominently employed for parameter-efficient fine-tuning of large language models (LLMs). However, the limited expressive capacity of LoRA, stemming from the low-rank constraint, has been recognized as a bottleneck, particularly in rigorous tasks like code generation and mathematical reasoning. To address this limitation, we introduce Rank-Sharing Low-Rank Adapt\u2026 \u25bd More Low-rank adaptation (LoRA) has been prominently employed for parameter-efficient fine-tuning of large language models (LLMs). However, the limited expressive capacity of LoRA, stemming from the low-rank constraint, has been recognized as a bottleneck, particularly in rigorous tasks like code generation and mathematical reasoning. To address this limitation, we introduce Rank-Sharing Low-Rank Adaptation (RaSA), an innovative extension that enhances the expressive capacity of LoRA by leveraging partial rank sharing across layers. By forming a shared rank pool and applying layer-specific weighting, RaSA effectively increases the number of ranks without augmenting parameter overhead. Our theoretically grounded and empirically validated approach demonstrates that RaSA not only maintains the core advantages of LoRA but also significantly boosts performance in challenging code and math tasks. Code, data and scripts are available at: https://github.com/zwhe99/RaSA. \u25b3 Less Submitted 16 March, 2025; originally announced March 2025. Comments: ICLR 2025 arXiv:2503.12559 [pdf, other] AdaReTaKe: Adaptive Redundancy Reduction to Perceive Longer for Video-language Understanding Authors: Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, Liqiang Nie Abstract: Multimodal Large Language Models (MLLMs) have revolutionized video understanding, yet are still limited by context length when processing long videos. Recent methods compress videos by leveraging visual redundancy uniformly, yielding promising results. Nevertheless, our quantitative analysis shows that redundancy varies significantly across time and model layers, necessitating a more flexible comp\u2026 \u25bd More Multimodal Large Language Models (MLLMs) have revolutionized video understanding, yet are still limited by context length when processing long videos. Recent methods compress videos by leveraging visual redundancy uniformly, yielding promising results. Nevertheless, our quantitative analysis shows that redundancy varies significantly across time and model layers, necessitating a more flexible compression strategy. We propose AdaReTaKe, a training-free method that flexibly reduces visual redundancy by allocating compression ratios among time and layers with theoretical guarantees. Integrated into state-of-the-art MLLMs, AdaReTaKe improves processing capacity from 256 to 2048 frames while preserving critical information. Experiments on VideoMME, MLVU, LongVideoBench, and LVBench datasets demonstrate that AdaReTaKe outperforms existing methods by 2.3% and 2.8% for 7B and 72B models, respectively, with even greater improvements of 5.9% and 6.0% on the longest LVBench. Our code is available at https://github.com/SCZwangxiao/video-FlexReduc.git. \u25b3 Less Submitted 16 March, 2025; originally announced March 2025. arXiv:2503.12527 [pdf, other] A Plug-and-Play Learning-based IMU Bias Factor for Robust Visual-Inertial Odometry Authors: Yang Yi, Kunqing Wang, Jinpu Zhang, Zhen Tan, Xiangke Wang, Hui Shen, Dewen Hu Abstract: The bias of low-cost Inertial Measurement Units (IMU) is a critical factor affecting the performance of Visual-Inertial Odometry (VIO). In particular, when visual tracking encounters errors, the optimized bias results may deviate significantly from the true values, adversely impacting the system's stability and localization precision. In this paper, we propose a novel plug-and-play framework featu\u2026 \u25bd More The bias of low-cost Inertial Measurement Units (IMU) is a critical factor affecting the performance of Visual-Inertial Odometry (VIO). In particular, when visual tracking encounters errors, the optimized bias results may deviate significantly from the true values, adversely impacting the system's stability and localization precision. In this paper, we propose a novel plug-and-play framework featuring the Inertial Prior Network (IPNet), which is designed to accurately estimate IMU bias. Recognizing the substantial impact of initial bias errors in low-cost inertial devices on system performance, our network directly leverages raw IMU data to estimate the mean bias, eliminating the dependency on historical estimates in traditional recursive predictions and effectively preventing error propagation. Furthermore, we introduce an iterative approach to calculate the mean value of the bias for network training, addressing the lack of bias labels in many visual-inertial datasets. The framework is evaluated on two public datasets and one self-collected dataset. Extensive experiments demonstrate that our method significantly enhances both localization precision and robustness, with the ATE-RMSE metric improving on average by 46\\%. The source code and video will be available at \\textcolor{red}{https://github.com/yiyscut/VIO-IPNet.git}. \u25b3 Less Submitted 16 March, 2025; originally announced March 2025. arXiv:2503.12340 [pdf, other] SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression Authors: Xin Wang, Samiul Alam, Zhongwei Wan, Hui Shen, Mi Zhang Abstract: Despite significant advancements, the practical deployment of Large Language Models (LLMs) is often hampered by their immense sizes, highlighting the need for effective compression techniques. Singular Value Decomposition (SVD) is a promising LLM compression technique. However, existing SVD-based compression methods fall short in reducing truncation losses, leading to less competitive performance\u2026 \u25bd More Despite significant advancements, the practical deployment of Large Language Models (LLMs) is often hampered by their immense sizes, highlighting the need for effective compression techniques. Singular Value Decomposition (SVD) is a promising LLM compression technique. However, existing SVD-based compression methods fall short in reducing truncation losses, leading to less competitive performance in compressed models. In this work, we introduce SVD-LLM V2, a SVD-based LLM compression method that optimizes singular value truncation in SVD compression with two techniques. First, SVD-LLM V2 proposes to use theoretical truncation loss of weight matrices to assign a unique compression ratio to each weight matrix at different layers to accommodate weight redundancy heterogeneity. Second, SVD-LLM V2 proposes loss-optimized weight truncation to ensure that the truncated singular values result in a lower and more stable truncation loss in practice. We evaluate SVD-LLM V2 on ten datasets and five LLMs at various scales. Our results show SVD-LLM V2 outperforms state-of-the-art SVD-based LLM compression methods. Our code is available at https://github.com/AIoT-MLSys-Lab/SVD-LLM \u25b3 Less Submitted 15 March, 2025; originally announced March 2025. Comments: NAACL 2025; Code available at https://github.com/AIoT-MLSys-Lab/SVD-LLM arXiv:2503.12335 [pdf, other] GS-I$^{3}$: Gaussian Splatting for Surface Reconstruction from Illumination-Inconsistent Images Authors: Tengfei Wang, Yongmao Hou, Zhaoning Zhang, Yiwei Xu, Zongqian Zhan, Xin Wang Abstract: Accurate geometric surface reconstruction, providing essential environmental information for navigation and manipulation tasks, is critical for enabling robotic self-exploration and interaction. Recently, 3D Gaussian Splatting (3DGS) has gained significant attention in the field of surface reconstruction due to its impressive geometric quality and computational efficiency. While recent relevant ad\u2026 \u25bd More Accurate geometric surface reconstruction, providing essential environmental information for navigation and manipulation tasks, is critical for enabling robotic self-exploration and interaction. Recently, 3D Gaussian Splatting (3DGS) has gained significant attention in the field of surface reconstruction due to its impressive geometric quality and computational efficiency. While recent relevant advancements in novel view synthesis under inconsistent illumination using 3DGS have shown promise, the challenge of robust surface reconstruction under such conditions is still being explored. To address this challenge, we propose a method called GS-3I. Specifically, to mitigate 3D Gaussian optimization bias caused by underexposed regions in single-view images, based on Convolutional Neural Network (CNN), a tone mapping correction framework is introduced. Furthermore, inconsistent lighting across multi-view images, resulting from variations in camera settings and complex scene illumination, often leads to geometric constraint mismatches and deviations in the reconstructed surface. To overcome this, we propose a normal compensation mechanism that integrates reference normals extracted from single-view image with normals computed from multi-view observations to effectively constrain geometric inconsistencies. Extensive experimental evaluations demonstrate that GS-3I can achieve robust and accurate surface reconstruction across complex illumination scenarios, highlighting its effectiveness and versatility in this critical challenge. https://github.com/TFwang-9527/GS-3I \u25b3 Less Submitted 18 March, 2025; v1 submitted 15 March, 2025; originally announced March 2025. Comments: Comments: This work has been submitted to the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025) for possible publication arXiv:2503.12051 [pdf, other] TLUE: A Tibetan Language Understanding Evaluation Benchmark Authors: Fan Gao, Cheng Huang, Nyima Tashi, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Xiao Feng, Yongbin Yu Abstract: Large language models (LLMs) have made tremendous progress in recent years, but low-resource languages, such as Tibetan, remain significantly underrepresented in their evaluation. Despite Tibetan being spoken by over seven million people, it has largely been neglected in the development and assessment of LLMs. To address this gap, we present TLUE (A Tibetan Language Understanding Evaluation Benchm\u2026 \u25bd More Large language models (LLMs) have made tremendous progress in recent years, but low-resource languages, such as Tibetan, remain significantly underrepresented in their evaluation. Despite Tibetan being spoken by over seven million people, it has largely been neglected in the development and assessment of LLMs. To address this gap, we present TLUE (A Tibetan Language Understanding Evaluation Benchmark), the first large-scale benchmark for assessing LLMs' capabilities in Tibetan. TLUE comprises two major components: (1) a comprehensive multi-task understanding benchmark spanning 5 domains and 67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate a diverse set of state-of-the-art LLMs. Experimental results demonstrate that most LLMs perform below the random baseline, highlighting the considerable challenges LLMs face in processing Tibetan, a low-resource language. TLUE provides an essential foundation for driving future research and progress in Tibetan language understanding and underscores the need for greater inclusivity in LLM development. \u25b3 Less Submitted 15 March, 2025; originally announced March 2025. Comments: 6 figures, 21 pages arXiv:2503.11965 [pdf, other] Revisiting Gradient Descent: A Dual-Weight Method for Improved Learning Authors: Xi Wang Abstract: We introduce a novel framework for learning in neural networks by decomposing each neuron's weight vector into two distinct parts, $W_1$ and $W_2$, thereby modeling contrastive information directly at the neuron level. Traditional gradient descent stores both positive (target) and negative (non-target) feature information in a single weight vector, often obscuring fine-grained distinctions. Our ap\u2026 \u25bd More We introduce a novel framework for learning in neural networks by decomposing each neuron's weight vector into two distinct parts, $W_1$ and $W_2$, thereby modeling contrastive information directly at the neuron level. Traditional gradient descent stores both positive (target) and negative (non-target) feature information in a single weight vector, often obscuring fine-grained distinctions. Our approach, by contrast, maintains separate updates for target and non-target features, ultimately forming a single effective weight $W = W_1 - W_2$ that is more robust to noise and class imbalance. Experimental results on both regression (California Housing, Wine Quality) and classification (MNIST, Fashion-MNIST, CIFAR-10) tasks suggest that this decomposition enhances generalization and resists overfitting, especially when training data are sparse or noisy. Crucially, the inference complexity remains the same as in the standard $WX + \\text{bias}$ setup, offering a practical solution for improved learning without additional inference-time overhead. \u25b3 Less Submitted 17 March, 2025; v1 submitted 14 March, 2025; originally announced March 2025. arXiv:2503.11647 [pdf, other] ReCamMaster: Camera-Controlled Generative Rendering from A Single Video Authors: Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, Di Zhang Abstract: Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-frame appearance and dynamic synchronization. To address this, we present ReCamMaster, a camera-contr\u2026 \u25bd More Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-frame appearance and dynamic synchronization. To address this, we present ReCamMaster, a camera-controlled generative video re-rendering framework that reproduces the dynamic scene of an input video at novel camera trajectories. The core innovation lies in harnessing the generative capabilities of pre-trained text-to-video models through a simple yet powerful video conditioning mechanism -- its capability often overlooked in current research. To overcome the scarcity of qualified training data, we construct a comprehensive multi-camera synchronized video dataset using Unreal Engine 5, which is carefully curated to follow real-world filming characteristics, covering diverse scenes and camera movements. It helps the model generalize to in-the-wild videos. Lastly, we further improve the robustness to diverse inputs through a meticulously designed training strategy. Extensive experiments tell that our method substantially outperforms existing state-of-the-art approaches and strong baselines. Our method also finds promising applications in video stabilization, super-resolution, and outpainting. Project page: https://jianhongbai.github.io/ReCamMaster/ \u25b3 Less Submitted 14 March, 2025; originally announced March 2025. Comments: Project page: https://jianhongbai.github.io/ReCamMaster/ arXiv:2503.11617 [pdf, other] ASMA-Tune: Unlocking LLMs' Assembly Code Comprehension via Structural-Semantic Instruction Tuning Authors: Xinyi Wang, Jiashui Wang, Peng Chen, Jinbo Su, Yanming Liu, Long Liu, Yangdong Wang, Qiyuan Chen, Kai Yun, Chunfu Jia Abstract: Analysis and comprehension of assembly code are crucial in various applications, such as reverse engineering. However, the low information density and lack of explicit syntactic structures in assembly code pose significant challenges. Pioneering approaches with masked language modeling (MLM)-based methods have been limited by facilitating natural language interaction. While recent methods based on\u2026 \u25bd More Analysis and comprehension of assembly code are crucial in various applications, such as reverse engineering. However, the low information density and lack of explicit syntactic structures in assembly code pose significant challenges. Pioneering approaches with masked language modeling (MLM)-based methods have been limited by facilitating natural language interaction. While recent methods based on decoder-focused large language models (LLMs) have significantly enhanced semantic representation, they still struggle to capture the nuanced and sparse semantics in assembly code. In this paper, we propose Assembly Augmented Tuning (ASMA-Tune), an end-to-end structural-semantic instruction-tuning framework. Our approach synergizes encoder architectures with decoder-based LLMs through projector modules to enable comprehensive code understanding. Experiments show that ASMA-Tune outperforms existing benchmarks, significantly enhancing assembly code comprehension and instruction-following abilities. Our model and dataset are public at https://github.com/wxy3596/ASMA-Tune. \u25b3 Less Submitted 14 March, 2025; originally announced March 2025. Comments: 19 pages, multiple figures arXiv:2503.11314 [pdf, other] Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large Language Models via Representation Engineering Authors: Xinyu Tang, Xiaolei Wang, Zhihao Lv, Yingqian Min, Wayne Xin Zhao, Binbin Hu, Ziqi Liu, Zhiqiang Zhang Abstract: Recent advancements in long chain-of-thoughts(long CoTs) have significantly improved the reasoning capabilities of large language models(LLMs). Existing work finds that the capability of long CoT reasoning can be efficiently elicited by tuning on only a few examples and can easily transfer to other tasks. This motivates us to investigate whether long CoT reasoning is a general capability for LLMs.\u2026 \u25bd More Recent advancements in long chain-of-thoughts(long CoTs) have significantly improved the reasoning capabilities of large language models(LLMs). Existing work finds that the capability of long CoT reasoning can be efficiently elicited by tuning on only a few examples and can easily transfer to other tasks. This motivates us to investigate whether long CoT reasoning is a general capability for LLMs. In this work, we conduct an empirical analysis for this question from the perspective of representation. We find that LLMs do encode long CoT reasoning as a general capability, with a clear distinction from vanilla CoTs. Furthermore, domain-specific representations are also required for the effective transfer of long CoT reasoning. Inspired by these findings, we propose GLoRE, a novel representation engineering method to unleash the general long CoT reasoning capabilities of LLMs. Extensive experiments demonstrate the effectiveness and efficiency of GLoRE in both in-domain and cross-domain scenarios. \u25b3 Less Submitted 14 March, 2025; originally announced March 2025. arXiv:2503.11240 [pdf, other] Towards Better Alignment: Training Diffusion Models with Reinforcement Learning Against Sparse Rewards Authors: Zijing Hu, Fengda Zhang, Long Chen, Kun Kuang, Jiahui Li, Kaifeng Gao, Jun Xiao, Xin Wang, Wenwu Zhu Abstract: Diffusion models have achieved remarkable success in text-to-image generation. However, their practical applications are hindered by the misalignment between generated images and corresponding text prompts. To tackle this issue, reinforcement learning (RL) has been considered for diffusion model fine-tuning. Yet, RL's effectiveness is limited by the challenge of sparse reward, where feedback is on\u2026 \u25bd More Diffusion models have achieved remarkable success in text-to-image generation. However, their practical applications are hindered by the misalignment between generated images and corresponding text prompts. To tackle this issue, reinforcement learning (RL) has been considered for diffusion model fine-tuning. Yet, RL's effectiveness is limited by the challenge of sparse reward, where feedback is only available at the end of the generation process. This makes it difficult to identify which actions during the denoising process contribute positively to the final generated image, potentially leading to ineffective or unnecessary denoising policies. To this end, this paper presents a novel RL-based framework that addresses the sparse reward problem when training diffusion models. Our framework, named $\\text{B}^2\\text{-DiffuRL}$, employs two strategies: \\textbf{B}ackward progressive training and \\textbf{B}ranch-based sampling. For one thing, backward progressive training focuses initially on the final timesteps of denoising process and gradually extends the training interval to earlier timesteps, easing the learning difficulty from sparse rewards. For another, we perform branch-based sampling for each training interval. By comparing the samples within the same branch, we can identify how much the policies of the current training interval contribute to the final image, which helps to learn effective policies instead of unnecessary ones. $\\text{B}^2\\text{-DiffuRL}$ is compatible with existing optimization algorithms. Extensive experiments demonstrate the effectiveness of $\\text{B}^2\\text{-DiffuRL}$ in improving prompt-image alignment and maintaining diversity in generated images. The code for this work is available. \u25b3 Less Submitted 14 March, 2025; originally announced March 2025. Comments: Accepted to CVPR 2025 arXiv:2503.10737 [pdf, other] Commenting Higher-level Code Unit: Full Code, Reduced Code, or Hierarchical Code Summarization Authors: Weisong Sun, Yiran Zhang, Jie Zhu, Zhihui Wang, Chunrong Fang, Yonglong Zhang, Yebo Feng, Jiangping Huang, Xingya Wang, Zhi Jin, Yang Liu Abstract: Commenting code is a crucial activity in software development, as it aids in facilitating future maintenance and updates. To enhance the efficiency of writing comments and reduce developers' workload, researchers has proposed various automated code summarization (ACS) techniques to automatically generate comments/summaries for given code units. However, these ACS techniques primarily focus on gene\u2026 \u25bd More Commenting code is a crucial activity in software development, as it aids in facilitating future maintenance and updates. To enhance the efficiency of writing comments and reduce developers' workload, researchers has proposed various automated code summarization (ACS) techniques to automatically generate comments/summaries for given code units. However, these ACS techniques primarily focus on generating summaries for code units at the method level. There is a significant lack of research on summarizing higher-level code units, such as file-level and module-level code units, despite the fact that summaries of these higher-level code units are highly useful for quickly gaining a macro-level understanding of software components and architecture. To fill this gap, in this paper, we conduct a systematic study on how to use LLMs for commenting higher-level code units, including file level and module level. These higher-level units are significantly larger than method-level ones, which poses challenges in handling long code inputs within LLM constraints and maintaining efficiency. To address these issues, we explore various summarization strategies for ACS of higher-level code units, which can be divided into three types: full code summarization, reduced code summarization, and hierarchical code summarization. The experimental results suggest that for summarizing file-level code units, using the full code is the most effective approach, with reduced code serving as a cost-efficient alternative. However, for summarizing module-level code units, hierarchical code summarization becomes the most promising strategy. In addition, inspired by the research on method-level ACS, we also investigate using the LLM as an evaluator to evaluate the quality of summaries of higher-level code units. The experimental results demonstrate that the LLM's evaluation results strongly correlate with human evaluations. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. MSC Class: 68-04 ACM Class: D.2.3; I.2.7 arXiv:2503.10723 [pdf, other] RankPO: Preference Optimization for Job-Talent Matching Authors: Yafei Zhang, Murray Wang, Yu Wang, Xiaohui Wang Abstract: Matching job descriptions (JDs) with suitable talent requires models capable of understanding not only textual similarities between JDs and candidate resumes but also contextual factors such as geographical location and academic seniority. To address this challenge, we propose a two-stage training framework for large language models (LLMs). In the first stage, a contrastive learning approach is us\u2026 \u25bd More Matching job descriptions (JDs) with suitable talent requires models capable of understanding not only textual similarities between JDs and candidate resumes but also contextual factors such as geographical location and academic seniority. To address this challenge, we propose a two-stage training framework for large language models (LLMs). In the first stage, a contrastive learning approach is used to train the model on a dataset constructed from real-world matching rules, such as geographical alignment and research area overlap. While effective, this model primarily learns patterns that defined by the matching rules. In the second stage, we introduce a novel preference-based fine-tuning method inspired by Direct Preference Optimization (DPO), termed Rank Preference Optimization (RankPO), to align the model with AI-curated pairwise preferences emphasizing textual understanding. Our experiments show that while the first-stage model achieves strong performance on rule-based data (nDCG@20 = 0.706), it lacks robust textual understanding (alignment with AI annotations = 0.46). By fine-tuning with RankPO, we achieve a balanced model that retains relatively good performance in the original tasks while significantly improving the alignment with AI preferences. The code and data are available at https://github.com/yflyzhang/RankPO. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. Comments: 15 pages, 3 figures, 7 tables arXiv:2503.10618 [pdf, other] DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture Design in Text to Image Generation Authors: Chen Chen, Rui Qian, Wenze Hu, Tsu-Jui Fu, Jialing Tong, Xinze Wang, Lezhi Li, Bowen Zhang, Alex Schwing, Wei Liu, Yinfei Yang Abstract: In this work, we empirically study Diffusion Transformers (DiTs) for text-to-image generation, focusing on architectural choices, text-conditioning strategies, and training protocols. We evaluate a range of DiT-based architectures--including PixArt-style and MMDiT variants--and compare them with a standard DiT variant which directly processes concatenated text and noise inputs. Surprisingly, our f\u2026 \u25bd More In this work, we empirically study Diffusion Transformers (DiTs) for text-to-image generation, focusing on architectural choices, text-conditioning strategies, and training protocols. We evaluate a range of DiT-based architectures--including PixArt-style and MMDiT variants--and compare them with a standard DiT variant which directly processes concatenated text and noise inputs. Surprisingly, our findings reveal that the performance of standard DiT is comparable with those specialized models, while demonstrating superior parameter-efficiency, especially when scaled up. Leveraging the layer-wise parameter sharing strategy, we achieve a further reduction of 66% in model size compared to an MMDiT architecture, with minimal performance impact. Building on an in-depth analysis of critical components such as text encoders and Variational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With supervised and reward fine-tuning, DiT-Air achieves state-of-the-art performance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly competitive, surpassing most existing models despite its compact size. \u25b3 Less Submitted 14 March, 2025; v1 submitted 13 March, 2025; originally announced March 2025. arXiv:2503.10596 [pdf, other] GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding Authors: Rui Hu, Lianghui Zhu, Yuxuan Zhang, Tianheng Cheng, Lei Liu, Heng Liu, Longjin Ran, Xiaoxin Chen, Wenyu Liu, Xinggang Wang Abstract: Pixel grounding, encompassing tasks such as Referring Expression Segmentation (RES), has garnered considerable attention due to its immense potential for bridging the gap between vision and language modalities. However, advancements in this domain are currently constrained by limitations inherent in existing datasets, including limited object categories, insufficient textual diversity, and a scarc\u2026 \u25bd More Pixel grounding, encompassing tasks such as Referring Expression Segmentation (RES), has garnered considerable attention due to its immense potential for bridging the gap between vision and language modalities. However, advancements in this domain are currently constrained by limitations inherent in existing datasets, including limited object categories, insufficient textual diversity, and a scarcity of high-quality annotations. To mitigate these limitations, we introduce GroundingSuite, which comprises: (1) an automated data annotation framework leveraging multiple Vision-Language Model (VLM) agents; (2) a large-scale training dataset encompassing 9.56 million diverse referring expressions and their corresponding segmentations; and (3) a meticulously curated evaluation benchmark consisting of 3,800 images. The GroundingSuite training dataset facilitates substantial performance improvements, enabling models trained on it to achieve state-of-the-art results. Specifically, a cIoU of 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the GroundingSuite annotation framework demonstrates superior efficiency compared to the current leading data annotation method, i.e., $4.5 \\times$ faster than the GLaMM. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. Comments: Work in progress. Code: https://github.com/hustvl/GroundingSuite arXiv:2503.10322 [pdf, other] Towards Fast, Memory-based and Data-Efficient Vision-Language Policy Authors: Haoxuan Li, Sixu Yan, Yuhan Li, Xinggang Wang Abstract: Vision Language Models (VLMs) pretrained on Internet-scale vision-language data have demonstrated the potential to transfer their knowledge to robotic learning. However, the existing paradigm encounters three critical challenges: (1) expensive inference cost resulting from large-scale model parameters, (2) frequent domain shifts caused by mismatched data modalities, and (3) limited capacity to han\u2026 \u25bd More Vision Language Models (VLMs) pretrained on Internet-scale vision-language data have demonstrated the potential to transfer their knowledge to robotic learning. However, the existing paradigm encounters three critical challenges: (1) expensive inference cost resulting from large-scale model parameters, (2) frequent domain shifts caused by mismatched data modalities, and (3) limited capacity to handle past or future experiences. In this work, we propose LiteVLP, a lightweight, memory-based, and general-purpose vision-language policy generation model. LiteVLP is built upon a pre-trained 1B-parameter VLM and fine-tuned on a tiny-scale and conversation-style robotic dataset. Through extensive experiments, we demonstrate that LiteVLP outperforms state-of-the-art vision-language policy on VIMA-Bench, with minimal training time. Furthermore, LiteVLP exhibits superior inference speed while maintaining exceptional high accuracy. In long-horizon manipulation tasks, LiteVLP also shows remarkable memory ability, outperforming the best-performing baseline model by 18.8%. These results highlight LiteVLP as a promising model to integrating the intelligence of VLMs into robotic learning. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. Comments: 11 pages, 7 figures, 6 tables arXiv:2503.10212 [pdf, other] MouseGPT: A Large-scale Vision-Language Model for Mouse Behavior Analysis Authors: Teng Xu, Taotao Zhou, Youjia Wang, Peng Yang, Simin Tang, Kuixiang Shao, Zifeng Tang, Yifei Liu, Xinyuan Chen, Hongshuang Wang, Xiaohui Wang, Huoqing Luo, Jingya Wang, Ji Hu, Jingyi Yu Abstract: Analyzing animal behavior is crucial in advancing neuroscience, yet quantifying and deciphering its intricate dynamics remains a significant challenge. Traditional machine vision approaches, despite their ability to detect spontaneous behaviors, fall short due to limited interpretability and reliance on manual labeling, which restricts the exploration of the full behavioral spectrum. Here, we intr\u2026 \u25bd More Analyzing animal behavior is crucial in advancing neuroscience, yet quantifying and deciphering its intricate dynamics remains a significant challenge. Traditional machine vision approaches, despite their ability to detect spontaneous behaviors, fall short due to limited interpretability and reliance on manual labeling, which restricts the exploration of the full behavioral spectrum. Here, we introduce MouseGPT, a Vision-Language Model (VLM) that integrates visual cues with natural language to revolutionize mouse behavior analysis. Built upon our first-of-its-kind dataset - incorporating pose dynamics and open-vocabulary behavioral annotations across over 42 million frames of diverse psychiatric conditions - MouseGPT provides a novel, context-rich method for comprehensive behavior interpretation. Our holistic analysis framework enables detailed behavior profiling, clustering, and novel behavior discovery, offering deep insights without the need for labor - intensive manual annotation. Evaluations reveal that MouseGPT surpasses existing models in precision, adaptability, and descriptive richness, positioning it as a transformative tool for ethology and for unraveling complex behavioral dynamics in animal models. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. Comments: 53 pages, 5 figures, 7 extended figures arXiv:2503.10080 [pdf, other] Bayesian Prompt Flow Learning for Zero-Shot Anomaly Detection Authors: Zhen Qu, Xian Tao, Xinyi Gong, Shichen Qu, Qiyu Chen, Zhengtao Zhang, Xingang Wang, Guiguang Ding Abstract: Recently, vision-language models (e.g. CLIP) have demonstrated remarkable performance in zero-shot anomaly detection (ZSAD). By leveraging auxiliary data during training, these models can directly perform cross-category anomaly detection on target datasets, such as detecting defects on industrial product surfaces or identifying tumors in organ tissues. Existing approaches typically construct text\u2026 \u25bd More Recently, vision-language models (e.g. CLIP) have demonstrated remarkable performance in zero-shot anomaly detection (ZSAD). By leveraging auxiliary data during training, these models can directly perform cross-category anomaly detection on target datasets, such as detecting defects on industrial product surfaces or identifying tumors in organ tissues. Existing approaches typically construct text prompts through either manual design or the optimization of learnable prompt vectors. However, these methods face several challenges: 1) handcrafted prompts require extensive expert knowledge and trial-and-error; 2) single-form learnable prompts struggle to capture complex anomaly semantics; and 3) an unconstrained prompt space limit generalization to unseen categories. To address these issues, we propose Bayesian Prompt Flow Learning (Bayes-PFL), which models the prompt space as a learnable probability distribution from a Bayesian perspective. Specifically, a prompt flow module is designed to learn both image-specific and image-agnostic distributions, which are jointly utilized to regularize the text prompt space and enhance the model's generalization on unseen categories. These learned distributions are then sampled to generate diverse text prompts, effectively covering the prompt space. Additionally, a residual cross-attention (RCA) module is introduced to better align dynamic text embeddings with fine-grained image features. Extensive experiments on 15 industrial and medical datasets demonstrate our method's superior performance. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. arXiv:2503.09642 [pdf, other] Open-Sora 2.0: Training a Commercial-Level Video Generation Model in $200k Authors: Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, Yuhui Wang, Anbang Ye, Gang Ren, Qianran Ma, Wanying Liang, Xiang Lian, Xiwen Wu, Yuting Zhong, Zhuangyan Li, Chaoyu Gong, Guojun Lei, Leijun Cheng, Limin Zhang, Minghao Li, Ruijie Zhang , et al. (7 additional authors not shown) Abstract: Video generation models have achieved remarkable progress in the past year. The quality of AI video continues to improve, but at the cost of larger model size, increased data quantity, and greater demand for training compute. In this report, we present Open-Sora 2.0, a commercial-level video generation model trained for only $200k. With this model, we demonstrate that the cost of training a top-pe\u2026 \u25bd More Video generation models have achieved remarkable progress in the past year. The quality of AI video continues to improve, but at the cost of larger model size, increased data quantity, and greater demand for training compute. In this report, we present Open-Sora 2.0, a commercial-level video generation model trained for only $200k. With this model, we demonstrate that the cost of training a top-performing video generation model is highly controllable. We detail all techniques that contribute to this efficiency breakthrough, including data curation, model architecture, training strategy, and system optimization. According to human evaluation results and VBench scores, Open-Sora 2.0 is comparable to global leading video generation models including the open-source HunyuanVideo and the closed-source Runway Gen-3 Alpha. By making Open-Sora 2.0 fully open-source, we aim to democratize access to advanced video generation technology, fostering broader innovation and creativity in content creation. All resources are publicly available at: https://github.com/hpcaitech/Open-Sora. \u25b3 Less Submitted 12 March, 2025; originally announced March 2025. arXiv:2503.09527 [pdf, other] CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games Authors: Peng Chen, Pi Bu, Yingyao Wang, Xinyi Wang, Ziming Wang, Jie Guo, Yingxiu Zhao, Qi Zhu, Jun Song, Siran Yang, Jiamang Wang, Bo Zheng Abstract: Recent advances in Vision-Language-Action models (VLAs) have expanded the capabilities of embodied intelligence. However, significant challenges remain in real-time decision-making in complex 3D environments, which demand second-level responses, high-resolution perception, and tactical reasoning under dynamic conditions. To advance the field, we introduce CombatVLA, an efficient VLA model optimize\u2026 \u25bd More Recent advances in Vision-Language-Action models (VLAs) have expanded the capabilities of embodied intelligence. However, significant challenges remain in real-time decision-making in complex 3D environments, which demand second-level responses, high-resolution perception, and tactical reasoning under dynamic conditions. To advance the field, we introduce CombatVLA, an efficient VLA model optimized for combat tasks in 3D action role-playing games(ARPGs). Specifically, our CombatVLA is a 3B model trained on video-action pairs collected by an action tracker, where the data is formatted as action-of-thought (AoT) sequences. Thereafter, CombatVLA seamlessly integrates into an action execution framework, allowing efficient inference through our truncated AoT strategy. Experimental results demonstrate that CombatVLA not only outperforms all existing models on the combat understanding benchmark but also achieves a 50-fold acceleration in game combat. Moreover, it has a higher task success rate than human players. We will open-source all resources, including the action tracker, dataset, benchmark, model weights, training code, and the implementation of the framework at https://combatvla.github.io/. \u25b3 Less Submitted 12 March, 2025; originally announced March 2025. arXiv:2503.09427 [pdf, other] Multimodal Language Modeling for High-Accuracy Single Cell Transcriptomics Analysis and Generation Authors: Yaorui Shi, Jiaqi Yang, Sihang Li, Junfeng Fang, Xiang Wang, Zhiyuan Liu, Yang Zhang Abstract: Pre-trained language models (PLMs) have revolutionized scientific research, yet their application to single-cell analysis remains limited. Text PLMs cannot process single-cell RNA sequencing data, while cell PLMs lack the ability to handle free text, restricting their use in multimodal tasks. Existing efforts to bridge these modalities often suffer from information loss or inadequate single-modal\u2026 \u25bd More Pre-trained language models (PLMs) have revolutionized scientific research, yet their application to single-cell analysis remains limited. Text PLMs cannot process single-cell RNA sequencing data, while cell PLMs lack the ability to handle free text, restricting their use in multimodal tasks. Existing efforts to bridge these modalities often suffer from information loss or inadequate single-modal pre-training, leading to suboptimal performances. To address these challenges, we propose Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a unified PLM for joint cell and text modeling. scMMGPT effectively integrates the state-of-the-art cell and text PLMs, facilitating cross-modal knowledge sharing for improved performance. To bridge the text-cell modality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes extensive pre-training on 27 million cells -- the largest dataset for multimodal cell-text PLMs to date. This large-scale pre-training enables scMMGPT to excel in joint cell-text tasks, achieving an 84\\% relative improvement of textual discrepancy for cell description generation, 20.5\\% higher accuracy for cell type annotation, and 4\\% improvement in $k$-NN accuracy for text-conditioned pseudo-cell generation, outperforming baselines. \u25b3 Less Submitted 12 March, 2025; originally announced March 2025. arXiv:2503.09102 [pdf, other] \"I Like Your Story!\": A Co-Creative Story-Crafting Game with a Persona-Driven Character Based on Generative AI Authors: Jiaying Fu, Xiruo Wang, Zhouyi Li, Kate Vi, Chuyan Xu, Yuqian Sun Abstract: While generative AI is advancing writing support tools, creative writing is often seen as the exclusive domain of skilled writers. This paper introduces \"1001 Nights\", a co-creative story-crafting game that transforms writing into a playful and rewarding activity. In this game, the AI agent takes on the role of a \"moody\" king with distinct storytelling preferences, not merely assisting but activel\u2026 \u25bd More While generative AI is advancing writing support tools, creative writing is often seen as the exclusive domain of skilled writers. This paper introduces \"1001 Nights\", a co-creative story-crafting game that transforms writing into a playful and rewarding activity. In this game, the AI agent takes on the role of a \"moody\" king with distinct storytelling preferences, not merely assisting but actively influencing the narrative. Players engage with the king agent through strategic storytelling, guiding him to mention weapon-related keywords, which materialize as battle equipment. The king agent provides dynamic feedback, expressing satisfaction or displeasure, prompting players to adjust their approach. By combining storytelling, game mechanics, and AI-driven responses, our system motivates creativity through playful constraints. Inspired by Oulipo's literary techniques, this approach demonstrates how AI-powered game experiences can make creative writing more accessible and engaging, encouraging players to explore their creative potential. \u25b3 Less Submitted 12 March, 2025; originally announced March 2025. Comments: 5 pages, 5 figures, In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA '25) arXiv:2503.09089 [pdf, other] LocAgent: Graph-Guided LLM Agents for Code Localization Authors: Zhaoling Chen, Xiangru Tang, Gangda Deng, Fang Wu, Jialong Wu, Zhiwei Jiang, Viktor Prasanna, Arman Cohan, Xingyao Wang Abstract: Code localization--identifying precisely where in a codebase changes need to be made--is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across h\u2026 \u25bd More Code localization--identifying precisely where in a codebase changes need to be made--is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across hierarchical structures and multiple dependencies. We introduce LocAgent, a framework that addresses code localization through graph-based representation. By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures (files, classes, functions) and their dependencies (imports, invocations, inheritance), enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning. Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization. Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent. \u25b3 Less Submitted 12 March, 2025; originally announced March 2025. arXiv:2503.08895 [pdf, other] Mutual Adaptation in Human-Robot Co-Transportation with Human Preference Uncertainty Authors: Al Jaber Mahmud, Weizi Li, Xuan Wang Abstract: Mutual adaptation can significantly enhance overall task performance in human-robot co-transportation by integrating both the robot's and human's understanding of the environment. While human modeling helps capture humans' subjective preferences, two challenges persist: (i) the uncertainty of human preference parameters and (ii) the need to balance adaptation strategies that benefit both humans an\u2026 \u25bd More Mutual adaptation can significantly enhance overall task performance in human-robot co-transportation by integrating both the robot's and human's understanding of the environment. While human modeling helps capture humans' subjective preferences, two challenges persist: (i) the uncertainty of human preference parameters and (ii) the need to balance adaptation strategies that benefit both humans and robots. In this paper, we propose a unified framework to address these challenges and improve task performance through mutual adaptation. First, instead of relying on fixed parameters, we model a probability distribution of human choices by incorporating a range of uncertain human parameters. Next, we introduce a time-varying stubbornness measure and a coordination mode transition model, which allows either the robot to lead the team's trajectory or, if a human's preferred path conflicts with the robot's plan and their stubbornness exceeds a threshold, the robot to transition to following the human. Finally, we introduce a pose optimization strategy to mitigate the uncertain human behaviors when they are leading. To validate the framework, we design and perform experiments with real human feedback. We then demonstrate, through simulations, the effectiveness of our models in enhancing task performance with mutual adaptation and pose optimization. \u25b3 Less Submitted 11 March, 2025; originally announced March 2025. Comments: 8 pages, 6 figures arXiv:2503.08686 [pdf, other] OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models Authors: Jialv Zou, Bencheng Liao, Qian Zhang, Wenyu Liu, Xinggang Wang Abstract: Recent advancements in unified multimodal understanding and visual generation (or multimodal generation) models have been hindered by their quadratic computational complexity and dependence on large-scale training data. We present OmniMamba, the first linear-architecture-based multimodal generation model that generates both text and images through a unified next-token prediction paradigm. The mode\u2026 \u25bd More Recent advancements in unified multimodal understanding and visual generation (or multimodal generation) models have been hindered by their quadratic computational complexity and dependence on large-scale training data. We present OmniMamba, the first linear-architecture-based multimodal generation model that generates both text and images through a unified next-token prediction paradigm. The model fully leverages Mamba-2's high computational and memory efficiency, extending its capabilities from text generation to multimodal generation. To address the data inefficiency of existing unified models, we propose two key innovations: (1) decoupled vocabularies to guide modality-specific generation, and (2) task-specific LoRA for parameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage training strategy to mitigate data imbalance between two tasks. Equipped with these techniques, OmniMamba achieves competitive performance with JanusFlow while surpassing Show-o across benchmarks, despite being trained on merely 2M image-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba stands out with outstanding inference efficiency, achieving up to a 119.2 times speedup and 63% GPU memory reduction for long-sequence generation compared to Transformer-based counterparts. Code and models are released at https://github.com/hustvl/OmniMamba \u25b3 Less Submitted 11 March, 2025; originally announced March 2025. arXiv:2503.08638 [pdf, other] YuE: Scaling Open Foundation Models for Long-Form Music Generation Authors: Ruibin Yuan, Hanfeng Lin, Shuyue Guo, Ge Zhang, Jiahao Pan, Yongyi Zang, Haohe Liu, Yiming Liang, Wenye Ma, Xingjian Du, Xinrun Du, Zhen Ye, Tianyu Zheng, Yinghao Ma, Minghao Liu, Zeyue Tian, Ziya Zhou, Liumeng Xue, Xingwei Qu, Yizhi Li, Shangda Wu, Tianhao Shen, Ziyang Ma, Jun Zhan, Chunhui Wang , et al. (32 additional authors not shown) Abstract: We tackle the task of long-form music generation--particularly the challenging \\textbf{lyrics-to-song} problem--by introducing YuE, a family of open foundation models based on the LLaMA2 architecture. Specifically, YuE scales to trillions of tokens and generates up to five minutes of music while maintaining lyrical alignment, coherent musical structure, and engaging vocal melodies with appropriate\u2026 \u25bd More We tackle the task of long-form music generation--particularly the challenging \\textbf{lyrics-to-song} problem--by introducing YuE, a family of open foundation models based on the LLaMA2 architecture. Specifically, YuE scales to trillions of tokens and generates up to five minutes of music while maintaining lyrical alignment, coherent musical structure, and engaging vocal melodies with appropriate accompaniment. It achieves this through (1) track-decoupled next-token prediction to overcome dense mixture signals, (2) structural progressive conditioning for long-context lyrical alignment, and (3) a multitask, multiphase pre-training recipe to converge and generalize. In addition, we redesign the in-context learning technique for music generation, enabling versatile style transfer (e.g., converting Japanese city pop into an English rap while preserving the original accompaniment) and bidirectional generation. Through extensive evaluation, we demonstrate that YuE matches or even surpasses some of the proprietary systems in musicality and vocal agility. In addition, fine-tuning YuE enables additional controls and enhanced support for tail languages. Furthermore, beyond generation, we show that YuE's learned representations can perform well on music understanding tasks, where the results of YuE match or exceed state-of-the-art methods on the MARBLE benchmark. Keywords: lyrics2song, song generation, long-form, foundation model, music generation \u25b3 Less Submitted 11 March, 2025; originally announced March 2025. Comments: https://github.com/multimodal-art-projection/YuE arXiv:2503.08568 [pdf, other] Privacy Law Enforcement Under Centralized Governance: A Qualitative Analysis of Four Years' Special Privacy Rectification Campaigns Authors: Tao Jing, Yao Li, Jingzhou Ye, Jie Wang, Xueqiang Wang Abstract: In recent years, major privacy laws like the GDPR have brought about positive changes. However, challenges remain in enforcing the laws, particularly due to under-resourced regulators facing a large number of potential privacy-violating software applications (apps) and the high costs of investigating them. Since 2019, China has launched a series of privacy enforcement campaigns known as Special Pr\u2026 \u25bd More In recent years, major privacy laws like the GDPR have brought about positive changes. However, challenges remain in enforcing the laws, particularly due to under-resourced regulators facing a large number of potential privacy-violating software applications (apps) and the high costs of investigating them. Since 2019, China has launched a series of privacy enforcement campaigns known as Special Privacy Rectification Campaigns (SPRCs) to address widespread privacy violations in its mobile application (app) ecosystem. Unlike the enforcement of the GDPR, SPRCs are characterized by large-scale privacy reviews and strict sanctions, under the strong control of central authorities. In SPRCs, central government authorities issue administrative orders to mobilize various resources for market-wide privacy reviews of mobile apps. They enforce strict sanctions by requiring privacy-violating apps to rectify issues within a short timeframe or face removal from app stores. While there are a few reports on SPRCs, the effectiveness and potential problems of this campaign-style privacy enforcement approach remain unclear to the community. In this study, we conducted 18 semi-structured interviews with app-related engineers involved in SPRCs to better understand the campaign-style privacy enforcement. Based on the interviews, we reported our findings on a variety of aspects of SPRCs, such as the processes that app engineers regularly follow to achieve privacy compliance in SPRCs, the challenges they encounter, the solutions they adopt to address these challenges, and the impacts of SPRCs, etc. We found that app engineers face a series of challenges in achieving privacy compliance in their apps... \u25b3 Less Submitted 11 March, 2025; originally announced March 2025. Comments: 18 pages, 5 figures, published to conference of USENIX Security '25 arXiv:2503.08339 [pdf] Diffusion Transformer Meets Random Masks: An Advanced PET Reconstruction Framework Authors: Bin Huang, Binzhong He, Yanhan Chen, Zhili Liu, Xinyue Wang, Binxuan Li, Qiegen Liu Abstract: Deep learning has significantly advanced PET image re-construction, achieving remarkable improvements in image quality through direct training on sinogram or image data. Traditional methods often utilize masks for inpainting tasks, but their incorporation into PET reconstruction frameworks introduces transformative potential. In this study, we pro-pose an advanced PET reconstruction framework call\u2026 \u25bd More Deep learning has significantly advanced PET image re-construction, achieving remarkable improvements in image quality through direct training on sinogram or image data. Traditional methods often utilize masks for inpainting tasks, but their incorporation into PET reconstruction frameworks introduces transformative potential. In this study, we pro-pose an advanced PET reconstruction framework called Diffusion tRansformer mEets rAndom Masks (DREAM). To the best of our knowledge, this is the first work to integrate mask mechanisms into both the sinogram domain and the latent space, pioneering their role in PET reconstruction and demonstrating their ability to enhance reconstruction fidelity and efficiency. The framework employs a high-dimensional stacking approach, transforming masked data from two to three dimensions to expand the solution space and enable the model to capture richer spatial rela-tionships. Additionally, a mask-driven latent space is de-signed to accelerate the diffusion process by leveraging sinogram-driven and mask-driven compact priors, which reduce computational complexity while preserving essen-tial data characteristics. A hierarchical masking strategy is also introduced, guiding the model from focusing on fi-ne-grained local details in the early stages to capturing broader global patterns over time. This progressive ap-proach ensures a balance between detailed feature preservation and comprehensive context understanding. Experimental results demonstrate that DREAM not only improves the overall quality of reconstructed PET images but also preserves critical clinical details, highlighting its potential to advance PET imaging technology. By inte-grating compact priors and hierarchical masking, DREAM offers a promising and efficient avenue for future research and application in PET imaging. The open-source code is available at: https://github.com/yqx7150/DREAM. \u25b3 Less Submitted 11 March, 2025; originally announced March 2025. arXiv:2503.08280 [pdf, other] OminiControl2: Efficient Conditioning for Diffusion Transformers Authors: Zhenxiong Tan, Qiaochu Xue, Xingyi Yang, Songhua Liu, Xinchao Wang Abstract: Fine-grained control of text-to-image diffusion transformer models (DiT) remains a critical challenge for practical deployment. While recent advances such as OminiControl and others have enabled a controllable generation of diverse control signals, these methods face significant computational inefficiency when handling long conditional inputs. We present OminiControl2, an efficient framework that\u2026 \u25bd More Fine-grained control of text-to-image diffusion transformer models (DiT) remains a critical challenge for practical deployment. While recent advances such as OminiControl and others have enabled a controllable generation of diverse control signals, these methods face significant computational inefficiency when handling long conditional inputs. We present OminiControl2, an efficient framework that achieves efficient image-conditional image generation. OminiControl2 introduces two key innovations: (1) a dynamic compression strategy that streamlines conditional inputs by preserving only the most semantically relevant tokens during generation, and (2) a conditional feature reuse mechanism that computes condition token features only once and reuses them across denoising steps. These architectural improvements preserve the original framework's parameter efficiency and multi-modal versatility while dramatically reducing computational costs. Our experiments demonstrate that OminiControl2 reduces conditional processing overhead by over 90% compared to its predecessor, achieving an overall 5.9$\\times$ speedup in multi-conditional generation scenarios. This efficiency enables the practical implementation of complex, multi-modal control for high-quality image synthesis with DiT models. \u25b3 Less Submitted 11 March, 2025; originally announced March 2025. arXiv:2503.08276 [pdf, other] PromptLNet: Region-Adaptive Aesthetic Enhancement via Prompt Guidance in Low-Light Enhancement Net Authors: Jun Yin, Yangfan He, Miao Zhang, Pengyu Zeng, Tianyi Wang, Shuai Lu, Xueqian Wang Abstract: Learning and improving large language models through human preference feedback has become a mainstream approach, but it has rarely been applied to the field of low-light image enhancement. Existing low-light enhancement evaluations typically rely on objective metrics (such as FID, PSNR, etc.), which often result in models that perform well objectively but lack aesthetic quality. Moreover, most low\u2026 \u25bd More Learning and improving large language models through human preference feedback has become a mainstream approach, but it has rarely been applied to the field of low-light image enhancement. Existing low-light enhancement evaluations typically rely on objective metrics (such as FID, PSNR, etc.), which often result in models that perform well objectively but lack aesthetic quality. Moreover, most low-light enhancement models are primarily designed for global brightening, lacking detailed refinement. Therefore, the generated images often require additional local adjustments, leading to research gaps in practical applications. To bridge this gap, we propose the following innovations: 1) We collect human aesthetic evaluation text pairs and aesthetic scores from multiple low-light image datasets (e.g., LOL, LOL2, LOM, DCIM, MEF, etc.) to train a low-light image aesthetic evaluation model, supplemented by an optimization algorithm designed to fine-tune the diffusion model. 2) We propose a prompt-driven brightness adjustment module capable of performing fine-grained brightness and aesthetic adjustments for specific instances or regions. 3) We evaluate our method alongside existing state-of-the-art algorithms on mainstream benchmarks. Experimental results show that our method not only outperforms traditional methods in terms of visual quality but also provides greater flexibility and controllability, paving the way for improved aesthetic quality. \u25b3 Less Submitted 11 March, 2025; originally announced March 2025."
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Zuccon,+G",
    "title": "Showing 1\u201350 of 68 results for author: Zuccon, G",
    "author": "Authors:\nHang Li, \n      \n      Xiao Wang, \n      \n      Bevan Koopman, \n      \n      Guido Zuccon",
    "body": "arXiv:2503.14887 [pdf, other] Pseudo-Relevance Feedback Can Improve Zero-Shot LLM-Based Dense Retrieval Authors: Hang Li, Xiao Wang, Bevan Koopman, Guido Zuccon Abstract: Pseudo-relevance feedback (PRF) refines queries by leveraging initially retrieved documents to improve retrieval effectiveness. In this paper, we investigate how large language models (LLMs) can facilitate PRF for zero-shot LLM-based dense retrieval, extending the recently proposed PromptReps method. Specifically, our approach uses LLMs to extract salient passage features-such as keywords and summ\u2026 \u25bd More Pseudo-relevance feedback (PRF) refines queries by leveraging initially retrieved documents to improve retrieval effectiveness. In this paper, we investigate how large language models (LLMs) can facilitate PRF for zero-shot LLM-based dense retrieval, extending the recently proposed PromptReps method. Specifically, our approach uses LLMs to extract salient passage features-such as keywords and summaries-from top-ranked documents, which are then integrated into PromptReps to produce enhanced query representations. Experiments on passage retrieval benchmarks demonstrate that incorporating PRF significantly boosts retrieval performance. Notably, smaller rankers with PRF can match the effectiveness of larger rankers without PRF, highlighting PRF's potential to improve LLM-driven search while maintaining an efficient balance between effectiveness and resource usage. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. arXiv:2503.06034 [pdf, other] Rank-R1: Enhancing Reasoning in LLM-based Document Rerankers via Reinforcement Learning Authors: Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin, Guido Zuccon Abstract: In this paper, we introduce Rank-R1, a novel LLM-based reranker that performs reasoning over both the user query and candidate documents before performing the ranking task. Existing document reranking methods based on large language models (LLMs) typically rely on prompting or fine-tuning LLMs to order or label candidate documents according to their relevance to a query. For Rank-R1, we use a rein\u2026 \u25bd More In this paper, we introduce Rank-R1, a novel LLM-based reranker that performs reasoning over both the user query and candidate documents before performing the ranking task. Existing document reranking methods based on large language models (LLMs) typically rely on prompting or fine-tuning LLMs to order or label candidate documents according to their relevance to a query. For Rank-R1, we use a reinforcement learning algorithm along with only a small set of relevance labels (without any reasoning supervision) to enhance the reasoning ability of LLM-based rerankers. Our hypothesis is that adding reasoning capabilities to the rerankers can improve their relevance assessement and ranking capabilities. Our experiments on the TREC DL and BRIGHT datasets show that Rank-R1 is highly effective, especially for complex queries. In particular, we find that Rank-R1 achieves effectiveness on in-domain datasets at par with that of supervised fine-tuning methods, but utilizing only 18\\% of the training data used by the fine-tuning methods. We also find that the model largely outperforms zero-shot and supervised fine-tuning when applied to out-of-domain datasets featuring complex queries, especially when a 14B-size model is used. Finally, we qualitatively observe that Rank-R1's reasoning process improves the explainability of the ranking results, opening new opportunities for search engine results presentation and fruition. \u25b3 Less Submitted 7 March, 2025; originally announced March 2025. arXiv:2503.05373 [pdf, other] Leveraging Semantic Type Dependencies for Clinical Named Entity Recognition Authors: Linh Le, Guido Zuccon, Gianluca Demartini, Genghong Zhao, Xia Zhang Abstract: Previous work on clinical relation extraction from free-text sentences leveraged information about semantic types from clinical knowledge bases as a part of entity representations. In this paper, we exploit additional evidence by also making use of domain-specific semantic type dependencies. We encode the relation between a span of tokens matching a Unified Medical Language System (UMLS) concept a\u2026 \u25bd More Previous work on clinical relation extraction from free-text sentences leveraged information about semantic types from clinical knowledge bases as a part of entity representations. In this paper, we exploit additional evidence by also making use of domain-specific semantic type dependencies. We encode the relation between a span of tokens matching a Unified Medical Language System (UMLS) concept and other tokens in the sentence. We implement our method and compare against different named entity recognition (NER) architectures (i.e., BiLSTM-CRF and BiLSTM-GCN-CRF) using different pre-trained clinical embeddings (i.e., BERT, BioBERT, UMLSBert). Our experimental results on clinical datasets show that in some cases NER effectiveness can be significantly improved by making use of domain-specific semantic type dependencies. Our work is also the first study generating a matrix encoding to make use of more than three dependencies in one pass for the NER task. \u25b3 Less Submitted 7 March, 2025; originally announced March 2025. Journal ref: AMIA - American Medical Informatics Association 2022 arXiv:2502.03400 [pdf, other] DenseReviewer: A Screening Prioritisation Tool for Systematic Review based on Dense Retrieval Authors: Xinyu Mao, Teerapong Leelanupab, Harrisen Scells, Guido Zuccon Abstract: Screening is a time-consuming and labour-intensive yet required task for medical systematic reviews, as tens of thousands of studies often need to be screened. Prioritising relevant studies to be screened allows downstream systematic review creation tasks to start earlier and save time. In previous work, we developed a dense retrieval method to prioritise relevant studies with reviewer feedback du\u2026 \u25bd More Screening is a time-consuming and labour-intensive yet required task for medical systematic reviews, as tens of thousands of studies often need to be screened. Prioritising relevant studies to be screened allows downstream systematic review creation tasks to start earlier and save time. In previous work, we developed a dense retrieval method to prioritise relevant studies with reviewer feedback during the title and abstract screening stage. Our method outperforms previous active learning methods in both effectiveness and efficiency. In this demo, we extend this prior work by creating (1) a web-based screening tool that enables end-users to screen studies exploiting state-of-the-art methods and (2) a Python library that integrates models and feedback mechanisms and allows researchers to develop and demonstrate new active learning methods. We describe the tool's design and showcase how it can aid screening. The tool is available at https://densereviewer.ielab.io. The source code is also open sourced at https://github.com/ielab/densereviewer. \u25b3 Less Submitted 5 February, 2025; originally announced February 2025. Comments: Accepted at ECIR 2025 arXiv:2501.16902 [pdf, other] Document Screenshot Retrievers are Vulnerable to Pixel Poisoning Attacks Authors: Shengyao Zhuang, Ekaterina Khramtsova, Xueguang Ma, Bevan Koopman, Jimmy Lin, Guido Zuccon Abstract: Recent advancements in dense retrieval have introduced vision-language model (VLM)-based retrievers, such as DSE and ColPali, which leverage document screenshots embedded as vectors to enable effective search and offer a simplified pipeline over traditional text-only methods. In this study, we propose three pixel poisoning attack methods designed to compromise VLM-based retrievers and evaluate the\u2026 \u25bd More Recent advancements in dense retrieval have introduced vision-language model (VLM)-based retrievers, such as DSE and ColPali, which leverage document screenshots embedded as vectors to enable effective search and offer a simplified pipeline over traditional text-only methods. In this study, we propose three pixel poisoning attack methods designed to compromise VLM-based retrievers and evaluate their effectiveness under various attack settings and parameter configurations. Our empirical results demonstrate that injecting even a single adversarial screenshot into the retrieval corpus can significantly disrupt search results, poisoning the top-10 retrieved documents for 41.9% of queries in the case of DSE and 26.4% for ColPali. These vulnerability rates notably exceed those observed with equivalent attacks on text-only retrievers. Moreover, when targeting a small set of known queries, the attack success rate raises, achieving complete success in certain cases. By exposing the vulnerabilities inherent in vision-language models, this work highlights the potential risks associated with their deployment. \u25b3 Less Submitted 28 January, 2025; originally announced January 2025. arXiv:2412.14457 [pdf, other] VISA: Retrieval Augmented Generation with Visual Source Attribution Authors: Xueguang Ma, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Wenhu Chen, Jimmy Lin Abstract: Generation with source attribution is important for enhancing the verifiability of retrieval-augmented generation (RAG) systems. However, existing approaches in RAG primarily link generated content to document-level references, making it challenging for users to locate evidence among multiple content-rich retrieved documents. To address this challenge, we propose Retrieval-Augmented Generation wit\u2026 \u25bd More Generation with source attribution is important for enhancing the verifiability of retrieval-augmented generation (RAG) systems. However, existing approaches in RAG primarily link generated content to document-level references, making it challenging for users to locate evidence among multiple content-rich retrieved documents. To address this challenge, we propose Retrieval-Augmented Generation with Visual Source Attribution (VISA), a novel approach that combines answer generation with visual source attribution. Leveraging large vision-language models (VLMs), VISA identifies the evidence and highlights the exact regions that support the generated answers with bounding boxes in the retrieved document screenshots. To evaluate its effectiveness, we curated two datasets: Wiki-VISA, based on crawled Wikipedia webpage screenshots, and Paper-VISA, derived from PubLayNet and tailored to the medical domain. Experimental results demonstrate the effectiveness of VISA for visual source attribution on documents' original look, as well as highlighting the challenges for improvement. Code, data, and model checkpoints will be released. \u25b3 Less Submitted 18 December, 2024; originally announced December 2024. arXiv:2411.17299 [pdf, other] 2D Matryoshka Training for Information Retrieval Authors: Shuai Wang, Shengyao Zhuang, Bevan Koopman, Guido Zuccon Abstract: 2D Matryoshka Training is an advanced embedding representation training approach designed to train an encoder model simultaneously across various layer-dimension setups. This method has demonstrated higher effectiveness in Semantic Text Similarity (STS) tasks over traditional training approaches when using sub-layers for embeddings. Despite its success, discrepancies exist between two published im\u2026 \u25bd More 2D Matryoshka Training is an advanced embedding representation training approach designed to train an encoder model simultaneously across various layer-dimension setups. This method has demonstrated higher effectiveness in Semantic Text Similarity (STS) tasks over traditional training approaches when using sub-layers for embeddings. Despite its success, discrepancies exist between two published implementations, leading to varied comparative results with baseline models. In this reproducibility study, we implement and evaluate both versions of 2D Matryoshka Training on STS tasks and extend our analysis to retrieval tasks. Our findings indicate that while both versions achieve higher effectiveness than traditional Matryoshka training on sub-dimensions, and traditional full-sized model training approaches, they do not outperform models trained separately on specific sub-layer and sub-dimension setups. Moreover, these results generalize well to retrieval tasks, both in supervised (MSMARCO) and zero-shot (BEIR) settings. Further explorations of different loss computations reveals more suitable implementations for retrieval tasks, such as incorporating full-dimension loss and training on a broader range of target dimensions. Conversely, some intuitive approaches, such as fixing document encoders to full model outputs, do not yield improvements. Our reproduction code is available at https://github.com/ielab/2DMSE-Reproduce. \u25b3 Less Submitted 26 November, 2024; originally announced November 2024. arXiv:2410.13230 [pdf, other] Starbucks: Improved Training for 2D Matryoshka Embeddings Authors: Shengyao Zhuang, Shuai Wang, Bevan Koopman, Guido Zuccon Abstract: Effective approaches that can scale embedding model depth (i.e. layers) and embedding size allow for the creation of models that are highly scalable across different computational resources and task requirements. While the recently proposed 2D Matryoshka training approach can efficiently produce a single embedding model such that its sub-layers and sub-dimensions can measure text similarity, its e\u2026 \u25bd More Effective approaches that can scale embedding model depth (i.e. layers) and embedding size allow for the creation of models that are highly scalable across different computational resources and task requirements. While the recently proposed 2D Matryoshka training approach can efficiently produce a single embedding model such that its sub-layers and sub-dimensions can measure text similarity, its effectiveness is significantly worse than if smaller models were trained separately. To address this issue, we propose Starbucks, a new training strategy for Matryoshka-like embedding models, which encompasses both the fine-tuning and pre-training phases. For the fine-tuning phase, we discover that, rather than sampling a random sub-layer and sub-dimensions for each training steps, providing a fixed list of layer-dimension pairs, from small size to large sizes, and computing the loss across all pairs significantly improves the effectiveness of 2D Matryoshka embedding models, bringing them on par with their separately trained counterparts. To further enhance performance, we introduce a new pre-training strategy, which applies masked autoencoder language modelling to sub-layers and sub-dimensions during pre-training, resulting in a stronger backbone for subsequent fine-tuning of the embedding model. Experimental results on both semantic text similarity and retrieval benchmarks demonstrate that the proposed pre-training and fine-tuning strategies significantly improved the effectiveness over 2D Matryoshka models, enabling Starbucks models to perform more efficiently and effectively than separately trained models. \u25b3 Less Submitted 18 October, 2024; v1 submitted 17 October, 2024; originally announced October 2024. arXiv:2410.06628 [pdf, other] Does Vec2Text Pose a New Corpus Poisoning Threat? Authors: Shengyao Zhuang, Bevan Koopman, Guido Zuccon Abstract: The emergence of Vec2Text -- a method for text embedding inversion -- has raised serious privacy concerns for dense retrieval systems which use text embeddings. This threat comes from the ability for an attacker with access to embeddings to reconstruct the original text. In this paper, we take a new look at Vec2Text and investigate how much of a threat it poses to the different attacks of corpus p\u2026 \u25bd More The emergence of Vec2Text -- a method for text embedding inversion -- has raised serious privacy concerns for dense retrieval systems which use text embeddings. This threat comes from the ability for an attacker with access to embeddings to reconstruct the original text. In this paper, we take a new look at Vec2Text and investigate how much of a threat it poses to the different attacks of corpus poisoning, whereby an attacker injects adversarial passages into a retrieval corpus with the intention of misleading dense retrievers. Theoretically, Vec2Text is far more dangerous than previous attack methods because it does not need access to the embedding model's weights and it can efficiently generate many adversarial passages. We show that under certain conditions, corpus poisoning with Vec2Text can pose a serious threat to dense retriever system integrity and user experience by injecting adversarial passaged into top ranked positions. Code and data are made available at https://github.com/ielab/vec2text-corpus-poisoning \u25b3 Less Submitted 9 October, 2024; originally announced October 2024. Comments: arXiv admin note: substantial text overlap with arXiv:2402.12784 arXiv:2408.02209 [pdf, other] Source-Free Domain-Invariant Performance Prediction Authors: Ekaterina Khramtsova, Mahsa Baktashmotlagh, Guido Zuccon, Xi Wang, Mathieu Salzmann Abstract: Accurately estimating model performance poses a significant challenge, particularly in scenarios where the source and target domains follow different data distributions. Most existing performance prediction methods heavily rely on the source data in their estimation process, limiting their applicability in a more realistic setting where only the trained model is accessible. The few methods that do\u2026 \u25bd More Accurately estimating model performance poses a significant challenge, particularly in scenarios where the source and target domains follow different data distributions. Most existing performance prediction methods heavily rely on the source data in their estimation process, limiting their applicability in a more realistic setting where only the trained model is accessible. The few methods that do not require source data exhibit considerably inferior performance. In this work, we propose a source-free approach centred on uncertainty-based estimation, using a generative model for calibration in the absence of source data. We establish connections between our approach for unsupervised calibration and temperature scaling. We then employ a gradient-based strategy to evaluate the correctness of the calibrated predictions. Our experiments on benchmark object recognition datasets reveal that existing source-based methods fall short with limited source sample availability. Furthermore, our approach significantly outperforms the current state-of-the-art source-free and source-based methods, affirming its effectiveness in domain-invariant performance estimation. \u25b3 Less Submitted 6 August, 2024; v1 submitted 4 August, 2024; originally announced August 2024. Comments: Accepted in ECCV 2024 arXiv:2407.06685 [pdf, other] Embark on DenseQuest: A System for Selecting the Best Dense Retriever for a Custom Collection Authors: Ekaterina Khramtsova, Teerapong Leelanupab, Shengyao Zhuang, Mahsa Baktashmotlagh, Guido Zuccon Abstract: In this demo we present a web-based application for selecting an effective pre-trained dense retriever to use on a private collection. Our system, DenseQuest, provides unsupervised selection and ranking capabilities to predict the best dense retriever among a pool of available dense retrievers, tailored to an uploaded target collection. DenseQuest implements a number of existing approaches, includ\u2026 \u25bd More In this demo we present a web-based application for selecting an effective pre-trained dense retriever to use on a private collection. Our system, DenseQuest, provides unsupervised selection and ranking capabilities to predict the best dense retriever among a pool of available dense retrievers, tailored to an uploaded target collection. DenseQuest implements a number of existing approaches, including a recent, highly effective method powered by Large Language Models (LLMs), which requires neither queries nor relevance judgments. The system is designed to be intuitive and easy to use for those information retrieval engineers and researchers who need to identify a general-purpose dense retrieval model to encode or search a new private target collection. Our demonstration illustrates conceptual architecture and the different use case scenarios of the system implemented on the cloud, enabling universal access and use. DenseQuest is available at https://densequest.ielab.io. \u25b3 Less Submitted 9 July, 2024; originally announced July 2024. Comments: SIGIR2024 demo paper arXiv:2407.00635 [pdf, other] Dense Retrieval with Continuous Explicit Feedback for Systematic Review Screening Prioritisation Authors: Xinyu Mao, Shengyao Zhuang, Bevan Koopman, Guido Zuccon Abstract: The goal of screening prioritisation in systematic reviews is to identify relevant documents with high recall and rank them in early positions for review. This saves reviewing effort if paired with a stopping criterion, and speeds up review completion if performed alongside downstream tasks. Recent studies have shown that neural models have good potential on this task, but their time-consuming fin\u2026 \u25bd More The goal of screening prioritisation in systematic reviews is to identify relevant documents with high recall and rank them in early positions for review. This saves reviewing effort if paired with a stopping criterion, and speeds up review completion if performed alongside downstream tasks. Recent studies have shown that neural models have good potential on this task, but their time-consuming fine-tuning and inference discourage their widespread use for screening prioritisation. In this paper, we propose an alternative approach that still relies on neural models, but leverages dense representations and relevance feedback to enhance screening prioritisation, without the need for costly model fine-tuning and inference. This method exploits continuous relevance feedback from reviewers during document screening to efficiently update the dense query representation, which is then applied to rank the remaining documents to be screened. We evaluate this approach across the CLEF TAR datasets for this task. Results suggest that the investigated dense query-driven approach is more efficient than directly using neural models and shows promising effectiveness compared to previous methods developed on the considered datasets. Our code is available at https://github.com/ielab/dense-screening-feedback. \u25b3 Less Submitted 17 July, 2024; v1 submitted 30 June, 2024; originally announced July 2024. Comments: Accepted at SIGIR 2024;typos corrected arXiv:2406.14117 [pdf, other] An Investigation of Prompt Variations for Zero-shot LLM-based Rankers Authors: Shuoqi Sun, Shengyao Zhuang, Shuai Wang, Guido Zuccon Abstract: We provide a systematic understanding of the impact of specific components and wordings used in prompts on the effectiveness of rankers based on zero-shot Large Language Models (LLMs). Several zero-shot ranking methods based on LLMs have recently been proposed. Among many aspects, methods differ across (1) the ranking algorithm they implement, e.g., pointwise vs. listwise, (2) the backbone LLMs us\u2026 \u25bd More We provide a systematic understanding of the impact of specific components and wordings used in prompts on the effectiveness of rankers based on zero-shot Large Language Models (LLMs). Several zero-shot ranking methods based on LLMs have recently been proposed. Among many aspects, methods differ across (1) the ranking algorithm they implement, e.g., pointwise vs. listwise, (2) the backbone LLMs used, e.g., GPT3.5 vs. FLAN-T5, (3) the components and wording used in prompts, e.g., the use or not of role-definition (role-playing) and the actual words used to express this. It is currently unclear whether performance differences are due to the underlying ranking algorithm, or because of spurious factors such as better choice of words used in prompts. This confusion risks to undermine future research. Through our large-scale experimentation and analysis, we find that ranking algorithms do contribute to differences between methods for zero-shot LLM ranking. However, so do the LLM backbones -- but even more importantly, the choice of prompt components and wordings affect the ranking. In fact, in our experiments, we find that, at times, these latter elements have more impact on the ranker's effectiveness than the actual ranking algorithms, and that differences among ranking methods become more blurred when prompt variations are considered. \u25b3 Less Submitted 24 January, 2025; v1 submitted 20 June, 2024; originally announced June 2024. arXiv:2405.07920 [pdf, other] A Systematic Investigation of Distilling Large Language Models into Cross-Encoders for Passage Re-ranking Authors: Ferdinand Schlatt, Maik Fr\u00f6be, Harrisen Scells, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Benno Stein, Martin Potthast, Matthias Hagen Abstract: Cross-encoders distilled from large language models (LLMs) are often more effective re-rankers than cross-encoders fine-tuned on manually labeled data. However, the distilled models usually do not reach their teacher LLM's effectiveness. To investigate whether best practices for fine-tuning cross-encoders on manually labeled data (e.g., hard-negative sampling, deep sampling, and listwise loss func\u2026 \u25bd More Cross-encoders distilled from large language models (LLMs) are often more effective re-rankers than cross-encoders fine-tuned on manually labeled data. However, the distilled models usually do not reach their teacher LLM's effectiveness. To investigate whether best practices for fine-tuning cross-encoders on manually labeled data (e.g., hard-negative sampling, deep sampling, and listwise loss functions) can help to improve LLM ranker distillation, we construct and release a new distillation dataset: Rank-DistiLLM. In our experiments, cross-encoders trained on Rank-DistiLLM reach the effectiveness of LLMs while being orders of magnitude more efficient. Our code and data is available at https://github.com/webis-de/msmarco-llm-distillation. \u25b3 Less Submitted 16 June, 2024; v1 submitted 13 May, 2024; originally announced May 2024. arXiv:2404.18424 [pdf, other] PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval Authors: Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin, Guido Zuccon Abstract: Utilizing large language models (LLMs) for zero-shot document ranking is done in one of two ways: (1) prompt-based re-ranking methods, which require no further training but are only feasible for re-ranking a handful of candidate documents due to computational costs; and (2) unsupervised contrastive trained dense retrieval methods, which can retrieve relevant documents from the entire corpus but re\u2026 \u25bd More Utilizing large language models (LLMs) for zero-shot document ranking is done in one of two ways: (1) prompt-based re-ranking methods, which require no further training but are only feasible for re-ranking a handful of candidate documents due to computational costs; and (2) unsupervised contrastive trained dense retrieval methods, which can retrieve relevant documents from the entire corpus but require a large amount of paired text data for contrastive training. In this paper, we propose PromptReps, which combines the advantages of both categories: no need for training and the ability to retrieve from the whole corpus. Our method only requires prompts to guide an LLM to generate query and document representations for effective document retrieval. Specifically, we prompt the LLMs to represent a given text using a single word, and then use the last token's hidden states and the corresponding logits associated with the prediction of the next token to construct a hybrid document retrieval system. The retrieval system harnesses both dense text embedding and sparse bag-of-words representations given by the LLM. Our experimental evaluation on the MSMARCO, TREC deep learning and BEIR zero-shot document retrieval datasets illustrates that this simple prompt-based LLM retrieval method can achieve a similar or higher retrieval effectiveness than state-of-the-art LLM embedding methods that are trained with large amounts of unsupervised data, especially when using a larger LLM. \u25b3 Less Submitted 18 October, 2024; v1 submitted 29 April, 2024; originally announced April 2024. Comments: EMNLP2024 main arXiv:2404.06912 [pdf, other] Set-Encoder: Permutation-Invariant Inter-Passage Attention for Listwise Passage Re-Ranking with Cross-Encoders Authors: Ferdinand Schlatt, Maik Fr\u00f6be, Harrisen Scells, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Benno Stein, Martin Potthast, Matthias Hagen Abstract: Existing cross-encoder re-rankers can be categorized as pointwise, pairwise, or listwise models. Pair- and listwise models allow passage interactions, which usually makes them more effective than pointwise models but also less efficient and less robust to input order permutations. To enable efficient permutation-invariant passage interactions during re-ranking, we propose a new cross-encoder archi\u2026 \u25bd More Existing cross-encoder re-rankers can be categorized as pointwise, pairwise, or listwise models. Pair- and listwise models allow passage interactions, which usually makes them more effective than pointwise models but also less efficient and less robust to input order permutations. To enable efficient permutation-invariant passage interactions during re-ranking, we propose a new cross-encoder architecture with inter-passage attention: the Set-Encoder. In Cranfield-style experiments on TREC Deep Learning and TIREx, the Set-Encoder is as effective as state-of-the-art listwise models while improving efficiency and robustness to input permutations. Interestingly, a pointwise model is similarly effective, but when additionally requiring the models to consider novelty, the Set-Encoder is more effective than its pointwise counterpart and retains its advantageous properties compared to other listwise models. Our code and models are publicly available at https://github.com/webis-de/set-encoder. \u25b3 Less Submitted 16 June, 2024; v1 submitted 10 April, 2024; originally announced April 2024. arXiv:2402.12784 [pdf, other] Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval Systems Authors: Shengyao Zhuang, Bevan Koopman, Xiaoran Chu, Guido Zuccon Abstract: The emergence of Vec2Text -- a method for text embedding inversion -- has raised serious privacy concerns for dense retrieval systems which use text embeddings, such as those offered by OpenAI and Cohere. This threat comes from the ability for a malicious attacker with access to embeddings to reconstruct the original text. In this paper, we investigate various factors related to embedding models t\u2026 \u25bd More The emergence of Vec2Text -- a method for text embedding inversion -- has raised serious privacy concerns for dense retrieval systems which use text embeddings, such as those offered by OpenAI and Cohere. This threat comes from the ability for a malicious attacker with access to embeddings to reconstruct the original text. In this paper, we investigate various factors related to embedding models that may impact text recoverability via Vec2Text. We explore factors such as distance metrics, pooling functions, bottleneck pre-training, training with noise addition, embedding quantization, and embedding dimensions, which were not considered in the original Vec2Text paper. Through a comprehensive analysis of these factors, our objective is to gain a deeper understanding of the key elements that affect the trade-offs between the text recoverability and retrieval effectiveness of dense retrieval systems, offering insights for practitioners designing privacy-aware dense retrieval systems. We also propose a simple embedding transformation fix that guarantees equal ranking effectiveness while mitigating the recoverability risk. Overall, this study reveals that Vec2Text could pose a threat to current dense retrieval systems, but there are some effective methods to patch such systems. \u25b3 Less Submitted 24 July, 2024; v1 submitted 20 February, 2024; originally announced February 2024. arXiv:2402.11891 [pdf, other] FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation Authors: Shuai Wang, Ekaterina Khramtsova, Shengyao Zhuang, Guido Zuccon Abstract: Federated search systems aggregate results from multiple search engines, selecting appropriate sources to enhance result quality and align with user intent. With the increasing uptake of Retrieval-Augmented Generation (RAG) pipelines, federated search can play a pivotal role in sourcing relevant information across heterogeneous data sources to generate informed responses. However, existing dataset\u2026 \u25bd More Federated search systems aggregate results from multiple search engines, selecting appropriate sources to enhance result quality and align with user intent. With the increasing uptake of Retrieval-Augmented Generation (RAG) pipelines, federated search can play a pivotal role in sourcing relevant information across heterogeneous data sources to generate informed responses. However, existing datasets, such as those developed in the past TREC FedWeb tracks, predate the RAG paradigm shift and lack representation of modern information retrieval challenges. To bridge this gap, we present FeB4RAG, a novel dataset specifically designed for federated search within RAG frameworks. This dataset, derived from 16 sub-collections of the widely used \\beir benchmarking collection, includes 790 information requests (akin to conversational queries) tailored for chatbot applications, along with top results returned by each resource and associated LLM-derived relevance judgements. Additionally, to support the need for this collection, we demonstrate the impact on response generation of a high quality federated search system for RAG compared to a naive approach to federated search. We do so by comparing answers generated through the RAG pipeline through a qualitative side-by-side comparison. Our collection fosters and supports the development and evaluation of new federated search methods, especially in the context of RAG pipelines. \u25b3 Less Submitted 19 February, 2024; originally announced February 2024. arXiv:2402.11757 [pdf, other] Large Language Models for Stemming: Promises, Pitfalls and Failures Authors: Shuai Wang, Shengyao Zhuang, Guido Zuccon Abstract: Text stemming is a natural language processing technique that is used to reduce words to their base form, also known as the root form. The use of stemming in IR has been shown to often improve the effectiveness of keyword-matching models such as BM25. However, traditional stemming methods, focusing solely on individual terms, overlook the richness of contextual information. Recognizing this gap, i\u2026 \u25bd More Text stemming is a natural language processing technique that is used to reduce words to their base form, also known as the root form. The use of stemming in IR has been shown to often improve the effectiveness of keyword-matching models such as BM25. However, traditional stemming methods, focusing solely on individual terms, overlook the richness of contextual information. Recognizing this gap, in this paper, we investigate the promising idea of using large language models (LLMs) to stem words by leveraging its capability of context understanding. With this respect, we identify three avenues, each characterised by different trade-offs in terms of computational cost, effectiveness and robustness : (1) use LLMs to stem the vocabulary for a collection, i.e., the set of unique words that appear in the collection (vocabulary stemming), (2) use LLMs to stem each document separately (contextual stemming), and (3) use LLMs to extract from each document entities that should not be stemmed, then use vocabulary stemming to stem the rest of the terms (entity-based contextual stemming). Through a series of empirical experiments, we compare the use of LLMs for stemming with that of traditional lexical stemmers such as Porter and Krovetz for English text. We find that while vocabulary stemming and contextual stemming fail to achieve higher effectiveness than traditional stemmers, entity-based contextual stemming can achieve a higher effectiveness than using Porter stemmer alone, under specific conditions. \u25b3 Less Submitted 18 February, 2024; originally announced February 2024. arXiv:2402.04853 [pdf, other] Leveraging LLMs for Unsupervised Dense Retriever Ranking Authors: Ekaterina Khramtsova, Shengyao Zhuang, Mahsa Baktashmotlagh, Guido Zuccon Abstract: In this paper we present Large Language Model Assisted Retrieval Model Ranking (LARMOR), an effective unsupervised approach that leverages LLMs for selecting which dense retriever to use on a test corpus (target). Dense retriever selection is crucial for many IR applications that rely on using dense retrievers trained on public corpora to encode or search a new, private target corpus. This is beca\u2026 \u25bd More In this paper we present Large Language Model Assisted Retrieval Model Ranking (LARMOR), an effective unsupervised approach that leverages LLMs for selecting which dense retriever to use on a test corpus (target). Dense retriever selection is crucial for many IR applications that rely on using dense retrievers trained on public corpora to encode or search a new, private target corpus. This is because when confronted with domain shift, where the downstream corpora, domains, or tasks of the target corpus differ from the domain/task the dense retriever was trained on, its performance often drops. Furthermore, when the target corpus is unlabeled, e.g., in a zero-shot scenario, the direct evaluation of the model on the target corpus becomes unfeasible. Unsupervised selection of the most effective pre-trained dense retriever becomes then a crucial challenge. Current methods for dense retriever selection are insufficient in handling scenarios with domain shift. Our proposed solution leverages LLMs to generate pseudo-relevant queries, labels and reference lists based on a set of documents sampled from the target corpus. Dense retrievers are then ranked based on their effectiveness on these generated pseudo-relevant signals. Notably, our method is the first approach that relies solely on the target corpus, eliminating the need for both training corpora and test labels. To evaluate the effectiveness of our method, we construct a large pool of state-of-the-art dense retrievers. The proposed approach outperforms existing baselines with respect to both dense retriever selection and ranking. We make our code and results publicly available at https://github.com/ielab/larmor/. \u25b3 Less Submitted 23 May, 2024; v1 submitted 7 February, 2024; originally announced February 2024. Comments: SIGIR2024 full paper arXiv:2401.17645 [pdf, other] ReSLLM: Large Language Models are Strong Resource Selectors for Federated Search Authors: Shuai Wang, Shengyao Zhuang, Bevan Koopman, Guido Zuccon Abstract: Federated search, which involves integrating results from multiple independent search engines, will become increasingly pivotal in the context of Retrieval-Augmented Generation pipelines empowering LLM-based applications such as chatbots. These systems often distribute queries among various search engines, ranging from specialized (e.g., PubMed) to general (e.g., Google), based on the nature of us\u2026 \u25bd More Federated search, which involves integrating results from multiple independent search engines, will become increasingly pivotal in the context of Retrieval-Augmented Generation pipelines empowering LLM-based applications such as chatbots. These systems often distribute queries among various search engines, ranging from specialized (e.g., PubMed) to general (e.g., Google), based on the nature of user utterances. A critical aspect of federated search is resource selection - the selection of appropriate resources prior to issuing the query to ensure high-quality and rapid responses, and contain costs associated with calling the external search engines. However, current SOTA resource selection methodologies primarily rely on feature-based learning approaches. These methods often involve the labour intensive and expensive creation of training labels for each resource. In contrast, LLMs have exhibited strong effectiveness as zero-shot methods across NLP and IR tasks. We hypothesise that in the context of federated search LLMs can assess the relevance of resources without the need for extensive predefined labels or features. In this paper, we propose ReSLLM. Our ReSLLM method exploits LLMs to drive the selection of resources in federated search in a zero-shot setting. In addition, we devise an unsupervised fine tuning protocol, the Synthetic Label Augmentation Tuning (SLAT), where the relevance of previously logged queries and snippets from resources is predicted using an off-the-shelf LLM and then in turn used to fine-tune ReSLLM with respect to resource selection. Our empirical evaluation and analysis details the factors influencing the effectiveness of LLMs in this context. The results showcase the merits of ReSLLM for resource selection: not only competitive effectiveness in the zero-shot setting, but also obtaining large when fine-tuned using SLAT-protocol. \u25b3 Less Submitted 31 January, 2024; originally announced January 2024. arXiv:2401.13509 [pdf, other] TPRF: A Transformer-based Pseudo-Relevance Feedback Model for Efficient and Effective Retrieval Authors: Hang Li, Chuting Yu, Ahmed Mourad, Bevan Koopman, Guido Zuccon Abstract: This paper considers Pseudo-Relevance Feedback (PRF) methods for dense retrievers in a resource constrained environment such as that of cheap cloud instances or embedded systems (e.g., smartphones and smartwatches), where memory and CPU are limited and GPUs are not present. For this, we propose a transformer-based PRF method (TPRF), which has a much smaller memory footprint and faster inference ti\u2026 \u25bd More This paper considers Pseudo-Relevance Feedback (PRF) methods for dense retrievers in a resource constrained environment such as that of cheap cloud instances or embedded systems (e.g., smartphones and smartwatches), where memory and CPU are limited and GPUs are not present. For this, we propose a transformer-based PRF method (TPRF), which has a much smaller memory footprint and faster inference time compared to other deep language models that employ PRF mechanisms, with a marginal effectiveness loss. TPRF learns how to effectively combine the relevance feedback signals from dense passage representations. Specifically, TPRF provides a mechanism for modelling relationships and weights between the query and the relevance feedback signals. The method is agnostic to the specific dense representation used and thus can be generally applied to any dense retriever. \u25b3 Less Submitted 6 December, 2024; v1 submitted 24 January, 2024; originally announced January 2024. arXiv:2401.13410 [pdf, other] How to Forget Clients in Federated Online Learning to Rank? Authors: Shuyi Wang, Bing Liu, Guido Zuccon Abstract: Data protection legislation like the European Union's General Data Protection Regulation (GDPR) establishes the \\textit{right to be forgotten}: a user (client) can request contributions made using their data to be removed from learned models. In this paper, we study how to remove the contributions made by a client participating in a Federated Online Learning to Rank (FOLTR) system. In a FOLTR syst\u2026 \u25bd More Data protection legislation like the European Union's General Data Protection Regulation (GDPR) establishes the \\textit{right to be forgotten}: a user (client) can request contributions made using their data to be removed from learned models. In this paper, we study how to remove the contributions made by a client participating in a Federated Online Learning to Rank (FOLTR) system. In a FOLTR system, a ranker is learned by aggregating local updates to the global ranking model. Local updates are learned in an online manner at a client-level using queries and implicit interactions that have occurred within that specific client. By doing so, each client's local data is not shared with other clients or with a centralised search service, while at the same time clients can benefit from an effective global ranking model learned from contributions of each client in the federation. In this paper, we study an effective and efficient unlearning method that can remove a client's contribution without compromising the overall ranker effectiveness and without needing to retrain the global ranker from scratch. A key challenge is how to measure whether the model has unlearned the contributions from the client $c^*$ that has requested removal. For this, we instruct $c^*$ to perform a poisoning attack (add noise to this client updates) and then we measure whether the impact of the attack is lessened when the unlearning process has taken place. Through experiments on four datasets, we demonstrate the effectiveness and efficiency of the unlearning strategy under different combinations of parameter settings. \u25b3 Less Submitted 24 January, 2024; originally announced January 2024. Comments: Accepted in ECIR 2024 arXiv:2401.08104 [pdf, other] A Reproducibility Study of Goldilocks: Just-Right Tuning of BERT for TAR Authors: Xinyu Mao, Bevan Koopman, Guido Zuccon Abstract: Screening documents is a tedious and time-consuming aspect of high-recall retrieval tasks, such as compiling a systematic literature review, where the goal is to identify all relevant documents for a topic. To help streamline this process, many Technology-Assisted Review (TAR) methods leverage active learning techniques to reduce the number of documents requiring review. BERT-based models have sho\u2026 \u25bd More Screening documents is a tedious and time-consuming aspect of high-recall retrieval tasks, such as compiling a systematic literature review, where the goal is to identify all relevant documents for a topic. To help streamline this process, many Technology-Assisted Review (TAR) methods leverage active learning techniques to reduce the number of documents requiring review. BERT-based models have shown high effectiveness in text classification, leading to interest in their potential use in TAR workflows. In this paper, we investigate recent work that examined the impact of further pre-training epochs on the effectiveness and efficiency of a BERT-based active learning pipeline. We first report that we could replicate the original experiments on two specific TAR datasets, confirming some of the findings: importantly, that further pre-training is critical to high effectiveness, but requires attention in terms of selecting the correct training epoch. We then investigate the generalisability of the pipeline on a different TAR task, that of medical systematic reviews. In this context, we show that there is no need for further pre-training if a domain-specific BERT backbone is used within the active learning pipeline. This finding provides practical implications for using the studied active learning pipeline within domain-specific TAR tasks. \u25b3 Less Submitted 15 January, 2024; originally announced January 2024. Comments: Accepted at ECIR 2024 (reproducibility) arXiv:2401.06320 [pdf, other] Zero-shot Generative Large Language Models for Systematic Review Screening Automation Authors: Shuai Wang, Harrisen Scells, Shengyao Zhuang, Martin Potthast, Bevan Koopman, Guido Zuccon Abstract: Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models~(LLMs\u2026 \u25bd More Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models~(LLMs) for automatic screening. We evaluate the effectiveness of eight different LLMs and investigate a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Our comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders LLMs practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches. \u25b3 Less Submitted 31 January, 2024; v1 submitted 11 January, 2024; originally announced January 2024. Comments: Accepted to ECIR2024 full paper (findings) arXiv:2401.01566 [pdf, other] Team IELAB at TREC Clinical Trial Track 2023: Enhancing Clinical Trial Retrieval with Neural Rankers and Large Language Models Authors: Shengyao Zhuang, Bevan Koopman, Guido Zuccon Abstract: We describe team ielab from CSIRO and The University of Queensland's approach to the 2023 TREC Clinical Trials Track. Our approach was to use neural rankers but to utilise Large Language Models to overcome the issue of lack of training data for such rankers. Specifically, we employ ChatGPT to generate relevant patient descriptions for randomly selected clinical trials from the corpus. This synthet\u2026 \u25bd More We describe team ielab from CSIRO and The University of Queensland's approach to the 2023 TREC Clinical Trials Track. Our approach was to use neural rankers but to utilise Large Language Models to overcome the issue of lack of training data for such rankers. Specifically, we employ ChatGPT to generate relevant patient descriptions for randomly selected clinical trials from the corpus. This synthetic dataset, combined with human-annotated training data from previous years, is used to train both dense and sparse retrievers based on PubmedBERT. Additionally, a cross-encoder re-ranker is integrated into the system. To further enhance the effectiveness of our approach, we prompting GPT-4 as a TREC annotator to provide judgments on our run files. These judgments are subsequently employed to re-rank the results. This architecture tightly integrates strong PubmedBERT-based rankers with the aid of SOTA Large Language Models, demonstrating a new approach to clinical trial retrieval. \u25b3 Less Submitted 3 January, 2024; originally announced January 2024. Comments: TREC Notebook arXiv:2311.07926 [pdf, other] VizPut: Insight-Aware Imputation of Incomplete Data for Visualization Recommendation Authors: Rischan Mafrur, Mohamed A Sharaf, Guido Zuccon Abstract: In insight recommendation systems, obtaining timely and high-quality recommended visual analytics over incomplete data is challenging due to the difficulties in cleaning and processing such data. Failing to address data incompleteness results in diminished recommendation quality, compelling users to impute the incomplete data to a cleaned version through a costly imputation strategy. This paper in\u2026 \u25bd More In insight recommendation systems, obtaining timely and high-quality recommended visual analytics over incomplete data is challenging due to the difficulties in cleaning and processing such data. Failing to address data incompleteness results in diminished recommendation quality, compelling users to impute the incomplete data to a cleaned version through a costly imputation strategy. This paper introduces VizPut scheme, an insight-aware selective imputation technique capable of determining which missing values should be imputed in incomplete data to optimize the effectiveness of recommended visualizations within a specified imputation budget. The VizPut scheme determines the optimal allocation of imputation operations with the objective of achieving maximal effectiveness in recommended visual analytics. We evaluate this approach using real-world datasets, and our experimental results demonstrate that VizPut effectively maximizes the efficacy of recommended visualizations within the user-defined imputation budget. \u25b3 Less Submitted 14 November, 2023; originally announced November 2023. Comments: This is part of my thesis chapter https://espace.library.uq.edu.au/view/UQ:812c680 arXiv:2311.04694 [pdf, other] Evaluating Generative Ad Hoc Information Retrieval Authors: Lukas Gienapp, Harrisen Scells, Niklas Deckers, Janek Bevendorff, Shuai Wang, Johannes Kiesel, Shahbaz Syed, Maik Fr\u00f6be, Guido Zuccon, Benno Stein, Matthias Hagen, Martin Potthast Abstract: Recent advances in large language models have enabled the development of viable generative retrieval systems. Instead of a traditional document ranking, generative retrieval systems often directly return a grounded generated text as a response to a query. Quantifying the utility of the textual responses is essential for appropriately evaluating such generative ad hoc retrieval. Yet, the establishe\u2026 \u25bd More Recent advances in large language models have enabled the development of viable generative retrieval systems. Instead of a traditional document ranking, generative retrieval systems often directly return a grounded generated text as a response to a query. Quantifying the utility of the textual responses is essential for appropriately evaluating such generative ad hoc retrieval. Yet, the established evaluation methodology for ranking-based ad hoc retrieval is not suited for the reliable and reproducible evaluation of generated responses. To lay a foundation for developing new evaluation methods for generative retrieval systems, we survey the relevant literature from the fields of information retrieval and natural language processing, identify search tasks and system architectures in generative retrieval, develop a new user model, and study its operationalization. \u25b3 Less Submitted 22 May, 2024; v1 submitted 8 November, 2023; originally announced November 2023. Comments: 14 pages, 6 figures, 1 table. Published at SIGIR'24 perspective paper track arXiv:2310.13243 [pdf, other] Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking Authors: Shengyao Zhuang, Bing Liu, Bevan Koopman, Guido Zuccon Abstract: In the field of information retrieval, Query Likelihood Models (QLMs) rank documents based on the probability of generating the query given the content of a document. Recently, advanced large language models (LLMs) have emerged as effective QLMs, showcasing promising ranking capabilities. This paper focuses on investigating the genuine zero-shot ranking effectiveness of recent LLMs, which are sole\u2026 \u25bd More In the field of information retrieval, Query Likelihood Models (QLMs) rank documents based on the probability of generating the query given the content of a document. Recently, advanced large language models (LLMs) have emerged as effective QLMs, showcasing promising ranking capabilities. This paper focuses on investigating the genuine zero-shot ranking effectiveness of recent LLMs, which are solely pre-trained on unstructured text data without supervised instruction fine-tuning. Our findings reveal the robust zero-shot ranking ability of such LLMs, highlighting that additional instruction fine-tuning may hinder effectiveness unless a question generation task is present in the fine-tuning dataset. Furthermore, we introduce a novel state-of-the-art ranking system that integrates LLM-based QLMs with a hybrid zero-shot retriever, demonstrating exceptional effectiveness in both zero-shot and few-shot scenarios. We make our codebase publicly available at https://github.com/ielab/llm-qlm. \u25b3 Less Submitted 19 October, 2023; originally announced October 2023. Comments: 5 pages arXiv:2310.09497 [pdf, other] A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models Authors: Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, Guido Zuccon Abstract: We propose a novel zero-shot document ranking approach based on Large Language Models (LLMs): the Setwise prompting approach. Our approach complements existing prompting approaches for LLM-based zero-shot ranking: Pointwise, Pairwise, and Listwise. Through the first-of-its-kind comparative evaluation within a consistent experimental framework and considering factors like model size, token consumpt\u2026 \u25bd More We propose a novel zero-shot document ranking approach based on Large Language Models (LLMs): the Setwise prompting approach. Our approach complements existing prompting approaches for LLM-based zero-shot ranking: Pointwise, Pairwise, and Listwise. Through the first-of-its-kind comparative evaluation within a consistent experimental framework and considering factors like model size, token consumption, latency, among others, we show that existing approaches are inherently characterised by trade-offs between effectiveness and efficiency. We find that while Pointwise approaches score high on efficiency, they suffer from poor effectiveness. Conversely, Pairwise approaches demonstrate superior effectiveness but incur high computational overhead. Our Setwise approach, instead, reduces the number of LLM inferences and the amount of prompt token consumption during the ranking procedure, compared to previous methods. This significantly improves the efficiency of LLM-based zero-shot ranking, while also retaining high zero-shot ranking effectiveness. We make our code and results publicly available at \\url{https://github.com/ielab/llm-rankers}. \u25b3 Less Submitted 30 May, 2024; v1 submitted 14 October, 2023; originally announced October 2023. Comments: SIGIR2024 full paper arXiv:2309.09403 [pdf, ps, other] Selecting which Dense Retriever to use for Zero-Shot Search Authors: Ekaterina Khramtsova, Shengyao Zhuang, Mahsa Baktashmotlagh, Xi Wang, Guido Zuccon Abstract: We propose the new problem of choosing which dense retrieval model to use when searching on a new collection for which no labels are available, i.e. in a zero-shot setting. Many dense retrieval models are readily available. Each model however is characterized by very differing search effectiveness -- not just on the test portion of the datasets in which the dense representations have been learned\u2026 \u25bd More We propose the new problem of choosing which dense retrieval model to use when searching on a new collection for which no labels are available, i.e. in a zero-shot setting. Many dense retrieval models are readily available. Each model however is characterized by very differing search effectiveness -- not just on the test portion of the datasets in which the dense representations have been learned but, importantly, also across different datasets for which data was not used to learn the dense representations. This is because dense retrievers typically require training on a large amount of labeled data to achieve satisfactory search effectiveness in a specific dataset or domain. Moreover, effectiveness gains obtained by dense retrievers on datasets for which they are able to observe labels during training, do not necessarily generalise to datasets that have not been observed during training. This is however a hard problem: through empirical experimentation we show that methods inspired by recent work in unsupervised performance evaluation with the presence of domain shift in the area of computer vision and machine learning are not effective for choosing highly performing dense retrievers in our setup. The availability of reliable methods for the selection of dense retrieval models in zero-shot settings that do not require the collection of labels for evaluation would allow to streamline the widespread adoption of dense retrieval. This is therefore an important new problem we believe the information retrieval community should consider. Implementation of methods, along with raw result files and analysis scripts are made publicly available at https://www.github.com/anonymized. \u25b3 Less Submitted 17 September, 2023; originally announced September 2023. arXiv:2309.09401 [pdf, other] ChatGPT Hallucinates when Attributing Answers Authors: Guido Zuccon, Bevan Koopman, Razia Shaik Abstract: Can ChatGPT provide evidence to support its answers? Does the evidence it suggests actually exist and does it really support its answer? We investigate these questions using a collection of domain-specific knowledge-based questions, specifically prompting ChatGPT to provide both an answer and supporting evidence in the form of references to external sources. We also investigate how different promp\u2026 \u25bd More Can ChatGPT provide evidence to support its answers? Does the evidence it suggests actually exist and does it really support its answer? We investigate these questions using a collection of domain-specific knowledge-based questions, specifically prompting ChatGPT to provide both an answer and supporting evidence in the form of references to external sources. We also investigate how different prompts impact answers and evidence. We find that ChatGPT provides correct or partially correct answers in about half of the cases (50.6% of the times), but its suggested references only exist 14% of the times. We further provide insights on the generated references that reveal common traits among the references that ChatGPT generates, and show how even if a reference provided by the model does exist, this reference often does not support the claims ChatGPT attributes to it. Our findings are important because (1) they are the first systematic analysis of the references created by ChatGPT in its answers; (2) they suggest that the model may leverage good quality information in producing correct answers, but is unable to attribute real evidence to support its answers. Prompts, raw result files and manual analysis are made publicly available. \u25b3 Less Submitted 17 September, 2023; originally announced September 2023. arXiv:2309.06131 [pdf, other] Annotating Data for Fine-Tuning a Neural Ranker? Current Active Learning Strategies are not Better than Random Selection Authors: Sophia Althammer, Guido Zuccon, Sebastian Hofst\u00e4tter, Suzan Verberne, Allan Hanbury Abstract: Search methods based on Pretrained Language Models (PLM) have demonstrated great effectiveness gains compared to statistical and early neural ranking models. However, fine-tuning PLM-based rankers requires a great amount of annotated training data. Annotating data involves a large manual effort and thus is expensive, especially in domain specific tasks. In this paper we investigate fine-tuning PLM\u2026 \u25bd More Search methods based on Pretrained Language Models (PLM) have demonstrated great effectiveness gains compared to statistical and early neural ranking models. However, fine-tuning PLM-based rankers requires a great amount of annotated training data. Annotating data involves a large manual effort and thus is expensive, especially in domain specific tasks. In this paper we investigate fine-tuning PLM-based rankers under limited training data and budget. We investigate two scenarios: fine-tuning a ranker from scratch, and domain adaptation starting with a ranker already fine-tuned on general data, and continuing fine-tuning on a target dataset. We observe a great variability in effectiveness when fine-tuning on different randomly selected subsets of training data. This suggests that it is possible to achieve effectiveness gains by actively selecting a subset of the training data that has the most positive effect on the rankers. This way, it would be possible to fine-tune effective PLM rankers at a reduced annotation budget. To investigate this, we adapt existing Active Learning (AL) strategies to the task of fine-tuning PLM rankers and investigate their effectiveness, also considering annotation and computational costs. Our extensive analysis shows that AL strategies do not significantly outperform random selection of training subsets in terms of effectiveness. We further find that gains provided by AL strategies come at the expense of more assessments (thus higher annotation costs) and AL strategies underperform random selection when comparing effectiveness given a fixed annotation cost. Our results highlight that ``optimal'' subsets of training data that provide high effectiveness at low annotation cost do exist, but current mainstream AL strategies applied to PLM rankers are not capable of identifying them. \u25b3 Less Submitted 12 September, 2023; originally announced September 2023. Comments: Accepted at SIGIR-AP 2023 arXiv:2309.05238 [pdf, other] Generating Natural Language Queries for More Effective Systematic Review Screening Prioritisation Authors: Shuai Wang, Harrisen Scells, Martin Potthast, Bevan Koopman, Guido Zuccon Abstract: Screening prioritisation in medical systematic reviews aims to rank the set of documents retrieved by complex Boolean queries. Prioritising the most important documents ensures that subsequent review steps can be carried out more efficiently and effectively. The current state of the art uses the final title of the review as a query to rank the documents using BERT-based neural rankers. However, th\u2026 \u25bd More Screening prioritisation in medical systematic reviews aims to rank the set of documents retrieved by complex Boolean queries. Prioritising the most important documents ensures that subsequent review steps can be carried out more efficiently and effectively. The current state of the art uses the final title of the review as a query to rank the documents using BERT-based neural rankers. However, the final title is only formulated at the end of the review process, which makes this approach impractical as it relies on ex post facto information. At the time of screening, only a rough working title is available, with which the BERT-based ranker performs significantly worse than with the final title. In this paper, we explore alternative sources of queries for prioritising screening, such as the Boolean query used to retrieve the documents to be screened and queries generated by instruction-based generative large-scale language models such as ChatGPT and Alpaca. Our best approach is not only viable based on the information available at the time of screening, but also has similar effectiveness to the final title. \u25b3 Less Submitted 23 November, 2023; v1 submitted 11 September, 2023; originally announced September 2023. Comments: Preprints for Accepted paper in SIGIR-AP-2023, note that this is updated from ACM published paper. The working title was wrong in the ACM-published version due to a bug in data preprocessing; however, this does not have any influence on the final conclusion/observation made from the paper arXiv:2307.01565 [pdf, other] An Analysis of Untargeted Poisoning Attack and Defense Methods for Federated Online Learning to Rank Systems Authors: Shuyi Wang, Guido Zuccon Abstract: Federated online learning to rank (FOLTR) aims to preserve user privacy by not sharing their searchable data and search interactions, while guaranteeing high search effectiveness, especially in contexts where individual users have scarce training data and interactions. For this, FOLTR trains learning to rank models in an online manner -- i.e. by exploiting users' interactions with the search syste\u2026 \u25bd More Federated online learning to rank (FOLTR) aims to preserve user privacy by not sharing their searchable data and search interactions, while guaranteeing high search effectiveness, especially in contexts where individual users have scarce training data and interactions. For this, FOLTR trains learning to rank models in an online manner -- i.e. by exploiting users' interactions with the search systems (queries, clicks), rather than labels -- and federatively -- i.e. by not aggregating interaction data in a central server for training purposes, but by training instances of a model on each user device on their own private data, and then sharing the model updates, not the data, across a set of users that have formed the federation. Existing FOLTR methods build upon advances in federated learning. While federated learning methods have been shown effective at training machine learning models in a distributed way without the need of data sharing, they can be susceptible to attacks that target either the system's security or its overall effectiveness. In this paper, we consider attacks on FOLTR systems that aim to compromise their search effectiveness. Within this scope, we experiment with and analyse data and model poisoning attack methods to showcase their impact on FOLTR search effectiveness. We also explore the effectiveness of defense methods designed to counteract attacks on FOLTR systems. We contribute an understanding of the effect of attack and defense methods for FOLTR systems, as well as identifying the key factors influencing their effectiveness. \u25b3 Less Submitted 4 July, 2023; originally announced July 2023. arXiv:2306.17614 [pdf, other] Outcome-based Evaluation of Systematic Review Automation Authors: Wojciech Kusa, Guido Zuccon, Petr Knoth, Allan Hanbury Abstract: Current methods of evaluating search strategies and automated citation screening for systematic literature reviews typically rely on counting the number of relevant and not relevant publications. This established practice, however, does not accurately reflect the reality of conducting a systematic review, because not all included publications have the same influence on the final outcome of the sys\u2026 \u25bd More Current methods of evaluating search strategies and automated citation screening for systematic literature reviews typically rely on counting the number of relevant and not relevant publications. This established practice, however, does not accurately reflect the reality of conducting a systematic review, because not all included publications have the same influence on the final outcome of the systematic review. More specifically, if an important publication gets excluded or included, this might significantly change the overall review outcome, while not including or excluding less influential studies may only have a limited impact. However, in terms of evaluation measures, all inclusion and exclusion decisions are treated equally and, therefore, failing to retrieve publications with little to no impact on the review outcome leads to the same decrease in recall as failing to retrieve crucial publications. We propose a new evaluation framework that takes into account the impact of the reported study on the overall systematic review outcome. We demonstrate the framework by extracting review meta-analysis data and estimating outcome effects using predictions from ranking runs on systematic reviews of interventions from CLEF TAR 2019 shared task. We further measure how closely the obtained outcomes are to the outcomes of the original review if the arbitrary rankings were used. We evaluate 74 runs using the proposed framework and compare the results with those obtained using standard IR measures. We find that accounting for the difference in review outcomes leads to a different assessment of the quality of a system than if traditional evaluation measures were used. Our analysis provides new insights into the evaluation of retrieval results in the context of systematic review automation, emphasising the importance of assessing the usefulness of each document beyond binary relevance. \u25b3 Less Submitted 30 June, 2023; originally announced June 2023. Comments: Accepted at ICTIR2023 arXiv:2306.16680 [pdf, other] Exploring the Representation Power of SPLADE Models Authors: Joel Mackenzie, Shengyao Zhuang, Guido Zuccon Abstract: The SPLADE (SParse Lexical AnD Expansion) model is a highly effective approach to learned sparse retrieval, where documents are represented by term impact scores derived from large language models. During training, SPLADE applies regularization to ensure postings lists are kept sparse -- with the aim of mimicking the properties of natural term distributions -- allowing efficient and effective lexi\u2026 \u25bd More The SPLADE (SParse Lexical AnD Expansion) model is a highly effective approach to learned sparse retrieval, where documents are represented by term impact scores derived from large language models. During training, SPLADE applies regularization to ensure postings lists are kept sparse -- with the aim of mimicking the properties of natural term distributions -- allowing efficient and effective lexical matching and ranking. However, we hypothesize that SPLADE may encode additional signals into common postings lists to further improve effectiveness. To explore this idea, we perform a number of empirical analyses where we re-train SPLADE with different, controlled vocabularies and measure how effective it is at ranking passages. Our findings suggest that SPLADE can effectively encode useful ranking signals in documents even when the vocabulary is constrained to terms that are not traditionally useful for ranking, such as stopwords or even random words. \u25b3 Less Submitted 29 June, 2023; originally announced June 2023. Comments: 4 pages, accepted at ICTIR2023 arXiv:2306.16668 [pdf, other] Beyond CO2 Emissions: The Overlooked Impact of Water Consumption of Information Retrieval Models Authors: Guido Zuccon, Harrisen Scells, Shengyao Zhuang Abstract: As in other fields of artificial intelligence, the information retrieval community has grown interested in investigating the power consumption associated with neural models, particularly models of search. This interest has become particularly relevant as the energy consumption of information retrieval models has risen with new neural models based on large language models, leading to an associated\u2026 \u25bd More As in other fields of artificial intelligence, the information retrieval community has grown interested in investigating the power consumption associated with neural models, particularly models of search. This interest has become particularly relevant as the energy consumption of information retrieval models has risen with new neural models based on large language models, leading to an associated increase of CO2 emissions, albeit relatively low compared to fields such as natural language processing. \u25b3 Less Submitted 28 June, 2023; originally announced June 2023. arXiv:2305.03950 [pdf, other] Augmenting Passage Representations with Query Generation for Enhanced Cross-Lingual Dense Retrieval Authors: Shengyao Zhuang, Linjun Shou, Guido Zuccon Abstract: Effective cross-lingual dense retrieval methods that rely on multilingual pre-trained language models (PLMs) need to be trained to encompass both the relevance matching task and the cross-language alignment task. However, cross-lingual data for training is often scarcely available. In this paper, rather than using more cross-lingual data for training, we propose to use cross-lingual query generati\u2026 \u25bd More Effective cross-lingual dense retrieval methods that rely on multilingual pre-trained language models (PLMs) need to be trained to encompass both the relevance matching task and the cross-language alignment task. However, cross-lingual data for training is often scarcely available. In this paper, rather than using more cross-lingual data for training, we propose to use cross-lingual query generation to augment passage representations with queries in languages other than the original passage language. These augmented representations are used at inference time so that the representation can encode more information across the different target languages. Training of a cross-lingual query generator does not require additional training data to that used for the dense retriever. The query generator training is also effective because the pre-training task for the generator (T5 text-to-text training) is very similar to the fine-tuning task (generation of a query). The use of the generator does not increase query latency at inference and can be combined with any cross-lingual dense retrieval method. Results from experiments on a benchmark cross-lingual information retrieval dataset show that our approach can improve the effectiveness of existing cross-lingual dense retrieval methods. Implementation of our methods, along with all generated query files are made publicly available at https://github.com/ielab/xQG4xDR. \u25b3 Less Submitted 6 May, 2023; originally announced May 2023. Comments: SIGIR2023 short paper arXiv:2304.08138 [pdf, other] Typos-aware Bottlenecked Pre-Training for Robust Dense Retrieval Authors: Shengyao Zhuang, Linjun Shou, Jian Pei, Ming Gong, Houxing Ren, Guido Zuccon, Daxin Jiang Abstract: Current dense retrievers (DRs) are limited in their ability to effectively process misspelled queries, which constitute a significant portion of query traffic in commercial search engines. The main issue is that the pre-trained language model-based encoders used by DRs are typically trained and fine-tuned using clean, well-curated text data. Misspelled queries are typically not found in the data u\u2026 \u25bd More Current dense retrievers (DRs) are limited in their ability to effectively process misspelled queries, which constitute a significant portion of query traffic in commercial search engines. The main issue is that the pre-trained language model-based encoders used by DRs are typically trained and fine-tuned using clean, well-curated text data. Misspelled queries are typically not found in the data used for training these models, and thus misspelled queries observed at inference time are out-of-distribution compared to the data used for training and fine-tuning. Previous efforts to address this issue have focused on \\textit{fine-tuning} strategies, but their effectiveness on misspelled queries remains lower than that of pipelines that employ separate state-of-the-art spell-checking components. To address this challenge, we propose ToRoDer (TypOs-aware bottlenecked pre-training for RObust DEnse Retrieval), a novel re-training strategy for DRs that increases their robustness to misspelled queries while preserving their effectiveness in downstream retrieval tasks. ToRoDer utilizes an encoder-decoder architecture where the encoder takes misspelled text with masked tokens as input and outputs bottlenecked information to the decoder. The decoder then takes as input the bottlenecked embeddings, along with token embeddings of the original text with the misspelled tokens masked out. The pre-training task is to recover the masked tokens for both the encoder and decoder. Our extensive experimental results and detailed ablation studies show that DRs pre-trained with ToRoDer exhibit significantly higher effectiveness on misspelled queries, sensibly closing the gap with pipelines that use a separate, complex spell-checker component, while retaining their effectiveness on correctly spelled queries. \u25b3 Less Submitted 26 November, 2023; v1 submitted 17 April, 2023; originally announced April 2023. Comments: 10 pages, accepted at SIGIR-AP arXiv:2302.13793 [pdf, other] Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness Authors: Guido Zuccon, Bevan Koopman Abstract: Generative pre-trained language models (GPLMs) like ChatGPT encode in the model's parameters knowledge the models observe during the pre-training phase. This knowledge is then used at inference to address the task specified by the user in their prompt. For example, for the question-answering task, the GPLMs leverage the knowledge and linguistic patterns learned at training to produce an answer to\u2026 \u25bd More Generative pre-trained language models (GPLMs) like ChatGPT encode in the model's parameters knowledge the models observe during the pre-training phase. This knowledge is then used at inference to address the task specified by the user in their prompt. For example, for the question-answering task, the GPLMs leverage the knowledge and linguistic patterns learned at training to produce an answer to a user question. Aside from the knowledge encoded in the model itself, answers produced by GPLMs can also leverage knowledge provided in the prompts. For example, a GPLM can be integrated into a retrieve-then-generate paradigm where a search engine is used to retrieve documents relevant to the question; the content of the documents is then transferred to the GPLM via the prompt. In this paper we study the differences in answer correctness generated by ChatGPT when leveraging the model's knowledge alone vs. in combination with the prompt knowledge. We study this in the context of consumers seeking health advice from the model. Aside from measuring the effectiveness of ChatGPT in this context, we show that the knowledge passed in the prompt can overturn the knowledge encoded in the model and this is, in our experiments, to the detriment of answer correctness. This work has important implications for the development of more robust and transparent question-answering systems based on generative pre-trained language models. \u25b3 Less Submitted 23 February, 2023; originally announced February 2023. arXiv:2302.03495 [pdf, other] Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search? Authors: Shuai Wang, Harrisen Scells, Bevan Koopman, Guido Zuccon Abstract: Systematic reviews are comprehensive reviews of the literature for a highly focused research question. These reviews are often treated as the highest form of evidence in evidence-based medicine, and are the key strategy to answer research questions in the medical field. To create a high-quality systematic review, complex Boolean queries are often constructed to retrieve studies for the review topi\u2026 \u25bd More Systematic reviews are comprehensive reviews of the literature for a highly focused research question. These reviews are often treated as the highest form of evidence in evidence-based medicine, and are the key strategy to answer research questions in the medical field. To create a high-quality systematic review, complex Boolean queries are often constructed to retrieve studies for the review topic. However, it often takes a long time for systematic review researchers to construct a high quality systematic review Boolean query, and often the resulting queries are far from effective. Poor queries may lead to biased or invalid reviews, because they missed to retrieve key evidence, or to extensive increase in review costs, because they retrieved too many irrelevant studies. Recent advances in Transformer-based generative models have shown great potential to effectively follow instructions from users and generate answers based on the instructions being made. In this paper, we investigate the effectiveness of the latest of such models, ChatGPT, in generating effective Boolean queries for systematic review literature search. Through a number of extensive experiments on standard test collections for the task, we find that ChatGPT is capable of generating queries that lead to high search precision, although trading-off this for recall. Overall, our study demonstrates the potential of ChatGPT in generating effective Boolean queries for systematic review literature search. The ability of ChatGPT to follow complex instructions and generate queries with high precision makes it a valuable tool for researchers conducting systematic reviews, particularly for rapid reviews where time is a constraint and often trading-off higher precision for lower recall is acceptable. \u25b3 Less Submitted 9 February, 2023; v1 submitted 2 February, 2023; originally announced February 2023. arXiv:2212.10762 [pdf, other] AgAsk: An Agent to Help Answer Farmer's Questions From Scientific Documents Authors: Bevan Koopman, Ahmed Mourad, Hang Li, Anton van der Vegt, Shengyao Zhuang, Simon Gibson, Yash Dang, David Lawrence, Guido Zuccon Abstract: Decisions in agriculture are increasingly data-driven; however, valuable agricultural knowledge is often locked away in free-text reports, manuals and journal articles. Specialised search systems are needed that can mine agricultural information to provide relevant answers to users' questions. This paper presents AgAsk -- an agent able to answer natural language agriculture questions by mining sci\u2026 \u25bd More Decisions in agriculture are increasingly data-driven; however, valuable agricultural knowledge is often locked away in free-text reports, manuals and journal articles. Specialised search systems are needed that can mine agricultural information to provide relevant answers to users' questions. This paper presents AgAsk -- an agent able to answer natural language agriculture questions by mining scientific documents. We carefully survey and analyse farmers' information needs. On the basis of these needs we release an information retrieval test collection comprising real questions, a large collection of scientific documents split in passages, and ground truth relevance assessments indicating which passages are relevant to each question. We implement and evaluate a number of information retrieval models to answer farmers questions, including two state-of-the-art neural ranking models. We show that neural rankers are highly effective at matching passages to questions in this context. Finally, we propose a deployment architecture for AgAsk that includes a client based on the Telegram messaging platform and retrieval model deployed on commodity hardware. The test collection we provide is intended to stimulate more research in methods to match natural language to answers in scientific documents. While the retrieval models were evaluated in the agriculture domain, they are generalisable and of interest to others working on similar problems. The test collection is available at: \\url{https://github.com/ielab/agvaluate}. \u25b3 Less Submitted 20 December, 2022; originally announced December 2022. Comments: 17 pages, submitted to IJDL arXiv:2212.09018 [pdf, other] MeSH Suggester: A Library and System for MeSH Term Suggestion for Systematic Review Boolean Query Construction Authors: Shuai Wang, Hang Li, Guido Zuccon Abstract: Boolean query construction is often critical for medical systematic review literature search. To create an effective Boolean query, systematic review researchers typically spend weeks coming up with effective query terms and combinations. One challenge to creating an effective systematic review Boolean query is the selection of effective MeSH Terms to include in the query. In our previous work, we\u2026 \u25bd More Boolean query construction is often critical for medical systematic review literature search. To create an effective Boolean query, systematic review researchers typically spend weeks coming up with effective query terms and combinations. One challenge to creating an effective systematic review Boolean query is the selection of effective MeSH Terms to include in the query. In our previous work, we created neural MeSH term suggestion methods and compared them to state-of-the-art MeSH term suggestion methods. We found neural MeSH term suggestion methods to be highly effective. In this demonstration, we build upon our previous work by creating (1) a Web-based MeSH term suggestion prototype system that allows users to obtain suggestions from a number of underlying methods and (2) a Python library that implements ours and others' MeSH term suggestion methods and that is aimed at researchers who want to further investigate, create or deploy such type of methods. We describe the architecture of the web-based system and how to use it for the MeSH term suggestion task. For the Python library, we describe how the library can be used for advancing further research and experimentation, and we validate the results of the methods contained in the library on standard datasets. Our web-based prototype system is available at http://ielab-mesh-suggest.uqcloud.net, while our Python library is at https://github.com/ielab/meshsuggestlib. \u25b3 Less Submitted 18 December, 2022; originally announced December 2022. arXiv:2212.09017 [pdf, other] Neural Rankers for Effective Screening Prioritisation in Medical Systematic Review Literature Search Authors: Shuai Wang, Harrisen Scells, Bevan Koopman, Guido Zuccon Abstract: Medical systematic reviews typically require assessing all the documents retrieved by a search. The reason is two-fold: the task aims for ``total recall''; and documents retrieved using Boolean search are an unordered set, and thus it is unclear how an assessor could examine only a subset. Screening prioritisation is the process of ranking the (unordered) set of retrieved documents, allowing asses\u2026 \u25bd More Medical systematic reviews typically require assessing all the documents retrieved by a search. The reason is two-fold: the task aims for ``total recall''; and documents retrieved using Boolean search are an unordered set, and thus it is unclear how an assessor could examine only a subset. Screening prioritisation is the process of ranking the (unordered) set of retrieved documents, allowing assessors to begin the downstream processes of the systematic review creation earlier, leading to earlier completion of the review, or even avoiding screening documents ranked least relevant. Screening prioritisation requires highly effective ranking methods. Pre-trained language models are state-of-the-art on many IR tasks but have yet to be applied to systematic review screening prioritisation. In this paper, we apply several pre-trained language models to the systematic review document ranking task, both directly and fine-tuned. An empirical analysis compares how effective neural methods compare to traditional methods for this task. We also investigate different types of document representations for neural methods and their impact on ranking performance. Our results show that BERT-based rankers outperform the current state-of-the-art screening prioritisation methods. However, BERT rankers and existing methods can actually be complementary, and thus, further improvements may be achieved if used in conjunction. \u25b3 Less Submitted 18 December, 2022; originally announced December 2022. arXiv:2211.16101 [pdf, other] Dependency-aware Self-training for Entity Alignment Authors: Bing Liu, Tiancheng Lan, Wen Hua, Guido Zuccon Abstract: Entity Alignment (EA), which aims to detect entity mappings (i.e. equivalent entity pairs) in different Knowledge Graphs (KGs), is critical for KG fusion. Neural EA methods dominate current EA research but still suffer from their reliance on labelled mappings. To solve this problem, a few works have explored boosting the training of EA models with self-training, which adds confidently predicted ma\u2026 \u25bd More Entity Alignment (EA), which aims to detect entity mappings (i.e. equivalent entity pairs) in different Knowledge Graphs (KGs), is critical for KG fusion. Neural EA methods dominate current EA research but still suffer from their reliance on labelled mappings. To solve this problem, a few works have explored boosting the training of EA models with self-training, which adds confidently predicted mappings into the training data iteratively. Though the effectiveness of self-training can be glimpsed in some specific settings, we still have very limited knowledge about it. One reason is the existing works concentrate on devising EA models and only treat self-training as an auxiliary tool. To fill this knowledge gap, we change the perspective to self-training to shed light on it. In addition, the existing self-training strategies have limited impact because they introduce either much False Positive noise or a low quantity of True Positive pseudo mappings. To improve self-training for EA, we propose exploiting the dependencies between entities, a particularity of EA, to suppress the noise without hurting the recall of True Positive mappings. Through extensive experiments, we show that the introduction of dependency makes the self-training strategy for EA reach a new level. The value of self-training in alleviating the reliance on annotation is actually much higher than what has been realised. Furthermore, we suggest future study on smart data annotation to break the ceiling of EA performance. \u25b3 Less Submitted 29 November, 2022; originally announced November 2022. Comments: WSDM 2023 arXiv:2211.15833 [pdf, other] Guiding Neural Entity Alignment with Compatibility Authors: Bing Liu, Harrisen Scells, Wen Hua, Guido Zuccon, Genghong Zhao, Xia Zhang Abstract: Entity Alignment (EA) aims to find equivalent entities between two Knowledge Graphs (KGs). While numerous neural EA models have been devised, they are mainly learned using labelled data only. In this work, we argue that different entities within one KG should have compatible counterparts in the other KG due to the potential dependencies among the entities. Making compatible predictions thus should\u2026 \u25bd More Entity Alignment (EA) aims to find equivalent entities between two Knowledge Graphs (KGs). While numerous neural EA models have been devised, they are mainly learned using labelled data only. In this work, we argue that different entities within one KG should have compatible counterparts in the other KG due to the potential dependencies among the entities. Making compatible predictions thus should be one of the goals of training an EA model along with fitting the labelled data: this aspect however is neglected in current methods. To power neural EA models with compatibility, we devise a training framework by addressing three problems: (1) how to measure the compatibility of an EA model; (2) how to inject the property of being compatible into an EA model; (3) how to optimise parameters of the compatibility model. Extensive experiments on widely-used datasets demonstrate the advantages of integrating compatibility within EA models. In fact, state-of-the-art neural EA models trained within our framework using just 5\\% of the labelled data can achieve comparable effectiveness with supervised training using 20\\% of the labelled data. \u25b3 Less Submitted 28 November, 2022; originally announced November 2022. Comments: EMNLP 2022 arXiv:2209.08687 [pdf, other] Automated MeSH Term Suggestion for Effective Query Formulation in Systematic Reviews Literature Search Authors: Shuai Wang, Harrisen Scells, Bevan Koopman, Guido Zuccon Abstract: High-quality medical systematic reviews require comprehensive literature searches to ensure the recommendations and outcomes are sufficiently reliable. Indeed, searching for relevant medical literature is a key phase in constructing systematic reviews and often involves domain (medical researchers) and search (information specialists) experts in developing the search queries. Queries in this conte\u2026 \u25bd More High-quality medical systematic reviews require comprehensive literature searches to ensure the recommendations and outcomes are sufficiently reliable. Indeed, searching for relevant medical literature is a key phase in constructing systematic reviews and often involves domain (medical researchers) and search (information specialists) experts in developing the search queries. Queries in this context are highly complex, based on Boolean logic, include free-text terms and index terms from standardised terminologies (e.g., the Medical Subject Headings (MeSH) thesaurus), and are difficult and time-consuming to build. The use of MeSH terms, in particular, has been shown to improve the quality of the search results. However, identifying the correct MeSH terms to include in a query is difficult: information experts are often unfamiliar with the MeSH database and unsure about the appropriateness of MeSH terms for a query. Naturally, the full value of the MeSH terminology is often not fully exploited. This article investigates methods to suggest MeSH terms based on an initial Boolean query that includes only free-text terms. In this context, we devise lexical and pre-trained language models based methods. These methods promise to automatically identify highly effective MeSH terms for inclusion in a systematic review query. Our study contributes an empirical evaluation of several MeSH term suggestion methods. We further contribute an extensive analysis of MeSH term suggestions for each method and how these suggestions impact the effectiveness of Boolean queries. \u25b3 Less Submitted 18 September, 2022; originally announced September 2022. Comments: This paper is currently in submission with Intelligent Systems with Applications Journal Technology-Assisted Review Systems Special issue and is under peer review. arXiv admin note: text overlap with arXiv:2112.00277 arXiv:2208.10366 [pdf, other] High-quality Task Division for Large-scale Entity Alignment Authors: Bing Liu, Wen Hua, Guido Zuccon, Genghong Zhao, Xia Zhang Abstract: Entity Alignment (EA) aims to match equivalent entities that refer to the same real-world objects and is a key step for Knowledge Graph (KG) fusion. Most neural EA models cannot be applied to large-scale real-life KGs due to their excessive consumption of GPU memory and time. One promising solution is to divide a large EA task into several subtasks such that each subtask only needs to match two sm\u2026 \u25bd More Entity Alignment (EA) aims to match equivalent entities that refer to the same real-world objects and is a key step for Knowledge Graph (KG) fusion. Most neural EA models cannot be applied to large-scale real-life KGs due to their excessive consumption of GPU memory and time. One promising solution is to divide a large EA task into several subtasks such that each subtask only needs to match two small subgraphs of the original KGs. However, it is challenging to divide the EA task without losing effectiveness. Existing methods display low coverage of potential mappings, insufficient evidence in context graphs, and largely differing subtask sizes. In this work, we design the DivEA framework for large-scale EA with high-quality task division. To include in the EA subtasks a high proportion of the potential mappings originally present in the large EA task, we devise a counterpart discovery method that exploits the locality principle of the EA task and the power of trained EA models. Unique to our counterpart discovery method is the explicit modelling of the chance of a potential mapping. We also introduce an evidence passing mechanism to quantify the informativeness of context entities and find the most informative context graphs with flexible control of the subtask size. Extensive experiments show that DivEA achieves higher EA performance than alternative state-of-the-art solutions. \u25b3 Less Submitted 22 August, 2022; originally announced August 2022. arXiv:2207.04220 [pdf, other] Rethinking Persistent Homology for Visual Recognition Authors: Ekaterina Khramtsova, Guido Zuccon, Xi Wang, Mahsa Baktashmotlagh Abstract: Persistent topological properties of an image serve as an additional descriptor providing an insight that might not be discovered by traditional neural networks. The existing research in this area focuses primarily on efficiently integrating topological properties of the data in the learning process in order to enhance the performance. However, there is no existing study to demonstrate all possibl\u2026 \u25bd More Persistent topological properties of an image serve as an additional descriptor providing an insight that might not be discovered by traditional neural networks. The existing research in this area focuses primarily on efficiently integrating topological properties of the data in the learning process in order to enhance the performance. However, there is no existing study to demonstrate all possible scenarios where introducing topological properties can boost or harm the performance. This paper performs a detailed analysis of the effectiveness of topological properties for image classification in various training scenarios, defined by: the number of training samples, the complexity of the training data and the complexity of the backbone network. We identify the scenarios that benefit the most from topological features, e.g., training simple networks on small datasets. Additionally, we discuss the problem of topological consistency of the datasets which is one of the major bottlenecks for using topological features for classification. We further demonstrate how the topological inconsistency can harm the performance for certain scenarios. \u25b3 Less Submitted 5 March, 2023; v1 submitted 9 July, 2022; originally announced July 2022. Comments: ICML 2022 Workshop on Topology, Algebra, and Geometry in Machine Learning"
  },
  {
    "url": "https://arxiv.org/format/2503.14802",
    "title": "arXiv.org >\n2503.14802 > format",
    "author": "Unknown",
    "body": "Help | Advanced Search Note: Many of the formats above are served gzipped (Content-Encoding: x-gzip). Your browser may silently uncompress after downloading so the files you see saved may appear uncompressed. Please report any problems to arXiv technical support and include the paper identifier. arXiv Operational Status Get status notifications via email or slack"
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Zaoad,+M+S",
    "title": "Showing 1\u20131 of 1 results for author: Zaoad, M S",
    "author": "Authors:\nMd Shahir Zaoad, \n      \n      Niamat Zawad, \n      \n      Priyanka Ranade, \n      \n      Richard Krogman, \n      \n      Latifur Khan, \n      \n      James Holt",
    "body": "arXiv:2503.14802 [pdf, other] Graph-Based Re-ranking: Emerging Techniques, Limitations, and Opportunities Authors: Md Shahir Zaoad, Niamat Zawad, Priyanka Ranade, Richard Krogman, Latifur Khan, James Holt Abstract: Knowledge graphs have emerged to be promising datastore candidates for context augmentation during Retrieval Augmented Generation (RAG). As a result, techniques in graph representation learning have been simultaneously explored alongside principal neural information retrieval approaches, such as two-phased retrieval, also known as re-ranking. While Graph Neural Networks (GNNs) have been proposed t\u2026 \u25bd More Knowledge graphs have emerged to be promising datastore candidates for context augmentation during Retrieval Augmented Generation (RAG). As a result, techniques in graph representation learning have been simultaneously explored alongside principal neural information retrieval approaches, such as two-phased retrieval, also known as re-ranking. While Graph Neural Networks (GNNs) have been proposed to demonstrate proficiency in graph learning for re-ranking, there are ongoing limitations in modeling and evaluating input graph structures for training and evaluation for passage and document ranking tasks. In this survey, we review emerging GNN-based ranking model architectures along with their corresponding graph representation construction methodologies. We conclude by providing recommendations on future research based on community-wide challenges and opportunities. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. MSC Class: N/A"
  },
  {
    "url": "https://arxiv.org/html/2503.14802v1",
    "title": "Graph-Based Re-ranking: \nEmerging Techniques, Limitations, and Opportunities",
    "author": "Graph-Based Re-ranking: \nEmerging Techniques, Limitations, and Opportunities\n\n\n\nMd Shahir Zaoad*1\n\n\u2003\u2003\nNiamat Zawad*1\n\n\u2003\u2003\nPriyanka Ranade2\n\n\u2003\u2003\nRichard Krogman2\n\n\u2003\u2003\nLatifur Khan1\n\n\u2003\u2003\nJames Holt2\n1Department of Computer Science, University of Texas, Dallas, USA.\n2 The Laboratory for Physical Sciences, Baltimore, MD, USA.\n111Equal Contribution.\n{mxz230002, nxz190009, lkhan}@utdallas.edu,\n{psranad, holt,rokrogm}@lps.umd.edu\n\n\n\n\nAbstract\nKnowledge graphs have emerged to be promising datastore candidates for context augmentation during Retrieval Augmented Generation (RAG). As a result, techniques in graph representation learning have been simultaneously explored alongside principal neural information retrieval approaches, such as two-phased retrieval, also known as re-ranking. While Graph Neural Networks (GNNs) have been proposed to demonstrate proficiency in graph learning for re-ranking, there are ongoing limitations in modeling and evaluating input graph structures for training and evaluation for passage and document ranking tasks. In this survey, we review emerging GNN-based ranking model architectures along with their corresponding graph representation construction methodologies. We conclude by providing recommendations on future research based on community-wide challenges and opportunities.\n\n\n\n1 Introduction\n\nRetrieval Augmented Generation (RAG) is an established research area that combines pretrained parametric and non-parametric memory for downstream language generation Lewis et al. (2020). Recently, there has been an emergence of using Knowledge Graphs as the external non-parametric datastore, in which structural information is queried to capture relational knowledge Dong et al. (2024b). Graph-RAG approaches typically follow a two-phased retrieval procedure, also known as re-ranking Peng et al. (2024). Two-phased retrieval approaches consist of a primary retrieval method in which given a search query, an initial set of probable responses is fetched using techniques such as approximate nearest neighbor (ANN) indexing, traditional keyword-based search, or embedding-based retrieval. The initial retrieval process generally often prioritizes compute efficiency over perfect accuracy, leading to prompting with irrelevant, noisy context and increased hallucination to the final output Glass et al. (2022). In a two-phase setup, re-ranking methods distill the initial set of retrieved documents by re-scoring the initial list according to a refined relevance score.\n\n\nQuerying the complex structure of the knowledge graph presents challenges for popular Large Language Model (LLM)-based re-ranker models. This challenge has inspired recent research in exploring the potential of Graph Neural Networks (GNNs) for exploiting structural information across entities and capturing relational knowledge for prompting a language model for generation.\n\n\nFor effective graph-based re-ranking, researchers have developed expansive and specialized methods focused primarily on developing (1) unique GNN model architectures and (2) constructing unique graph representation structures specifically optimized for retrieval tasks. However, many of these methods have only recently emerged, leading to gaps in community-wide evaluation of model architectures and best practices. Additionally, these methods have not yet been collated into a systematic review. To fill this gap, we provide a comprehensive overview of emerging GNN-based ranking model architectures and their corresponding graph representation construction methodologies. We conclude by providing recommendations on future research based on our findings and analysis of limitations.\n\n\n\n\n2 Graph-Based Retrieval and Re-ranking\n\nRe-ranking problems abstractly consider a situation where there is a query q\ud835\udc5eqitalic_q and a finite set D={di}i\u2208[n]\ud835\udc37subscriptsubscript\ud835\udc51\ud835\udc56\ud835\udc56delimited-[]\ud835\udc5bD=\\{d_{i}\\}_{i\\in[n]}italic_D = { italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i \u2208 [ italic_n ] end_POSTSUBSCRIPT of documents, assumed to have been selected a priori from an arbitrary retrieval process.\nThe goal is to obtain a re-ranking of the elements of D\ud835\udc37Ditalic_D, via a scoring function rq:D\u2192(0,1):subscript\ud835\udc5f\ud835\udc5e\u2192\ud835\udc3701r_{q}:D\\rightarrow(0,1)italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT : italic_D \u2192 ( 0 , 1 ) where rq\u2062(di)\u2208(0,1)subscript\ud835\udc5f\ud835\udc5esubscript\ud835\udc51\ud835\udc5601r_{q}(d_{i})\\in(0,1)italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) \u2208 ( 0 , 1 ) represents the degree of the relevance of the document disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to the given query q\ud835\udc5eqitalic_q.\nThe re-ranked document is the new sequence D\u2032superscript\ud835\udc37\u2032D^{\\prime}italic_D start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT consisting of the elements disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT sorted in order of increasing relevance score rq\u2062(di)subscript\ud835\udc5f\ud835\udc5esubscript\ud835\udc51\ud835\udc56r_{q}(d_{i})italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). The re-ranking process is popularly executed through neural language modeling methods. However, an emerging research area involves using graph models such as Graph Neural Networks (GNNs) for re-ranking. In this scenario, for each pairing of the query q\ud835\udc5eqitalic_q and a document d\u2208D\ud835\udc51\ud835\udc37d\\in Ditalic_d \u2208 italic_D, we may attribute a graph representation G=(V,E)\ud835\udc3a\ud835\udc49\ud835\udc38G=(V,E)italic_G = ( italic_V , italic_E ) that may be specified in multiple ways.\n\n\nAfter a graph representation G\ud835\udc3aGitalic_G of the data is established, one may then initialize a randomized feature representation on the nodes and adjacency matrix.\nA GNN can then be applied to this data to obtain a higher level representation of the nodes, condensed into a single feature vector (often mean-pooling) to obtain a uniform representation for the total graph. This representation is used in the computation of the relevance score rq\u2062(d)subscript\ud835\udc5f\ud835\udc5e\ud835\udc51r_{q}(d)italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( italic_d ). An overview of the components of a graph-based re-ranking pipeline is displayed in Figure 1.\n\n\nFigure 1: The overall re-ranking pipeline. (1) The input query is passed to the retriever, (2) which then retrieves the top n\ud835\udc5bnitalic_n documents. (3) The query and the documents are processed by the encoder. (4) The embeddings are passed to the relevance matcher for similarity calculations (5) which are then utilized for creating edges between nodes. (6) The graph information is then passed to the re-ranker to (7) get the final re-ranked list of documents.\n\n\n\n\n3 Re-ranking Datasets\n\nThere are a number of generic Information Retrieval (IR) datasets that have been well-studied and long term standards for a wide range of downstream tasks including passage & document ranking and question-answering Thakur et al. . This section provides a broad overview of a select subset of popular examples that are modified for training and evaluating graph-based re-ranking techniques discussed in the rest of this paper.\n\n\n\n3.0.1 Passage and Document Retrieval\n\nRe-ranking is typically presented as a subtask within generic passage and document-level retrieval benchmarks Bajaj et al. (2016). Document retrieval tasks aim to rank a set of documents based on similarity to a query, while passage retrieval tasks aim to rank relevant document-parts or pieces of text (passages) rather than an entire ranked set of documents.\n\n\nThe Microsoft Machine Reading Comprehension (MS MARCO) dataset is an example of a benchmark that provides test and evaluation datasets for both document and passage-level retrieval tasks Bajaj et al. (2016). MSMARCO is derived from sampled Bing search queries and corresponding web pages and is a widely used benchmark for passage and document ranking. MSMARCO has established itself as a standard in the re-ranking community. It has been reused within several community-wide shared tasks such as those presented Text REtrieval Conference (TREC) Voorhees and Harman (2005). Similar document and passage-level datasets have been developed for domain-specific retrieval tasks. For example, CLEF-IP Piroi et al. (2011) is a domain-specific retrieval dataset that focuses on information retrieval within the Intellectual Property (IP) domain.\n\n\n\n\n3.0.2 Ranking Algorithms\n\nRanking algorithms can generally be categorized into Pointwise, Pairwise, and Listwise approaches Cao et al. (2007). The difference among the three categories depend on the number of documents considered at a time in the loss function during training. Pointwise approaches consider single document similarity, Pairwise focus on similarity of a pair of documents to a query, and lastly Listwise approaches consider a similarity of a list of documents Cao et al. (2007). In the next section, we document current graph-based re-ranking models, organized in the above categories.\n\n\n\n\n\n4 Graph Re-ranking Models\n\nThe widespread impact of Pre-trained Language Models (PLM) has also extended to the information retrieval field. In recent years, IR has greatly benefited from various state-of-the-art (SOTA) retrieval frameworks. However, graph-based retrieval remains relatively unexplored in the PLM era. Therefore, we aim to present a comprehensive overview of contemporary graph-based retrieval methods, emphasizing but not limited to PLMs. Table 1 presents a summary of these approaches, focusing on the key aspects of both the graph-based and IR approaches. In the table, the Task represents the retrieval goal (Document/Passage retrieval). Point, Pair, and List stands for the respective re-ranking strategies, are defined in Section 3.0.2. Doc, Doc-part, and Entity are named based on the node\u2019s content. Finally, the Relation column indicates whether the approach directly incorporates inter-document or inter-passage relationships in the re-ranking process.\n\n\n\n\n\nModel\nTask\nPoint\nPair\nList\nDoc\nDoc-part\nEntity\nRelation\n\n\n\n\nG-RAG Dong et al. (2024a)\n\nD\n\u2713\n\n\n\u2713\n\n\nY\n\n\nGNRR Di\u00a0Francesco et al. (2024)\n\nD\n\u2713\n\n\n\u2713\n\n\nY\n\n\nGAR MacAvaney et al. (2022b)\n\nD\n\u2713\n\n\n\u2713\n\n\nN\n\n\nMiM-LiM Albarede et al. (2022)\n\nP\n\u2713\n\n\n\u2713\n\n\nY\n\n\nGCN-reRanker Vollmers et al. \n\nD\n\u2713\n\n\n\n\n\u2713\nY\n\n\nIDRQA Zhang et al. (2021)\n\nD\n\u2713\n\n\n\n\n\u2713\nY\n\n\nKERM Dong et al. (2022)\n\nP\n\u2713\n\n\n\n\n\u2713\nY\n\n\nTRM Veningston and Shanmugalakshmi (2014)\n\nD\n\u2713\n\n\n\n\u2713\n\nY\n\n\nPRP-Graph Luo et al. (2024)\n\nD\n\n\u2713\n\n\u2713\n\n\nY\n\n\nQDG Frayling et al. (2024)\n\nD\n\u2713\n\n\n\n\u2713\n\nN\n\n\nKGPR Fang et al. (2023)\n\nP\n\u2713\n\n\n\n\n\u2713\nY\n\n\nSPR-PageRank Gienapp et al. (2022)\n\nD\n\n\u2713\n\n\u2713\n\n\nY\n\n\nGraphMonoT5 Gupta and Demner-Fushman (2024)\n\nD\n\u2713\n\n\n\n\n\u2713\nN\n\n\nPassageRankReed and Madabushi (2020)\n\nD\n\u2713\n\n\n\n\u2713\n\nN\n\n\nKG-FiD Yu et al. (2021)\n\nP\n\u2713\n\n\n\n\n\u2713\nY\n\n\nGARAgent MacAvaney et al. (2022a)\n\nD\n\u2713\n\n\n\u2713\n\n\nN\n\n\nFairness-Aware Jaenich et al. (2024)\n\nD\n\u2713\n\n\n\u2713\n\n\nN\n\n\nDoc-CohesionSarwar and O\u2019Riordan (2021)\n\nD\n\u2713\n\n\n\n\u2713\n\nY\n\n\nSlideGARRathee et al. (2025)\n\nD\n\n\n\u2713\n\u2713\n\n\nY\n\n\nGBRM Deng et al. (2009)\n\nD\n\u2713\n\n\n\u2713\n\n\nY\n\n\n\nTable 1: Comparative Overview of Graph-Based Re-Ranking Techniques. In the header, D = Document, P = Passage, Y = Yes, N = No.\n\n\n\n4.1 Pointwise Re-ranking\n\nPassageRank Reed and Madabushi (2020) a graph-based passage ranking pipeline represents each passage as a node while the edge is represented by similarity scores between the nodes in a directed graph. Scores of each node Visubscript\ud835\udc49\ud835\udc56V_{i}italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is calculated as,\n\n\n\n\n\nW\u2062(Vi)=(1\u2212d)+d\u2062\u2211Vj\u2208I\u2062n\u2062(Vi)wj\u2062i\u00d7W\u2062(Vj)\u2211Vk\u2208O\u2062u\u2062t\u2062(Vj)wj\u2062k\ud835\udc4asubscript\ud835\udc49\ud835\udc561\ud835\udc51\ud835\udc51subscriptsubscript\ud835\udc49\ud835\udc57\ud835\udc3c\ud835\udc5bsubscript\ud835\udc49\ud835\udc56subscript\ud835\udc64\ud835\udc57\ud835\udc56\ud835\udc4asubscript\ud835\udc49\ud835\udc57subscriptsubscript\ud835\udc49\ud835\udc58\ud835\udc42\ud835\udc62\ud835\udc61subscript\ud835\udc49\ud835\udc57subscript\ud835\udc64\ud835\udc57\ud835\udc58W(V_{i})=(1-d)+d\\sum_{V_{j}\\in In(V_{i})}\\frac{w_{ji}\\times W(V_{j})}{\\sum_{V_%\n{k}\\in Out(V_{j})}w_{jk}}italic_W ( italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = ( 1 - italic_d ) + italic_d \u2211 start_POSTSUBSCRIPT italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u2208 italic_I italic_n ( italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT divide start_ARG italic_w start_POSTSUBSCRIPT italic_j italic_i end_POSTSUBSCRIPT \u00d7 italic_W ( italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_V start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2208 italic_O italic_u italic_t ( italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_j italic_k end_POSTSUBSCRIPT end_ARG\n\n(1)\n\n\n\n\nwhere I\u2062n\u2062(Vi)\ud835\udc3c\ud835\udc5bsubscript\ud835\udc49\ud835\udc56In(V_{i})italic_I italic_n ( italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) is set of vertices pointing toward Visubscript\ud835\udc49\ud835\udc56V_{i}italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, O\u2062u\u2062t\u2062(Vi)\ud835\udc42\ud835\udc62\ud835\udc61subscript\ud835\udc49\ud835\udc56Out(V_{i})italic_O italic_u italic_t ( italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) is set of vertices where Visubscript\ud835\udc49\ud835\udc56V_{i}italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT points to, wj\u2062isubscript\ud835\udc64\ud835\udc57\ud835\udc56w_{ji}italic_w start_POSTSUBSCRIPT italic_j italic_i end_POSTSUBSCRIPT is the weight between node i and j, and d\ud835\udc51ditalic_d is the damping factor. Finally, a Bidirectional Encoder Representations from Transformers (BERT) encoder model re-ranks the documents Devlin (2018).\n\n\nSimilar to the previous pointwise approach, traditional two-phased retrievers suffer from recall limitations where the re-ranker\u2019s performance is subjected to the initially retrieved documents. To address this, MacAvaney et al. (2022b) propose Graph Adaptive Re-ranking (GAR), based on the clustering hypothesis Jardine and van Rijsbergen (1971), which suggest closely related documents being relevant to a given query. The authors implement a feedback process to incrementally update the pool of candidate documents with the neighbors from a corpus graph, a directed graph encoding the similarity between documents. This allows for the re-ranking of documents otherwise skipped due to the re-ranking budget. GAR takes an initial pool of candidate documents R0subscript\ud835\udc450R_{0}italic_R start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, a batch size b\ud835\udc4fbitalic_b, a re-ranking budget RBsubscript\ud835\udc45\ud835\udc35R_{B}italic_R start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT, and a corpus graph as input. The output of the process is a re-ranked pool R1subscript\ud835\udc451R_{1}italic_R start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. The re-ranking further leverages a dynamically updated re-ranking pool P\ud835\udc43Pitalic_P and graph frontier F\ud835\udc39Fitalic_F. P\ud835\udc43Pitalic_P is initialized with the document pool and the F\ud835\udc39Fitalic_F is kept empty. At each iteration, top-b\ud835\udc4fbitalic_b documents are re-ranked, represented by B\ud835\udc35Bitalic_B. Consequently, the neighbors of B\ud835\udc35Bitalic_B are extracted from the corpus graph and are inserted into F\ud835\udc39Fitalic_F. The process repeats, however, instead of only scoring top-b\ud835\udc4fbitalic_b documents from the initial candidate pool the process scores documents alternatively from the initial candidate pool and the frontier. This process continues until the re-ranking budget permits.\n\n\nAnother work Jaenich et al. (2024) extends GAR while focusing on fair exposure of individual document groups in the candidate list. This experiment leverages groups from an already labeled dataset. The proposed approach modifies the original GAR based on two categories of policies: the pre-retrieval policies concerned with modifying the corpus graph and the in-process policies that modify the GAR pipeline itself. Policy-1 dictates that only documents from different groups can share an edge in the corpus graph. Policy-2 relaxes policy-1 by allowing a limited number of documents from each group in the corpus graph. Policy-3 modifies the set of neighbors corresponding to each re-ranked document, disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT in a batch before adding them to the frontier. A document is discarded from the neighbor set if it belongs to the same group as disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Similar to Policy 2, policy-4 defines a quota for each group of documents in the frontier. Rather than only selecting the top-b documents in each batch, policy-5 enforces selecting a specific number of highest-scoring documents from each group. It ensures that the neighbors in the corpus graph have sufficient diversity. In addition to selecting a batch based on the highest-scoring document from each group, policy-6 further considers the order in which the documents were inserted into the frontier leading to prioritizing the documents scored in the first iteration.\n\n\n\n\n4.2 Inter-Document Relationship\n\nWhile many approaches inspired by the clustering hypothesis leverage inter-document similarity, they are limited by larger proportions of irrelevant passages. Sarwar and O\u2019Riordan (2021) propose a novel graph-based method to model the inter-passage similarities for re-ranking documents. The model represents the cohesion of each document by capturing the topic-shift of passages, where passages related to the same topic represent high document cohesion. The process starts with decomposing each document into passages. It then generates a graph with the passages as nodes and their relevance as edges. The cohesion score is the average similarity of all possible pairs of passages in a document. The document re-ranking is implemented in one of three settings: i) s\u2062i\u2062m\u2062(di,q)\u00d7C\u2062(di)\ud835\udc60\ud835\udc56\ud835\udc5asubscript\ud835\udc51\ud835\udc56\ud835\udc5e\ud835\udc36subscript\ud835\udc51\ud835\udc56sim(d_{i},q)\\times C(d_{i})italic_s italic_i italic_m ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q ) \u00d7 italic_C ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), ii) s\u2062i\u2062m\u2062(di,q)+C\u2062(di)\ud835\udc60\ud835\udc56\ud835\udc5asubscript\ud835\udc51\ud835\udc56\ud835\udc5e\ud835\udc36subscript\ud835\udc51\ud835\udc56sim(d_{i},q)+C(d_{i})italic_s italic_i italic_m ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q ) + italic_C ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), or iii) s\u2062i\u2062m\u2062(di,q)+C\u2062(d\u2062i)\u00d7X\ud835\udc60\ud835\udc56\ud835\udc5asubscript\ud835\udc51\ud835\udc56\ud835\udc5e\ud835\udc36\ud835\udc51\ud835\udc56\ud835\udc4bsim(d_{i},q)+{C(di)\\times X}italic_s italic_i italic_m ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q ) + italic_C ( italic_d italic_i ) \u00d7 italic_X, where X\u2208[0.0\u22121.0]\ud835\udc4bdelimited-[]0.01.0X\\in[0.0-1.0]italic_X \u2208 [ 0.0 - 1.0 ] allows selective inclusion of cohesion, s\u2062i\u2062m\u2062(di,q)\ud835\udc60\ud835\udc56\ud835\udc5asubscript\ud835\udc51\ud835\udc56\ud835\udc5esim(d_{i},q)italic_s italic_i italic_m ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q ) is query-similarity, and C\u2062(i)\ud835\udc36\ud835\udc56C(i)italic_C ( italic_i ) is the cohesion score.\n\n\nThe previously discussed methods did not leverage the structural information in the corpus graphs to update document feature representations. To overcome this, Yu et al. Yu et al. (2021) propose KG-FiD, a Knowledge Graph (KG) enhanced FiD that incorporates GNN to leverage the inter-passage relationship. It introduces a two-stage re-ranking pipeline. The first stage is concerned with re-ranking the N0subscript\ud835\udc410N_{0}italic_N start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT passages returned by the initial retriever, the top N1subscript\ud835\udc411N_{1}italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT of which is leveraged in the second stage. The second stage re-ranks top N2subscript\ud835\udc412N_{2}italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT passages from N1subscript\ud835\udc411N_{1}italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and generates an answer.\n\n\nAnother method of modeling inter-document relationships for GNN learning is via Abstract Meaning Representation (AMR) graphs. Dong et al. (2024a) develop G-RAG, which models the inter-document relationship via an Abstract Meaning Representation (AMR) graph.\nThe feature representations of the nodes that appear in the shortest path between the query and target document are aggregated for contextually augmenting the document node representation, facilitating the final re-ranking process.\n\n\nGraph Neural Re-ranking (GNRR) Di\u00a0Francesco et al. (2024) is another method that addresses document relations by allowing each query to incorporate document distribution during the inference process. This approach models the document relationship using corpus sub-graphs and encodes these representations using GNN. GNRR comprises 3 main phases: i) Data Retrieval, ii) Subgraph Construction, and iii) Features and Score Computation. Given a query q and the document corpus C\ud835\udc36Citalic_C, a sparse retrieval fetches top-1000q documents R0subscript\ud835\udc450R_{0}italic_R start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and builds a semantic corpus graph, GCsubscript\ud835\udc3a\ud835\udc36G_{C}italic_G start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT leveraging TCT-ColBERT Lin et al. (2020). TCT-ColBERT also encodes the query. GCsubscript\ud835\udc3a\ud835\udc36G_{C}italic_G start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT then facilitate the generation of a query-induced subgraph, GC\u2062q=(Vq,Eq)subscript\ud835\udc3a\ud835\udc36\ud835\udc5esubscript\ud835\udc49\ud835\udc5esubscript\ud835\udc38\ud835\udc5eG_{Cq}=(V_{q},E_{q})italic_G start_POSTSUBSCRIPT italic_C italic_q end_POSTSUBSCRIPT = ( italic_V start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) where, Vq\u2208R0subscript\ud835\udc49\ud835\udc5esubscript\ud835\udc450V_{q}\\in R_{0}italic_V start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT \u2208 italic_R start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. Therefore, GC\u2062qsubscript\ud835\udc3a\ud835\udc36\ud835\udc5eG_{Cq}italic_G start_POSTSUBSCRIPT italic_C italic_q end_POSTSUBSCRIPT retains the lexical information from the sparse retriever and the structural information of the corpus graph. The node representations of GC\u2062qsubscript\ud835\udc3a\ud835\udc36\ud835\udc5eG_{Cq}italic_G start_POSTSUBSCRIPT italic_C italic_q end_POSTSUBSCRIPT are computed using an element-wise product between the query and each document. While the document-interactions are modeled by the GNN, a Multi-layer Perceptron (MLP) independently calculates the relevance of each query-document pair. Leveraging the merged form of these two representations a score module estimates the final ranks.\n\n\nA common practice in passage retrieval and re-ranking tasks is to leverage contextual information to enhance performance. To this end, Albarede et al. (2022) proposed a merge and late interaction model based on Graph Attention Network (GAT) for passage contextualization. They introduced a novel document graph representation based on inter- and intra-document similarities.\n\n\nZhang et al. (2021) proposed Iterative Document Re-ranking (IDR), an integral component of their ODQA framework, that addresses the lexical overlap problem leveraging document relationship. IDR models the inter-document relationship by constructing an entity-centric document graph where two documents share an edge based on the common entities. The graph-based re-ranking module in IDR comprises 4 components: i) Contextual Encoding, ii) Graph Attention, iii) Multi-document Fusion, and iv) Document Filter. The process starts with encoding each question q\ud835\udc5eqitalic_q, and document dksubscript\ud835\udc51\ud835\udc58d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT followed by the concatenation of the document representations v\ud835\udc63vitalic_v. A document graph is then constructed where the node representation eisubscript\ud835\udc52\ud835\udc56e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT for each shared entity Eisubscript\ud835\udc38\ud835\udc56E_{i}italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is generated by pooling token embeddings from v\ud835\udc63vitalic_v as ei=p\u2062o\u2062o\u2062l\u2062i\u2062n\u2062g\u2062(t1(i),t2(i),\u2026,t|Ei|(i))subscript\ud835\udc52\ud835\udc56\ud835\udc5d\ud835\udc5c\ud835\udc5c\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc54superscriptsubscript\ud835\udc611\ud835\udc56superscriptsubscript\ud835\udc612\ud835\udc56\u2026superscriptsubscript\ud835\udc61subscript\ud835\udc38\ud835\udc56\ud835\udc56e_{i}=pooling(t_{1}^{(i)},t_{2}^{(i)},\u2026,t_{|E_{i}|}^{(i)})italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_p italic_o italic_o italic_l italic_i italic_n italic_g ( italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , \u2026 , italic_t start_POSTSUBSCRIPT | italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ); here, tj(i)superscriptsubscript\ud835\udc61\ud835\udc57\ud835\udc56t_{j}^{(i)}italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT is the j\ud835\udc57jitalic_j-th token in Eisubscript\ud835\udc38\ud835\udc56E_{i}italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. A GAT processes the document graph by updating each entity representation. The next stage, multi-document fusion, updates the non-entity tokens by projecting them over the entity tokens to generate v\u2032superscript\ud835\udc63\u2032v^{\\prime}italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT which is fed into a transformer layer to obtain the fused representation vector v~~\ud835\udc63\\tilde{v}over~ start_ARG italic_v end_ARG. Finally, leveraging the fused representation a binary classifier scores the documents.\n\n\n\n\n4.3 External Knowledge Integration\n\nThe structural information of knowledge graphs provides an effective means to capture the relationship between documents. Motivated by its importance in re-ranking, Vollmers et al.  applied a Graph Attention Network (GAT) as a cross-encoder over the proposed query-document knowledge sub-graph. Within the graph, the nodes represent entities extracted from a query-document pair, while the edges represent their semantic relationship. The proposed approach computes the query-document relevance in 3 steps: entity extraction and linking, subgraph retrieval, and GAT-based re-ranking. In the first step, FLAIRAkbik et al. (2018) is utilized to extract entities from the documents, while Llama3 is used to disambiguate and extract entities from the queries as it has higher accuracy with shorter text. Following this, GENRE De Cao et al. (2021), an autoregressive model is used to link the extracted entities with their corresponding URIs in a knowledge graph. The next step is subgraph extraction. To this end, the Subgraph Retrieval Toolkit (SRTK) Shen (2023); Zhang et al. (2022) is used to provide a list of RDF triples from the target KG against a list of entities. In the final step, the GAT cross-encoder is used to encode the subgraph where the output node embedding of the query and the documents are used to calculate the relevance score.\n\n\nCross-encoder is the driving force of many SOTA re-ranking pipelines. However, by itself, the cross-encoder lacks background knowledge, a critical element for effective passage retrieval, especially in domain-specific tasks. Therefore, Fang et al. (2023) propose KGPR, a KG-enhanced cross-encoder for passage re-ranking. At its core KGPR uses LUKE Yamada et al. (2020), an entity-aware pre-trained language model, to incorporate KG to facilitate background information. LUKE-based cross-encoder calculates query-passage relevance utilizing query, passage, and their corresponding entity embedding. However, by default, the LUKE model disregards the entity relations. Therefore, the proposed method leverages a knowledge subgraph extracted from the Freebase Bollacker et al. (2008). The subgraph extraction is carried out in two phases: entity linking and subgraph retrieval. The former identifies entities from both the query and the passage to link them with Freebase nodes. To this end, KGPR utilizes ELQ Li et al. (2020), an entity-linking model for questions. The extracted passage entities are then traced up to 1-hop from the query entities in the Firebase to extract the subgraph Gq,dsubscript\ud835\udc3a\ud835\udc5e\ud835\udc51G_{q,d}italic_G start_POSTSUBSCRIPT italic_q , italic_d end_POSTSUBSCRIPT, while filtering out edges not related to passage entities. The authors introduce an additional embedding for relations in a KG triple. During the re-ranking process, the triple embedding servers as an additional input for LUKE, effectively infusing background knowledge.\n\n\nThe inherent noise associated with existing KGs makes them suboptimal for re-ranking tasks. To address this, Dong et al. (2022) introduced a re-ranking-centric knowledge meta graph distillation module with their Knowledge Enhanced Re-ranking Model (KERM). Additionally, the authors introduced a knowledge injector, an aggregation module to bridge the semantic gap that emerges during the aggregation of implicit and explicit knowledge. At first, KERM employs TransE Bordes et al. (2013), a knowledge graph embedding model, to prune noises from a global knowledge graph, G\ud835\udc3aGitalic_G to obtain G\u2032superscript\ud835\udc3a\u2032G^{\\prime}italic_G start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT. In the pruning process, only the top-\u03c0\ud835\udf0b\\piitalic_\u03c0 neighbors of each entity are considered based on the distance metric, dist(eh,et)=1/{E(eh).E(r)+E(eh).E(et)+E(r).E(et)}dist(e_{h},e_{t})=1/\\{E(e_{h}).E(r)+E(e_{h}).E(e_{t})+E(r).E(e_{t})\\}italic_d italic_i italic_s italic_t ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = 1 / { italic_E ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) . italic_E ( italic_r ) + italic_E ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) . italic_E ( italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + italic_E ( italic_r ) . italic_E ( italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) } where, ehsubscript\ud835\udc52\u210ee_{h}italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT, r\ud835\udc5fritalic_r, and etsubscript\ud835\udc52\ud835\udc61e_{t}italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT are a head entity, relation, and tail entity of a triplet, respectively. Following this, the bipartite meta-graph, Gq,psubscript\ud835\udc3a\ud835\udc5e\ud835\udc5dG_{q,p}italic_G start_POSTSUBSCRIPT italic_q , italic_p end_POSTSUBSCRIPT is constructed in three phases. It begins by selecting the most relevant sentence to the query, followed by extracting its entities, and finally linking them with the top-k hops from G\u2032superscript\ud835\udc3a\u2032G^{\\prime}italic_G start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT using Breadth-First Search (BFS). At this stage, a knowledge injector is utilized to further enhance the explicit knowledge. A Graph Meta Network (GMN) module, a multi-layer GNN integrated into the knowledge injector, dynamically enriches the meta-graph by utilizing the encoded textual information (i.e., implicit knowledge). It leads to the mutual enhancement of both the text and the knowledge embedding. This enhanced representation facilitates cross-encoder\u2019s re-ranking process.\n\n\nGupta and Demner-Fushman (2024) also addresses the shortcomings of PLM-based re-rankers on domain-specific tasks. The proposed approach, GraphMonoT5, fuses external knowledge from KG into PLMs to facilitate biomedical document re-ranking. On top of the default encoder-decoder T5, the GraphMonoT5 complements the encoder with a GNN to account for the external knowledge modeled in KG. Entities extracted from each query-document pair are linked to a KG followed by the subgraph extraction, connecting all the nodes within 2-hop paths of corresponding entities. The GraphMonoT5 introduces a novel interaction module aimed at fusing the text and the graph embeddings leveraging interaction tokens, ti\u2062n\u2062tsubscript\ud835\udc61\ud835\udc56\ud835\udc5b\ud835\udc61t_{int}italic_t start_POSTSUBSCRIPT italic_i italic_n italic_t end_POSTSUBSCRIPT, interaction nodes, ni\u2062n\u2062tsubscript\ud835\udc5b\ud835\udc56\ud835\udc5b\ud835\udc61n_{int}italic_n start_POSTSUBSCRIPT italic_i italic_n italic_t end_POSTSUBSCRIPT, and their corresponding embeddings. The interaction embeddings are merged into a single representation, which together with the text and node embedding from a query-document pair, is fed into the GraphMonoT5 decoder to yield a ranking soccer. The documents are then re-ranked based on these scores.\n\n\nVeningston and Shanmugalakshmi (2014) proposed a document re-ranking strategy leveraging a term graph that models word relations among documents. The term graph, GTsubscript\ud835\udc3a\ud835\udc47G_{T}italic_G start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT = (VTsubscript\ud835\udc49\ud835\udc47V_{T}italic_V start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT, ETsubscript\ud835\udc38\ud835\udc47E_{T}italic_E start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT), is built using frequent item-sets, FSsubscript\ud835\udc39\ud835\udc46F_{S}italic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT which represents the document with its most frequent terms. In the graph, VT\u2208fSsubscript\ud835\udc49\ud835\udc47subscript\ud835\udc53\ud835\udc46V_{T}\\in f_{S}italic_V start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT \u2208 italic_f start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT where fs\u2208FSsubscript\ud835\udc53\ud835\udc60subscript\ud835\udc39\ud835\udc46f_{s}\\in F_{S}italic_f start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT \u2208 italic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT is an unique item, and ET\u2286VT\u2062i\u00d7VT\u2062jsubscript\ud835\udc38\ud835\udc47subscript\ud835\udc49\ud835\udc47\ud835\udc56subscript\ud835\udc49\ud835\udc47\ud835\udc57E_{T}\\subseteq V_{Ti}\\times V_{Tj}italic_E start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT \u2286 italic_V start_POSTSUBSCRIPT italic_T italic_i end_POSTSUBSCRIPT \u00d7 italic_V start_POSTSUBSCRIPT italic_T italic_j end_POSTSUBSCRIPT only if VT\u2062isubscript\ud835\udc49\ud835\udc47\ud835\udc56V_{Ti}italic_V start_POSTSUBSCRIPT italic_T italic_i end_POSTSUBSCRIPT and VT\u2062jsubscript\ud835\udc49\ud835\udc47\ud835\udc57V_{Tj}italic_V start_POSTSUBSCRIPT italic_T italic_j end_POSTSUBSCRIPT share the same item-set. The edge weight corresponds to the largest support value, s\u2062u\u2062p\u2062p\u2062o\u2062r\u2062td\ud835\udc60\ud835\udc62\ud835\udc5d\ud835\udc5d\ud835\udc5c\ud835\udc5fsubscript\ud835\udc61\ud835\udc51support_{d}italic_s italic_u italic_p italic_p italic_o italic_r italic_t start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT of the item-set, FS\u2062dsubscript\ud835\udc39\ud835\udc46\ud835\udc51F_{Sd}italic_F start_POSTSUBSCRIPT italic_S italic_d end_POSTSUBSCRIPT containing both the corresponding nodes. S\u2062u\u2062p\u2062p\u2062o\u2062r\u2062td\ud835\udc46\ud835\udc62\ud835\udc5d\ud835\udc5d\ud835\udc5c\ud835\udc5fsubscript\ud835\udc61\ud835\udc51Support_{d}italic_S italic_u italic_p italic_p italic_o italic_r italic_t start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT is calculated as \u2211i=1nfS\u2062d\u2062(ti)/\u2211j=1N\u2211i=1nfd\u2062j\u2062(ti)superscriptsubscript\ud835\udc561\ud835\udc5bsubscript\ud835\udc53\ud835\udc46\ud835\udc51subscript\ud835\udc61\ud835\udc56superscriptsubscript\ud835\udc571\ud835\udc41superscriptsubscript\ud835\udc561\ud835\udc5bsubscript\ud835\udc53\ud835\udc51\ud835\udc57subscript\ud835\udc61\ud835\udc56\\sum_{i=1}^{n}f_{Sd}(t_{i})/\\sum_{j=1}^{N}\\sum_{i=1}^{n}f_{dj}(t_{i})\u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT italic_S italic_d end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) / \u2211 start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT italic_d italic_j end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) where fd\u2062j\u2062(ti)subscript\ud835\udc53\ud835\udc51\ud835\udc57subscript\ud835\udc61\ud835\udc56f_{dj}(t_{i})italic_f start_POSTSUBSCRIPT italic_d italic_j end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) is the frequency of term tisubscript\ud835\udc61\ud835\udc56t_{i}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT in document d\ud835\udc51ditalic_d, n\ud835\udc5bnitalic_n is the number of terms in item-set, and N\ud835\udc41Nitalic_N is total number of item-sets. The term graph is leveraged in document re-ranking following either of the two approaches: i) Term Rank-based document re-ranking, or ii) Term Distance matrix-based document re-ranking. The former is based on the PageRank algorithm Brin and Page (1998) that computes the term\u2019s rankings. Term ranks, calculated via PageRank, are utilized to re-rank the documents based on the shared terms between the query and the documents. The latter re-ranking approach leverages the term distance matrix, an adjacency matrix representing the distance between any two terms as the least number of hops. A document is ranked higher if its terms are at a closer distance to the terms of a given query.\n\n\n\n\n4.4 Pairwise Re-ranking\n\nSo far, we\u2019ve only focused on pointwise re-ranking, which evaluates each document in isolation. In contrast, pairwise re-ranking scores n\ud835\udc5bnitalic_n documents by evaluating all n\u00d7n\ud835\udc5b\ud835\udc5bn\\times nitalic_n \u00d7 italic_n pairs, capturing subtle nuances during re-ranking. PLM-based pair-wise re-ranking has demonstrated remarkable performance. However, the high inference overhead diminishes its feasibility. To address this, Gienapp et al. (2022) employs a subset of pairs sampled from the full set of document pairs. Consequently, the aggregation process that computes final rankings from the preference probabilities of document pairs, becomes even more crucial in the proposed approach. One of the proposed aggregation approaches draws inspiration from the PageRank algorithm. To this end, the sampled subset of document C\ud835\udc36Citalic_C is converted into a directed graph where nodes represent documents while edges represent the preference probabilities. For a given pair of documents (di,dj)subscript\ud835\udc51\ud835\udc56subscript\ud835\udc51\ud835\udc57(d_{i},d_{j})( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ), the preference probability, pi\u2062jsubscript\ud835\udc5d\ud835\udc56\ud835\udc57p_{ij}italic_p start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT represents the likelihood of disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT outranking djsubscript\ud835\udc51\ud835\udc57d_{j}italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT. The re-ranking is facilitated by the PageRank algorithm adapted for weighted edges. In the main re-ranking pipeline, the proposed approach utilizes a mono-duo architecture. Initially, monoT5, a pointwise re-ranker, ranks the top-1000 documents. This is followed by the duoT5, pairwise re-ranker, which re-ranks the top-50 results from the previous step.\n\n\nPairwise Ranking Prompting (PRP) is a zero-shot re-ranking method based on Large Language Models (LLM). However, contemporary PRP methods do not account for the uncertainty associated with the labels. To overcome this limitation, Luo et al. (2024) proposed a PRP-graph coupled with a novel scoring PRP unit. The PRP-graph functions in two distinct phases. First, it generates a ranking graph discussed in section Document-level graphs. Inspired by the weighted PageRank algorithm, the vertices of the document graph are iteratively updated to reflect the final re-ranking score.\n\n\nDeng et al. (2009) present a unique approach to re-rank documents. The documents in the corpus are initially ranked using p\u2062(q\u2223\u03b8d)=\u220ft\u2208qp\u2062(t\u2223\u03b8d)n\u2062(t,q)\ud835\udc5dconditional\ud835\udc5esubscript\ud835\udf03\ud835\udc51subscriptproduct\ud835\udc61\ud835\udc5e\ud835\udc5dsuperscriptconditional\ud835\udc61subscript\ud835\udf03\ud835\udc51\ud835\udc5b\ud835\udc61\ud835\udc5ep(q\\mid\\theta_{d})=\\prod_{t\\in q}p(t\\mid\\theta_{d})^{n(t,q)}italic_p ( italic_q \u2223 italic_\u03b8 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) = \u220f start_POSTSUBSCRIPT italic_t \u2208 italic_q end_POSTSUBSCRIPT italic_p ( italic_t \u2223 italic_\u03b8 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_n ( italic_t , italic_q ) end_POSTSUPERSCRIPT, where p\u2062(q\u2223\u03b8d)\ud835\udc5dconditional\ud835\udc5esubscript\ud835\udf03\ud835\udc51p(q\\mid\\theta_{d})italic_p ( italic_q \u2223 italic_\u03b8 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) is the maximum likelihood estimation of the term t\ud835\udc61titalic_t in a document d\ud835\udc51ditalic_d, and n\u2062(t,q)\ud835\udc5b\ud835\udc61\ud835\udc5en(t,q)italic_n ( italic_t , italic_q ) is the number of times that term t\ud835\udc61titalic_t occurs in query q\ud835\udc5eqitalic_q. A document is ranked higher if it has a higher likelihood of generating the query. After a latent space graph is created(explained in Section Document-level graphs), the following cost function is optimized to re-rank the documents.\n\n\n\n\nR\u2062(F,q,G)\ud835\udc45\ud835\udc39\ud835\udc5e\ud835\udc3a\\displaystyle R(F,q,G)italic_R ( italic_F , italic_q , italic_G )\n=12\u2062\u2211i,j=1nwi\u2062j\u2062\u2016f\u2062(di,q)Di\u2062i\u2212f\u2062(dj,q)Dj\u2062j\u20162absent12superscriptsubscript\ud835\udc56\ud835\udc571\ud835\udc5bsubscript\ud835\udc64\ud835\udc56\ud835\udc57superscriptnorm\ud835\udc53subscript\ud835\udc51\ud835\udc56\ud835\udc5esubscript\ud835\udc37\ud835\udc56\ud835\udc56\ud835\udc53subscript\ud835\udc51\ud835\udc57\ud835\udc5esubscript\ud835\udc37\ud835\udc57\ud835\udc572\\displaystyle=\\frac{1}{2}\\sum_{i,j=1}^{n}w_{ij}\\left\\|\\frac{f(d_{i},q)}{\\sqrt{%\nD_{ii}}}-\\frac{f(d_{j},q)}{\\sqrt{D_{jj}}}\\right\\|^{2}= divide start_ARG 1 end_ARG start_ARG 2 end_ARG \u2211 start_POSTSUBSCRIPT italic_i , italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_w start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT \u2225 divide start_ARG italic_f ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q ) end_ARG start_ARG square-root start_ARG italic_D start_POSTSUBSCRIPT italic_i italic_i end_POSTSUBSCRIPT end_ARG end_ARG - divide start_ARG italic_f ( italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_q ) end_ARG start_ARG square-root start_ARG italic_D start_POSTSUBSCRIPT italic_j italic_j end_POSTSUBSCRIPT end_ARG end_ARG \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\n\n(2)\n\n\n\n\n+\u03bc\u2062\u2211i=1n\u2016f\u2062(di,q)\u2212f0\u2062(di,q)\u20162,\ud835\udf07superscriptsubscript\ud835\udc561\ud835\udc5bsuperscriptnorm\ud835\udc53subscript\ud835\udc51\ud835\udc56\ud835\udc5esuperscript\ud835\udc530subscript\ud835\udc51\ud835\udc56\ud835\udc5e2\\displaystyle\\quad+\\mu\\sum_{i=1}^{n}\\left\\|f(d_{i},q)-f^{0}(d_{i},q)\\right\\|^{%\n2},+ italic_\u03bc \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT \u2225 italic_f ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q ) - italic_f start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q ) \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,\n\n\n\n\nThe first sum 12\u2062\u2211i,j=1nwi\u2062j\u2062\u2016f\u2062(di,q)Di\u2062i\u2212f\u2062(dj,q)Dj\u2062j\u2016212superscriptsubscript\ud835\udc56\ud835\udc571\ud835\udc5bsubscript\ud835\udc64\ud835\udc56\ud835\udc57superscriptnorm\ud835\udc53subscript\ud835\udc51\ud835\udc56\ud835\udc5esubscript\ud835\udc37\ud835\udc56\ud835\udc56\ud835\udc53subscript\ud835\udc51\ud835\udc57\ud835\udc5esubscript\ud835\udc37\ud835\udc57\ud835\udc572\\frac{1}{2}\\sum_{i,j=1}^{n}w_{ij}\\left\\|\\frac{f(d_{i},q)}{\\sqrt{D_{ii}}}-\\frac%\n{f(d_{j},q)}{\\sqrt{D_{jj}}}\\right\\|^{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARG \u2211 start_POSTSUBSCRIPT italic_i , italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_w start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT \u2225 divide start_ARG italic_f ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q ) end_ARG start_ARG square-root start_ARG italic_D start_POSTSUBSCRIPT italic_i italic_i end_POSTSUBSCRIPT end_ARG end_ARG - divide start_ARG italic_f ( italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_q ) end_ARG start_ARG square-root start_ARG italic_D start_POSTSUBSCRIPT italic_j italic_j end_POSTSUBSCRIPT end_ARG end_ARG \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\nensures that documents with high similarity should have similar ranking scores. This is the global consistency term, where the cost increases if similar documents end up with very different ranking scores. The second sum \u03bc\u2062\u2211i=1n\u2016f\u2062(di,q)\u2212f0\u2062(di,q)\u20162\ud835\udf07superscriptsubscript\ud835\udc561\ud835\udc5bsuperscriptnorm\ud835\udc53subscript\ud835\udc51\ud835\udc56\ud835\udc5esuperscript\ud835\udc530subscript\ud835\udc51\ud835\udc56\ud835\udc5e2\\quad\\mu\\sum_{i=1}^{n}\\left\\|f(d_{i},q)-f^{0}(d_{i},q)\\right\\|^{2}italic_\u03bc \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT \u2225 italic_f ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q ) - italic_f start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q ) \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTensures that the refined ranking scores stay close to the initial ranking scores. This term prevents the re-ranking from deviating too much from the original scores.\n\n\n\n\n4.5 Listwise Re-ranking\n\nRathee et al. (2025) extends GARMacAvaney et al. (2022b) to the LLM-based listwise re-ranker. The existing GAR, relying on the Probability Ranking Principle, does not consider the document relationship while calculating the relevance score. Therefore, the proposed approach SlideGAR adapts the GAR from pointwise re-ranking to listwise setting to account for document relationship. After retrieving an initial ranked list of documents using BM25, SlideGar applies a sliding window approach with window size w\ud835\udc64witalic_w and step size b\ud835\udc4fbitalic_b. The top w\ud835\udc64witalic_w documents from the initial ranking are selected, and, similar to GAR, the graph frontier is updated with their neighboring nodes. However, unlike GAR, which ranks documents individually, SlideGar ranks batches of documents simultaneously using a listwise LLM re-ranker such as RankZephyrPradeep et al. (2023). The ranking process alternates between the initial ranked list and the graph frontier, ensuring a more adaptive and context-aware document retrieval strategy.\n\n\n\n\n\n5 Graph Construction for Graph-based Reranking\n\nGraph-based re-ranking methods enhance retrieval performance by incorporating document relationships, surpassing traditional ranking models. The re-ranking models discussed thus far rely on graph data structures, which can be broadly categorized into document-level and entity-level graphs. In document-level graphs, edges represent relationships between documents/document parts, whereas entity-level graphs establish connections between individual tokens or concepts. This section explores some of the notable strategies used to construct both types of graphs, detailing their formation, edge weighting, and relevance to re-ranking tasks.\n\n\n\nDefinition\n\nLet G = (V, E, A, R, Adj) be a directed/undirected graph, where V is the set of nodes and E\u2286V\u00d7V\ud835\udc38\ud835\udc49\ud835\udc49E\\subseteq V\\times Vitalic_E \u2286 italic_V \u00d7 italic_V is the set of edges. The number of nodes in G is denoted by |V|=N\ud835\udc49\ud835\udc41|V|=N| italic_V | = italic_N, and the number of edges by |E|\ud835\udc38|E|| italic_E |. A is the set of node types while R denotes the set of edge types. The set of edges E can also be expressed as the adjacency matrix A\u2062d\u2062j\u2208{0,1}N\u00d7N\ud835\udc34\ud835\udc51\ud835\udc57superscript01\ud835\udc41\ud835\udc41Adj\\in\\{0,1\\}^{N\\times N}italic_A italic_d italic_j \u2208 { 0 , 1 } start_POSTSUPERSCRIPT italic_N \u00d7 italic_N end_POSTSUPERSCRIPT, where A\u2062d\u2062ju\u2062v\ud835\udc34\ud835\udc51subscript\ud835\udc57\ud835\udc62\ud835\udc63Adj_{uv}italic_A italic_d italic_j start_POSTSUBSCRIPT italic_u italic_v end_POSTSUBSCRIPT = 1 if nodes(u,v)\u2208E\ud835\udc62\ud835\udc63\ud835\udc38(u,v)\\in E( italic_u , italic_v ) \u2208 italic_E\nare connected, and A\u2062d\u2062ju\u2062v\ud835\udc34\ud835\udc51subscript\ud835\udc57\ud835\udc62\ud835\udc63Adj_{uv}italic_A italic_d italic_j start_POSTSUBSCRIPT italic_u italic_v end_POSTSUBSCRIPT= 0 otherwise.\n\n\n\nDocument-level graphs\n\nIn Di\u00a0Francesco et al. (2024); MacAvaney et al. (2022b) each pair of documents (disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, djsubscript\ud835\udc51\ud835\udc57d_{j}italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT) in the graph corpus shares a connection based on the\ncosine similarity between their document encodings. After the graph structure for each query is established, the features of the nodes are defined by performing an element-wise product between the query representation and each document representation. Therefore the node feature for d\u2062o\u2062c\u2062u\u2062m\u2062e\u2062n\u2062ti\ud835\udc51\ud835\udc5c\ud835\udc50\ud835\udc62\ud835\udc5a\ud835\udc52\ud835\udc5bsubscript\ud835\udc61\ud835\udc56document_{i}italic_d italic_o italic_c italic_u italic_m italic_e italic_n italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is given by \ud835\udc31i=\ud835\udc33q\u2299\ud835\udc33disubscript\ud835\udc31\ud835\udc56direct-productsubscript\ud835\udc33\ud835\udc5esubscript\ud835\udc33subscript\ud835\udc51\ud835\udc56\\mathbf{x}_{i}=\\mathbf{z}_{q}\\odot\\mathbf{z}_{d_{i}}bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_z start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT \u2299 bold_z start_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT, \ud835\udc33qsubscript\ud835\udc33\ud835\udc5e\\mathbf{z}_{q}bold_z start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT and \ud835\udc33disubscript\ud835\udc33subscript\ud835\udc51\ud835\udc56\\mathbf{z}_{d_{i}}bold_z start_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT being the query and document encodings respectively. Zhang et al. (2021) utilizes the Named Entity Recognition system to extract entities and connect documents if they have shared entities.\n\n\nInstead of merely linking nodes based on document similarity, Albarede et al. (2022) introduces a more structured approach by dividing document nodes into section nodes (non-textual units with titles) and passage nodes (textual units without titles). Additionally, the framework defines eight distinct edge types, consisting of four primary relations and their respective inverses:\n(1) Order relation \u2013 captures the sequential arrangement between passage nodes.\n(2) Structural relation \u2013 represents connections between a passage and its parent section node or between two section nodes.\n(3) Internal relation \u2013 links nodes within the same document.\n(4) External relation \u2013 connects nodes across different documents.\nThis hierarchical and relational structure enhances the granularity and contextual understanding of document graphs.\n\n\nG-RAG Dong et al. (2024a) considers each document to be a text block of 100 words. Each question is concatenated with documents in the corpus. AMRBART Bai et al. (2022) is utilized to create AMR graphsBanarescu et al. (2013). From the graphs, the connection information between different documents is incorporated into the edge features in the subsequent document-level graph. Thus an undirected\ndocument graph Gqsubscript\ud835\udc3a\ud835\udc5eG_{q}italic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT = {V, E} based on AMRs {Gq\u2062p\u20621subscript\ud835\udc3a\ud835\udc5e\ud835\udc5d1G_{qp1}italic_G start_POSTSUBSCRIPT italic_q italic_p 1 end_POSTSUBSCRIPT, \u00b7 \u00b7 \u00b7 , Gq\u2062p\u2062nsubscript\ud835\udc3a\ud835\udc5e\ud835\udc5d\ud835\udc5bG_{qpn}italic_G start_POSTSUBSCRIPT italic_q italic_p italic_n end_POSTSUBSCRIPT} is established. Each node vi\u2208Vsubscript\ud835\udc63\ud835\udc56\ud835\udc49v_{i}\\in Vitalic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 italic_V corresponds to the document pisubscript\ud835\udc5d\ud835\udc56p_{i}italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. For vi,vj\u2208Vsubscript\ud835\udc63\ud835\udc56subscript\ud835\udc63\ud835\udc57\ud835\udc49v_{i},v_{j}\\in Vitalic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u2208 italic_V where i\u2260j\ud835\udc56\ud835\udc57i\\neq jitalic_i \u2260 italic_j if AMRs Gq\u2062p\u2062isubscript\ud835\udc3a\ud835\udc5e\ud835\udc5d\ud835\udc56G_{qpi}italic_G start_POSTSUBSCRIPT italic_q italic_p italic_i end_POSTSUBSCRIPT and Gq\u2062p\u2062jsubscript\ud835\udc3a\ud835\udc5e\ud835\udc5d\ud835\udc57G_{qpj}italic_G start_POSTSUBSCRIPT italic_q italic_p italic_j end_POSTSUBSCRIPT have common nodes, there will be an undirected edge between visubscript\ud835\udc63\ud835\udc56v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and vjsubscript\ud835\udc63\ud835\udc57v_{j}italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT in the document-level graph.\n\n\nDeng et al. (2009) incorporates link information in a document-level latent space graph. The content matrix term C\u2208\u211dn\u00d7m\ud835\udc36superscript\u211d\ud835\udc5b\ud835\udc5aC\\in\\mathbb{R}^{n\\times m}italic_C \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_m end_POSTSUPERSCRIPT is a sparse matrix whose rows\nrepresent documents and columns represent terms,\nwhere m\ud835\udc5amitalic_m is the number of terms. The document-author bipartite graph can similarly be described by a matrix A\u2208\u211dn\u00d7l\ud835\udc34superscript\u211d\ud835\udc5b\ud835\udc59A\\in\\mathbb{R}^{n\\times l}italic_A \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_l end_POSTSUPERSCRIPT, which is also a sparse matrix whose rows correspond to documents and whose columns correspond to authors, where l\ud835\udc59litalic_l is the number of authors. These matrices are mapped to a shared latent space X\ud835\udc4bXitalic_X through joint factorizationZhu et al. (2007), thus combining the authorship and content information. The edge weights are determined using a heat kernel as follows\n\n\n\nwi\u2062j=exp\u2061(\u2212\u2016xi\u2212xj\u201622\u2062\u03c32)subscript\ud835\udc64\ud835\udc56\ud835\udc57superscriptnormsubscript\ud835\udc65\ud835\udc56subscript\ud835\udc65\ud835\udc5722superscript\ud835\udf0e2w_{ij}=\\exp\\left(-\\frac{\\|x_{i}-x_{j}\\|^{2}}{2\\sigma^{2}}\\right)italic_w start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = roman_exp ( - divide start_ARG \u2225 italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 italic_\u03c3 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG )\n\n(3)\n\n\nwhere xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and xjsubscript\ud835\udc65\ud835\udc57x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT are the latent representations of documents i\ud835\udc56iitalic_i and j\ud835\udc57jitalic_j and \u03c3\ud835\udf0e\\sigmaitalic_\u03c3 is the heat kernel that controls the spread or reach of the edges between nodes in the graph. Specifically, it affects how sensitive the weight of the edge wi\u2062jsubscript\ud835\udc64\ud835\udc56\ud835\udc57w_{ij}italic_w start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT is to the Euclidean distance between two nodes. Smaller \u03c3\ud835\udf0e\\sigmaitalic_\u03c3 will correspond to far apart nodes having edge weights closer to zero and vice versa.\n\n\nLuo et al. (2024) investigates the application of Pairwise Ranking Prompts Qin et al. (2023) for graph construction. A document disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and its closest subsequent document djsubscript\ud835\udc51\ud835\udc57d_{j}italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT are selected and a bidirectional relationship is established through directed edges di\u2192jsubscript\ud835\udc51\u2192\ud835\udc56\ud835\udc57d_{i\\to j}italic_d start_POSTSUBSCRIPT italic_i \u2192 italic_j end_POSTSUBSCRIPT and dj\u2192isubscript\ud835\udc51\u2192\ud835\udc57\ud835\udc56d_{j\\to i}italic_d start_POSTSUBSCRIPT italic_j \u2192 italic_i end_POSTSUBSCRIPT. The edge weights are assigned based on PRP-derived preference scores si\u2192jsubscript\ud835\udc60\u2192\ud835\udc56\ud835\udc57s_{i\\to j}italic_s start_POSTSUBSCRIPT italic_i \u2192 italic_j end_POSTSUBSCRIPT and sj\u2192isubscript\ud835\udc60\u2192\ud835\udc57\ud835\udc56s_{j\\to i}italic_s start_POSTSUBSCRIPT italic_j \u2192 italic_i end_POSTSUBSCRIPT, quantifying the relative ranking between the documents. scores are iteratively updated over r rounds\nusing the following update rules\n\n\n\n\nSirsuperscriptsubscript\ud835\udc46\ud835\udc56\ud835\udc5f\\displaystyle S_{i}^{r}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT\n=Sir\u22121+sj\u2192i\u00d7Sjr\u22121rabsentsuperscriptsubscript\ud835\udc46\ud835\udc56\ud835\udc5f1subscript\ud835\udc60\u2192\ud835\udc57\ud835\udc56superscriptsubscript\ud835\udc46\ud835\udc57\ud835\udc5f1\ud835\udc5f\\displaystyle=S_{i}^{r-1}+s_{j\\to i}\\times\\frac{S_{j}^{r-1}}{r}= italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r - 1 end_POSTSUPERSCRIPT + italic_s start_POSTSUBSCRIPT italic_j \u2192 italic_i end_POSTSUBSCRIPT \u00d7 divide start_ARG italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r - 1 end_POSTSUPERSCRIPT end_ARG start_ARG italic_r end_ARG\n\n(4)\n\n\n\nSjrsuperscriptsubscript\ud835\udc46\ud835\udc57\ud835\udc5f\\displaystyle S_{j}^{r}italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT\n=Sjr\u22121+si\u2192j\u00d7Sir\u22121rabsentsuperscriptsubscript\ud835\udc46\ud835\udc57\ud835\udc5f1subscript\ud835\udc60\u2192\ud835\udc56\ud835\udc57superscriptsubscript\ud835\udc46\ud835\udc56\ud835\udc5f1\ud835\udc5f\\displaystyle=S_{j}^{r-1}+s_{i\\to j}\\times\\frac{S_{i}^{r-1}}{r}= italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r - 1 end_POSTSUPERSCRIPT + italic_s start_POSTSUBSCRIPT italic_i \u2192 italic_j end_POSTSUBSCRIPT \u00d7 divide start_ARG italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r - 1 end_POSTSUPERSCRIPT end_ARG start_ARG italic_r end_ARG\n\n\n\n\nAfter r rounds, the final ranking graph G is obtained, where node scores reflect the cumulative ranking adjustments over multiple iterations.\n\n\n\nEntity-level graphs\n\nDeng et al. (2009) propose a two-stage process for graph construction. The process begins with node embedding initialization using TransE Bordes et al. (2013). The pairwise distances between embeddings are computed, and only the top n closest nodes are retained while the rest are pruned. Next, relevant sentence retrieval is performed by measuring the similarity between Word2Vec Mikolov et al. (2013) embeddings of the query and sentences from the corpus. Common entities between the query and retrieved sentences are identified, and a Breadth-First Search (BFS) is conducted to locate all nodes within K-hops of these entities. The resulting nodes are then used to construct a meta graph, capturing the refined structure of the knowledge graph for downstream tasks.Gupta and Demner-Fushman (2024); Yu et al. (2021) follow a similar approach for knowledge graph entity alignment and subgraph creation. Fang et al. (2023) also retrieves entities from a knowledge graph, selectively retaining only the edges that connect entities appearing in either the query or the passage.\n\n\n\n\n6 Conclusion and Summary of Findings\n\n\n6.1 Limitations and Discussion\n\nGraph-based retrieval is an established research area that has produced a wide variety of datasets used primarily for Knowledge Graph Question Answering (KGQA) downstream tasks. KGQA datasets are currently designed for one/multi-hop entity and relationship classification tasks. Current datasets are not well suited for more complex tasks such as passage and document ranking/re-ranking. As a result, there has been an emergence of curated graph construction methodologies and datasets used for graph-based passage and document ranking tasks (Section 5).\n\n\nThe diversity of these methods leads to an incongruity across evaluation techniques that aim to measure the performance of current methods. This scenario differs from established benchmark standards developed for language model retrievers. Examples of established benchmark datasets used to evaluate language model-based ranking tasks are provided in Section 3. These datasets were derived from large collections of unstructured data and preprocessed into query and target pairs. Unlike language models, Graph-based techniques such as GNNs additionally require a transformation of unstructured text samples into adjacency matrices of nodes and edges that can be used for training (See Section 2). Section 4 outlines the range of methods demonstrated for first curating the adjacency matrix and additionally, incorporating ranking-based features such as node/edge similarity scores into the graph.\n\n\nThe design and quality of the input representation can drastically change architectural considerations for proposed graph-based ranking models, ultimately impacting downstream ranking performance. Despite ongoing progress in this area, a standard benchmark to measure performance has not yet been developed to evaluate graph-based passage and document ranking tasks. For each of the methods presented in Section 4, downstream performance is evaluated on established benchmark datasets like MSMARCO, originally developed specifically for language model based ranking tasks. In traditional settings, datasets such as MSMARCO are typically used for both train and test workloads, with separate tracks for further data augmentation and/or zero-shot performance criteria.\n\n\n\n\n6.2 Future Work\n\nIn graph-based settings, we have observed that authors derive their own methods for (1) generating an adjacency matrix for the train set, (2) potentially augmenting the train set, with additional external data, and lastly, (3) performing evaluation on the downstream ranking task. We also have documented settings in which the distribution of the training input graph is either only minimally overlapping, or differs completely than that of a particular downstream task. In this case, evaluation assumes a transfer learning process, and is based on zero-shot performance. While relying on downstream performance provides a reference for comparing graph-based ranking models, the community should move towards creating a static benchmark that addresses limitations such as evaluating model architectures and the data curation process, as well as reproducibility.\n\n\n\n\nReferences\n\n\nAkbik et al. [2018]\n\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n\n\nContextual string embeddings for sequence labeling.\n\n\nIn COLING 2018, 27th International Conference on Computational Linguistics, pages 1638\u20131649, 2018.\n\n\n\n\nAlbarede et al. [2022]\n\nLucas Albarede, Philippe Mulhem, Lorraine Goeuriot, Claude Le\u00a0Pape-Gardeux, Sylvain Marie, and Trinidad Chardin-Segui.\n\n\nPassage retrieval on structured documents using graph attention networks.\n\n\nIn European Conference on Information Retrieval, pages 13\u201321. Springer, 2022.\n\n\n\n\nBai et al. [2022]\n\nXuefeng Bai, Yulong Chen, and Yue Zhang.\n\n\nGraph pre-training for amr parsing and generation.\n\n\narXiv preprint arXiv:2203.07836, 2022.\n\n\n\n\nBajaj et al. [2016]\n\nPayal Bajaj, Daniel Campos, Nick Craswell, Li\u00a0Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et\u00a0al.\n\n\nMs marco: A human generated machine reading comprehension dataset.\n\n\narXiv e-prints, pages arXiv\u20131611, 2016.\n\n\n\n\nBanarescu et al. [2013]\n\nLaura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider.\n\n\nAbstract meaning representation for sembanking.\n\n\nIn Proceedings of the 7th linguistic annotation workshop and interoperability with discourse, pages 178\u2013186, 2013.\n\n\n\n\nBollacker et al. [2008]\n\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.\n\n\nFreebase: a collaboratively created graph database for structuring human knowledge.\n\n\nIn Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247\u20131250, 2008.\n\n\n\n\nBordes et al. [2013]\n\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.\n\n\nTranslating embeddings for modeling multi-relational data.\n\n\nAdvances in neural information processing systems, 26, 2013.\n\n\n\n\nBrin and Page [1998]\n\nSergey Brin and Lawrence Page.\n\n\nThe anatomy of a large-scale hypertextual web search engine.\n\n\nComputer networks and ISDN systems, 30(1-7):107\u2013117, 1998.\n\n\n\n\nCao et al. [2007]\n\nZhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li.\n\n\nLearning to rank: from pairwise approach to listwise approach.\n\n\nIn Proceedings of the 24th international conference on Machine learning, pages 129\u2013136, 2007.\n\n\n\n\nDe Cao et al. [2021]\n\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni.\n\n\nAutoregressive entity retrieval.\n\n\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\n\n\n\n\nDeng et al. [2009]\n\nHongbo Deng, Michael\u00a0R Lyu, and Irwin King.\n\n\nEffective latent space graph-based re-ranking model with global consistency.\n\n\nIn Proceedings of the second acm international conference on web search and data mining, pages 212\u2013221, 2009.\n\n\n\n\nDevlin [2018]\n\nJacob Devlin.\n\n\nBert: Pre-training of deep bidirectional transformers for language understanding.\n\n\narXiv preprint arXiv:1810.04805, 2018.\n\n\n\n\nDi\u00a0Francesco et al. [2024]\n\nAndrea\u00a0Giuseppe Di\u00a0Francesco, Christian Giannetti, Nicola Tonellotto, and Fabrizio Silvestri.\n\n\nGraph neural re-ranking via corpus graph.\n\n\narXiv preprint arXiv:2406.11720, 2024.\n\n\n\n\nDong et al. [2022]\n\nQian Dong, Yiding Liu, Suqi Cheng, Shuaiqiang Wang, Zhicong Cheng, Shuzi Niu, and Dawei Yin.\n\n\nIncorporating explicit knowledge in pre-trained language models for passage re-ranking.\n\n\nIn Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1490\u20131501, 2022.\n\n\n\n\nDong et al. [2024a]\n\nJialin Dong, Bahare Fatemi, Bryan Perozzi, Lin\u00a0F Yang, and Anton Tsitsulin.\n\n\nDon\u2019t forget to connect! improving rag with graph-based reranking.\n\n\narXiv preprint arXiv:2405.18414, 2024.\n\n\n\n\nDong et al. [2024b]\n\nYuxin Dong, Shuo Wang, Hongye Zheng, Jiajing Chen, Zhenhong Zhang, and Chihang Wang.\n\n\nAdvanced rag models with graph structures: Optimizing complex knowledge reasoning and text generation.\n\n\nIn 2024 5th International Symposium on Computer Engineering and Intelligent Communications (ISCEIC), pages 626\u2013630. IEEE, 2024.\n\n\n\n\nFang et al. [2023]\n\nJinyuan Fang, Zaiqiao Meng, and Craig Macdonald.\n\n\nKgpr: Knowledge graph enhanced passage ranking.\n\n\nIn Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pages 3880\u20133885, 2023.\n\n\n\n\nFrayling et al. [2024]\n\nErlend Frayling, Sean MacAvaney, Craig Macdonald, and Iadh Ounis.\n\n\nEffective adhoc retrieval through traversal of a query-document graph.\n\n\nIn European Conference on Information Retrieval, pages 89\u2013104. Springer, 2024.\n\n\n\n\nGienapp et al. [2022]\n\nLukas Gienapp, Maik Fr\u00f6be, Matthias Hagen, and Martin Potthast.\n\n\nSparse pairwise re-ranking with pre-trained transformers.\n\n\nIn Proceedings of the 2022 ACM SIGIR International Conference on Theory of Information Retrieval, pages 72\u201380, 2022.\n\n\n\n\nGlass et al. [2022]\n\nMichael Glass, Gaetano Rossiello, Md\u00a0Faisal\u00a0Mahbub Chowdhury, Ankita\u00a0Rajaram Naik, Pengshan Cai, and Alfio Gliozzo.\n\n\nRe2g: Retrieve, rerank, generate.\n\n\nIn Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2022.\n\n\n\n\nGupta and Demner-Fushman [2024]\n\nDeepak Gupta and Dina Demner-Fushman.\n\n\nEmpowering language model with guided knowledge fusion for biomedical document re-ranking.\n\n\nIn International Conference on Artificial Intelligence in Medicine, pages 251\u2013260. Springer, 2024.\n\n\n\n\nJaenich et al. [2024]\n\nThomas Jaenich, Graham McDonald, and Iadh Ounis.\n\n\nFairness-aware exposure allocation via adaptive reranking.\n\n\nIn Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1504\u20131513, 2024.\n\n\n\n\nJardine and van Rijsbergen [1971]\n\nNick Jardine and Cornelis\u00a0Joost van Rijsbergen.\n\n\nThe use of hierarchic clustering in information retrieval.\n\n\nInformation storage and retrieval, 7(5):217\u2013240, 1971.\n\n\n\n\nLewis et al. [2020]\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et\u00a0al.\n\n\nRetrieval-augmented generation for knowledge-intensive nlp tasks.\n\n\nAdvances in Neural Information Processing Systems, 33:9459\u20139474, 2020.\n\n\n\n\nLi et al. [2020]\n\nBelinda\u00a0Z Li, Sewon Min, Srinivasan Iyer, Yashar Mehdad, and Wen-tau Yih.\n\n\nEfficient one-pass end-to-end entity linking for questions.\n\n\narXiv preprint arXiv:2010.02413, 2020.\n\n\n\n\nLin et al. [2020]\n\nSheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.\n\n\nDistilling dense representations for ranking using tightly-coupled teachers.\n\n\narXiv preprint arXiv:2010.11386, 2020.\n\n\n\n\nLuo et al. [2024]\n\nJian Luo, Xuanang Chen, Ben He, and Le\u00a0Sun.\n\n\nPrp-graph: Pairwise ranking prompting to llms with graph aggregation for effective text re-ranking.\n\n\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5766\u20135776, 2024.\n\n\n\n\nMacAvaney et al. [2022a]\n\nSean MacAvaney, Nicola Tonellotto, and Craig Macdonald.\n\n\nAdaptive re-ranking as an information-seeking agent.\n\n\n2022.\n\n\n\n\nMacAvaney et al. [2022b]\n\nSean MacAvaney, Nicola Tonellotto, and Craig Macdonald.\n\n\nAdaptive re-ranking with a corpus graph.\n\n\nIn Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pages 1491\u20131500, 2022.\n\n\n\n\nMikolov et al. [2013]\n\nTom\u00e1\u0161 Mikolov, Wen-tau Yih, and Geoffrey Zweig.\n\n\nLinguistic regularities in continuous space word representations.\n\n\nIn Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies, pages 746\u2013751, 2013.\n\n\n\n\nPeng et al. [2024]\n\nBoci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, and Siliang Tang.\n\n\nGraph retrieval-augmented generation: A survey.\n\n\narXiv e-prints, pages arXiv\u20132408, 2024.\n\n\n\n\nPiroi et al. [2011]\n\nFlorina Piroi, Mihai Lupu, Allan Hanbury, and Veronika Zenz.\n\n\nClef-ip 2011: Retrieval in the intellectual property domain.\n\n\nIn CLEF (notebook papers/labs/workshop), 2011.\n\n\n\n\nPradeep et al. [2023]\n\nRonak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin.\n\n\nRankzephyr: Effective and robust zero-shot listwise reranking is a breeze!\n\n\narXiv preprint arXiv:2312.02724, 2023.\n\n\n\n\nQin et al. [2023]\n\nZhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le\u00a0Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, et\u00a0al.\n\n\nLarge language models are effective text rankers with pairwise ranking prompting.\n\n\narXiv preprint arXiv:2306.17563, 2023.\n\n\n\n\nRathee et al. [2025]\n\nMandeep Rathee, Sean MacAvaney, and Avishek Anand.\n\n\nGuiding retrieval using llm-based listwise rankers.\n\n\narXiv preprint arXiv:2501.09186, 2025.\n\n\n\n\nReed and Madabushi [2020]\n\nKyle Reed and Harish\u00a0Tayyar Madabushi.\n\n\nFaster bert-based re-ranking through candidate passage extraction.\n\n\nIn TREC, 2020.\n\n\n\n\nSarwar and O\u2019Riordan [2021]\n\nGhulam Sarwar and Colm O\u2019Riordan.\n\n\nA graph-based approach at passage level to investigate the cohesiveness of documents.\n\n\nIn DATA, pages 115\u2013123, 2021.\n\n\n\n\nShen [2023]\n\nYuanchun Shen.\n\n\nSrtk: A toolkit for semantic-relevant subgraph retrieval.\n\n\narXiv preprint arXiv:2305.04101, 2023.\n\n\n\n\n[39]\n\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych.\n\n\nBeir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models.\n\n\n\n\nVeningston and Shanmugalakshmi [2014]\n\nK\u00a0Veningston and R\u00a0Shanmugalakshmi.\n\n\nInformation retrieval by document re-ranking using term association graph.\n\n\nIn Proceedings of the 2014 international conference on interdisciplinary advances in applied computing, pages 1\u20138, 2014.\n\n\n\n\n[41]\n\nDaniel Vollmers, Manzoor Ali, Hamada\u00a0M Zahera, and Axel-Cyrille\u00a0Ngonga Ngomo.\n\n\nDocument reranking using gat-cross encoder.\n\n\n\n\nVoorhees and Harman [2005]\n\nEllen\u00a0M Voorhees and Donna\u00a0K Harman.\n\n\nThe text retrieval conference.\n\n\nTREC: Experiment and evaluation in information retrieval, pages 3\u201319, 2005.\n\n\n\n\nYamada et al. [2020]\n\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto.\n\n\nLuke: Deep contextualized entity representations with entity-aware self-attention.\n\n\narXiv preprint arXiv:2010.01057, 2020.\n\n\n\n\nYu et al. [2021]\n\nDonghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao Yu, Shuohang Wang, Yichong Xu, Xiang Ren, Yiming Yang, and Michael Zeng.\n\n\nKg-fid: Infusing knowledge graph in fusion-in-decoder for open-domain question answering.\n\n\narXiv preprint arXiv:2110.04330, 2021.\n\n\n\n\nZhang et al. [2021]\n\nYuyu Zhang, Ping Nie, Arun Ramamurthy, and Le\u00a0Song.\n\n\nAnswering any-hop open-domain questions with iterative document reranking.\n\n\nIn Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 481\u2013490, 2021.\n\n\n\n\nZhang et al. [2022]\n\nJing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie Tang, Cuiping Li, and Hong Chen.\n\n\nSubgraph retrieval enhanced model for multi-hop knowledge base question answering.\n\n\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5773\u20135784, 2022.\n\n\n\n\nZhu et al. [2007]\n\nShenghuo Zhu, Kai Yu, Yun Chi, and Yihong Gong.\n\n\nCombining content and link for classification using matrix factorization.\n\n\nIn Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 487\u2013494, 2007.",
    "body": "Knowledge graphs have emerged to be promising datastore candidates for context augmentation during Retrieval Augmented Generation (RAG). As a result, techniques in graph representation learning have been simultaneously explored alongside principal neural information retrieval approaches, such as two-phased retrieval, also known as re-ranking. While Graph Neural Networks (GNNs) have been proposed to demonstrate proficiency in graph learning for re-ranking, there are ongoing limitations in modeling and evaluating input graph structures for training and evaluation for passage and document ranking tasks. In this survey, we review emerging GNN-based ranking model architectures along with their corresponding graph representation construction methodologies. We conclude by providing recommendations on future research based on community-wide challenges and opportunities. Retrieval Augmented Generation (RAG) is an established research area that combines pretrained parametric and non-parametric memory for downstream language generation Lewis et al. (2020). Recently, there has been an emergence of using Knowledge Graphs as the external non-parametric datastore, in which structural information is queried to capture relational knowledge Dong et al. (2024b). Graph-RAG approaches typically follow a two-phased retrieval procedure, also known as re-ranking Peng et al. (2024). Two-phased retrieval approaches consist of a primary retrieval method in which given a search query, an initial set of probable responses is fetched using techniques such as approximate nearest neighbor (ANN) indexing, traditional keyword-based search, or embedding-based retrieval. The initial retrieval process generally often prioritizes compute efficiency over perfect accuracy, leading to prompting with irrelevant, noisy context and increased hallucination to the final output Glass et al. (2022). In a two-phase setup, re-ranking methods distill the initial set of retrieved documents by re-scoring the initial list according to a refined relevance score. Querying the complex structure of the knowledge graph presents challenges for popular Large Language Model (LLM)-based re-ranker models. This challenge has inspired recent research in exploring the potential of Graph Neural Networks (GNNs) for exploiting structural information across entities and capturing relational knowledge for prompting a language model for generation. For effective graph-based re-ranking, researchers have developed expansive and specialized methods focused primarily on developing (1) unique GNN model architectures and (2) constructing unique graph representation structures specifically optimized for retrieval tasks. However, many of these methods have only recently emerged, leading to gaps in community-wide evaluation of model architectures and best practices. Additionally, these methods have not yet been collated into a systematic review. To fill this gap, we provide a comprehensive overview of emerging GNN-based ranking model architectures and their corresponding graph representation construction methodologies. We conclude by providing recommendations on future research based on our findings and analysis of limitations. Re-ranking problems abstractly consider a situation where there is a query q\ud835\udc5eqitalic_q and a finite set D={di}i\u2208[n]\ud835\udc37subscriptsubscript\ud835\udc51\ud835\udc56\ud835\udc56delimited-[]\ud835\udc5bD=\\{d_{i}\\}_{i\\in[n]}italic_D = { italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i \u2208 [ italic_n ] end_POSTSUBSCRIPT of documents, assumed to have been selected a priori from an arbitrary retrieval process. The goal is to obtain a re-ranking of the elements of D\ud835\udc37Ditalic_D, via a scoring function rq:D\u2192(0,1):subscript\ud835\udc5f\ud835\udc5e\u2192\ud835\udc3701r_{q}:D\\rightarrow(0,1)italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT : italic_D \u2192 ( 0 , 1 ) where rq\u2062(di)\u2208(0,1)subscript\ud835\udc5f\ud835\udc5esubscript\ud835\udc51\ud835\udc5601r_{q}(d_{i})\\in(0,1)italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) \u2208 ( 0 , 1 ) represents the degree of the relevance of the document disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to the given query q\ud835\udc5eqitalic_q. The re-ranked document is the new sequence D\u2032superscript\ud835\udc37\u2032D^{\\prime}italic_D start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT consisting of the elements disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT sorted in order of increasing relevance score rq\u2062(di)subscript\ud835\udc5f\ud835\udc5esubscript\ud835\udc51\ud835\udc56r_{q}(d_{i})italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). The re-ranking process is popularly executed through neural language modeling methods. However, an emerging research area involves using graph models such as Graph Neural Networks (GNNs) for re-ranking. In this scenario, for each pairing of the query q\ud835\udc5eqitalic_q and a document d\u2208D\ud835\udc51\ud835\udc37d\\in Ditalic_d \u2208 italic_D, we may attribute a graph representation G=(V,E)\ud835\udc3a\ud835\udc49\ud835\udc38G=(V,E)italic_G = ( italic_V , italic_E ) that may be specified in multiple ways. After a graph representation G\ud835\udc3aGitalic_G of the data is established, one may then initialize a randomized feature representation on the nodes and adjacency matrix. A GNN can then be applied to this data to obtain a higher level representation of the nodes, condensed into a single feature vector (often mean-pooling) to obtain a uniform representation for the total graph. This representation is used in the computation of the relevance score rq\u2062(d)subscript\ud835\udc5f\ud835\udc5e\ud835\udc51r_{q}(d)italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( italic_d ). An overview of the components of a graph-based re-ranking pipeline is displayed in Figure 1. There are a number of generic Information Retrieval (IR) datasets that have been well-studied and long term standards for a wide range of downstream tasks including passage & document ranking and question-answering Thakur et al. . This section provides a broad overview of a select subset of popular examples that are modified for training and evaluating graph-based re-ranking techniques discussed in the rest of this paper. Re-ranking is typically presented as a subtask within generic passage and document-level retrieval benchmarks Bajaj et al. (2016). Document retrieval tasks aim to rank a set of documents based on similarity to a query, while passage retrieval tasks aim to rank relevant document-parts or pieces of text (passages) rather than an entire ranked set of documents. The Microsoft Machine Reading Comprehension (MS MARCO) dataset is an example of a benchmark that provides test and evaluation datasets for both document and passage-level retrieval tasks Bajaj et al. (2016). MSMARCO is derived from sampled Bing search queries and corresponding web pages and is a widely used benchmark for passage and document ranking. MSMARCO has established itself as a standard in the re-ranking community. It has been reused within several community-wide shared tasks such as those presented Text REtrieval Conference (TREC) Voorhees and Harman (2005). Similar document and passage-level datasets have been developed for domain-specific retrieval tasks. For example, CLEF-IP Piroi et al. (2011) is a domain-specific retrieval dataset that focuses on information retrieval within the Intellectual Property (IP) domain. Ranking algorithms can generally be categorized into Pointwise, Pairwise, and Listwise approaches Cao et al. (2007). The difference among the three categories depend on the number of documents considered at a time in the loss function during training. Pointwise approaches consider single document similarity, Pairwise focus on similarity of a pair of documents to a query, and lastly Listwise approaches consider a similarity of a list of documents Cao et al. (2007). In the next section, we document current graph-based re-ranking models, organized in the above categories. The widespread impact of Pre-trained Language Models (PLM) has also extended to the information retrieval field. In recent years, IR has greatly benefited from various state-of-the-art (SOTA) retrieval frameworks. However, graph-based retrieval remains relatively unexplored in the PLM era. Therefore, we aim to present a comprehensive overview of contemporary graph-based retrieval methods, emphasizing but not limited to PLMs. Table 1 presents a summary of these approaches, focusing on the key aspects of both the graph-based and IR approaches. In the table, the Task represents the retrieval goal (Document/Passage retrieval). Point, Pair, and List stands for the respective re-ranking strategies, are defined in Section 3.0.2. Doc, Doc-part, and Entity are named based on the node\u2019s content. Finally, the Relation column indicates whether the approach directly incorporates inter-document or inter-passage relationships in the re-ranking process. PassageRank Reed and Madabushi (2020) a graph-based passage ranking pipeline represents each passage as a node while the edge is represented by similarity scores between the nodes in a directed graph. Scores of each node Visubscript\ud835\udc49\ud835\udc56V_{i}italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is calculated as, where I\u2062n\u2062(Vi)\ud835\udc3c\ud835\udc5bsubscript\ud835\udc49\ud835\udc56In(V_{i})italic_I italic_n ( italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) is set of vertices pointing toward Visubscript\ud835\udc49\ud835\udc56V_{i}italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, O\u2062u\u2062t\u2062(Vi)\ud835\udc42\ud835\udc62\ud835\udc61subscript\ud835\udc49\ud835\udc56Out(V_{i})italic_O italic_u italic_t ( italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) is set of vertices where Visubscript\ud835\udc49\ud835\udc56V_{i}italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT points to, wj\u2062isubscript\ud835\udc64\ud835\udc57\ud835\udc56w_{ji}italic_w start_POSTSUBSCRIPT italic_j italic_i end_POSTSUBSCRIPT is the weight between node i and j, and d\ud835\udc51ditalic_d is the damping factor. Finally, a Bidirectional Encoder Representations from Transformers (BERT) encoder model re-ranks the documents Devlin (2018). Similar to the previous pointwise approach, traditional two-phased retrievers suffer from recall limitations where the re-ranker\u2019s performance is subjected to the initially retrieved documents. To address this, MacAvaney et al. (2022b) propose Graph Adaptive Re-ranking (GAR), based on the clustering hypothesis Jardine and van Rijsbergen (1971), which suggest closely related documents being relevant to a given query. The authors implement a feedback process to incrementally update the pool of candidate documents with the neighbors from a corpus graph, a directed graph encoding the similarity between documents. This allows for the re-ranking of documents otherwise skipped due to the re-ranking budget. GAR takes an initial pool of candidate documents R0subscript\ud835\udc450R_{0}italic_R start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, a batch size b\ud835\udc4fbitalic_b, a re-ranking budget RBsubscript\ud835\udc45\ud835\udc35R_{B}italic_R start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT, and a corpus graph as input. The output of the process is a re-ranked pool R1subscript\ud835\udc451R_{1}italic_R start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. The re-ranking further leverages a dynamically updated re-ranking pool P\ud835\udc43Pitalic_P and graph frontier F\ud835\udc39Fitalic_F. P\ud835\udc43Pitalic_P is initialized with the document pool and the F\ud835\udc39Fitalic_F is kept empty. At each iteration, top-b\ud835\udc4fbitalic_b documents are re-ranked, represented by B\ud835\udc35Bitalic_B. Consequently, the neighbors of B\ud835\udc35Bitalic_B are extracted from the corpus graph and are inserted into F\ud835\udc39Fitalic_F. The process repeats, however, instead of only scoring top-b\ud835\udc4fbitalic_b documents from the initial candidate pool the process scores documents alternatively from the initial candidate pool and the frontier. This process continues until the re-ranking budget permits. Another work Jaenich et al. (2024) extends GAR while focusing on fair exposure of individual document groups in the candidate list. This experiment leverages groups from an already labeled dataset. The proposed approach modifies the original GAR based on two categories of policies: the pre-retrieval policies concerned with modifying the corpus graph and the in-process policies that modify the GAR pipeline itself. Policy-1 dictates that only documents from different groups can share an edge in the corpus graph. Policy-2 relaxes policy-1 by allowing a limited number of documents from each group in the corpus graph. Policy-3 modifies the set of neighbors corresponding to each re-ranked document, disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT in a batch before adding them to the frontier. A document is discarded from the neighbor set if it belongs to the same group as disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Similar to Policy 2, policy-4 defines a quota for each group of documents in the frontier. Rather than only selecting the top-b documents in each batch, policy-5 enforces selecting a specific number of highest-scoring documents from each group. It ensures that the neighbors in the corpus graph have sufficient diversity. In addition to selecting a batch based on the highest-scoring document from each group, policy-6 further considers the order in which the documents were inserted into the frontier leading to prioritizing the documents scored in the first iteration. While many approaches inspired by the clustering hypothesis leverage inter-document similarity, they are limited by larger proportions of irrelevant passages. Sarwar and O\u2019Riordan (2021) propose a novel graph-based method to model the inter-passage similarities for re-ranking documents. The model represents the cohesion of each document by capturing the topic-shift of passages, where passages related to the same topic represent high document cohesion. The process starts with decomposing each document into passages. It then generates a graph with the passages as nodes and their relevance as edges. The cohesion score is the average similarity of all possible pairs of passages in a document. The document re-ranking is implemented in one of three settings: i) s\u2062i\u2062m\u2062(di,q)\u00d7C\u2062(di)\ud835\udc60\ud835\udc56\ud835\udc5asubscript\ud835\udc51\ud835\udc56\ud835\udc5e\ud835\udc36subscript\ud835\udc51\ud835\udc56sim(d_{i},q)\\times C(d_{i})italic_s italic_i italic_m ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q ) \u00d7 italic_C ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), ii) s\u2062i\u2062m\u2062(di,q)+C\u2062(di)\ud835\udc60\ud835\udc56\ud835\udc5asubscript\ud835\udc51\ud835\udc56\ud835\udc5e\ud835\udc36subscript\ud835\udc51\ud835\udc56sim(d_{i},q)+C(d_{i})italic_s italic_i italic_m ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q ) + italic_C ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), or iii) s\u2062i\u2062m\u2062(di,q)+C\u2062(d\u2062i)\u00d7X\ud835\udc60\ud835\udc56\ud835\udc5asubscript\ud835\udc51\ud835\udc56\ud835\udc5e\ud835\udc36\ud835\udc51\ud835\udc56\ud835\udc4bsim(d_{i},q)+{C(di)\\times X}italic_s italic_i italic_m ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q ) + italic_C ( italic_d italic_i ) \u00d7 italic_X, where X\u2208[0.0\u22121.0]\ud835\udc4bdelimited-[]0.01.0X\\in[0.0-1.0]italic_X \u2208 [ 0.0 - 1.0 ] allows selective inclusion of cohesion, s\u2062i\u2062m\u2062(di,q)\ud835\udc60\ud835\udc56\ud835\udc5asubscript\ud835\udc51\ud835\udc56\ud835\udc5esim(d_{i},q)italic_s italic_i italic_m ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q ) is query-similarity, and C\u2062(i)\ud835\udc36\ud835\udc56C(i)italic_C ( italic_i ) is the cohesion score. The previously discussed methods did not leverage the structural information in the corpus graphs to update document feature representations. To overcome this, Yu et al. Yu et al. (2021) propose KG-FiD, a Knowledge Graph (KG) enhanced FiD that incorporates GNN to leverage the inter-passage relationship. It introduces a two-stage re-ranking pipeline. The first stage is concerned with re-ranking the N0subscript\ud835\udc410N_{0}italic_N start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT passages returned by the initial retriever, the top N1subscript\ud835\udc411N_{1}italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT of which is leveraged in the second stage. The second stage re-ranks top N2subscript\ud835\udc412N_{2}italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT passages from N1subscript\ud835\udc411N_{1}italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and generates an answer. Another method of modeling inter-document relationships for GNN learning is via Abstract Meaning Representation (AMR) graphs. Dong et al. (2024a) develop G-RAG, which models the inter-document relationship via an Abstract Meaning Representation (AMR) graph. The feature representations of the nodes that appear in the shortest path between the query and target document are aggregated for contextually augmenting the document node representation, facilitating the final re-ranking process. Graph Neural Re-ranking (GNRR) Di Francesco et al. (2024) is another method that addresses document relations by allowing each query to incorporate document distribution during the inference process. This approach models the document relationship using corpus sub-graphs and encodes these representations using GNN. GNRR comprises 3 main phases: i) Data Retrieval, ii) Subgraph Construction, and iii) Features and Score Computation. Given a query q and the document corpus C\ud835\udc36Citalic_C, a sparse retrieval fetches top-1000q documents R0subscript\ud835\udc450R_{0}italic_R start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and builds a semantic corpus graph, GCsubscript\ud835\udc3a\ud835\udc36G_{C}italic_G start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT leveraging TCT-ColBERT Lin et al. (2020). TCT-ColBERT also encodes the query. GCsubscript\ud835\udc3a\ud835\udc36G_{C}italic_G start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT then facilitate the generation of a query-induced subgraph, GC\u2062q=(Vq,Eq)subscript\ud835\udc3a\ud835\udc36\ud835\udc5esubscript\ud835\udc49\ud835\udc5esubscript\ud835\udc38\ud835\udc5eG_{Cq}=(V_{q},E_{q})italic_G start_POSTSUBSCRIPT italic_C italic_q end_POSTSUBSCRIPT = ( italic_V start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) where, Vq\u2208R0subscript\ud835\udc49\ud835\udc5esubscript\ud835\udc450V_{q}\\in R_{0}italic_V start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT \u2208 italic_R start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. Therefore, GC\u2062qsubscript\ud835\udc3a\ud835\udc36\ud835\udc5eG_{Cq}italic_G start_POSTSUBSCRIPT italic_C italic_q end_POSTSUBSCRIPT retains the lexical information from the sparse retriever and the structural information of the corpus graph. The node representations of GC\u2062qsubscript\ud835\udc3a\ud835\udc36\ud835\udc5eG_{Cq}italic_G start_POSTSUBSCRIPT italic_C italic_q end_POSTSUBSCRIPT are computed using an element-wise product between the query and each document. While the document-interactions are modeled by the GNN, a Multi-layer Perceptron (MLP) independently calculates the relevance of each query-document pair. Leveraging the merged form of these two representations a score module estimates the final ranks. A common practice in passage retrieval and re-ranking tasks is to leverage contextual information to enhance performance. To this end, Albarede et al. (2022) proposed a merge and late interaction model based on Graph Attention Network (GAT) for passage contextualization. They introduced a novel document graph representation based on inter- and intra-document similarities. Zhang et al. (2021) proposed Iterative Document Re-ranking (IDR), an integral component of their ODQA framework, that addresses the lexical overlap problem leveraging document relationship. IDR models the inter-document relationship by constructing an entity-centric document graph where two documents share an edge based on the common entities. The graph-based re-ranking module in IDR comprises 4 components: i) Contextual Encoding, ii) Graph Attention, iii) Multi-document Fusion, and iv) Document Filter. The process starts with encoding each question q\ud835\udc5eqitalic_q, and document dksubscript\ud835\udc51\ud835\udc58d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT followed by the concatenation of the document representations v\ud835\udc63vitalic_v. A document graph is then constructed where the node representation eisubscript\ud835\udc52\ud835\udc56e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT for each shared entity Eisubscript\ud835\udc38\ud835\udc56E_{i}italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is generated by pooling token embeddings from v\ud835\udc63vitalic_v as ei=p\u2062o\u2062o\u2062l\u2062i\u2062n\u2062g\u2062(t1(i),t2(i),\u2026,t|Ei|(i))subscript\ud835\udc52\ud835\udc56\ud835\udc5d\ud835\udc5c\ud835\udc5c\ud835\udc59\ud835\udc56\ud835\udc5b\ud835\udc54superscriptsubscript\ud835\udc611\ud835\udc56superscriptsubscript\ud835\udc612\ud835\udc56\u2026superscriptsubscript\ud835\udc61subscript\ud835\udc38\ud835\udc56\ud835\udc56e_{i}=pooling(t_{1}^{(i)},t_{2}^{(i)},\u2026,t_{|E_{i}|}^{(i)})italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_p italic_o italic_o italic_l italic_i italic_n italic_g ( italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , \u2026 , italic_t start_POSTSUBSCRIPT | italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ); here, tj(i)superscriptsubscript\ud835\udc61\ud835\udc57\ud835\udc56t_{j}^{(i)}italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT is the j\ud835\udc57jitalic_j-th token in Eisubscript\ud835\udc38\ud835\udc56E_{i}italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. A GAT processes the document graph by updating each entity representation. The next stage, multi-document fusion, updates the non-entity tokens by projecting them over the entity tokens to generate v\u2032superscript\ud835\udc63\u2032v^{\\prime}italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT which is fed into a transformer layer to obtain the fused representation vector v~~\ud835\udc63\\tilde{v}over~ start_ARG italic_v end_ARG. Finally, leveraging the fused representation a binary classifier scores the documents. The structural information of knowledge graphs provides an effective means to capture the relationship between documents. Motivated by its importance in re-ranking, Vollmers et al. applied a Graph Attention Network (GAT) as a cross-encoder over the proposed query-document knowledge sub-graph. Within the graph, the nodes represent entities extracted from a query-document pair, while the edges represent their semantic relationship. The proposed approach computes the query-document relevance in 3 steps: entity extraction and linking, subgraph retrieval, and GAT-based re-ranking. In the first step, FLAIRAkbik et al. (2018) is utilized to extract entities from the documents, while Llama3 is used to disambiguate and extract entities from the queries as it has higher accuracy with shorter text. Following this, GENRE De Cao et al. (2021), an autoregressive model is used to link the extracted entities with their corresponding URIs in a knowledge graph. The next step is subgraph extraction. To this end, the Subgraph Retrieval Toolkit (SRTK) Shen (2023); Zhang et al. (2022) is used to provide a list of RDF triples from the target KG against a list of entities. In the final step, the GAT cross-encoder is used to encode the subgraph where the output node embedding of the query and the documents are used to calculate the relevance score. Cross-encoder is the driving force of many SOTA re-ranking pipelines. However, by itself, the cross-encoder lacks background knowledge, a critical element for effective passage retrieval, especially in domain-specific tasks. Therefore, Fang et al. (2023) propose KGPR, a KG-enhanced cross-encoder for passage re-ranking. At its core KGPR uses LUKE Yamada et al. (2020), an entity-aware pre-trained language model, to incorporate KG to facilitate background information. LUKE-based cross-encoder calculates query-passage relevance utilizing query, passage, and their corresponding entity embedding. However, by default, the LUKE model disregards the entity relations. Therefore, the proposed method leverages a knowledge subgraph extracted from the Freebase Bollacker et al. (2008). The subgraph extraction is carried out in two phases: entity linking and subgraph retrieval. The former identifies entities from both the query and the passage to link them with Freebase nodes. To this end, KGPR utilizes ELQ Li et al. (2020), an entity-linking model for questions. The extracted passage entities are then traced up to 1-hop from the query entities in the Firebase to extract the subgraph Gq,dsubscript\ud835\udc3a\ud835\udc5e\ud835\udc51G_{q,d}italic_G start_POSTSUBSCRIPT italic_q , italic_d end_POSTSUBSCRIPT, while filtering out edges not related to passage entities. The authors introduce an additional embedding for relations in a KG triple. During the re-ranking process, the triple embedding servers as an additional input for LUKE, effectively infusing background knowledge. The inherent noise associated with existing KGs makes them suboptimal for re-ranking tasks. To address this, Dong et al. (2022) introduced a re-ranking-centric knowledge meta graph distillation module with their Knowledge Enhanced Re-ranking Model (KERM). Additionally, the authors introduced a knowledge injector, an aggregation module to bridge the semantic gap that emerges during the aggregation of implicit and explicit knowledge. At first, KERM employs TransE Bordes et al. (2013), a knowledge graph embedding model, to prune noises from a global knowledge graph, G\ud835\udc3aGitalic_G to obtain G\u2032superscript\ud835\udc3a\u2032G^{\\prime}italic_G start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT. In the pruning process, only the top-\u03c0\ud835\udf0b\\piitalic_\u03c0 neighbors of each entity are considered based on the distance metric, dist(eh,et)=1/{E(eh).E(r)+E(eh).E(et)+E(r).E(et)}dist(e_{h},e_{t})=1/\\{E(e_{h}).E(r)+E(e_{h}).E(e_{t})+E(r).E(e_{t})\\}italic_d italic_i italic_s italic_t ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = 1 / { italic_E ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) . italic_E ( italic_r ) + italic_E ( italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) . italic_E ( italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + italic_E ( italic_r ) . italic_E ( italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) } where, ehsubscript\ud835\udc52\u210ee_{h}italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT, r\ud835\udc5fritalic_r, and etsubscript\ud835\udc52\ud835\udc61e_{t}italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT are a head entity, relation, and tail entity of a triplet, respectively. Following this, the bipartite meta-graph, Gq,psubscript\ud835\udc3a\ud835\udc5e\ud835\udc5dG_{q,p}italic_G start_POSTSUBSCRIPT italic_q , italic_p end_POSTSUBSCRIPT is constructed in three phases. It begins by selecting the most relevant sentence to the query, followed by extracting its entities, and finally linking them with the top-k hops from G\u2032superscript\ud835\udc3a\u2032G^{\\prime}italic_G start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT using Breadth-First Search (BFS). At this stage, a knowledge injector is utilized to further enhance the explicit knowledge. A Graph Meta Network (GMN) module, a multi-layer GNN integrated into the knowledge injector, dynamically enriches the meta-graph by utilizing the encoded textual information (i.e., implicit knowledge). It leads to the mutual enhancement of both the text and the knowledge embedding. This enhanced representation facilitates cross-encoder\u2019s re-ranking process. Gupta and Demner-Fushman (2024) also addresses the shortcomings of PLM-based re-rankers on domain-specific tasks. The proposed approach, GraphMonoT5, fuses external knowledge from KG into PLMs to facilitate biomedical document re-ranking. On top of the default encoder-decoder T5, the GraphMonoT5 complements the encoder with a GNN to account for the external knowledge modeled in KG. Entities extracted from each query-document pair are linked to a KG followed by the subgraph extraction, connecting all the nodes within 2-hop paths of corresponding entities. The GraphMonoT5 introduces a novel interaction module aimed at fusing the text and the graph embeddings leveraging interaction tokens, ti\u2062n\u2062tsubscript\ud835\udc61\ud835\udc56\ud835\udc5b\ud835\udc61t_{int}italic_t start_POSTSUBSCRIPT italic_i italic_n italic_t end_POSTSUBSCRIPT, interaction nodes, ni\u2062n\u2062tsubscript\ud835\udc5b\ud835\udc56\ud835\udc5b\ud835\udc61n_{int}italic_n start_POSTSUBSCRIPT italic_i italic_n italic_t end_POSTSUBSCRIPT, and their corresponding embeddings. The interaction embeddings are merged into a single representation, which together with the text and node embedding from a query-document pair, is fed into the GraphMonoT5 decoder to yield a ranking soccer. The documents are then re-ranked based on these scores. Veningston and Shanmugalakshmi (2014) proposed a document re-ranking strategy leveraging a term graph that models word relations among documents. The term graph, GTsubscript\ud835\udc3a\ud835\udc47G_{T}italic_G start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT = (VTsubscript\ud835\udc49\ud835\udc47V_{T}italic_V start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT, ETsubscript\ud835\udc38\ud835\udc47E_{T}italic_E start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT), is built using frequent item-sets, FSsubscript\ud835\udc39\ud835\udc46F_{S}italic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT which represents the document with its most frequent terms. In the graph, VT\u2208fSsubscript\ud835\udc49\ud835\udc47subscript\ud835\udc53\ud835\udc46V_{T}\\in f_{S}italic_V start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT \u2208 italic_f start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT where fs\u2208FSsubscript\ud835\udc53\ud835\udc60subscript\ud835\udc39\ud835\udc46f_{s}\\in F_{S}italic_f start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT \u2208 italic_F start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT is an unique item, and ET\u2286VT\u2062i\u00d7VT\u2062jsubscript\ud835\udc38\ud835\udc47subscript\ud835\udc49\ud835\udc47\ud835\udc56subscript\ud835\udc49\ud835\udc47\ud835\udc57E_{T}\\subseteq V_{Ti}\\times V_{Tj}italic_E start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT \u2286 italic_V start_POSTSUBSCRIPT italic_T italic_i end_POSTSUBSCRIPT \u00d7 italic_V start_POSTSUBSCRIPT italic_T italic_j end_POSTSUBSCRIPT only if VT\u2062isubscript\ud835\udc49\ud835\udc47\ud835\udc56V_{Ti}italic_V start_POSTSUBSCRIPT italic_T italic_i end_POSTSUBSCRIPT and VT\u2062jsubscript\ud835\udc49\ud835\udc47\ud835\udc57V_{Tj}italic_V start_POSTSUBSCRIPT italic_T italic_j end_POSTSUBSCRIPT share the same item-set. The edge weight corresponds to the largest support value, s\u2062u\u2062p\u2062p\u2062o\u2062r\u2062td\ud835\udc60\ud835\udc62\ud835\udc5d\ud835\udc5d\ud835\udc5c\ud835\udc5fsubscript\ud835\udc61\ud835\udc51support_{d}italic_s italic_u italic_p italic_p italic_o italic_r italic_t start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT of the item-set, FS\u2062dsubscript\ud835\udc39\ud835\udc46\ud835\udc51F_{Sd}italic_F start_POSTSUBSCRIPT italic_S italic_d end_POSTSUBSCRIPT containing both the corresponding nodes. S\u2062u\u2062p\u2062p\u2062o\u2062r\u2062td\ud835\udc46\ud835\udc62\ud835\udc5d\ud835\udc5d\ud835\udc5c\ud835\udc5fsubscript\ud835\udc61\ud835\udc51Support_{d}italic_S italic_u italic_p italic_p italic_o italic_r italic_t start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT is calculated as \u2211i=1nfS\u2062d\u2062(ti)/\u2211j=1N\u2211i=1nfd\u2062j\u2062(ti)superscriptsubscript\ud835\udc561\ud835\udc5bsubscript\ud835\udc53\ud835\udc46\ud835\udc51subscript\ud835\udc61\ud835\udc56superscriptsubscript\ud835\udc571\ud835\udc41superscriptsubscript\ud835\udc561\ud835\udc5bsubscript\ud835\udc53\ud835\udc51\ud835\udc57subscript\ud835\udc61\ud835\udc56\\sum_{i=1}^{n}f_{Sd}(t_{i})/\\sum_{j=1}^{N}\\sum_{i=1}^{n}f_{dj}(t_{i})\u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT italic_S italic_d end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) / \u2211 start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT italic_d italic_j end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) where fd\u2062j\u2062(ti)subscript\ud835\udc53\ud835\udc51\ud835\udc57subscript\ud835\udc61\ud835\udc56f_{dj}(t_{i})italic_f start_POSTSUBSCRIPT italic_d italic_j end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) is the frequency of term tisubscript\ud835\udc61\ud835\udc56t_{i}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT in document d\ud835\udc51ditalic_d, n\ud835\udc5bnitalic_n is the number of terms in item-set, and N\ud835\udc41Nitalic_N is total number of item-sets. The term graph is leveraged in document re-ranking following either of the two approaches: i) Term Rank-based document re-ranking, or ii) Term Distance matrix-based document re-ranking. The former is based on the PageRank algorithm Brin and Page (1998) that computes the term\u2019s rankings. Term ranks, calculated via PageRank, are utilized to re-rank the documents based on the shared terms between the query and the documents. The latter re-ranking approach leverages the term distance matrix, an adjacency matrix representing the distance between any two terms as the least number of hops. A document is ranked higher if its terms are at a closer distance to the terms of a given query. So far, we\u2019ve only focused on pointwise re-ranking, which evaluates each document in isolation. In contrast, pairwise re-ranking scores n\ud835\udc5bnitalic_n documents by evaluating all n\u00d7n\ud835\udc5b\ud835\udc5bn\\times nitalic_n \u00d7 italic_n pairs, capturing subtle nuances during re-ranking. PLM-based pair-wise re-ranking has demonstrated remarkable performance. However, the high inference overhead diminishes its feasibility. To address this, Gienapp et al. (2022) employs a subset of pairs sampled from the full set of document pairs. Consequently, the aggregation process that computes final rankings from the preference probabilities of document pairs, becomes even more crucial in the proposed approach. One of the proposed aggregation approaches draws inspiration from the PageRank algorithm. To this end, the sampled subset of document C\ud835\udc36Citalic_C is converted into a directed graph where nodes represent documents while edges represent the preference probabilities. For a given pair of documents (di,dj)subscript\ud835\udc51\ud835\udc56subscript\ud835\udc51\ud835\udc57(d_{i},d_{j})( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ), the preference probability, pi\u2062jsubscript\ud835\udc5d\ud835\udc56\ud835\udc57p_{ij}italic_p start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT represents the likelihood of disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT outranking djsubscript\ud835\udc51\ud835\udc57d_{j}italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT. The re-ranking is facilitated by the PageRank algorithm adapted for weighted edges. In the main re-ranking pipeline, the proposed approach utilizes a mono-duo architecture. Initially, monoT5, a pointwise re-ranker, ranks the top-1000 documents. This is followed by the duoT5, pairwise re-ranker, which re-ranks the top-50 results from the previous step. Pairwise Ranking Prompting (PRP) is a zero-shot re-ranking method based on Large Language Models (LLM). However, contemporary PRP methods do not account for the uncertainty associated with the labels. To overcome this limitation, Luo et al. (2024) proposed a PRP-graph coupled with a novel scoring PRP unit. The PRP-graph functions in two distinct phases. First, it generates a ranking graph discussed in section Document-level graphs. Inspired by the weighted PageRank algorithm, the vertices of the document graph are iteratively updated to reflect the final re-ranking score. Deng et al. (2009) present a unique approach to re-rank documents. The documents in the corpus are initially ranked using p\u2062(q\u2223\u03b8d)=\u220ft\u2208qp\u2062(t\u2223\u03b8d)n\u2062(t,q)\ud835\udc5dconditional\ud835\udc5esubscript\ud835\udf03\ud835\udc51subscriptproduct\ud835\udc61\ud835\udc5e\ud835\udc5dsuperscriptconditional\ud835\udc61subscript\ud835\udf03\ud835\udc51\ud835\udc5b\ud835\udc61\ud835\udc5ep(q\\mid\\theta_{d})=\\prod_{t\\in q}p(t\\mid\\theta_{d})^{n(t,q)}italic_p ( italic_q \u2223 italic_\u03b8 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) = \u220f start_POSTSUBSCRIPT italic_t \u2208 italic_q end_POSTSUBSCRIPT italic_p ( italic_t \u2223 italic_\u03b8 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_n ( italic_t , italic_q ) end_POSTSUPERSCRIPT, where p\u2062(q\u2223\u03b8d)\ud835\udc5dconditional\ud835\udc5esubscript\ud835\udf03\ud835\udc51p(q\\mid\\theta_{d})italic_p ( italic_q \u2223 italic_\u03b8 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) is the maximum likelihood estimation of the term t\ud835\udc61titalic_t in a document d\ud835\udc51ditalic_d, and n\u2062(t,q)\ud835\udc5b\ud835\udc61\ud835\udc5en(t,q)italic_n ( italic_t , italic_q ) is the number of times that term t\ud835\udc61titalic_t occurs in query q\ud835\udc5eqitalic_q. A document is ranked higher if it has a higher likelihood of generating the query. After a latent space graph is created(explained in Section Document-level graphs), the following cost function is optimized to re-rank the documents. The first sum 12\u2062\u2211i,j=1nwi\u2062j\u2062\u2016f\u2062(di,q)Di\u2062i\u2212f\u2062(dj,q)Dj\u2062j\u2016212superscriptsubscript\ud835\udc56\ud835\udc571\ud835\udc5bsubscript\ud835\udc64\ud835\udc56\ud835\udc57superscriptnorm\ud835\udc53subscript\ud835\udc51\ud835\udc56\ud835\udc5esubscript\ud835\udc37\ud835\udc56\ud835\udc56\ud835\udc53subscript\ud835\udc51\ud835\udc57\ud835\udc5esubscript\ud835\udc37\ud835\udc57\ud835\udc572\\frac{1}{2}\\sum_{i,j=1}^{n}w_{ij}\\left\\|\\frac{f(d_{i},q)}{\\sqrt{D_{ii}}}-\\frac% {f(d_{j},q)}{\\sqrt{D_{jj}}}\\right\\|^{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARG \u2211 start_POSTSUBSCRIPT italic_i , italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_w start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT \u2225 divide start_ARG italic_f ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q ) end_ARG start_ARG square-root start_ARG italic_D start_POSTSUBSCRIPT italic_i italic_i end_POSTSUBSCRIPT end_ARG end_ARG - divide start_ARG italic_f ( italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_q ) end_ARG start_ARG square-root start_ARG italic_D start_POSTSUBSCRIPT italic_j italic_j end_POSTSUBSCRIPT end_ARG end_ARG \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ensures that documents with high similarity should have similar ranking scores. This is the global consistency term, where the cost increases if similar documents end up with very different ranking scores. The second sum \u03bc\u2062\u2211i=1n\u2016f\u2062(di,q)\u2212f0\u2062(di,q)\u20162\ud835\udf07superscriptsubscript\ud835\udc561\ud835\udc5bsuperscriptnorm\ud835\udc53subscript\ud835\udc51\ud835\udc56\ud835\udc5esuperscript\ud835\udc530subscript\ud835\udc51\ud835\udc56\ud835\udc5e2\\quad\\mu\\sum_{i=1}^{n}\\left\\|f(d_{i},q)-f^{0}(d_{i},q)\\right\\|^{2}italic_\u03bc \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT \u2225 italic_f ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q ) - italic_f start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q ) \u2225 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTensures that the refined ranking scores stay close to the initial ranking scores. This term prevents the re-ranking from deviating too much from the original scores. Rathee et al. (2025) extends GARMacAvaney et al. (2022b) to the LLM-based listwise re-ranker. The existing GAR, relying on the Probability Ranking Principle, does not consider the document relationship while calculating the relevance score. Therefore, the proposed approach SlideGAR adapts the GAR from pointwise re-ranking to listwise setting to account for document relationship. After retrieving an initial ranked list of documents using BM25, SlideGar applies a sliding window approach with window size w\ud835\udc64witalic_w and step size b\ud835\udc4fbitalic_b. The top w\ud835\udc64witalic_w documents from the initial ranking are selected, and, similar to GAR, the graph frontier is updated with their neighboring nodes. However, unlike GAR, which ranks documents individually, SlideGar ranks batches of documents simultaneously using a listwise LLM re-ranker such as RankZephyrPradeep et al. (2023). The ranking process alternates between the initial ranked list and the graph frontier, ensuring a more adaptive and context-aware document retrieval strategy. Graph-based re-ranking methods enhance retrieval performance by incorporating document relationships, surpassing traditional ranking models. The re-ranking models discussed thus far rely on graph data structures, which can be broadly categorized into document-level and entity-level graphs. In document-level graphs, edges represent relationships between documents/document parts, whereas entity-level graphs establish connections between individual tokens or concepts. This section explores some of the notable strategies used to construct both types of graphs, detailing their formation, edge weighting, and relevance to re-ranking tasks. Let G = (V, E, A, R, Adj) be a directed/undirected graph, where V is the set of nodes and E\u2286V\u00d7V\ud835\udc38\ud835\udc49\ud835\udc49E\\subseteq V\\times Vitalic_E \u2286 italic_V \u00d7 italic_V is the set of edges. The number of nodes in G is denoted by |V|=N\ud835\udc49\ud835\udc41|V|=N| italic_V | = italic_N, and the number of edges by |E|\ud835\udc38|E|| italic_E |. A is the set of node types while R denotes the set of edge types. The set of edges E can also be expressed as the adjacency matrix A\u2062d\u2062j\u2208{0,1}N\u00d7N\ud835\udc34\ud835\udc51\ud835\udc57superscript01\ud835\udc41\ud835\udc41Adj\\in\\{0,1\\}^{N\\times N}italic_A italic_d italic_j \u2208 { 0 , 1 } start_POSTSUPERSCRIPT italic_N \u00d7 italic_N end_POSTSUPERSCRIPT, where A\u2062d\u2062ju\u2062v\ud835\udc34\ud835\udc51subscript\ud835\udc57\ud835\udc62\ud835\udc63Adj_{uv}italic_A italic_d italic_j start_POSTSUBSCRIPT italic_u italic_v end_POSTSUBSCRIPT = 1 if nodes(u,v)\u2208E\ud835\udc62\ud835\udc63\ud835\udc38(u,v)\\in E( italic_u , italic_v ) \u2208 italic_E are connected, and A\u2062d\u2062ju\u2062v\ud835\udc34\ud835\udc51subscript\ud835\udc57\ud835\udc62\ud835\udc63Adj_{uv}italic_A italic_d italic_j start_POSTSUBSCRIPT italic_u italic_v end_POSTSUBSCRIPT= 0 otherwise. In Di Francesco et al. (2024); MacAvaney et al. (2022b) each pair of documents (disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, djsubscript\ud835\udc51\ud835\udc57d_{j}italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT) in the graph corpus shares a connection based on the cosine similarity between their document encodings. After the graph structure for each query is established, the features of the nodes are defined by performing an element-wise product between the query representation and each document representation. Therefore the node feature for d\u2062o\u2062c\u2062u\u2062m\u2062e\u2062n\u2062ti\ud835\udc51\ud835\udc5c\ud835\udc50\ud835\udc62\ud835\udc5a\ud835\udc52\ud835\udc5bsubscript\ud835\udc61\ud835\udc56document_{i}italic_d italic_o italic_c italic_u italic_m italic_e italic_n italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is given by \ud835\udc31i=\ud835\udc33q\u2299\ud835\udc33disubscript\ud835\udc31\ud835\udc56direct-productsubscript\ud835\udc33\ud835\udc5esubscript\ud835\udc33subscript\ud835\udc51\ud835\udc56\\mathbf{x}_{i}=\\mathbf{z}_{q}\\odot\\mathbf{z}_{d_{i}}bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_z start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT \u2299 bold_z start_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT, \ud835\udc33qsubscript\ud835\udc33\ud835\udc5e\\mathbf{z}_{q}bold_z start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT and \ud835\udc33disubscript\ud835\udc33subscript\ud835\udc51\ud835\udc56\\mathbf{z}_{d_{i}}bold_z start_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT being the query and document encodings respectively. Zhang et al. (2021) utilizes the Named Entity Recognition system to extract entities and connect documents if they have shared entities. Instead of merely linking nodes based on document similarity, Albarede et al. (2022) introduces a more structured approach by dividing document nodes into section nodes (non-textual units with titles) and passage nodes (textual units without titles). Additionally, the framework defines eight distinct edge types, consisting of four primary relations and their respective inverses: (1) Order relation \u2013 captures the sequential arrangement between passage nodes. (2) Structural relation \u2013 represents connections between a passage and its parent section node or between two section nodes. (3) Internal relation \u2013 links nodes within the same document. (4) External relation \u2013 connects nodes across different documents. This hierarchical and relational structure enhances the granularity and contextual understanding of document graphs. G-RAG Dong et al. (2024a) considers each document to be a text block of 100 words. Each question is concatenated with documents in the corpus. AMRBART Bai et al. (2022) is utilized to create AMR graphsBanarescu et al. (2013). From the graphs, the connection information between different documents is incorporated into the edge features in the subsequent document-level graph. Thus an undirected document graph Gqsubscript\ud835\udc3a\ud835\udc5eG_{q}italic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT = {V, E} based on AMRs {Gq\u2062p\u20621subscript\ud835\udc3a\ud835\udc5e\ud835\udc5d1G_{qp1}italic_G start_POSTSUBSCRIPT italic_q italic_p 1 end_POSTSUBSCRIPT, \u00b7 \u00b7 \u00b7 , Gq\u2062p\u2062nsubscript\ud835\udc3a\ud835\udc5e\ud835\udc5d\ud835\udc5bG_{qpn}italic_G start_POSTSUBSCRIPT italic_q italic_p italic_n end_POSTSUBSCRIPT} is established. Each node vi\u2208Vsubscript\ud835\udc63\ud835\udc56\ud835\udc49v_{i}\\in Vitalic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 italic_V corresponds to the document pisubscript\ud835\udc5d\ud835\udc56p_{i}italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. For vi,vj\u2208Vsubscript\ud835\udc63\ud835\udc56subscript\ud835\udc63\ud835\udc57\ud835\udc49v_{i},v_{j}\\in Vitalic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u2208 italic_V where i\u2260j\ud835\udc56\ud835\udc57i\\neq jitalic_i \u2260 italic_j if AMRs Gq\u2062p\u2062isubscript\ud835\udc3a\ud835\udc5e\ud835\udc5d\ud835\udc56G_{qpi}italic_G start_POSTSUBSCRIPT italic_q italic_p italic_i end_POSTSUBSCRIPT and Gq\u2062p\u2062jsubscript\ud835\udc3a\ud835\udc5e\ud835\udc5d\ud835\udc57G_{qpj}italic_G start_POSTSUBSCRIPT italic_q italic_p italic_j end_POSTSUBSCRIPT have common nodes, there will be an undirected edge between visubscript\ud835\udc63\ud835\udc56v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and vjsubscript\ud835\udc63\ud835\udc57v_{j}italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT in the document-level graph. Deng et al. (2009) incorporates link information in a document-level latent space graph. The content matrix term C\u2208\u211dn\u00d7m\ud835\udc36superscript\u211d\ud835\udc5b\ud835\udc5aC\\in\\mathbb{R}^{n\\times m}italic_C \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_m end_POSTSUPERSCRIPT is a sparse matrix whose rows represent documents and columns represent terms, where m\ud835\udc5amitalic_m is the number of terms. The document-author bipartite graph can similarly be described by a matrix A\u2208\u211dn\u00d7l\ud835\udc34superscript\u211d\ud835\udc5b\ud835\udc59A\\in\\mathbb{R}^{n\\times l}italic_A \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_l end_POSTSUPERSCRIPT, which is also a sparse matrix whose rows correspond to documents and whose columns correspond to authors, where l\ud835\udc59litalic_l is the number of authors. These matrices are mapped to a shared latent space X\ud835\udc4bXitalic_X through joint factorizationZhu et al. (2007), thus combining the authorship and content information. The edge weights are determined using a heat kernel as follows where xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and xjsubscript\ud835\udc65\ud835\udc57x_{j}italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT are the latent representations of documents i\ud835\udc56iitalic_i and j\ud835\udc57jitalic_j and \u03c3\ud835\udf0e\\sigmaitalic_\u03c3 is the heat kernel that controls the spread or reach of the edges between nodes in the graph. Specifically, it affects how sensitive the weight of the edge wi\u2062jsubscript\ud835\udc64\ud835\udc56\ud835\udc57w_{ij}italic_w start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT is to the Euclidean distance between two nodes. Smaller \u03c3\ud835\udf0e\\sigmaitalic_\u03c3 will correspond to far apart nodes having edge weights closer to zero and vice versa. Luo et al. (2024) investigates the application of Pairwise Ranking Prompts Qin et al. (2023) for graph construction. A document disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and its closest subsequent document djsubscript\ud835\udc51\ud835\udc57d_{j}italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT are selected and a bidirectional relationship is established through directed edges di\u2192jsubscript\ud835\udc51\u2192\ud835\udc56\ud835\udc57d_{i\\to j}italic_d start_POSTSUBSCRIPT italic_i \u2192 italic_j end_POSTSUBSCRIPT and dj\u2192isubscript\ud835\udc51\u2192\ud835\udc57\ud835\udc56d_{j\\to i}italic_d start_POSTSUBSCRIPT italic_j \u2192 italic_i end_POSTSUBSCRIPT. The edge weights are assigned based on PRP-derived preference scores si\u2192jsubscript\ud835\udc60\u2192\ud835\udc56\ud835\udc57s_{i\\to j}italic_s start_POSTSUBSCRIPT italic_i \u2192 italic_j end_POSTSUBSCRIPT and sj\u2192isubscript\ud835\udc60\u2192\ud835\udc57\ud835\udc56s_{j\\to i}italic_s start_POSTSUBSCRIPT italic_j \u2192 italic_i end_POSTSUBSCRIPT, quantifying the relative ranking between the documents. scores are iteratively updated over r rounds using the following update rules After r rounds, the final ranking graph G is obtained, where node scores reflect the cumulative ranking adjustments over multiple iterations. Deng et al. (2009) propose a two-stage process for graph construction. The process begins with node embedding initialization using TransE Bordes et al. (2013). The pairwise distances between embeddings are computed, and only the top n closest nodes are retained while the rest are pruned. Next, relevant sentence retrieval is performed by measuring the similarity between Word2Vec Mikolov et al. (2013) embeddings of the query and sentences from the corpus. Common entities between the query and retrieved sentences are identified, and a Breadth-First Search (BFS) is conducted to locate all nodes within K-hops of these entities. The resulting nodes are then used to construct a meta graph, capturing the refined structure of the knowledge graph for downstream tasks.Gupta and Demner-Fushman (2024); Yu et al. (2021) follow a similar approach for knowledge graph entity alignment and subgraph creation. Fang et al. (2023) also retrieves entities from a knowledge graph, selectively retaining only the edges that connect entities appearing in either the query or the passage. Graph-based retrieval is an established research area that has produced a wide variety of datasets used primarily for Knowledge Graph Question Answering (KGQA) downstream tasks. KGQA datasets are currently designed for one/multi-hop entity and relationship classification tasks. Current datasets are not well suited for more complex tasks such as passage and document ranking/re-ranking. As a result, there has been an emergence of curated graph construction methodologies and datasets used for graph-based passage and document ranking tasks (Section 5). The diversity of these methods leads to an incongruity across evaluation techniques that aim to measure the performance of current methods. This scenario differs from established benchmark standards developed for language model retrievers. Examples of established benchmark datasets used to evaluate language model-based ranking tasks are provided in Section 3. These datasets were derived from large collections of unstructured data and preprocessed into query and target pairs. Unlike language models, Graph-based techniques such as GNNs additionally require a transformation of unstructured text samples into adjacency matrices of nodes and edges that can be used for training (See Section 2). Section 4 outlines the range of methods demonstrated for first curating the adjacency matrix and additionally, incorporating ranking-based features such as node/edge similarity scores into the graph. The design and quality of the input representation can drastically change architectural considerations for proposed graph-based ranking models, ultimately impacting downstream ranking performance. Despite ongoing progress in this area, a standard benchmark to measure performance has not yet been developed to evaluate graph-based passage and document ranking tasks. For each of the methods presented in Section 4, downstream performance is evaluated on established benchmark datasets like MSMARCO, originally developed specifically for language model based ranking tasks. In traditional settings, datasets such as MSMARCO are typically used for both train and test workloads, with separate tracks for further data augmentation and/or zero-shot performance criteria. In graph-based settings, we have observed that authors derive their own methods for (1) generating an adjacency matrix for the train set, (2) potentially augmenting the train set, with additional external data, and lastly, (3) performing evaluation on the downstream ranking task. We also have documented settings in which the distribution of the training input graph is either only minimally overlapping, or differs completely than that of a particular downstream task. In this case, evaluation assumes a transfer learning process, and is based on zero-shot performance. While relying on downstream performance provides a reference for comparing graph-based ranking models, the community should move towards creating a static benchmark that addresses limitations such as evaluating model architectures and the data curation process, as well as reproducibility."
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Zawad,+N",
    "title": "Showing 1\u20132 of 2 results for author: Zawad, N",
    "author": "Authors:\nMd Shahir Zaoad, \n      \n      Niamat Zawad, \n      \n      Priyanka Ranade, \n      \n      Richard Krogman, \n      \n      Latifur Khan, \n      \n      James Holt",
    "body": "arXiv:2503.14802 [pdf, other] Graph-Based Re-ranking: Emerging Techniques, Limitations, and Opportunities Authors: Md Shahir Zaoad, Niamat Zawad, Priyanka Ranade, Richard Krogman, Latifur Khan, James Holt Abstract: Knowledge graphs have emerged to be promising datastore candidates for context augmentation during Retrieval Augmented Generation (RAG). As a result, techniques in graph representation learning have been simultaneously explored alongside principal neural information retrieval approaches, such as two-phased retrieval, also known as re-ranking. While Graph Neural Networks (GNNs) have been proposed t\u2026 \u25bd More Knowledge graphs have emerged to be promising datastore candidates for context augmentation during Retrieval Augmented Generation (RAG). As a result, techniques in graph representation learning have been simultaneously explored alongside principal neural information retrieval approaches, such as two-phased retrieval, also known as re-ranking. While Graph Neural Networks (GNNs) have been proposed to demonstrate proficiency in graph learning for re-ranking, there are ongoing limitations in modeling and evaluating input graph structures for training and evaluation for passage and document ranking tasks. In this survey, we review emerging GNN-based ranking model architectures along with their corresponding graph representation construction methodologies. We conclude by providing recommendations on future research based on community-wide challenges and opportunities. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. MSC Class: N/A arXiv:1812.03632 [pdf, other] Statement networks: a power structure narrative as depicted by newspapers Authors: Shoumik Sharar Chowdhury, Nazmus Saquib, Niamat Zawad, Manash Kumar Mandal, Syed Haque Abstract: We report a data mining pipeline and subsequent analysis to understand the core periphery power structure created in three national newspapers in Bangladesh, as depicted by statements made by people appearing in news. Statements made by one actor about another actor can be considered a form of public conversation. Named entity recognition techniques can be used to create a temporal actor network f\u2026 \u25bd More We report a data mining pipeline and subsequent analysis to understand the core periphery power structure created in three national newspapers in Bangladesh, as depicted by statements made by people appearing in news. Statements made by one actor about another actor can be considered a form of public conversation. Named entity recognition techniques can be used to create a temporal actor network from such conversations, which shows some unique structure, and reveals much room for improvement in news reporting and also the top actors' conversation preferences. Our results indicate there is a presence of cliquishness between powerful political leaders when it comes to their appearance in news. We also show how these cohesive cores form through the news articles, and how, over a decade, news cycles change the actors belonging in these groups. \u25b3 Less Submitted 10 December, 2018; originally announced December 2018. Comments: Presented at NeurIPS 2018 Workshop on Machine Learning for the Developing World"
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Ranade,+P",
    "title": "Showing 1\u20136 of 6 results for author: Ranade, P",
    "author": "Authors:\nMd Shahir Zaoad, \n      \n      Niamat Zawad, \n      \n      Priyanka Ranade, \n      \n      Richard Krogman, \n      \n      Latifur Khan, \n      \n      James Holt",
    "body": "arXiv:2503.14802 [pdf, other] Graph-Based Re-ranking: Emerging Techniques, Limitations, and Opportunities Authors: Md Shahir Zaoad, Niamat Zawad, Priyanka Ranade, Richard Krogman, Latifur Khan, James Holt Abstract: Knowledge graphs have emerged to be promising datastore candidates for context augmentation during Retrieval Augmented Generation (RAG). As a result, techniques in graph representation learning have been simultaneously explored alongside principal neural information retrieval approaches, such as two-phased retrieval, also known as re-ranking. While Graph Neural Networks (GNNs) have been proposed t\u2026 \u25bd More Knowledge graphs have emerged to be promising datastore candidates for context augmentation during Retrieval Augmented Generation (RAG). As a result, techniques in graph representation learning have been simultaneously explored alongside principal neural information retrieval approaches, such as two-phased retrieval, also known as re-ranking. While Graph Neural Networks (GNNs) have been proposed to demonstrate proficiency in graph learning for re-ranking, there are ongoing limitations in modeling and evaluating input graph structures for training and evaluation for passage and document ranking tasks. In this survey, we review emerging GNN-based ranking model architectures along with their corresponding graph representation construction methodologies. We conclude by providing recommendations on future research based on community-wide challenges and opportunities. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. MSC Class: N/A arXiv:2310.13848 [pdf, other] FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction Authors: Priyanka Ranade, Anupam Joshi Abstract: Narrative construction is the process of representing disparate event information into a logical plot structure that models an end to end story. Intelligence analysis is an example of a domain that can benefit tremendously from narrative construction techniques, particularly in aiding analysts during the largely manual and costly process of synthesizing event information into comprehensive intelli\u2026 \u25bd More Narrative construction is the process of representing disparate event information into a logical plot structure that models an end to end story. Intelligence analysis is an example of a domain that can benefit tremendously from narrative construction techniques, particularly in aiding analysts during the largely manual and costly process of synthesizing event information into comprehensive intelligence reports. Manual intelligence report generation is often prone to challenges such as integrating dynamic event information, writing fine-grained queries, and closing information gaps. This motivates the development of a system that retrieves and represents critical aspects of events in a form that aids in automatic generation of intelligence reports. We introduce a Retrieval Augmented Generation (RAG) approach to augment prompting of an autoregressive decoder by retrieving structured information asserted in a knowledge graph to generate targeted information based on a narrative plot model. We apply our approach to the problem of neural intelligence report generation and introduce FABULA, framework to augment intelligence analysis workflows using RAG. An analyst can use FABULA to query an Event Plot Graph (EPG) to retrieve relevant event plot points, which can be used to augment prompting of a Large Language Model (LLM) during intelligence report generation. Our evaluation studies show that the plot points included in the generated intelligence reports have high semantic relevance, high coherency, and low data redundancy. \u25b3 Less Submitted 1 June, 2024; v1 submitted 20 October, 2023; originally announced October 2023. Journal ref: 2023 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM) arXiv:2310.05350 [pdf] Scaling Studies for Efficient Parameter Search and Parallelism for Large Language Model Pre-training Authors: Michael Benington, Leo Phan, Chris Pierre Paul, Evan Shoemaker, Priyanka Ranade, Torstein Collett, Grant Hodgson Perez, Christopher Krieger Abstract: AI accelerator processing capabilities and memory constraints largely dictate the scale in which machine learning workloads (e.g., training and inference) can be executed within a desirable time frame. Training a state of the art, transformer-based model today requires use of GPU-accelerated high performance computers with high-speed interconnects. As datasets and models continue to increase in si\u2026 \u25bd More AI accelerator processing capabilities and memory constraints largely dictate the scale in which machine learning workloads (e.g., training and inference) can be executed within a desirable time frame. Training a state of the art, transformer-based model today requires use of GPU-accelerated high performance computers with high-speed interconnects. As datasets and models continue to increase in size, computational requirements and memory demands for AI also continue to grow. These challenges have inspired the development of distributed algorithm and circuit-based optimization techniques that enable the ability to progressively scale models in multi-node environments, efficiently minimize neural network cost functions for faster convergence, and store more parameters into a set number of available resources. In our research project, we focus on parallel and distributed machine learning algorithm development, specifically for optimizing the data processing and pre-training of a set of 5 encoder-decoder LLMs, ranging from 580 million parameters to 13 billion parameters. We performed a fine-grained study to quantify the relationships between three ML parallelism methods, specifically exploring Microsoft DeepSpeed Zero Redundancy Optimizer (ZeRO) stages. \u25b3 Less Submitted 10 October, 2023; v1 submitted 8 October, 2023; originally announced October 2023. Journal ref: Supercomputing 2023 (SC23) Student Research Poster Track arXiv:2208.01693 [pdf, other] Recognizing and Extracting Cybersecurtity-relevant Entities from Text Authors: Casey Hanks, Michael Maiden, Priyanka Ranade, Tim Finin, Anupam Joshi Abstract: Cyber Threat Intelligence (CTI) is information describing threat vectors, vulnerabilities, and attacks and is often used as training data for AI-based cyber defense systems such as Cybersecurity Knowledge Graphs (CKG). There is a strong need to develop community-accessible datasets to train existing AI-based cybersecurity pipelines to efficiently and accurately extract meaningful insights from CTI\u2026 \u25bd More Cyber Threat Intelligence (CTI) is information describing threat vectors, vulnerabilities, and attacks and is often used as training data for AI-based cyber defense systems such as Cybersecurity Knowledge Graphs (CKG). There is a strong need to develop community-accessible datasets to train existing AI-based cybersecurity pipelines to efficiently and accurately extract meaningful insights from CTI. We have created an initial unstructured CTI corpus from a variety of open sources that we are using to train and test cybersecurity entity models using the spaCy framework and exploring self-learning methods to automatically recognize cybersecurity entities. We also describe methods to apply cybersecurity domain entity linking with existing world knowledge from Wikidata. Our future work will survey and test spaCy NLP tools and create methods for continuous integration of new information extracted from text. \u25b3 Less Submitted 2 August, 2022; originally announced August 2022. Journal ref: Workshop on Machine Learning for Cybersecurity, 2022 International Conference on Machine Learning arXiv:2102.04351 [pdf, other] Generating Fake Cyber Threat Intelligence Using Transformer-Based Models Authors: Priyanka Ranade, Aritran Piplai, Sudip Mittal, Anupam Joshi, Tim Finin Abstract: Cyber-defense systems are being developed to automatically ingest Cyber Threat Intelligence (CTI) that contains semi-structured data and/or text to populate knowledge graphs. A potential risk is that fake CTI can be generated and spread through Open-Source Intelligence (OSINT) communities or on the Web to effect a data poisoning attack on these systems. Adversaries can use fake CTI examples as tra\u2026 \u25bd More Cyber-defense systems are being developed to automatically ingest Cyber Threat Intelligence (CTI) that contains semi-structured data and/or text to populate knowledge graphs. A potential risk is that fake CTI can be generated and spread through Open-Source Intelligence (OSINT) communities or on the Web to effect a data poisoning attack on these systems. Adversaries can use fake CTI examples as training input to subvert cyber defense systems, forcing the model to learn incorrect inputs to serve their malicious needs. In this paper, we automatically generate fake CTI text descriptions using transformers. We show that given an initial prompt sentence, a public language model like GPT-2 with fine-tuning, can generate plausible CTI text with the ability of corrupting cyber-defense systems. We utilize the generated fake CTI text to perform a data poisoning attack on a Cybersecurity Knowledge Graph (CKG) and a cybersecurity corpus. The poisoning attack introduced adverse impacts such as returning incorrect reasoning outputs, representation poisoning, and corruption of other dependent AI-based cyber defense systems. We evaluate with traditional approaches and conduct a human evaluation study with cybersecurity professionals and threat hunters. Based on the study, professional threat hunters were equally likely to consider our fake generated CTI as true. \u25b3 Less Submitted 18 June, 2021; v1 submitted 8 February, 2021; originally announced February 2021. Comments: In Proceedings of International Joint Conference on Neural Networks 2021 (IJCNN 2021), July 2021 arXiv:1807.07517 [pdf, other] Using Deep Neural Networks to Translate Multi-lingual Threat Intelligence Authors: Priyanka Ranade, Sudip Mittal, Anupam Joshi, Karuna Joshi Abstract: The multilingual nature of the Internet increases complications in the cybersecurity community's ongoing efforts to strategically mine threat intelligence from OSINT data on the web. OSINT sources such as social media, blogs, and dark web vulnerability markets exist in diverse languages and hinder security analysts, who are unable to draw conclusions from intelligence in languages they don't under\u2026 \u25bd More The multilingual nature of the Internet increases complications in the cybersecurity community's ongoing efforts to strategically mine threat intelligence from OSINT data on the web. OSINT sources such as social media, blogs, and dark web vulnerability markets exist in diverse languages and hinder security analysts, who are unable to draw conclusions from intelligence in languages they don't understand. Although third party translation engines are growing stronger, they are unsuited for private security environments. First, sensitive intelligence is not a permitted input to third party engines due to privacy and confidentiality policies. In addition, third party engines produce generalized translations that tend to lack exclusive cybersecurity terminology. In this paper, we address these issues and describe our system that enables threat intelligence understanding across unfamiliar languages. We create a neural network based system that takes in cybersecurity data in a different language and outputs the respective English translation. The English translation can then be understood by an analyst, and can also serve as input to an AI based cyber-defense system that can take mitigative action. As a proof of concept, we have created a pipeline which takes Russian threats and generates its corresponding English, RDF, and vectorized representations. Our network optimizes translations on specifically, cybersecurity data. \u25b3 Less Submitted 19 July, 2018; originally announced July 2018."
  },
  {
    "url": "https://arxiv.org/abs/2503.14800",
    "title": "Computer Science > Information Retrieval",
    "author": "Authors:Ghadir Alselwi, Hao Xue, Shoaib Jameel, Basem Suleiman, Flora D. Salim, Imran Razzak",
    "body": "Help | Advanced Search arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. arXiv Operational Status Get status notifications via email or slack"
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Krogman,+R",
    "title": "Showing 1\u20131 of 1 results for author: Krogman, R",
    "author": "Authors:\nMd Shahir Zaoad, \n      \n      Niamat Zawad, \n      \n      Priyanka Ranade, \n      \n      Richard Krogman, \n      \n      Latifur Khan, \n      \n      James Holt",
    "body": "arXiv:2503.14802 [pdf, other] Graph-Based Re-ranking: Emerging Techniques, Limitations, and Opportunities Authors: Md Shahir Zaoad, Niamat Zawad, Priyanka Ranade, Richard Krogman, Latifur Khan, James Holt Abstract: Knowledge graphs have emerged to be promising datastore candidates for context augmentation during Retrieval Augmented Generation (RAG). As a result, techniques in graph representation learning have been simultaneously explored alongside principal neural information retrieval approaches, such as two-phased retrieval, also known as re-ranking. While Graph Neural Networks (GNNs) have been proposed t\u2026 \u25bd More Knowledge graphs have emerged to be promising datastore candidates for context augmentation during Retrieval Augmented Generation (RAG). As a result, techniques in graph representation learning have been simultaneously explored alongside principal neural information retrieval approaches, such as two-phased retrieval, also known as re-ranking. While Graph Neural Networks (GNNs) have been proposed to demonstrate proficiency in graph learning for re-ranking, there are ongoing limitations in modeling and evaluating input graph structures for training and evaluation for passage and document ranking tasks. In this survey, we review emerging GNN-based ranking model architectures along with their corresponding graph representation construction methodologies. We conclude by providing recommendations on future research based on community-wide challenges and opportunities. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. MSC Class: N/A"
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Holt,+J",
    "title": "Showing 1\u201332 of 32 results for author: Holt, J",
    "author": "Authors:\nMd Shahir Zaoad, \n      \n      Niamat Zawad, \n      \n      Priyanka Ranade, \n      \n      Richard Krogman, \n      \n      Latifur Khan, \n      \n      James Holt",
    "body": "arXiv:2503.14802 [pdf, other] Graph-Based Re-ranking: Emerging Techniques, Limitations, and Opportunities Authors: Md Shahir Zaoad, Niamat Zawad, Priyanka Ranade, Richard Krogman, Latifur Khan, James Holt Abstract: Knowledge graphs have emerged to be promising datastore candidates for context augmentation during Retrieval Augmented Generation (RAG). As a result, techniques in graph representation learning have been simultaneously explored alongside principal neural information retrieval approaches, such as two-phased retrieval, also known as re-ranking. While Graph Neural Networks (GNNs) have been proposed t\u2026 \u25bd More Knowledge graphs have emerged to be promising datastore candidates for context augmentation during Retrieval Augmented Generation (RAG). As a result, techniques in graph representation learning have been simultaneously explored alongside principal neural information retrieval approaches, such as two-phased retrieval, also known as re-ranking. While Graph Neural Networks (GNNs) have been proposed to demonstrate proficiency in graph learning for re-ranking, there are ongoing limitations in modeling and evaluating input graph structures for training and evaluation for passage and document ranking tasks. In this survey, we review emerging GNN-based ranking model architectures along with their corresponding graph representation construction methodologies. We conclude by providing recommendations on future research based on community-wide challenges and opportunities. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. MSC Class: N/A arXiv:2502.02759 [pdf, other] ClarAVy: A Tool for Scalable and Accurate Malware Family Labeling Authors: Robert J. Joyce, Derek Everett, Maya Fuchs, Edward Raff, James Holt Abstract: Determining the family to which a malicious file belongs is an essential component of cyberattack investigation, attribution, and remediation. Performing this task manually is time consuming and requires expert knowledge. Automated tools using that label malware using antivirus detections lack accuracy and/or scalability, making them insufficient for real-world applications. Three pervasive shortc\u2026 \u25bd More Determining the family to which a malicious file belongs is an essential component of cyberattack investigation, attribution, and remediation. Performing this task manually is time consuming and requires expert knowledge. Automated tools using that label malware using antivirus detections lack accuracy and/or scalability, making them insufficient for real-world applications. Three pervasive shortcomings in these tools are responsible: (1) incorrect parsing of antivirus detections, (2) errors during family alias resolution, and (3) an inappropriate antivirus aggregation strategy. To address each of these, we created our own malware family labeling tool called ClarAVy. ClarAVy utilizes a Variational Bayesian approach to aggregate detections from a collection of antivirus products into accurate family labels. Our tool scales to enormous malware datasets, and we evaluated it by labeling $\\approx$40 million malicious files. ClarAVy has 8 and 12 percentage points higher accuracy than the prior leading tool in labeling the MOTIF and MalPedia datasets, respectively. \u25b3 Less Submitted 4 February, 2025; originally announced February 2025. arXiv:2411.18516 [pdf, other] Living off the Analyst: Harvesting Features from Yara Rules for Malware Detection Authors: Siddhant Gupta, Fred Lu, Andrew Barlow, Edward Raff, Francis Ferraro, Cynthia Matuszek, Charles Nicholas, James Holt Abstract: A strategy used by malicious actors is to \"live off the land,\" where benign systems and tools already available on a victim's systems are used and repurposed for the malicious actor's intent. In this work, we ask if there is a way for anti-virus developers to similarly re-purpose existing work to improve their malware detection capability. We show that this is plausible via YARA rules, which use h\u2026 \u25bd More A strategy used by malicious actors is to \"live off the land,\" where benign systems and tools already available on a victim's systems are used and repurposed for the malicious actor's intent. In this work, we ask if there is a way for anti-virus developers to similarly re-purpose existing work to improve their malware detection capability. We show that this is plausible via YARA rules, which use human-written signatures to detect specific malware families, functionalities, or other markers of interest. By extracting sub-signatures from publicly available YARA rules, we assembled a set of features that can more effectively discriminate malicious samples from benign ones. Our experiments demonstrate that these features add value beyond traditional features on the EMBER 2018 dataset. Manual analysis of the added sub-signatures shows a power-law behavior in a combination of features that are specific and unique, as well as features that occur often. A prior expectation may be that the features would be limited in being overly specific to unique malware families. This behavior is observed, and is apparently useful in practice. In addition, we also find sub-signatures that are dual-purpose (e.g., detecting virtual machine environments) or broadly generic (e.g., DLL imports). \u25b3 Less Submitted 27 November, 2024; originally announced November 2024. Comments: To appear in BigData'24 CyberHunt 2024 arXiv:2410.23601 [pdf, other] Stabilizing Linear Passive-Aggressive Online Learning with Weighted Reservoir Sampling Authors: Skyler Wu, Fred Lu, Edward Raff, James Holt Abstract: Online learning methods, like the seminal Passive-Aggressive (PA) classifier, are still highly effective for high-dimensional streaming data, out-of-core processing, and other throughput-sensitive applications. Many such algorithms rely on fast adaptation to individual errors as a key to their convergence. While such algorithms enjoy low theoretical regret, in real-world deployment they can be sen\u2026 \u25bd More Online learning methods, like the seminal Passive-Aggressive (PA) classifier, are still highly effective for high-dimensional streaming data, out-of-core processing, and other throughput-sensitive applications. Many such algorithms rely on fast adaptation to individual errors as a key to their convergence. While such algorithms enjoy low theoretical regret, in real-world deployment they can be sensitive to individual outliers that cause the algorithm to over-correct. When such outliers occur at the end of the data stream, this can cause the final solution to have unexpectedly low accuracy. We design a weighted reservoir sampling (WRS) approach to obtain a stable ensemble model from the sequence of solutions without requiring additional passes over the data, hold-out sets, or a growing amount of memory. Our key insight is that good solutions tend to be error-free for more iterations than bad solutions, and thus, the number of passive rounds provides an estimate of a solution's relative quality. Our reservoir thus contains $K$ previous intermediate weight vectors with high survival times. We demonstrate our WRS approach on the Passive-Aggressive Classifier (PAC) and First-Order Sparse Online Learning (FSOL), where our method consistently and significantly outperforms the unmodified approach. We show that the risk of the ensemble classifier is bounded with respect to the regret of the underlying online learning method. \u25b3 Less Submitted 30 October, 2024; originally announced October 2024. Comments: To appear in the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) arXiv:2410.22677 [pdf, other] Is Function Similarity Over-Engineered? Building a Benchmark Authors: Rebecca Saul, Chang Liu, Noah Fleischmann, Richard Zak, Kristopher Micinski, Edward Raff, James Holt Abstract: Binary analysis is a core component of many critical security tasks, including reverse engineering, malware analysis, and vulnerability detection. Manual analysis is often time-consuming, but identifying commonly-used or previously-seen functions can reduce the time it takes to understand a new file. However, given the complexity of assembly, and the NP-hard nature of determining function equivale\u2026 \u25bd More Binary analysis is a core component of many critical security tasks, including reverse engineering, malware analysis, and vulnerability detection. Manual analysis is often time-consuming, but identifying commonly-used or previously-seen functions can reduce the time it takes to understand a new file. However, given the complexity of assembly, and the NP-hard nature of determining function equivalence, this task is extremely difficult. Common approaches often use sophisticated disassembly and decompilation tools, graph analysis, and other expensive pre-processing steps to perform function similarity searches over some corpus. In this work, we identify a number of discrepancies between the current research environment and the underlying application need. To remedy this, we build a new benchmark, REFuSE-Bench, for binary function similarity detection consisting of high-quality datasets and tests that better reflect real-world use cases. In doing so, we address issues like data duplication and accurate labeling, experiment with real malware, and perform the first serious evaluation of ML binary function similarity models on Windows data. Our benchmark reveals that a new, simple basline, one which looks at only the raw bytes of a function, and requires no disassembly or other pre-processing, is able to achieve state-of-the-art performance in multiple settings. Our findings challenge conventional assumptions that complex models with highly-engineered features are being used to their full potential, and demonstrate that simpler approaches can provide significant value. \u25b3 Less Submitted 29 October, 2024; originally announced October 2024. Comments: To appear in the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks arXiv:2410.22669 [pdf, other] A Walsh Hadamard Derived Linear Vector Symbolic Architecture Authors: Mohammad Mahmudul Alam, Alexander Oberle, Edward Raff, Stella Biderman, Tim Oates, James Holt Abstract: Vector Symbolic Architectures (VSAs) are one approach to developing Neuro-symbolic AI, where two vectors in $\\mathbb{R}^d$ are `bound' together to produce a new vector in the same space. VSAs support the commutativity and associativity of this binding operation, along with an inverse operation, allowing one to construct symbolic-style manipulations over real-valued vectors. Most VSAs were develope\u2026 \u25bd More Vector Symbolic Architectures (VSAs) are one approach to developing Neuro-symbolic AI, where two vectors in $\\mathbb{R}^d$ are `bound' together to produce a new vector in the same space. VSAs support the commutativity and associativity of this binding operation, along with an inverse operation, allowing one to construct symbolic-style manipulations over real-valued vectors. Most VSAs were developed before deep learning and automatic differentiation became popular and instead focused on efficacy in hand-designed systems. In this work, we introduce the Hadamard-derived linear Binding (HLB), which is designed to have favorable computational efficiency, and efficacy in classic VSA tasks, and perform well in differentiable systems. Code is available at https://github.com/FutureComputing4AI/Hadamard-derived-Linear-Binding \u25b3 Less Submitted 29 October, 2024; originally announced October 2024. Comments: To appear in the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) arXiv:2407.06346 [pdf, other] High-Dimensional Distributed Sparse Classification with Scalable Communication-Efficient Global Updates Authors: Fred Lu, Ryan R. Curtin, Edward Raff, Francis Ferraro, James Holt Abstract: As the size of datasets used in statistical learning continues to grow, distributed training of models has attracted increasing attention. These methods partition the data and exploit parallelism to reduce memory and runtime, but suffer increasingly from communication costs as the data size or the number of iterations grows. Recent work on linear models has shown that a surrogate likelihood can be\u2026 \u25bd More As the size of datasets used in statistical learning continues to grow, distributed training of models has attracted increasing attention. These methods partition the data and exploit parallelism to reduce memory and runtime, but suffer increasingly from communication costs as the data size or the number of iterations grows. Recent work on linear models has shown that a surrogate likelihood can be optimized locally to iteratively improve on an initial solution in a communication-efficient manner. However, existing versions of these methods experience multiple shortcomings as the data size becomes massive, including diverging updates and efficiently handling sparsity. In this work we develop solutions to these problems which enable us to learn a communication-efficient distributed logistic regression model even beyond millions of features. In our experiments we demonstrate a large improvement in accuracy over distributed algorithms with only a few distributed update steps needed, and similar or faster runtimes. Our code is available at \\url{https://github.com/FutureComputing4AI/ProxCSL}. \u25b3 Less Submitted 8 July, 2024; originally announced July 2024. Comments: KDD 2024, Research Track arXiv:2406.01753 [pdf, other] Optimizing the Optimal Weighted Average: Efficient Distributed Sparse Classification Authors: Fred Lu, Ryan R. Curtin, Edward Raff, Francis Ferraro, James Holt Abstract: While distributed training is often viewed as a solution to optimizing linear models on increasingly large datasets, inter-machine communication costs of popular distributed approaches can dominate as data dimensionality increases. Recent work on non-interactive algorithms shows that approximate solutions for linear models can be obtained efficiently with only a single round of communication among\u2026 \u25bd More While distributed training is often viewed as a solution to optimizing linear models on increasingly large datasets, inter-machine communication costs of popular distributed approaches can dominate as data dimensionality increases. Recent work on non-interactive algorithms shows that approximate solutions for linear models can be obtained efficiently with only a single round of communication among machines. However, this approximation often degenerates as the number of machines increases. In this paper, building on the recent optimal weighted average method, we introduce a new technique, ACOWA, that allows an extra round of communication to achieve noticeably better approximation quality with minor runtime increases. Results show that for sparse distributed logistic regression, ACOWA obtains solutions that are more faithful to the empirical risk minimizer and attain substantially higher accuracy than other distributed algorithms. \u25b3 Less Submitted 3 June, 2024; originally announced June 2024. Comments: Under review arXiv:2405.03991 [pdf, other] Assemblage: Automatic Binary Dataset Construction for Machine Learning Authors: Chang Liu, Rebecca Saul, Yihao Sun, Edward Raff, Maya Fuchs, Townsend Southard Pantano, James Holt, Kristopher Micinski Abstract: Binary code is pervasive, and binary analysis is a key task in reverse engineering, malware classification, and vulnerability discovery. Unfortunately, while there exist large corpora of malicious binaries, obtaining high-quality corpora of benign binaries for modern systems has proven challenging (e.g., due to licensing issues). Consequently, machine learning based pipelines for binary analysis u\u2026 \u25bd More Binary code is pervasive, and binary analysis is a key task in reverse engineering, malware classification, and vulnerability discovery. Unfortunately, while there exist large corpora of malicious binaries, obtaining high-quality corpora of benign binaries for modern systems has proven challenging (e.g., due to licensing issues). Consequently, machine learning based pipelines for binary analysis utilize either costly commercial corpora (e.g., VirusTotal) or open-source binaries (e.g., coreutils) available in limited quantities. To address these issues, we present Assemblage: an extensible cloud-based distributed system that crawls, configures, and builds Windows PE binaries to obtain high-quality binary corpuses suitable for training state-of-the-art models in binary analysis. We have run Assemblage on AWS over the past year, producing 890k Windows PE and 428k Linux ELF binaries across 29 configurations. Assemblage is designed to be both reproducible and extensible, enabling users to publish \"recipes\" for their datasets, and facilitating the extraction of a wide array of features. We evaluated Assemblage by using its data to train modern learning-based pipelines for compiler provenance and binary function similarity. Our results illustrate the practical need for robust corpora of high-quality Windows PE binaries in training modern learning-based binary analyses. Assemblage code is open sourced under the MIT license, and the dataset can be downloaded from https://assemblage-dataset.net \u25b3 Less Submitted 2 November, 2024; v1 submitted 7 May, 2024; originally announced May 2024. Comments: To appear in the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks arXiv:2403.17978 [pdf, other] Holographic Global Convolutional Networks for Long-Range Prediction Tasks in Malware Detection Authors: Mohammad Mahmudul Alam, Edward Raff, Stella Biderman, Tim Oates, James Holt Abstract: Malware detection is an interesting and valuable domain to work in because it has significant real-world impact and unique machine-learning challenges. We investigate existing long-range techniques and benchmarks and find that they're not very suitable in this problem area. In this paper, we introduce Holographic Global Convolutional Networks (HGConv) that utilize the properties of Holographic Red\u2026 \u25bd More Malware detection is an interesting and valuable domain to work in because it has significant real-world impact and unique machine-learning challenges. We investigate existing long-range techniques and benchmarks and find that they're not very suitable in this problem area. In this paper, we introduce Holographic Global Convolutional Networks (HGConv) that utilize the properties of Holographic Reduced Representations (HRR) to encode and decode features from sequence elements. Unlike other global convolutional methods, our method does not require any intricate kernel computation or crafted kernel design. HGConv kernels are defined as simple parameters learned through backpropagation. The proposed method has achieved new SOTA results on Microsoft Malware Classification Challenge, Drebin, and EMBER malware benchmarks. With log-linear complexity in sequence length, the empirical results demonstrate substantially faster run-time by HGConv compared to other methods achieving far more efficient scaling even with sequence length $\\geq 100,000$. \u25b3 Less Submitted 23 March, 2024; originally announced March 2024. Comments: To appear in Proceedings of the 27th International Conference on Artificial Intelligence and Statistics (AISTATS) 2024, Valencia, Spain arXiv:2312.15813 [pdf, other] Small Effect Sizes in Malware Detection? Make Harder Train/Test Splits! Authors: Tirth Patel, Fred Lu, Edward Raff, Charles Nicholas, Cynthia Matuszek, James Holt Abstract: Industry practitioners care about small improvements in malware detection accuracy because their models are deployed to hundreds of millions of machines, meaning a 0.1\\% change can cause an overwhelming number of false positives. However, academic research is often restrained to public datasets on the order of ten thousand samples and is too small to detect improvements that may be relevant to ind\u2026 \u25bd More Industry practitioners care about small improvements in malware detection accuracy because their models are deployed to hundreds of millions of machines, meaning a 0.1\\% change can cause an overwhelming number of false positives. However, academic research is often restrained to public datasets on the order of ten thousand samples and is too small to detect improvements that may be relevant to industry. Working within these constraints, we devise an approach to generate a benchmark of configurable difficulty from a pool of available samples. This is done by leveraging malware family information from tools like AVClass to construct training/test splits that have different generalization rates, as measured by a secondary model. Our experiments will demonstrate that using a less accurate secondary model with disparate features is effective at producing benchmarks for a more sophisticated target model that is under evaluation. We also ablate against alternative designs to show the need for our approach. \u25b3 Less Submitted 25 December, 2023; originally announced December 2023. Comments: To appear in Conference on Applied Machine Learning for Information Security 2023 arXiv:2310.17867 [pdf, other] Reproducibility in Multiple Instance Learning: A Case For Algorithmic Unit Tests Authors: Edward Raff, James Holt Abstract: Multiple Instance Learning (MIL) is a sub-domain of classification problems with positive and negative labels and a \"bag\" of inputs, where the label is positive if and only if a positive element is contained within the bag, and otherwise is negative. Training in this context requires associating the bag-wide label to instance-level information, and implicitly contains a causal assumption and asymm\u2026 \u25bd More Multiple Instance Learning (MIL) is a sub-domain of classification problems with positive and negative labels and a \"bag\" of inputs, where the label is positive if and only if a positive element is contained within the bag, and otherwise is negative. Training in this context requires associating the bag-wide label to instance-level information, and implicitly contains a causal assumption and asymmetry to the task (i.e., you can't swap the labels without changing the semantics). MIL problems occur in healthcare (one malignant cell indicates cancer), cyber security (one malicious executable makes an infected computer), and many other tasks. In this work, we examine five of the most prominent deep-MIL models and find that none of them respects the standard MIL assumption. They are able to learn anti-correlated instances, i.e., defaulting to \"positive\" labels until seeing a negative counter-example, which should not be possible for a correct MIL model. We suspect that enhancements and other works derived from these models will share the same issue. In any context in which these models are being used, this creates the potential for learning incorrect models, which creates risk of operational failure. We identify and demonstrate this problem via a proposed \"algorithmic unit test\", where we create synthetic datasets that can be solved by a MIL respecting model, and which clearly reveal learning that violates MIL assumptions. The five evaluated methods each fail one or more of these tests. This provides a model-agnostic way to identify violations of modeling assumptions, which we hope will be useful for future development and evaluation of MIL models. \u25b3 Less Submitted 26 October, 2023; originally announced October 2023. Comments: To appear in the 37th Conference on Neural Information Processing Systems (NeurIPS 2023) arXiv:2310.11706 [pdf, other] MalDICT: Benchmark Datasets on Malware Behaviors, Platforms, Exploitation, and Packers Authors: Robert J. Joyce, Edward Raff, Charles Nicholas, James Holt Abstract: Existing research on malware classification focuses almost exclusively on two tasks: distinguishing between malicious and benign files and classifying malware by family. However, malware can be categorized according to many other types of attributes, and the ability to identify these attributes in newly-emerging malware using machine learning could provide significant value to analysts. In particu\u2026 \u25bd More Existing research on malware classification focuses almost exclusively on two tasks: distinguishing between malicious and benign files and classifying malware by family. However, malware can be categorized according to many other types of attributes, and the ability to identify these attributes in newly-emerging malware using machine learning could provide significant value to analysts. In particular, we have identified four tasks which are under-represented in prior work: classification by behaviors that malware exhibit, platforms that malware run on, vulnerabilities that malware exploit, and packers that malware are packed with. To obtain labels for training and evaluating ML classifiers on these tasks, we created an antivirus (AV) tagging tool called ClarAVy. ClarAVy's sophisticated AV label parser distinguishes itself from prior AV-based taggers, with the ability to accurately parse 882 different AV label formats used by 90 different AV products. We are releasing benchmark datasets for each of these four classification tasks, tagged using ClarAVy and comprising nearly 5.5 million malicious files in total. Our malware behavior dataset includes 75 distinct tags - nearly 7x more than the only prior benchmark dataset with behavioral tags. To our knowledge, we are the first to release datasets with malware platform and packer tags. \u25b3 Less Submitted 18 October, 2023; originally announced October 2023. arXiv:2307.13855 [pdf, other] Exploring the Sharpened Cosine Similarity Authors: Skyler Wu, Fred Lu, Edward Raff, James Holt Abstract: Convolutional layers have long served as the primary workhorse for image classification. Recently, an alternative to convolution was proposed using the Sharpened Cosine Similarity (SCS), which in theory may serve as a better feature detector. While multiple sources report promising results, there has not been to date a full-scale empirical analysis of neural network performance using these new lay\u2026 \u25bd More Convolutional layers have long served as the primary workhorse for image classification. Recently, an alternative to convolution was proposed using the Sharpened Cosine Similarity (SCS), which in theory may serve as a better feature detector. While multiple sources report promising results, there has not been to date a full-scale empirical analysis of neural network performance using these new layers. In our work, we explore SCS's parameter behavior and potential as a drop-in replacement for convolutions in multiple CNN architectures benchmarked on CIFAR-10. We find that while SCS may not yield significant increases in accuracy, it may learn more interpretable representations. We also find that, in some circumstances, SCS may confer a slight increase in adversarial robustness. \u25b3 Less Submitted 25 July, 2023; originally announced July 2023. Comments: Accepted to I Can't Believe It's Not Better Workshop (ICBINB) at NeurIPS 2022 arXiv:2305.19534 [pdf, other] Recasting Self-Attention with Holographic Reduced Representations Authors: Mohammad Mahmudul Alam, Edward Raff, Stella Biderman, Tim Oates, James Holt Abstract: In recent years, self-attention has become the dominant paradigm for sequence modeling in a variety of domains. However, in domains with very long sequence lengths the $\\mathcal{O}(T^2)$ memory and $\\mathcal{O}(T^2 H)$ compute costs can make using transformers infeasible. Motivated by problems in malware detection, where sequence lengths of $T \\geq 100,000$ are a roadblock to deep learning, we re-\u2026 \u25bd More In recent years, self-attention has become the dominant paradigm for sequence modeling in a variety of domains. However, in domains with very long sequence lengths the $\\mathcal{O}(T^2)$ memory and $\\mathcal{O}(T^2 H)$ compute costs can make using transformers infeasible. Motivated by problems in malware detection, where sequence lengths of $T \\geq 100,000$ are a roadblock to deep learning, we re-cast self-attention using the neuro-symbolic approach of Holographic Reduced Representations (HRR). In doing so we perform the same high-level strategy of the standard self-attention: a set of queries matching against a set of keys, and returning a weighted response of the values for each key. Implemented as a ``Hrrformer'' we obtain several benefits including $\\mathcal{O}(T H \\log H)$ time complexity, $\\mathcal{O}(T H)$ space complexity, and convergence in $10\\times$ fewer epochs. Nevertheless, the Hrrformer achieves near state-of-the-art accuracy on LRA benchmarks and we are able to learn with just a single layer. Combined, these benefits make our Hrrformer the first viable Transformer for such long malware classification sequences and up to $280\\times$ faster to train on the Long Range Arena benchmark. Code is available at \\url{https://github.com/NeuromorphicComputationResearchProgram/Hrrformer} \u25b3 Less Submitted 30 May, 2023; originally announced May 2023. Comments: To appear in Proceedings of the 40th International Conference on Machine Learning (ICML) arXiv:2301.06163 [pdf, other] A Coreset Learning Reality Check Authors: Fred Lu, Edward Raff, James Holt Abstract: Subsampling algorithms are a natural approach to reduce data size before fitting models on massive datasets. In recent years, several works have proposed methods for subsampling rows from a data matrix while maintaining relevant information for classification. While these works are supported by theory and limited experiments, to date there has not been a comprehensive evaluation of these methods.\u2026 \u25bd More Subsampling algorithms are a natural approach to reduce data size before fitting models on massive datasets. In recent years, several works have proposed methods for subsampling rows from a data matrix while maintaining relevant information for classification. While these works are supported by theory and limited experiments, to date there has not been a comprehensive evaluation of these methods. In our work, we directly compare multiple methods for logistic regression drawn from the coreset and optimal subsampling literature and discover inconsistencies in their effectiveness. In many cases, methods do not outperform simple uniform subsampling. \u25b3 Less Submitted 15 January, 2023; originally announced January 2023. Comments: To appear in the Thirty-Seventh AAAI Conference on Artificial Intelligence arXiv:2212.02663 [pdf, other] Efficient Malware Analysis Using Metric Embeddings Authors: Ethan M. Rudd, David Krisiloff, Scott Coull, Daniel Olszewski, Edward Raff, James Holt Abstract: In this paper, we explore the use of metric learning to embed Windows PE files in a low-dimensional vector space for downstream use in a variety of applications, including malware detection, family classification, and malware attribute tagging. Specifically, we enrich labeling on malicious and benign PE files using computationally expensive, disassembly-based malicious capabilities. Using these ca\u2026 \u25bd More In this paper, we explore the use of metric learning to embed Windows PE files in a low-dimensional vector space for downstream use in a variety of applications, including malware detection, family classification, and malware attribute tagging. Specifically, we enrich labeling on malicious and benign PE files using computationally expensive, disassembly-based malicious capabilities. Using these capabilities, we derive several different types of metric embeddings utilizing an embedding neural network trained via contrastive loss, Spearman rank correlation, and combinations thereof. We then examine performance on a variety of transfer tasks performed on the EMBER and SOREL datasets, demonstrating that for several tasks, low-dimensional, computationally efficient metric embeddings maintain performance with little decay, which offers the potential to quickly retrain for a variety of transfer tasks at significantly reduced storage overhead. We conclude with an examination of practical considerations for the use of our proposed embedding approach, such as robustness to adversarial evasion and introduction of task-specific auxiliary objectives to improve performance on mission critical tasks. \u25b3 Less Submitted 5 December, 2022; originally announced December 2022. Comments: Pre-print of a manuscript submitted to the ACM Digital Threats: Research and Practice (DTRAP) Special Issue on Applied Machine Learning for Information Security. 19 Pages arXiv:2211.13250 [pdf, other] Lempel-Ziv Networks Authors: Rebecca Saul, Mohammad Mahmudul Alam, John Hurwitz, Edward Raff, Tim Oates, James Holt Abstract: Sequence processing has long been a central area of machine learning research. Recurrent neural nets have been successful in processing sequences for a number of tasks; however, they are known to be both ineffective and computationally expensive when applied to very long sequences. Compression-based methods have demonstrated more robustness when processing such sequences -- in particular, an appro\u2026 \u25bd More Sequence processing has long been a central area of machine learning research. Recurrent neural nets have been successful in processing sequences for a number of tasks; however, they are known to be both ineffective and computationally expensive when applied to very long sequences. Compression-based methods have demonstrated more robustness when processing such sequences -- in particular, an approach pairing the Lempel-Ziv Jaccard Distance (LZJD) with the k-Nearest Neighbor algorithm has shown promise on long sequence problems (up to $T=200,000,000$ steps) involving malware classification. Unfortunately, use of LZJD is limited to discrete domains. To extend the benefits of LZJD to a continuous domain, we investigate the effectiveness of a deep-learning analog of the algorithm, the Lempel-Ziv Network. While we achieve successful proof of concept, we are unable to improve meaningfully on the performance of a standard LSTM across a variety of datasets and sequence processing tasks. In addition to presenting this negative result, our work highlights the problem of sub-par baseline tuning in newer research areas. \u25b3 Less Submitted 23 November, 2022; originally announced November 2022. Comments: I Can't Believe It's Not Better Workshop at NeurIPS 2022 arXiv:2206.05893 [pdf, other] Deploying Convolutional Networks on Untrusted Platforms Using 2D Holographic Reduced Representations Authors: Mohammad Mahmudul Alam, Edward Raff, Tim Oates, James Holt Abstract: Due to the computational cost of running inference for a neural network, the need to deploy the inferential steps on a third party's compute environment or hardware is common. If the third party is not fully trusted, it is desirable to obfuscate the nature of the inputs and outputs, so that the third party can not easily determine what specific task is being performed. Provably secure protocols fo\u2026 \u25bd More Due to the computational cost of running inference for a neural network, the need to deploy the inferential steps on a third party's compute environment or hardware is common. If the third party is not fully trusted, it is desirable to obfuscate the nature of the inputs and outputs, so that the third party can not easily determine what specific task is being performed. Provably secure protocols for leveraging an untrusted party exist but are too computational demanding to run in practice. We instead explore a different strategy of fast, heuristic security that we call Connectionist Symbolic Pseudo Secrets. By leveraging Holographic Reduced Representations (HRR), we create a neural network with a pseudo-encryption style defense that empirically shows robustness to attack, even under threat models that unrealistically favor the adversary. \u25b3 Less Submitted 12 June, 2022; originally announced June 2022. Comments: To appear in the Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022 arXiv:2206.03265 [pdf, other] Marvolo: Programmatic Data Augmentation for Practical ML-Driven Malware Detection Authors: Michael D. Wong, Edward Raff, James Holt, Ravi Netravali Abstract: Data augmentation has been rare in the cyber security domain due to technical difficulties in altering data in a manner that is semantically consistent with the original data. This shortfall is particularly onerous given the unique difficulty of acquiring benign and malicious training data that runs into copyright restrictions, and that institutions like banks and governments receive targeted malw\u2026 \u25bd More Data augmentation has been rare in the cyber security domain due to technical difficulties in altering data in a manner that is semantically consistent with the original data. This shortfall is particularly onerous given the unique difficulty of acquiring benign and malicious training data that runs into copyright restrictions, and that institutions like banks and governments receive targeted malware that will never exist in large quantities. We present MARVOLO, a binary mutator that programmatically grows malware (and benign) datasets in a manner that boosts the accuracy of ML-driven malware detectors. MARVOLO employs semantics-preserving code transformations that mimic the alterations that malware authors and defensive benign developers routinely make in practice , allowing us to generate meaningful augmented data. Crucially, semantics-preserving transformations also enable MARVOLO to safely propagate labels from original to newly-generated data samples without mandating expensive reverse engineering of binaries. Further, MARVOLO embeds several key optimizations that keep costs low for practitioners by maximizing the density of diverse data samples generated within a given time (or resource) budget. Experiments using wide-ranging commercial malware datasets and a recent ML-driven malware detector show that MARVOLO boosts accuracies by up to 5%, while operating on only a small fraction (15%) of the potential input binaries. \u25b3 Less Submitted 7 June, 2022; originally announced June 2022. Comments: 15 pages, 7 figures arXiv:2203.05037 [pdf] Human-GDPR Interaction: Practical Experiences of Accessing Personal Data Authors: Alex Bowyer, Jack Holt, Josephine Go Jefferies, Rob Wilson, David Kirk, Jan David Smeddinck Abstract: In our data-centric world, most services rely on collecting and using personal data. The EU's General Data Protection Regulation (GDPR) aims to enhance individuals' control over their data, but its practical impact is not well understood. We present a 10-participant study, where each participant filed 4-5 data access requests. Through interviews accompanying these requests and discussions scrutini\u2026 \u25bd More In our data-centric world, most services rely on collecting and using personal data. The EU's General Data Protection Regulation (GDPR) aims to enhance individuals' control over their data, but its practical impact is not well understood. We present a 10-participant study, where each participant filed 4-5 data access requests. Through interviews accompanying these requests and discussions scrutinising returned data, it appears that GDPR falls short of its goals due to non-compliance and low-quality responses. Participants found their hopes to understand providers' data practices or harness their own data unmet. This causes increased distrust without any subjective improvement in power, although more transparent providers do earn greater trust. We propose designing more effective, data-inclusive and open policies and data access systems to improve both customer relations and individual agency, and also that wider public use of GDPR rights could help with delivering accountability and motivating providers to improve data practices. \u25b3 Less Submitted 9 March, 2022; originally announced March 2022. Comments: 19 pages, to be published in CHI Conference on Human Factors in Computing Systems (CHI '22), April 29 - May 05, 2022, New Orleans, LA, USA arXiv:2202.14010 Proceedings of the Artificial Intelligence for Cyber Security (AICS) Workshop at AAAI 2022 Authors: James Holt, Edward Raff, Ahmad Ridley, Dennis Ross, Arunesh Sinha, Diane Staheli, William Streilen, Milind Tambe, Yevgeniy Vorobeychik, Allan Wollaber Abstract: The workshop will focus on the application of AI to problems in cyber security. Cyber systems generate large volumes of data, utilizing this effectively is beyond human capabilities. Additionally, adversaries continue to develop new attacks. Hence, AI methods are required to understand and protect the cyber domain. These challenges are widely studied in enterprise networks, but there are many gaps\u2026 \u25bd More The workshop will focus on the application of AI to problems in cyber security. Cyber systems generate large volumes of data, utilizing this effectively is beyond human capabilities. Additionally, adversaries continue to develop new attacks. Hence, AI methods are required to understand and protect the cyber domain. These challenges are widely studied in enterprise networks, but there are many gaps in research and practice as well as novel problems in other domains. In general, AI techniques are still not widely adopted in the real world. Reasons include: (1) a lack of certification of AI for security, (2) a lack of formal study of the implications of practical constraints (e.g., power, memory, storage) for AI systems in the cyber domain, (3) known vulnerabilities such as evasion, poisoning attacks, (4) lack of meaningful explanations for security analysts, and (5) lack of analyst trust in AI solutions. There is a need for the research community to develop novel solutions for these practical issues. \u25b3 Less Submitted 1 March, 2022; v1 submitted 28 February, 2022; originally announced February 2022. arXiv:2202.08985 [pdf, ps, other] Out of Distribution Data Detection Using Dropout Bayesian Neural Networks Authors: Andre T. Nguyen, Fred Lu, Gary Lopez Munoz, Edward Raff, Charles Nicholas, James Holt Abstract: We explore the utility of information contained within a dropout based Bayesian neural network (BNN) for the task of detecting out of distribution (OOD) data. We first show how previous attempts to leverage the randomized embeddings induced by the intermediate layers of a dropout BNN can fail due to the distance metric used. We introduce an alternative approach to measuring embedding uncertainty,\u2026 \u25bd More We explore the utility of information contained within a dropout based Bayesian neural network (BNN) for the task of detecting out of distribution (OOD) data. We first show how previous attempts to leverage the randomized embeddings induced by the intermediate layers of a dropout BNN can fail due to the distance metric used. We introduce an alternative approach to measuring embedding uncertainty, justify its use theoretically, and demonstrate how incorporating embedding uncertainty improves OOD data identification across three tasks: image classification, language classification, and malware detection. \u25b3 Less Submitted 17 February, 2022; originally announced February 2022. arXiv:2109.02157 [pdf, other] Learning with Holographic Reduced Representations Authors: Ashwinkumar Ganesan, Hang Gao, Sunil Gandhi, Edward Raff, Tim Oates, James Holt, Mark McLean Abstract: Holographic Reduced Representations (HRR) are a method for performing symbolic AI on top of real-valued vectors by associating each vector with an abstract concept, and providing mathematical operations to manipulate vectors as if they were classic symbolic objects. This method has seen little use outside of older symbolic AI work and cognitive science. Our goal is to revisit this approach to unde\u2026 \u25bd More Holographic Reduced Representations (HRR) are a method for performing symbolic AI on top of real-valued vectors by associating each vector with an abstract concept, and providing mathematical operations to manipulate vectors as if they were classic symbolic objects. This method has seen little use outside of older symbolic AI work and cognitive science. Our goal is to revisit this approach to understand if it is viable for enabling a hybrid neural-symbolic approach to learning as a differentiable component of a deep learning architecture. HRRs today are not effective in a differentiable solution due to numerical instability, a problem we solve by introducing a projection step that forces the vectors to exist in a well behaved point in space. In doing so we improve the concept retrieval efficacy of HRRs by over $100\\times$. Using multi-label classification we demonstrate how to leverage the symbolic HRR properties to develop an output layer and loss function that is able to learn effectively, and allows us to investigate some of the pros and cons of an HRR neuro-symbolic learning approach. Our code can be found at https://github.com/NeuromorphicComputationResearchProgram/Learning-with-Holographic-Reduced-Representations \u25b3 Less Submitted 28 December, 2021; v1 submitted 5 September, 2021; originally announced September 2021. Comments: To appear in the 35th Conference on Neural Information Processing Systems (NeurIPS 2021) arXiv:2108.04081 [pdf, other] Leveraging Uncertainty for Improved Static Malware Detection Under Extreme False Positive Constraints Authors: Andre T. Nguyen, Edward Raff, Charles Nicholas, James Holt Abstract: The detection of malware is a critical task for the protection of computing environments. This task often requires extremely low false positive rates (FPR) of 0.01% or even lower, for which modern machine learning has no readily available tools. We introduce the first broad investigation of the use of uncertainty for malware detection across multiple datasets, models, and feature types. We show ho\u2026 \u25bd More The detection of malware is a critical task for the protection of computing environments. This task often requires extremely low false positive rates (FPR) of 0.01% or even lower, for which modern machine learning has no readily available tools. We introduce the first broad investigation of the use of uncertainty for malware detection across multiple datasets, models, and feature types. We show how ensembling and Bayesian treatments of machine learning methods for static malware detection allow for improved identification of model errors, uncovering of new malware families, and predictive performance under extreme false positive constraints. In particular, we improve the true positive rate (TPR) at an actual realized FPR of 1e-5 from an expected 0.69 for previous methods to 0.80 on the best performing model class on the Sophos industry scale dataset. We additionally demonstrate how previous works have used an evaluation protocol that can lead to misleading results. \u25b3 Less Submitted 9 August, 2021; originally announced August 2021. Report number: IJCAI-ACD/2021/102 arXiv:2104.07807 [pdf] From Personal Data to Digital Legacy: Exploring Conflicts in the Sharing, Security and Privacy of Post-mortem Data Authors: Jack Holt, James Nicholson, Jan David Smeddinck Abstract: As digital technologies become more prevalent there is a growing awareness of the importance of good security and privacy practices. The tools and techniques used to achieve this are typically designed with the living user in mind, with little consideration of how they should or will perform after the user has died. We report on two workshops carried out with users of password managers to explore\u2026 \u25bd More As digital technologies become more prevalent there is a growing awareness of the importance of good security and privacy practices. The tools and techniques used to achieve this are typically designed with the living user in mind, with little consideration of how they should or will perform after the user has died. We report on two workshops carried out with users of password managers to explore their views on the post-mortem sharing, security and privacy of a range of common digital assets. We discuss a post-mortem privacy paradox where users recognise value in planning for their digital legacy, yet avoid actively doing so. Importantly, our findings highlight a tension between the use of recommended security tools during life and facilitating appropriate post-mortem access to chosen assets. We offer design recommendations to facilitate and encourage digital legacy planning while promoting good security habits during life. \u25b3 Less Submitted 15 April, 2021; originally announced April 2021. Comments: WWW '21 arXiv:2010.12080 [pdf, other] Getting Passive Aggressive About False Positives: Patching Deployed Malware Detectors Authors: Edward Raff, Bobby Filar, James Holt Abstract: False positives (FPs) have been an issue of extreme importance for anti-virus (AV) systems for decades. As more security vendors turn to machine learning, alert deluge has hit critical mass with over 20% of all alerts resulting in FPs and, in some organizations, the number reaches half of all alerts. This increase has resulted in fatigue, frustration, and, worst of all, neglect from security worke\u2026 \u25bd More False positives (FPs) have been an issue of extreme importance for anti-virus (AV) systems for decades. As more security vendors turn to machine learning, alert deluge has hit critical mass with over 20% of all alerts resulting in FPs and, in some organizations, the number reaches half of all alerts. This increase has resulted in fatigue, frustration, and, worst of all, neglect from security workers on SOC teams. A foundational cause for FPs is that vendors must build one global system to try and satisfy all customers, but have no method to adjust to individual local environments. This leads to outrageous, albeit technically correct, characterization of their platforms being 99.9% effective. Once these systems are deployed the idiosyncrasies of individual, local environments expose blind spots that lead to FPs and uncertainty. We propose a strategy for fixing false positives in production after a model has already been deployed. For too long the industry has tried to combat these problems with inefficient, and at times, dangerous allowlist techniques and excessive model retraining which is no longer enough. We propose using a technique called passive-aggressive learning to alter a malware detection model to an individual's environment, eliminating false positives without sharing any customer sensitive information. We will show how to use passive-aggressive learning to solve a collection of notoriously difficult false positives from a production environment without compromising the malware model's accuracy, reducing the total number of FP alerts by an average of 23x. \u25b3 Less Submitted 22 October, 2020; originally announced October 2020. Comments: to appear in IEEE International Conference on Data Mining Workshop (ICDM) on Deep Learning for Cyber Threat Intelligence (DL-CTI) arXiv:2009.03779 [pdf, other] Automatic Yara Rule Generation Using Biclustering Authors: Edward Raff, Richard Zak, Gary Lopez Munoz, William Fleming, Hyrum S. Anderson, Bobby Filar, Charles Nicholas, James Holt Abstract: Yara rules are a ubiquitous tool among cybersecurity practitioners and analysts. Developing high-quality Yara rules to detect a malware family of interest can be labor- and time-intensive, even for expert users. Few tools exist and relatively little work has been done on how to automate the generation of Yara rules for specific families. In this paper, we leverage large n-grams ($n \\geq 8$) combin\u2026 \u25bd More Yara rules are a ubiquitous tool among cybersecurity practitioners and analysts. Developing high-quality Yara rules to detect a malware family of interest can be labor- and time-intensive, even for expert users. Few tools exist and relatively little work has been done on how to automate the generation of Yara rules for specific families. In this paper, we leverage large n-grams ($n \\geq 8$) combined with a new biclustering algorithm to construct simple Yara rules more effectively than currently available software. Our method, AutoYara, is fast, allowing for deployment on low-resource equipment for teams that deploy to remote networks. Our results demonstrate that AutoYara can help reduce analyst workload by producing rules with useful true-positive rates while maintaining low false-positive rates, sometimes matching or even outperforming human analysts. In addition, real-world testing by malware analysts indicates AutoYara could reduce analyst time spent constructing Yara rules by 44-86%, allowing them to spend their time on the more advanced malware that current tools can't handle. Code will be made available at https://github.com/NeuromorphicComputationResearchProgram . \u25b3 Less Submitted 5 September, 2020; originally announced September 2020. Comments: to be published in the 13th ACM Workshop on Artificial Intelligence and Security (AISec) arXiv:1905.02497 [pdf, other] RelExt: Relation Extraction using Deep Learning approaches for Cybersecurity Knowledge Graph Improvement Authors: Aditya Pingle, Aritran Piplai, Sudip Mittal, Anupam Joshi, James Holt, Richard Zak Abstract: Security Analysts that work in a `Security Operations Center' (SoC) play a major role in ensuring the security of the organization. The amount of background knowledge they have about the evolving and new attacks makes a significant difference in their ability to detect attacks. Open source threat intelligence sources, like text descriptions about cyber-attacks, can be stored in a structured fashio\u2026 \u25bd More Security Analysts that work in a `Security Operations Center' (SoC) play a major role in ensuring the security of the organization. The amount of background knowledge they have about the evolving and new attacks makes a significant difference in their ability to detect attacks. Open source threat intelligence sources, like text descriptions about cyber-attacks, can be stored in a structured fashion in a cybersecurity knowledge graph. A cybersecurity knowledge graph can be paramount in aiding a security analyst to detect cyber threats because it stores a vast range of cyber threat information in the form of semantic triples which can be queried. A semantic triple contains two cybersecurity entities with a relationship between them. In this work, we propose a system to create semantic triples over cybersecurity text, using deep learning approaches to extract possible relationships. We use the set of semantic triples generated through our system to assert in a cybersecurity knowledge graph. Security Analysts can retrieve this data from the knowledge graph, and use this information to form a decision about a cyber-attack. \u25b3 Less Submitted 16 May, 2019; v1 submitted 7 May, 2019; originally announced May 2019. arXiv:1112.4253 [pdf, other] Simple and Robust Binary Self-Location Patterns Authors: Alfred M. Bruckstein, Tuvi Etzion, Raja Giryes, Noam Gordon, Robert J. Holt, Doron Shuldiner Abstract: A simple method to generate a two-dimensional binary grid pattern, which allows for absolute and accurate self-location in a finite planar region, is proposed. The pattern encodes position information in a local way so that reading a small number of its black or white pixels at any place provides sufficient data from which the location can be decoded both efficiently and robustly. A simple method to generate a two-dimensional binary grid pattern, which allows for absolute and accurate self-location in a finite planar region, is proposed. The pattern encodes position information in a local way so that reading a small number of its black or white pixels at any place provides sufficient data from which the location can be decoded both efficiently and robustly. \u25b3 Less Submitted 19 December, 2011; originally announced December 2011. arXiv:cs/0506085 [pdf, ps, other] On the Job Training Authors: Jason E. Holt Abstract: We propose a new framework for building and evaluating machine learning algorithms. We argue that many real-world problems require an agent which must quickly learn to respond to demands, yet can continue to perform and respond to new training throughout its useful life. We give a framework for how such agents can be built, describe several metrics for evaluating them, and show that subtle chang\u2026 \u25bd More We propose a new framework for building and evaluating machine learning algorithms. We argue that many real-world problems require an agent which must quickly learn to respond to demands, yet can continue to perform and respond to new training throughout its useful life. We give a framework for how such agents can be built, describe several metrics for evaluating them, and show that subtle changes in system construction can significantly affect agent performance. \u25b3 Less Submitted 22 June, 2005; originally announced June 2005. Comments: 8 pages, submitted to NIPS 2005 ACM Class: K.3.2 arXiv:cs/0506084 [pdf, ps, other] The One Page Model Checker Authors: Jason E. Holt Abstract: We show how standard IPC mechanisms can be used with the fork() system call to perform explicit state model checking on all interleavings of a multithreaded application. We specifically show how to check for deadlock and race conditions in programs with two threads. Our techniques are easy to apply to other languages, and require only the most rudimentary parsing of the target language. Our fund\u2026 \u25bd More We show how standard IPC mechanisms can be used with the fork() system call to perform explicit state model checking on all interleavings of a multithreaded application. We specifically show how to check for deadlock and race conditions in programs with two threads. Our techniques are easy to apply to other languages, and require only the most rudimentary parsing of the target language. Our fundamental system fits in one page of C code. \u25b3 Less Submitted 22 June, 2005; originally announced June 2005. Comments: 12 pages ACM Class: F.3.1"
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Khan,+L",
    "title": "Showing 1\u201340 of 40 results for author: Khan, L",
    "author": "Authors:\nMd Shahir Zaoad, \n      \n      Niamat Zawad, \n      \n      Priyanka Ranade, \n      \n      Richard Krogman, \n      \n      Latifur Khan, \n      \n      James Holt",
    "body": "arXiv:2503.14802 [pdf, other] Graph-Based Re-ranking: Emerging Techniques, Limitations, and Opportunities Authors: Md Shahir Zaoad, Niamat Zawad, Priyanka Ranade, Richard Krogman, Latifur Khan, James Holt Abstract: Knowledge graphs have emerged to be promising datastore candidates for context augmentation during Retrieval Augmented Generation (RAG). As a result, techniques in graph representation learning have been simultaneously explored alongside principal neural information retrieval approaches, such as two-phased retrieval, also known as re-ranking. While Graph Neural Networks (GNNs) have been proposed t\u2026 \u25bd More Knowledge graphs have emerged to be promising datastore candidates for context augmentation during Retrieval Augmented Generation (RAG). As a result, techniques in graph representation learning have been simultaneously explored alongside principal neural information retrieval approaches, such as two-phased retrieval, also known as re-ranking. While Graph Neural Networks (GNNs) have been proposed to demonstrate proficiency in graph learning for re-ranking, there are ongoing limitations in modeling and evaluating input graph structures for training and evaluation for passage and document ranking tasks. In this survey, we review emerging GNN-based ranking model architectures along with their corresponding graph representation construction methodologies. We conclude by providing recommendations on future research based on community-wide challenges and opportunities. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. MSC Class: N/A arXiv:2502.20979 [pdf, other] Real-Time Aerial Fire Detection on Resource-Constrained Devices Using Knowledge Distillation Authors: Sabina Jangirova, Branislava Jankovic, Waseem Ullah, Latif U. Khan, Mohsen Guizani Abstract: Wildfire catastrophes cause significant environmental degradation, human losses, and financial damage. To mitigate these severe impacts, early fire detection and warning systems are crucial. Current systems rely primarily on fixed CCTV cameras with a limited field of view, restricting their effectiveness in large outdoor environments. The fusion of intelligent fire detection with remote sensing im\u2026 \u25bd More Wildfire catastrophes cause significant environmental degradation, human losses, and financial damage. To mitigate these severe impacts, early fire detection and warning systems are crucial. Current systems rely primarily on fixed CCTV cameras with a limited field of view, restricting their effectiveness in large outdoor environments. The fusion of intelligent fire detection with remote sensing improves coverage and mobility, enabling monitoring in remote and challenging areas. Existing approaches predominantly utilize convolutional neural networks and vision transformer models. While these architectures provide high accuracy in fire detection, their computational complexity limits real-time performance on edge devices such as UAVs. In our work, we present a lightweight fire detection model based on MobileViT-S, compressed through the distillation of knowledge from a stronger teacher model. The ablation study highlights the impact of a teacher model and the chosen distillation technique on the model's performance improvement. We generate activation map visualizations using Grad-CAM to confirm the model's ability to focus on relevant fire regions. The high accuracy and efficiency of the proposed model make it well-suited for deployment on satellites, UAVs, and IoT devices for effective fire detection. Experiments on common fire benchmarks demonstrate that our model suppresses the state-of-the-art model by 0.44%, 2.00% while maintaining a compact model size. Our model delivers the highest processing speed among existing works, achieving real-time performance on resource-constrained devices. \u25b3 Less Submitted 28 February, 2025; originally announced February 2025. arXiv:2502.07855 [pdf, other] Vision-Language Models for Edge Networks: A Comprehensive Survey Authors: Ahmed Sharshar, Latif U. Khan, Waseem Ullah, Mohsen Guizani Abstract: Vision Large Language Models (VLMs) combine visual understanding with natural language processing, enabling tasks like image captioning, visual question answering, and video analysis. While VLMs show impressive capabilities across domains such as autonomous vehicles, smart surveillance, and healthcare, their deployment on resource-constrained edge devices remains challenging due to processing powe\u2026 \u25bd More Vision Large Language Models (VLMs) combine visual understanding with natural language processing, enabling tasks like image captioning, visual question answering, and video analysis. While VLMs show impressive capabilities across domains such as autonomous vehicles, smart surveillance, and healthcare, their deployment on resource-constrained edge devices remains challenging due to processing power, memory, and energy limitations. This survey explores recent advancements in optimizing VLMs for edge environments, focusing on model compression techniques, including pruning, quantization, knowledge distillation, and specialized hardware solutions that enhance efficiency. We provide a detailed discussion of efficient training and fine-tuning methods, edge deployment challenges, and privacy considerations. Additionally, we discuss the diverse applications of lightweight VLMs across healthcare, environmental monitoring, and autonomous systems, illustrating their growing impact. By highlighting key design strategies, current challenges, and offering recommendations for future directions, this survey aims to inspire further research into the practical deployment of VLMs, ultimately making advanced AI accessible in resource-limited settings. \u25b3 Less Submitted 11 February, 2025; originally announced February 2025. arXiv:2501.12087 [pdf, other] UAV-Assisted Real-Time Disaster Detection Using Optimized Transformer Model Authors: Branislava Jankovic, Sabina Jangirova, Waseem Ullah, Latif U. Khan, Mohsen Guizani Abstract: Dangerous surroundings and difficult-to-reach landscapes introduce significant complications for adequate disaster management and recuperation. These problems can be solved by engaging unmanned aerial vehicles (UAVs) provided with embedded platforms and optical sensors. In this work, we focus on enabling onboard aerial image processing to ensure proper and real-time disaster detection. Such a sett\u2026 \u25bd More Dangerous surroundings and difficult-to-reach landscapes introduce significant complications for adequate disaster management and recuperation. These problems can be solved by engaging unmanned aerial vehicles (UAVs) provided with embedded platforms and optical sensors. In this work, we focus on enabling onboard aerial image processing to ensure proper and real-time disaster detection. Such a setting usually causes challenges due to the limited hardware resources of UAVs. However, privacy, connectivity, and latency issues can be avoided. We suggest a UAV-assisted edge framework for disaster detection, leveraging our proposed model optimized for onboard real-time aerial image classification. The optimization of the model is achieved using post-training quantization techniques. To address the limited number of disaster cases in existing benchmark datasets and therefore ensure real-world adoption of our model, we construct a novel dataset, DisasterEye, featuring disaster scenes captured by UAVs and individuals on-site. Experimental results reveal the efficacy of our model, reaching high accuracy with lowered inference latency and memory use on both traditional machines and resource-limited devices. This shows that the scalability and adaptability of our method make it a powerful solution for real-time disaster management on resource-constrained UAV platforms. \u25b3 Less Submitted 28 February, 2025; v1 submitted 21 January, 2025; originally announced January 2025. arXiv:2501.05495 [pdf, other] LSEBMCL: A Latent Space Energy-Based Model for Continual Learning Authors: Xiaodi Li, Dingcheng Li, Rujun Gao, Mahmoud Zamani, Latifur Khan Abstract: Continual learning has become essential in many practical applications such as online news summaries and product classification. The primary challenge is known as catastrophic forgetting, a phenomenon where a model inadvertently discards previously learned knowledge when it is trained on new tasks. Existing solutions involve storing exemplars from previous classes, regularizing parameters during t\u2026 \u25bd More Continual learning has become essential in many practical applications such as online news summaries and product classification. The primary challenge is known as catastrophic forgetting, a phenomenon where a model inadvertently discards previously learned knowledge when it is trained on new tasks. Existing solutions involve storing exemplars from previous classes, regularizing parameters during the fine-tuning process, or assigning different model parameters to each task. The proposed solution LSEBMCL (Latent Space Energy-Based Model for Continual Learning) in this work is to use energy-based models (EBMs) to prevent catastrophic forgetting by sampling data points from previous tasks when training on new ones. The EBM is a machine learning model that associates an energy value with each input data point. The proposed method uses an EBM layer as an outer-generator in the continual learning framework for NLP tasks. The study demonstrates the efficacy of EBM in NLP tasks, achieving state-of-the-art results in all experiments. \u25b3 Less Submitted 9 January, 2025; originally announced January 2025. Comments: In the 7th International Conference on Artificial Intelligence in Information and Communication (ICAIIC 2025) arXiv:2412.15060 [pdf, other] ConfliBERT: A Language Model for Political Conflict Authors: Patrick T. Brandt, Sultan Alsarra, Vito J. D`Orazio, Dagmar Heintze, Latifur Khan, Shreyas Meher, Javier Osorio, Marcus Sianan Abstract: Conflict scholars have used rule-based approaches to extract information about political violence from news reports and texts. Recent Natural Language Processing developments move beyond rigid rule-based approaches. We review our recent ConfliBERT language model (Hu et al. 2022) to process political and violence related texts. The model can be used to extract actor and action classifications from\u2026 \u25bd More Conflict scholars have used rule-based approaches to extract information about political violence from news reports and texts. Recent Natural Language Processing developments move beyond rigid rule-based approaches. We review our recent ConfliBERT language model (Hu et al. 2022) to process political and violence related texts. The model can be used to extract actor and action classifications from texts about political conflict. When fine-tuned, results show that ConfliBERT has superior performance in accuracy, precision and recall over other large language models (LLM) like Google's Gemma 2 (9B), Meta's Llama 3.1 (7B), and Alibaba's Qwen 2.5 (14B) within its relevant domains. It is also hundreds of times faster than these more generalist LLMs. These results are illustrated using texts from the BBC, re3d, and the Global Terrorism Dataset (GTD). \u25b3 Less Submitted 19 December, 2024; originally announced December 2024. Comments: 30 pages, 4 figures, 5 tables arXiv:2402.12319 [pdf, other] Dynamic Environment Responsive Online Meta-Learning with Fairness Awareness Authors: Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, Feng Chen Abstract: The fairness-aware online learning framework has emerged as a potent tool within the context of continuous lifelong learning. In this scenario, the learner's objective is to progressively acquire new tasks as they arrive over time, while also guaranteeing statistical parity among various protected sub-populations, such as race and gender, when it comes to the newly introduced tasks. A significant\u2026 \u25bd More The fairness-aware online learning framework has emerged as a potent tool within the context of continuous lifelong learning. In this scenario, the learner's objective is to progressively acquire new tasks as they arrive over time, while also guaranteeing statistical parity among various protected sub-populations, such as race and gender, when it comes to the newly introduced tasks. A significant limitation of current approaches lies in their heavy reliance on the i.i.d (independent and identically distributed) assumption concerning data, leading to a static regret analysis of the framework. Nevertheless, it's crucial to note that achieving low static regret does not necessarily translate to strong performance in dynamic environments characterized by tasks sampled from diverse distributions. In this paper, to tackle the fairness-aware online learning challenge in evolving settings, we introduce a unique regret measure, FairSAR, by incorporating long-term fairness constraints into a strongly adapted loss regret framework. Moreover, to determine an optimal model parameter at each time step, we introduce an innovative adaptive fairness-aware online meta-learning algorithm, referred to as FairSAOML. This algorithm possesses the ability to adjust to dynamic environments by effectively managing bias control and model accuracy. The problem is framed as a bi-level convex-concave optimization, considering both the model's primal and dual parameters, which pertain to its accuracy and fairness attributes, respectively. Theoretical analysis yields sub-linear upper bounds for both loss regret and the cumulative violation of fairness constraints. Our experimental evaluation on various real-world datasets in dynamic environments demonstrates that our proposed FairSAOML algorithm consistently outperforms alternative approaches rooted in the most advanced prior online learning methods. \u25b3 Less Submitted 19 February, 2024; originally announced February 2024. Comments: Accepted by TKDD, extended from KDD 2022. arXiv admin note: substantial text overlap with arXiv:2205.11264 arXiv:2311.13816 [pdf, other] Algorithmic Fairness Generalization under Covariate and Dependence Shifts Simultaneously Authors: Chen Zhao, Kai Jiang, Xintao Wu, Haoliang Wang, Latifur Khan, Christan Grant, Feng Chen Abstract: The endeavor to preserve the generalization of a fair and invariant classifier across domains, especially in the presence of distribution shifts, becomes a significant and intricate challenge in machine learning. In response to this challenge, numerous effective algorithms have been developed with a focus on addressing the problem of fairness-aware domain generalization. These algorithms are desig\u2026 \u25bd More The endeavor to preserve the generalization of a fair and invariant classifier across domains, especially in the presence of distribution shifts, becomes a significant and intricate challenge in machine learning. In response to this challenge, numerous effective algorithms have been developed with a focus on addressing the problem of fairness-aware domain generalization. These algorithms are designed to navigate various types of distribution shifts, with a particular emphasis on covariate and dependence shifts. In this context, covariate shift pertains to changes in the marginal distribution of input features, while dependence shift involves alterations in the joint distribution of the label variable and sensitive attributes. In this paper, we introduce a simple but effective approach that aims to learn a fair and invariant classifier by simultaneously addressing both covariate and dependence shifts across domains. We assert the existence of an underlying transformation model can transform data from one domain to another, while preserving the semantics related to non-sensitive attributes and classes. By augmenting various synthetic data domains through the model, we learn a fair and invariant classifier in source domains. This classifier can then be generalized to unknown target domains, maintaining both model prediction and fairness concerns. Extensive empirical studies on four benchmark datasets demonstrate that our approach surpasses state-of-the-art methods. \u25b3 Less Submitted 21 May, 2024; v1 submitted 23 November, 2023; originally announced November 2023. Comments: Accepted by KDD 2024 research track arXiv:2308.07876 [pdf, other] Leveraging Codebook Knowledge with NLI and ChatGPT for Zero-Shot Political Relation Classification Authors: Yibo Hu, Erick Skorupa Parolin, Latifur Khan, Patrick T. Brandt, Javier Osorio, Vito J. D'Orazio Abstract: Is it possible accurately classify political relations within evolving event ontologies without extensive annotations? This study investigates zero-shot learning methods that use expert knowledge from existing annotation codebook, and evaluates the performance of advanced ChatGPT (GPT-3.5/4) and a natural language inference (NLI)-based model called ZSP. ChatGPT uses codebook's labeled summaries as\u2026 \u25bd More Is it possible accurately classify political relations within evolving event ontologies without extensive annotations? This study investigates zero-shot learning methods that use expert knowledge from existing annotation codebook, and evaluates the performance of advanced ChatGPT (GPT-3.5/4) and a natural language inference (NLI)-based model called ZSP. ChatGPT uses codebook's labeled summaries as prompts, whereas ZSP breaks down the classification task into context, event mode, and class disambiguation to refine task-specific hypotheses. This decomposition enhances interpretability, efficiency, and adaptability to schema changes. The experiments reveal ChatGPT's strengths and limitations, and crucially show ZSP's outperformance of dictionary-based methods and its competitive edge over some supervised models. These findings affirm the value of ZSP for validating event records and advancing ontology development. Our study underscores the efficacy of leveraging transfer learning and existing domain expertise to enhance research efficiency and scalability. \u25b3 Less Submitted 6 June, 2024; v1 submitted 15 August, 2023; originally announced August 2023. Comments: ACL 2024 arXiv:2306.01007 [pdf, other] Towards Fair Disentangled Online Learning for Changing Environments Authors: Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, Christan Grant, Feng Chen Abstract: In the problem of online learning for changing environments, data are sequentially received one after another over time, and their distribution assumptions may vary frequently. Although existing methods demonstrate the effectiveness of their learning algorithms by providing a tight bound on either dynamic regret or adaptive regret, most of them completely ignore learning with model fairness, defin\u2026 \u25bd More In the problem of online learning for changing environments, data are sequentially received one after another over time, and their distribution assumptions may vary frequently. Although existing methods demonstrate the effectiveness of their learning algorithms by providing a tight bound on either dynamic regret or adaptive regret, most of them completely ignore learning with model fairness, defined as the statistical parity across different sub-population (e.g., race and gender). Another drawback is that when adapting to a new environment, an online learner needs to update model parameters with a global change, which is costly and inefficient. Inspired by the sparse mechanism shift hypothesis, we claim that changing environments in online learning can be attributed to partial changes in learned parameters that are specific to environments and the rest remain invariant to changing environments. To this end, in this paper, we propose a novel algorithm under the assumption that data collected at each time can be disentangled with two representations, an environment-invariant semantic factor and an environment-specific variation factor. The semantic factor is further used for fair prediction under a group fairness constraint. To evaluate the sequence of model parameters generated by the learner, a novel regret is proposed in which it takes a mixed form of dynamic and static regret metrics followed by a fairness-aware long-term constraint. The detailed analysis provides theoretical guarantees for loss regret and violation of cumulative fairness constraints. Empirical evaluations on real-world datasets demonstrate our proposed method sequentially outperforms baseline methods in model accuracy and fairness. \u25b3 Less Submitted 16 July, 2023; v1 submitted 31 May, 2023; originally announced June 2023. Comments: Accepted by KDD 2023 arXiv:2305.15422 [pdf] Facial Expression Recognition at the Edge: CPU vs GPU vs VPU vs TPU Authors: Mohammadreza Mohammadi, Heath Smith, Lareb Khan, Ramtin Zand Abstract: Facial Expression Recognition (FER) plays an important role in human-computer interactions and is used in a wide range of applications. Convolutional Neural Networks (CNN) have shown promise in their ability to classify human facial expressions, however, large CNNs are not well-suited to be implemented on resource- and energy-constrained IoT devices. In this work, we present a hierarchical framewo\u2026 \u25bd More Facial Expression Recognition (FER) plays an important role in human-computer interactions and is used in a wide range of applications. Convolutional Neural Networks (CNN) have shown promise in their ability to classify human facial expressions, however, large CNNs are not well-suited to be implemented on resource- and energy-constrained IoT devices. In this work, we present a hierarchical framework for developing and optimizing hardware-aware CNNs tuned for deployment at the edge. We perform a comprehensive analysis across various edge AI accelerators including NVIDIA Jetson Nano, Intel Neural Compute Stick, and Coral TPU. Using the proposed strategy, we achieved a peak accuracy of 99.49% when testing on the CK+ facial expression recognition dataset. Additionally, we achieved a minimum inference latency of 0.39 milliseconds and a minimum power consumption of 0.52 Watts. \u25b3 Less Submitted 16 May, 2023; originally announced May 2023. arXiv:2303.13055 [pdf, other] Reimagining Application User Interface (UI) Design using Deep Learning Methods: Challenges and Opportunities Authors: Subtain Malik, Muhammad Tariq Saeed, Marya Jabeen Zia, Shahzad Rasool, Liaquat Ali Khan, Mian Ilyas Ahmed Abstract: In this paper, we present a review of the recent work in deep learning methods for user interface design. The survey encompasses well known deep learning techniques (deep neural networks, convolutional neural networks, recurrent neural networks, autoencoders, and generative adversarial networks) and datasets widely used to design user interface applications. We highlight important problems and eme\u2026 \u25bd More In this paper, we present a review of the recent work in deep learning methods for user interface design. The survey encompasses well known deep learning techniques (deep neural networks, convolutional neural networks, recurrent neural networks, autoencoders, and generative adversarial networks) and datasets widely used to design user interface applications. We highlight important problems and emerging research frontiers in this field. We believe that the use of deep learning for user interface design automation tasks could be one of the high potential fields for the advancement of the software development industry. \u25b3 Less Submitted 23 March, 2023; originally announced March 2023. Comments: A review paper on studies of UI design techniques and deep learning arXiv:2301.11441 [pdf, other] Metaverse for Wireless Systems: Architecture, Advances, Standardization, and Open Challenges Authors: Latif U. Khan, Mohsen Guizani, Dusit Niyato, Ala Al-Fuqaha, Merouane Debbah Abstract: The growing landscape of emerging wireless applications is a key driver toward the development of novel wireless system designs. Such a design can be based on the metaverse that uses a virtual model of the physical world systems along with other schemes/technologies (e.g., optimization theory, machine learning, and blockchain). A metaverse using a virtual model performs proactive intelligent analy\u2026 \u25bd More The growing landscape of emerging wireless applications is a key driver toward the development of novel wireless system designs. Such a design can be based on the metaverse that uses a virtual model of the physical world systems along with other schemes/technologies (e.g., optimization theory, machine learning, and blockchain). A metaverse using a virtual model performs proactive intelligent analytics prior to a user request for efficient management of the wireless system resources. Additionally, a metaverse will enable self-sustainability to operate wireless systems with the least possible intervention from network operators. Although the metaverse can offer many benefits, it faces some challenges as well. Therefore, in this tutorial, we discuss the role of a metaverse in enabling wireless applications. We present an overview, key enablers, design aspects (i.e., metaverse for wireless and wireless for metaverse), and a novel high-level architecture of metaverse-based wireless systems. We discuss metaverse management, reliability, and security of the metaverse-based system. Furthermore, we discuss recent advances and standardization of metaverse-enabled wireless system. Finally, we outline open challenges and present possible solutions. \u25b3 Less Submitted 22 June, 2023; v1 submitted 26 January, 2023; originally announced January 2023. arXiv:2301.08824 [pdf, ps, other] An Automated Vulnerability Detection Framework for Smart Contracts Authors: Feng Mi, Chen Zhao, Zhuoyi Wang, Sadaf MD Halim, Xiaodi Li, Zhouxiang Wu, Latifur Khan, Bhavani Thuraisingham Abstract: With the increase of the adoption of blockchain technology in providing decentralized solutions to various problems, smart contracts have become more popular to the point that billions of US Dollars are currently exchanged every day through such technology. Meanwhile, various vulnerabilities in smart contracts have been exploited by attackers to steal cryptocurrencies worth millions of dollars. Th\u2026 \u25bd More With the increase of the adoption of blockchain technology in providing decentralized solutions to various problems, smart contracts have become more popular to the point that billions of US Dollars are currently exchanged every day through such technology. Meanwhile, various vulnerabilities in smart contracts have been exploited by attackers to steal cryptocurrencies worth millions of dollars. The automatic detection of smart contract vulnerabilities therefore is an essential research problem. Existing solutions to this problem particularly rely on human experts to define features or different rules to detect vulnerabilities. However, this often causes many vulnerabilities to be ignored, and they are inefficient in detecting new vulnerabilities. In this study, to overcome such challenges, we propose a framework to automatically detect vulnerabilities in smart contracts on the blockchain. More specifically, first, we utilize novel feature vector generation techniques from bytecode of smart contract since the source code of smart contracts are rarely available in public. Next, the collected vectors are fed into our novel metric learning-based deep neural network(DNN) to get the detection result. We conduct comprehensive experiments on large-scale benchmarks, and the quantitative results demonstrate the effectiveness and efficiency of our approach. \u25b3 Less Submitted 20 January, 2023; originally announced January 2023. arXiv:2211.03703 [pdf, other] Machine Learning for Wireless Metaverse: Fundamentals, Use Case, and Future Directions Authors: Latif U. Khan, Ibrar Yaqoob, Khaled Salah, Choong Seon Hong, Dusit Niyato, Zhu Han, Mohsen Guizani Abstract: Today's wireless systems are posing key challenges in terms of quality of service and quality of physical experience. Metaverse has the potential to reshape, transform, and add innovations to the existing wireless systems. A metaverse is a collective virtual open space that can enable wireless systems using digital twins, digital avatars, and interactive experience technologies. Machine learning (\u2026 \u25bd More Today's wireless systems are posing key challenges in terms of quality of service and quality of physical experience. Metaverse has the potential to reshape, transform, and add innovations to the existing wireless systems. A metaverse is a collective virtual open space that can enable wireless systems using digital twins, digital avatars, and interactive experience technologies. Machine learning (ML) is indispensable for modeling twins, avatars, and deploying interactive experience technologies. In this paper, we present the role of ML in enabling metaverse-based wireless systems. We discuss key fundamental concepts for advancing ML in the metaverse-based wireless systems. Moreover, we present a case study of deep reinforcement learning for metaverse sensing. Finally, we discuss the future directions along with potential solutions. \u25b3 Less Submitted 10 March, 2025; v1 submitted 7 November, 2022; originally announced November 2022. arXiv:2210.09917 [pdf, other] Controllable Fake Document Infilling for Cyber Deception Authors: Yibo Hu, Yu Lin, Erick Skorupa Parolin, Latifur Khan, Kevin Hamlen Abstract: Recent works in cyber deception study how to deter malicious intrusion by generating multiple fake versions of a critical document to impose costs on adversaries who need to identify the correct information. However, existing approaches are context-agnostic, resulting in sub-optimal and unvaried outputs. We propose a novel context-aware model, Fake Document Infilling (FDI), by converting the probl\u2026 \u25bd More Recent works in cyber deception study how to deter malicious intrusion by generating multiple fake versions of a critical document to impose costs on adversaries who need to identify the correct information. However, existing approaches are context-agnostic, resulting in sub-optimal and unvaried outputs. We propose a novel context-aware model, Fake Document Infilling (FDI), by converting the problem to a controllable mask-then-infill procedure. FDI masks important concepts of varied lengths in the document, then infills a realistic but fake alternative considering both the previous and future contexts. We conduct comprehensive evaluations on technical documents and news stories. Results show that FDI outperforms the baselines in generating highly believable fakes with moderate modification to protect critical information and deceive adversaries. \u25b3 Less Submitted 28 October, 2022; v1 submitted 18 October, 2022; originally announced October 2022. Comments: Findings of EMNLP 2022 arXiv:2208.05558 [pdf, other] Federated Learning for Digital Twin-Based Vehicular Networks: Architecture and Challenges Authors: Latif U. Khan, Ehzaz Mustafa, Junaid Shuja, Faisal Rehman, Kashif Bilal, Zhu Han, Choong Seon Hong Abstract: Emerging intelligent transportation applications, such as accident reporting, lane change assistance, collision avoidance, and infotainment, will be based on diverse requirements (e.g., latency, reliability, quality of physical experience). To fulfill such requirements, there is a significant need to deploy a digital twin-based intelligent transportation system. Although the twin-based implementat\u2026 \u25bd More Emerging intelligent transportation applications, such as accident reporting, lane change assistance, collision avoidance, and infotainment, will be based on diverse requirements (e.g., latency, reliability, quality of physical experience). To fulfill such requirements, there is a significant need to deploy a digital twin-based intelligent transportation system. Although the twin-based implementation of vehicular networks can offer performance optimization. Modeling twins is a significantly challenging task. Machine learning (ML) can be a preferable solution to model such a virtual model, and specifically federated learning (FL) is a distributed learning scheme that can better preserve privacy compared to centralized ML. Although FL can offer performance enhancement, it requires careful design. Therefore, in this article, we present an overview of FL for the twin-based vehicular network. A general architecture showing FL for the twin-based vehicular network is proposed. Our proposed architecture consists of two spaces, such as twin space and a physical space. The physical space consists of all the physical entities (e.g., cars and edge servers) required for vehicular networks, whereas the twin space refers to the logical space that is used for the deployment of twins. A twin space can be implemented either using edge servers and cloud servers. We also outline a few use cases of FL for the twin-based vehicular network. Finally, the paper is concluded and an outlook on open challenges is presented. \u25b3 Less Submitted 10 August, 2022; originally announced August 2022. arXiv:2207.00413 [pdf, other] Metaverse for Wireless Systems: Vision, Enablers, Architecture, and Future Directions Authors: Latif U. Khan, Zhu Han, Dusit Niyato, Mohsen Guizani, Choong Seon Hong Abstract: Recently, significant research efforts have been initiated to enable the next-generation, namely, the sixth-generation (6G) wireless systems. In this article, we present a vision of metaverse towards effectively enabling the development of 6G wireless systems. A metaverse will use virtual representation (e.g., digital twin), digital avatars, and interactive experience technologies (e.g., extended\u2026 \u25bd More Recently, significant research efforts have been initiated to enable the next-generation, namely, the sixth-generation (6G) wireless systems. In this article, we present a vision of metaverse towards effectively enabling the development of 6G wireless systems. A metaverse will use virtual representation (e.g., digital twin), digital avatars, and interactive experience technologies (e.g., extended reality) to assist analyses, optimizations, and operations of various wireless applications. Specifically, the metaverse can offer virtual wireless system operations through the digital twin that allows network designers, mobile developers, and telecommunications engineers to monitor, observe, analyze, and simulations their solutions collaboratively and virtually. We first introduce a general architecture for metaverse-based wireless systems. We discuss key driving applications, design trends, and key enablers of metaverse-based wireless systems. Finally, we present several open challenges and their potential solutions. \u25b3 Less Submitted 12 June, 2023; v1 submitted 20 June, 2022; originally announced July 2022. arXiv:2205.11264 [pdf, other] Adaptive Fairness-Aware Online Meta-Learning for Changing Environments Authors: Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, Feng Chen Abstract: The fairness-aware online learning framework has arisen as a powerful tool for the continual lifelong learning setting. The goal for the learner is to sequentially learn new tasks where they come one after another over time and the learner ensures the statistic parity of the new coming task across different protected sub-populations (e.g. race and gender). A major drawback of existing methods is t\u2026 \u25bd More The fairness-aware online learning framework has arisen as a powerful tool for the continual lifelong learning setting. The goal for the learner is to sequentially learn new tasks where they come one after another over time and the learner ensures the statistic parity of the new coming task across different protected sub-populations (e.g. race and gender). A major drawback of existing methods is that they make heavy use of the i.i.d assumption for data and hence provide static regret analysis for the framework. However, low static regret cannot imply a good performance in changing environments where tasks are sampled from heterogeneous distributions. To address the fairness-aware online learning problem in changing environments, in this paper, we first construct a novel regret metric FairSAR by adding long-term fairness constraints onto a strongly adapted loss regret. Furthermore, to determine a good model parameter at each round, we propose a novel adaptive fairness-aware online meta-learning algorithm, namely FairSAOML, which is able to adapt to changing environments in both bias control and model precision. The problem is formulated in the form of a bi-level convex-concave optimization with respect to the model's primal and dual parameters that are associated with the model's accuracy and fairness, respectively. The theoretic analysis provides sub-linear upper bounds for both loss regret and violation of cumulative fairness constraints. Our experimental evaluation on different real-world datasets with settings of changing environments suggests that the proposed FairSAOML significantly outperforms alternatives based on the best prior online learning approaches. \u25b3 Less Submitted 26 May, 2022; v1 submitted 20 May, 2022; originally announced May 2022. Comments: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2022. arXiv admin note: text overlap with arXiv:2108.09435 arXiv:2202.06248 [pdf] A Tech Hybrid-Recommendation Engine and Personalized Notification: An integrated tool to assist users through Recommendations (Project ATHENA) Authors: Lordjette Leigh M. Lecaros, Concepcion L. Khan Abstract: Project ATHENA aims to develop an application to address information overload, primarily focused on Recommendation Systems (RSs) with the personalization and user experience design of a modern system. Two machine learning (ML) algorithms were used: (1) TF-IDF for Content-based filtering (CBF); (2) Classification with Matrix Factorization- Singular Value Decomposition(SVD) applied with Collaborativ\u2026 \u25bd More Project ATHENA aims to develop an application to address information overload, primarily focused on Recommendation Systems (RSs) with the personalization and user experience design of a modern system. Two machine learning (ML) algorithms were used: (1) TF-IDF for Content-based filtering (CBF); (2) Classification with Matrix Factorization- Singular Value Decomposition(SVD) applied with Collaborative filtering (CF) and mean (normalization) for prediction accuracy of the CF. Data sampling in academic Research and Development of Philippine Council for Agriculture, Aquatic, and Natural Resources Research and Development (PCAARRD) e-Library and Project SARAI publications plus simulated data used as training sets to generate a recommendation of items that uses the three RS filtering (CF, CBF, and personalized version of item recommendations). Series of Testing and TAM performed and discussed. Findings allow users to engage in online information and quickly evaluate retrieved items produced by the application. Compatibility-testing (CoT) shows the application is compatible with all major browsers and mobile-friendly. Performance-testing (PT) recommended v-parameter specs and TAM evaluations results indicate strongly associated with overall positive feedback, thoroughly enough to address the information-overload problem as the core of the paper. A modular architecture presented addressing the information overload, primarily focused on RSs with the personalization and design of modern systems. Developers utilized Two ML algorithms and prototyped a simplified version of the architecture. Series of testing (CoT and PT) and evaluations with TAM were performed and discussed. Project ATHENA added a UX feature design of a modern system. \u25b3 Less Submitted 13 February, 2022; originally announced February 2022. Comments: 15 pages Report number: Vol. 6, pp. 925-939 arXiv:2202.02559 [pdf, other] Digital Twin of Wireless Systems: Overview, Taxonomy, Challenges, and Opportunities Authors: Latif U. Khan, Zhu Han, Walid Saad, Ekram Hossain, Mohsen Guizani, Choong Seon Hong Abstract: Future wireless services must be focused on improving the quality of life by enabling various applications, such as extended reality, brain-computer interaction, and healthcare. These applications have diverse performance requirements (e.g., user-defined quality of experience metrics, latency, and reliability) that are challenging to be fulfilled by existing wireless systems. To meet the diverse r\u2026 \u25bd More Future wireless services must be focused on improving the quality of life by enabling various applications, such as extended reality, brain-computer interaction, and healthcare. These applications have diverse performance requirements (e.g., user-defined quality of experience metrics, latency, and reliability) that are challenging to be fulfilled by existing wireless systems. To meet the diverse requirements of the emerging applications, the concept of a digital twin has been recently proposed. A digital twin uses a virtual representation along with security-related technologies (e.g., blockchain), communication technologies (e.g., 6G), computing technologies (e.g., edge computing), and machine learning, so as to enable the smart applications. In this tutorial, we present a comprehensive overview on digital twins for wireless systems. First, we present an overview of fundamental concepts (i.e., design aspects, high-level architecture, and frameworks) of digital twin of wireless systems. Second, a comprehensive taxonomy is devised for both different aspects. These aspects are twins for wireless and wireless for twins. For the twins for wireless aspect, we consider parameters, such as twin objects design, prototyping, deployment trends, physical devices design, interface design, incentive mechanism, twins isolation, and decoupling. On the other hand, for wireless for twins, parameters such as, twin objects access aspects, security and privacy, and air interface design are considered. Finally, open research challenges and opportunities are presented along with causes and possible solutions. \u25b3 Less Submitted 5 February, 2022; originally announced February 2022. arXiv:2112.03796 [pdf, other] New Lower Bounds on the Capacity of Optical Fiber Channels via Optimized Shaping and Detection Authors: Marco Secondini, Stella Civelli, Enrico Forestieri, Lareb Zar Khan Abstract: Constellation shaping is a practical and effective technique to improve the performance and the rate adaptivity of optical communication systems. In principle, it could also be used to mitigate the impact of nonlinear effects, possibly increasing the information rate beyond the current limit dictated by fiber nonlinearity. However, this appealing idea is frustrated by the difficulty of designing a\u2026 \u25bd More Constellation shaping is a practical and effective technique to improve the performance and the rate adaptivity of optical communication systems. In principle, it could also be used to mitigate the impact of nonlinear effects, possibly increasing the information rate beyond the current limit dictated by fiber nonlinearity. However, this appealing idea is frustrated by the difficulty of designing an effective shaping strategy that takes into account the nonlinearity and long memory of the fiber channel, as well as the possible interplay with other nonlinearity mitigation strategies. As a result, only little progress has been made so far, while the optimal shaping distribution and the ultimate channel capacity remain unknown. In this work, we describe a novel technique to optimize the shaping distribution in a very general setting and high-dimensional space. For a simplified block-memoryless nonlinear optical channel, the capacity lower bound obtained by the proposed technique can be expressed analytically, establishing the conditions for an unbounded growth of capacity with power. In a more realistic scenario, the technique can be implemented by a rejection sampling algorithm driven by a suitable cost function, and the corresponding achievable information rate estimated numerically. The combination of the proposed technique with an improved (non-Gaussian) decoding metric yields a new capacity lower bound for the dual-polarization WDM channel. \u25b3 Less Submitted 7 December, 2021; originally announced December 2021. Comments: Submitted to IEEE Journal of Lightwave Technology on November 30th, 2021 arXiv:2107.07114 [pdf, other] Uncertainty-Aware Reliable Text Classification Authors: Yibo Hu, Latifur Khan Abstract: Deep neural networks have significantly contributed to the success in predictive accuracy for classification tasks. However, they tend to make over-confident predictions in real-world settings, where domain shifting and out-of-distribution (OOD) examples exist. Most research on uncertainty estimation focuses on computer vision because it provides visual validation on uncertainty quality. However,\u2026 \u25bd More Deep neural networks have significantly contributed to the success in predictive accuracy for classification tasks. However, they tend to make over-confident predictions in real-world settings, where domain shifting and out-of-distribution (OOD) examples exist. Most research on uncertainty estimation focuses on computer vision because it provides visual validation on uncertainty quality. However, few have been presented in the natural language process domain. Unlike Bayesian methods that indirectly infer uncertainty through weight uncertainties, current evidential uncertainty-based methods explicitly model the uncertainty of class probabilities through subjective opinions. They further consider inherent uncertainty in data with different root causes, vacuity (i.e., uncertainty due to a lack of evidence) and dissonance (i.e., uncertainty due to conflicting evidence). In our paper, we firstly apply evidential uncertainty in OOD detection for text classification tasks. We propose an inexpensive framework that adopts both auxiliary outliers and pseudo off-manifold samples to train the model with prior knowledge of a certain class, which has high vacuity for OOD samples. Extensive empirical experiments demonstrate that our model based on evidential uncertainty outperforms other counterparts for detecting OOD examples. Our approach can be easily deployed to traditional recurrent neural networks and fine-tuned pre-trained transformers. \u25b3 Less Submitted 15 July, 2021; originally announced July 2021. Comments: KDD 2021 arXiv:2105.09641 [pdf, other] A Dispersed Federated Learning Framework for 6G-Enabled Autonomous Driving Cars Authors: Latif U. Khan, Yan Kyaw Tun, Madyan Alsenwi, Muhammad Imran, Zhu Han, Choong Seon Hong Abstract: Sixth-Generation (6G)-based Internet of Everything applications (e.g. autonomous driving cars) have witnessed a remarkable interest. Autonomous driving cars using federated learning (FL) has the ability to enable different smart services. Although FL implements distributed machine learning model training without the requirement to move the data of devices to a centralized server, it its own implem\u2026 \u25bd More Sixth-Generation (6G)-based Internet of Everything applications (e.g. autonomous driving cars) have witnessed a remarkable interest. Autonomous driving cars using federated learning (FL) has the ability to enable different smart services. Although FL implements distributed machine learning model training without the requirement to move the data of devices to a centralized server, it its own implementation challenges such as robustness, centralized server security, communication resources constraints, and privacy leakage due to the capability of a malicious aggregation server to infer sensitive information of end-devices. To address the aforementioned limitations, a dispersed federated learning (DFL) framework for autonomous driving cars is proposed to offer robust, communication resource-efficient, and privacy-aware learning. A mixed-integer non-linear (MINLP) optimization problem is formulated to jointly minimize the loss in federated learning model accuracy due to packet errors and transmission latency. Due to the NP-hard and non-convex nature of the formulated MINLP problem, we propose the Block Successive Upper-bound Minimization (BSUM) based solution. Furthermore, the performance comparison of the proposed scheme with three baseline schemes has been carried out. Extensive numerical results are provided to show the validity of the proposed BSUM-based scheme. \u25b3 Less Submitted 20 May, 2021; originally announced May 2021. arXiv:2104.06313 [pdf, other] SetConv: A New Approach for Learning from Imbalanced Data Authors: Yang Gao, Yi-Fan Li, Yu Lin, Charu Aggarwal, Latifur Khan Abstract: For many real-world classification problems, e.g., sentiment classification, most existing machine learning methods are biased towards the majority class when the Imbalance Ratio (IR) is high. To address this problem, we propose a set convolution (SetConv) operation and an episodic training strategy to extract a single representative for each class, so that classifiers can later be trained on a ba\u2026 \u25bd More For many real-world classification problems, e.g., sentiment classification, most existing machine learning methods are biased towards the majority class when the Imbalance Ratio (IR) is high. To address this problem, we propose a set convolution (SetConv) operation and an episodic training strategy to extract a single representative for each class, so that classifiers can later be trained on a balanced class distribution. We prove that our proposed algorithm is permutation-invariant despite the order of inputs, and experiments on multiple large-scale benchmark text datasets show the superiority of our proposed framework when compared to other SOTA methods. \u25b3 Less Submitted 3 April, 2021; originally announced April 2021. Comments: Accepted by EMNLP 2020 (11 pages, 9 figures) arXiv:2104.01495 [pdf, other] Towards Self-Adaptive Metric Learning On the Fly Authors: Yang Gao, Yi-Fan Li, Swarup Chandra, Latifur Khan, Bhavani Thuraisingham Abstract: Good quality similarity metrics can significantly facilitate the performance of many large-scale, real-world applications. Existing studies have proposed various solutions to learn a Mahalanobis or bilinear metric in an online fashion by either restricting distances between similar (dissimilar) pairs to be smaller (larger) than a given lower (upper) bound or requiring similar instances to be separ\u2026 \u25bd More Good quality similarity metrics can significantly facilitate the performance of many large-scale, real-world applications. Existing studies have proposed various solutions to learn a Mahalanobis or bilinear metric in an online fashion by either restricting distances between similar (dissimilar) pairs to be smaller (larger) than a given lower (upper) bound or requiring similar instances to be separated from dissimilar instances with a given margin. However, these linear metrics learned by leveraging fixed bounds or margins may not perform well in real-world applications, especially when data distributions are complex. We aim to address the open challenge of \"Online Adaptive Metric Learning\" (OAML) for learning adaptive metric functions on the fly. Unlike traditional online metric learning methods, OAML is significantly more challenging since the learned metric could be non-linear and the model has to be self-adaptive as more instances are observed. In this paper, we present a new online metric learning framework that attempts to tackle the challenge by learning an ANN-based metric with adaptive model complexity from a stream of constraints. In particular, we propose a novel Adaptive-Bound Triplet Loss (ABTL) to effectively utilize the input constraints and present a novel Adaptive Hedge Update (AHU) method for online updating the model parameters. We empirically validate the effectiveness and efficacy of our framework on various applications such as real-world image classification, facial verification, and image retrieval. \u25b3 Less Submitted 3 April, 2021; originally announced April 2021. Comments: Accepted by WWW 2019 (Long Paper, Oral) arXiv:2102.12169 [pdf, other] Digital-Twin-Enabled 6G: Vision, Architectural Trends, and Future Directions Authors: Latif U. Khan, Walid Saad, Dusit Niyato, Zhu Han, Choong Seon Hong Abstract: Internet of Everything (IoE) applications such as haptics, human-computer interaction, and extended reality, using the sixth-generation (6G) of wireless systems have diverse requirements in terms of latency, reliability, data rate, and user-defined performance metrics. Therefore, enabling IoE applications over 6G requires a new framework that can be used to manage, operate, and optimize the 6G wir\u2026 \u25bd More Internet of Everything (IoE) applications such as haptics, human-computer interaction, and extended reality, using the sixth-generation (6G) of wireless systems have diverse requirements in terms of latency, reliability, data rate, and user-defined performance metrics. Therefore, enabling IoE applications over 6G requires a new framework that can be used to manage, operate, and optimize the 6G wireless system and its underlying IoE services. Such a new framework for 6G can be based on digital twins. Digital twins use a virtual representation of the 6G physical system along with the associated algorithms (e.g., machine learning, optimization), communication technologies (e.g., millimeter-wave and terahertz communication), computing systems (e.g., edge computing and cloud computing), as well as privacy and security-related technologists (e.g., blockchain). First, we present the key design requirements for enabling 6G through the use of a digital twin. Next, the architectural components and trends such as edge-based twins, cloud-based-twins, and edge-cloud-based twins are presented. Furthermore, we provide a comparative description of various twins. Finally, we outline and recommend guidelines for several future research directions. \u25b3 Less Submitted 13 November, 2021; v1 submitted 24 February, 2021; originally announced February 2021. Comments: This work has been submitted to the IEEE for possible publication arXiv:2009.13012 [pdf, other] Federated Learning for Internet of Things: Recent Advances, Taxonomy, and Open Challenges Authors: Latif U. Khan, Walid Saad, Zhu Han, Ekram Hossain, Choong Seon Hong Abstract: The Internet of Things (IoT) will be ripe for the deployment of novel machine learning algorithms for both network and application management. However, given the presence of massively distributed and private datasets, it is challenging to use classical centralized learning algorithms in the IoT. To overcome this challenge, federated learning can be a promising solution that enables on-device machi\u2026 \u25bd More The Internet of Things (IoT) will be ripe for the deployment of novel machine learning algorithms for both network and application management. However, given the presence of massively distributed and private datasets, it is challenging to use classical centralized learning algorithms in the IoT. To overcome this challenge, federated learning can be a promising solution that enables on-device machine learning without the need to migrate the private end-user data to a central cloud. In federated learning, only learning model updates are transferred between end-devices and the aggregation server. Although federated learning can offer better privacy preservation than centralized machine learning, it has still privacy concerns. In this paper, first, we present the recent advances of federated learning towards enabling federated learning-powered IoT applications. A set of metrics such as sparsification, robustness, quantization, scalability, security, and privacy, is delineated in order to rigorously evaluate the recent advances. Second, we devise a taxonomy for federated learning over IoT networks. Third, we propose two IoT use cases of dispersed federated learning that can offer better privacy preservation than federated learning. Finally, we present several open research challenges with their possible solutions. \u25b3 Less Submitted 18 June, 2021; v1 submitted 27 September, 2020; originally announced September 2020. Comments: This paper has been accepted for publication in IEEE Communications Surveys and Tutorials arXiv:2009.12675 [pdf, other] A Primal-Dual Subgradient Approachfor Fair Meta Learning Authors: Chen Zhao, Feng Chen, Zhuoyi Wang, Latifur Khan Abstract: The problem of learning to generalize to unseen classes during training, known as few-shot classification, has attracted considerable attention. Initialization based methods, such as the gradient-based model agnostic meta-learning (MAML), tackle the few-shot learning problem by \"learning to fine-tune\". The goal of these approaches is to learn proper model initialization, so that the classifiers fo\u2026 \u25bd More The problem of learning to generalize to unseen classes during training, known as few-shot classification, has attracted considerable attention. Initialization based methods, such as the gradient-based model agnostic meta-learning (MAML), tackle the few-shot learning problem by \"learning to fine-tune\". The goal of these approaches is to learn proper model initialization, so that the classifiers for new classes can be learned from a few labeled examples with a small number of gradient update steps. Few shot meta-learning is well-known with its fast-adapted capability and accuracy generalization onto unseen tasks. Learning fairly with unbiased outcomes is another significant hallmark of human intelligence, which is rarely touched in few-shot meta-learning. In this work, we propose a Primal-Dual Fair Meta-learning framework, namely PDFM, which learns to train fair machine learning models using only a few examples based on data from related tasks. The key idea is to learn a good initialization of a fair model's primal and dual parameters so that it can adapt to a new fair learning task via a few gradient update steps. Instead of manually tuning the dual parameters as hyperparameters via a grid search, PDFM optimizes the initialization of the primal and dual parameters jointly for fair meta-learning via a subgradient primal-dual approach. We further instantiate examples of bias controlling using mean difference and decision boundary covariance as fairness constraints to each task for supervised regression and classification, respectively. We demonstrate the versatility of our proposed approach by applying our approach to various real-world datasets. Our experiments show substantial improvements over the best prior work for this setting. \u25b3 Less Submitted 8 March, 2021; v1 submitted 26 September, 2020; originally announced September 2020. Comments: 20th IEEE International Conference on Data Mining (ICDM 2020) arXiv:2008.05286 [pdf, other] Secure IoT Data Analytics in Cloud via Intel SGX Authors: Md Shihabul Islam, Mustafa Safa Ozdayi, Latifur Khan, Murat Kantarcioglu Abstract: The growing adoption of IoT devices in our daily life is engendering a data deluge, mostly private information that needs careful maintenance and secure storage system to ensure data integrity and protection. Also, the prodigious IoT ecosystem has provided users with opportunities to automate systems by interconnecting their devices and other services with rule-based programs. The cloud services t\u2026 \u25bd More The growing adoption of IoT devices in our daily life is engendering a data deluge, mostly private information that needs careful maintenance and secure storage system to ensure data integrity and protection. Also, the prodigious IoT ecosystem has provided users with opportunities to automate systems by interconnecting their devices and other services with rule-based programs. The cloud services that are used to store and process sensitive IoT data turn out to be vulnerable to outside threats. Hence, sensitive IoT data and rule-based programs need to be protected against cyberattacks. To address this important challenge, in this paper, we propose a framework to maintain confidentiality and integrity of IoT data and rule-based program execution. We design the framework to preserve data privacy utilizing Trusted Execution Environment (TEE) such as Intel SGX, and end-to-end data encryption mechanism. We evaluate the framework by executing rule-based programs in the SGX securely with both simulated and real IoT device data. \u25b3 Less Submitted 10 August, 2020; originally announced August 2020. arXiv:2008.05189 [pdf, other] Dispersed Federated Learning: Vision, Taxonomy, and Future Directions Authors: Latif U. Khan, Walid Saad, Zhu Han, Choong Seon Hong Abstract: The ongoing deployment of the Internet of Things (IoT)-based smart applications is spurring the adoption of machine learning as a key technology enabler. To overcome the privacy and overhead challenges of centralized machine learning, there has been a significant recent interest in the concept of federated learning. Federated learning offers on-device, privacy-preserving machine learning without t\u2026 \u25bd More The ongoing deployment of the Internet of Things (IoT)-based smart applications is spurring the adoption of machine learning as a key technology enabler. To overcome the privacy and overhead challenges of centralized machine learning, there has been a significant recent interest in the concept of federated learning. Federated learning offers on-device, privacy-preserving machine learning without the need to transfer end-devices data to a third party location. However, federated learning still has privacy concerns due to sensitive information inferring capability of the aggregation server using end-devices local learning models. Furthermore, the federated learning process might fail due to a failure in the aggregation server (e.g., due to a malicious attack or physical defect). Other than privacy and robustness issues, federated learning over IoT networks requires a significant amount of communication resources for training. To cope with these issues, we propose a novel concept of dispersed federated learning (DFL) that is based on the true decentralization. We opine that DFL will serve as a practical implementation of federated learning for various IoT-based smart applications such as smart industries and intelligent transportation systems. First, the fundamentals of the DFL are presented. Second, a taxonomy is devised with a qualitative analysis of various DFL schemes. Third, a DFL framework for IoT networks is proposed with a matching theory-based solution. Finally, an outlook on future research directions is presented. \u25b3 Less Submitted 17 June, 2021; v1 submitted 12 August, 2020; originally announced August 2020. Comments: This paper has been accepted for publication in IEEE Wireless Communications arXiv:2004.00387 [pdf, other] Deep Learning on Knowledge Graph for Recommender System: A Survey Authors: Yang Gao, Yi-Fan Li, Yu Lin, Hang Gao, Latifur Khan Abstract: Recent advances in research have demonstrated the effectiveness of knowledge graphs (KG) in providing valuable external knowledge to improve recommendation systems (RS). A knowledge graph is capable of encoding high-order relations that connect two objects with one or multiple related attributes. With the help of the emerging Graph Neural Networks (GNN), it is possible to extract both object chara\u2026 \u25bd More Recent advances in research have demonstrated the effectiveness of knowledge graphs (KG) in providing valuable external knowledge to improve recommendation systems (RS). A knowledge graph is capable of encoding high-order relations that connect two objects with one or multiple related attributes. With the help of the emerging Graph Neural Networks (GNN), it is possible to extract both object characteristics and relations from KG, which is an essential factor for successful recommendations. In this paper, we provide a comprehensive survey of the GNN-based knowledge-aware deep recommender systems. Specifically, we discuss the state-of-the-art frameworks with a focus on their core component, i.e., the graph embedding module, and how they address practical recommendation issues such as scalability, cold-start and so on. We further summarize the commonly-used benchmark datasets, evaluation metrics as well as open-source codes. Finally, we conclude the survey and propose potential research directions in this rapidly growing field. \u25b3 Less Submitted 25 March, 2020; originally announced April 2020. Comments: 6 figures, 5 tables arXiv:1911.05642 [pdf, other] Federated Learning for Edge Networks: Resource Optimization and Incentive Mechanism Authors: Latif U. Khan, Shashi Raj Pandey, Nguyen H. Tran, Walid Saad, Zhu Han, Minh N. H. Nguyen, Choong Seon Hong Abstract: Recent years have witnessed a rapid proliferation of smart Internet of Things (IoT) devices. IoT devices with intelligence require the use of effective machine learning paradigms. Federated learning can be a promising solution for enabling IoT-based smart applications. In this paper, we present the primary design aspects for enabling federated learning at network edge. We model the incentive-based\u2026 \u25bd More Recent years have witnessed a rapid proliferation of smart Internet of Things (IoT) devices. IoT devices with intelligence require the use of effective machine learning paradigms. Federated learning can be a promising solution for enabling IoT-based smart applications. In this paper, we present the primary design aspects for enabling federated learning at network edge. We model the incentive-based interaction between a global server and participating devices for federated learning via a Stackelberg game to motivate the participation of the devices in the federated learning process. We present several open research challenges with their possible solutions. Finally, we provide an outlook on future research. \u25b3 Less Submitted 7 September, 2020; v1 submitted 5 November, 2019; originally announced November 2019. Comments: The first two authors contributed equally. This article has been accepted for publication in IEEE Communications Magazine arXiv:1910.00785 [pdf, other] Energy Harvesting in 5G Networks: Taxonomy, Requirements, Challenges, and Future Directions Authors: Muhammad Imran, Latif U. Khan, Ibrar Yaqoob, Ejaz Ahmed, Muhammad Ahsan Qureshi, Arif Ahmed Abstract: Consciousness of energy saving is increasing in fifth-generation (5G) wireless networks due to the high energy consumption issue. Energy harvesting technology is a possible appealing solution for ultimately prolonging the lifetime of devices and networks. Although considerable research efforts have been conducted in the context of using energy harvesting technology in 5G wireless networks, these e\u2026 \u25bd More Consciousness of energy saving is increasing in fifth-generation (5G) wireless networks due to the high energy consumption issue. Energy harvesting technology is a possible appealing solution for ultimately prolonging the lifetime of devices and networks. Although considerable research efforts have been conducted in the context of using energy harvesting technology in 5G wireless networks, these efforts are in their infancy, and a tutorial on this topic is still lacking. This study aims to discuss the beneficial role of energy harvesting technology in 5G networks. We categorize and classify the literature available on energy harvesting in 5G networks by devising a taxonomy based on energy sources; energy harvesting devices, phases, and models; energy conversion methods, and energy propagation medium. The key requirements for enabling energy harvesting in 5G networks are also outlined. Several core research challenges that remain to be addressed are discussed. Furthermore, future research directions are provided. \u25b3 Less Submitted 2 October, 2019; originally announced October 2019. arXiv:1909.08747 [pdf, other] Edge-Computing-Enabled Smart Cities: A Comprehensive Survey Authors: Latif U. Khan, Ibrar Yaqoob, Nguyen H. Tran, S. M. Ahsan Kazmi, Tri Nguyen Dang, Choong Seon Hong Abstract: Recent years have disclosed a remarkable proliferation of compute-intensive applications in smart cities. Such applications continuously generate enormous amounts of data which demand strict latency-aware computational processing capabilities. Although edge computing is an appealing technology to compensate for stringent latency related issues, its deployment engenders new challenges. In this surv\u2026 \u25bd More Recent years have disclosed a remarkable proliferation of compute-intensive applications in smart cities. Such applications continuously generate enormous amounts of data which demand strict latency-aware computational processing capabilities. Although edge computing is an appealing technology to compensate for stringent latency related issues, its deployment engenders new challenges. In this survey, we highlight the role of edge computing in realizing the vision of smart cities. First, we analyze the evolution of edge computing paradigms. Subsequently, we critically review the state-of-the-art literature focusing on edge computing applications in smart cities. Later, we categorize and classify the literature by devising a comprehensive and meticulous taxonomy. Furthermore, we identify and discuss key requirements, and enumerate recently reported synergies of edge computing enabled smart cities. Finally, several indispensable open challenges along with their causes and guidelines are discussed, serving as future research directions. \u25b3 Less Submitted 12 October, 2020; v1 submitted 11 September, 2019; originally announced September 2019. arXiv:1901.07867 [pdf] Context based Analysis of Lexical Semantics for Hindi Language Authors: Mohd Zeeshan Ansari, Lubna Khan Abstract: A word having multiple senses in a text introduces the lexical semantic task to find out which particular sense is appropriate for the given context. One such task is Word sense disambiguation which refers to the identification of the most appropriate meaning of the polysemous word in a given context using computational algorithms. The language processing research in Hindi, the official language o\u2026 \u25bd More A word having multiple senses in a text introduces the lexical semantic task to find out which particular sense is appropriate for the given context. One such task is Word sense disambiguation which refers to the identification of the most appropriate meaning of the polysemous word in a given context using computational algorithms. The language processing research in Hindi, the official language of India, and other Indian languages is restricted by unavailability of the standard corpus. For Hindi word sense disambiguation also, the large corpus is not available. In this work, we prepared the text containing new senses of certain words leading to the enrichment of the sense-tagged Hindi corpus of sixty polysemous words. Furthermore, we analyzed two novel lexical associations for Hindi word sense disambiguation based on the contextual features of the polysemous word. The evaluation of these methods is carried out over learning algorithms and favorable results are achieved. \u25b3 Less Submitted 23 January, 2019; originally announced January 2019. Comments: Accepted in NGCT-2018 arXiv:1811.05141 Co-Representation Learning For Classification and Novel Class Detection via Deep Networks Authors: Zhuoyi Wang, Zelun Kong, Hemeng Tao, Swarup Chandra, Latifur Khan Abstract: One of the key challenges of performing label prediction over a data stream concerns with the emergence of instances belonging to unobserved class labels over time. Previously, this problem has been addressed by detecting such instances and using them for appropriate classifier adaptation. The fundamental aspect of a novel-class detection strategy relies on the ability of comparison among observed\u2026 \u25bd More One of the key challenges of performing label prediction over a data stream concerns with the emergence of instances belonging to unobserved class labels over time. Previously, this problem has been addressed by detecting such instances and using them for appropriate classifier adaptation. The fundamental aspect of a novel-class detection strategy relies on the ability of comparison among observed instances to discriminate them into known and unknown classes. Therefore, studies in the past have proposed various metrics suitable for comparison over the observed feature space. Unfortunately, these similarity measures fail to reliably identify distinct regions in observed feature spaces useful for class discrimination and novel-class detection, especially in streams containing high-dimensional data instances such as images and texts. In this paper, we address this key challenge by proposing a semi-supervised multi-task learning framework called \\sysname{} which aims to intrinsically search for a latent space suitable for detecting labels of instances from both known and unknown classes. We empirically measure the performance of \\sysname{} over multiple real-world image and text datasets and demonstrate its superiority by comparing its performance with existing semi-supervised methods. \u25b3 Less Submitted 27 January, 2019; v1 submitted 13 November, 2018; originally announced November 2018. Comments: The paper absence of relative theoretical prove, some supplement experiment should also be add arXiv:1810.03966 [pdf, other] Adaptive Image Stream Classification via Convolutional Neural Network with Intrinsic Similarity Metrics Authors: Yang Gao, Swarup Chandra, Zhuoyi Wang, Latifur Khan Abstract: When performing data classification over a stream of continuously occurring instances, a key challenge is to develop an open-world classifier that anticipates instances from an unknown class. Studies addressing this problem, typically called novel class detection, have considered classification methods that reactively adapt to such changes along the stream. Importantly, they rely on the property o\u2026 \u25bd More When performing data classification over a stream of continuously occurring instances, a key challenge is to develop an open-world classifier that anticipates instances from an unknown class. Studies addressing this problem, typically called novel class detection, have considered classification methods that reactively adapt to such changes along the stream. Importantly, they rely on the property of cohesion and separation among instances in feature space. Instances belonging to the same class are assumed to be closer to each other (cohesion) than those belonging to different classes (separation). Unfortunately, this assumption may not have large support when dealing with high dimensional data such as images. In this paper, we address this key challenge by proposing a semisupervised multi-task learning framework called CSIM which aims to intrinsically search for a latent space suitable for detecting labels of instances from both known and unknown classes. Particularly, we utilize a convolution neural network layer that aids in the learning of a latent feature space suitable for novel class detection. We empirically measure the performance of CSIM over multiple realworld image datasets and demonstrate its superiority by comparing its performance with existing semi-supervised methods. \u25b3 Less Submitted 27 September, 2018; originally announced October 2018. Comments: 10 pages; KDD'18 Deep Learning Day, August 2018, London, UK arXiv:1401.4436 [pdf] Cause Identification from Aviation Safety Incident Reports via Weakly Supervised Semantic Lexicon Construction Authors: Muhammad Arshad Ul Abedin, Vincent Ng, Latifur Khan Abstract: The Aviation Safety Reporting System collects voluntarily submitted reports on aviation safety incidents to facilitate research work aiming to reduce such incidents. To effectively reduce these incidents, it is vital to accurately identify why these incidents occurred. More precisely, given a set of possible causes, or shaping factors, this task of cause identification involves identifying all and\u2026 \u25bd More The Aviation Safety Reporting System collects voluntarily submitted reports on aviation safety incidents to facilitate research work aiming to reduce such incidents. To effectively reduce these incidents, it is vital to accurately identify why these incidents occurred. More precisely, given a set of possible causes, or shaping factors, this task of cause identification involves identifying all and only those shaping factors that are responsible for the incidents described in a report. We investigate two approaches to cause identification. Both approaches exploit information provided by a semantic lexicon, which is automatically constructed via Thelen and Riloffs Basilisk framework augmented with our linguistic and algorithmic modifications. The first approach labels a report using a simple heuristic, which looks for the words and phrases acquired during the semantic lexicon learning process in the report. The second approach recasts cause identification as a text classification problem, employing supervised and transductive text classification algorithms to learn models from incident reports labeled with shaping factors and using the models to label unseen reports. Our experiments show that both the heuristic-based approach and the learning-based approach (when given sufficient training data) outperform the baseline system significantly. \u25b3 Less Submitted 15 January, 2014; originally announced January 2014. Journal ref: Journal Of Artificial Intelligence Research, Volume 38, pages 569-631, 2010 arXiv:0710.3979 [pdf] Toward Trusted Sharing of Network Packet Traces Using Anonymization: Single-Field Privacy/Analysis Tradeoffs Authors: William Yurcik, Clay Woolam, Greg Hellings, Latifur Khan, Bhavani Thuraisingham Abstract: Network data needs to be shared for distributed security analysis. Anonymization of network data for sharing sets up a fundamental tradeoff between privacy protection versus security analysis capability. This privacy/analysis tradeoff has been acknowledged by many researchers but this is the first paper to provide empirical measurements to characterize the privacy/analysis tradeoff for an enterp\u2026 \u25bd More Network data needs to be shared for distributed security analysis. Anonymization of network data for sharing sets up a fundamental tradeoff between privacy protection versus security analysis capability. This privacy/analysis tradeoff has been acknowledged by many researchers but this is the first paper to provide empirical measurements to characterize the privacy/analysis tradeoff for an enterprise dataset. Specifically we perform anonymization options on single-fields within network packet traces and then make measurements using intrusion detection system alarms as a proxy for security analysis capability. Our results show: (1) two fields have a zero sum tradeoff (more privacy lessens security analysis and vice versa) and (2) eight fields have a more complex tradeoff (that is not zero sum) in which both privacy and analysis can both be simultaneously accomplished. \u25b3 Less Submitted 26 October, 2007; v1 submitted 22 October, 2007; originally announced October 2007. Comments: 8 pages,1 figure, 4 tables ACM Class: C.2.0; C.2.3; C.2.m; D.3.4; K.6.5"
  },
  {
    "url": "https://arxiv.org/format/2503.14800",
    "title": "arXiv.org >\n2503.14800 > format",
    "author": "Unknown",
    "body": "Help | Advanced Search Note: Many of the formats above are served gzipped (Content-Encoding: x-gzip). Your browser may silently uncompress after downloading so the files you see saved may appear uncompressed. Please report any problems to arXiv technical support and include the paper identifier. arXiv Operational Status Get status notifications via email or slack"
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Alselwi,+G",
    "title": "Showing 1\u20131 of 1 results for author: Alselwi, G",
    "author": "Authors:\nGhadir Alselwi, \n      \n      Hao Xue, \n      \n      Shoaib Jameel, \n      \n      Basem Suleiman, \n      \n      Flora D. Salim, \n      \n      Imran Razzak",
    "body": "arXiv:2503.14800 [pdf, other] Long Context Modeling with Ranked Memory-Augmented Retrieval Authors: Ghadir Alselwi, Hao Xue, Shoaib Jameel, Basem Suleiman, Flora D. Salim, Imran Razzak Abstract: Effective long-term memory management is crucial for language models handling extended contexts. We introduce a novel framework that dynamically ranks memory entries based on relevance. Unlike previous works, our model introduces a novel relevance scoring and a pointwise re-ranking model for key-value embeddings, inspired by learning-to-rank techniques in information retrieval. Enhanced Ranked Mem\u2026 \u25bd More Effective long-term memory management is crucial for language models handling extended contexts. We introduce a novel framework that dynamically ranks memory entries based on relevance. Unlike previous works, our model introduces a novel relevance scoring and a pointwise re-ranking model for key-value embeddings, inspired by learning-to-rank techniques in information retrieval. Enhanced Ranked Memory Augmented Retrieval ERMAR achieves state-of-the-art results on standard benchmarks. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025."
  },
  {
    "url": "https://arxiv.org/html/2503.14800v1",
    "title": "Long Context Modeling with Ranked Memory-Augmented Retrieval",
    "author": "Ghadir Alselwi1,\nHao Xue1,\nShoaib Jameel2,\nBasem Suleiman1,\nFlora D. Salim1,\nImran Razzak3,1\n1University of New South Wales, Sydney, NSW, Austalia \n2University of Southampton, Southampton, UK \n3Mohamed Bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE \n{g.alselwi,hao.xue1,b.suleiman, flora.salim}@unsw.edu.au \nm.s.jameel@southampton.ac.uk, imran.razzak@mbzuai.ac.ae",
    "body": "Effective long-term memory management is crucial for language models handling extended contexts. We introduce a novel framework that dynamically ranks memory entries based on relevance. Unlike previous works, our model introduces a novel relevance scoring and a pointwise re-ranking model for key-value embeddings, inspired by learning-to-rank techniques in information retrieval. Enhanced Ranked Memory Augmented Retrieval (ERMAR) achieves state-of-the-art results on standard benchmarks. Long Context Modeling with Ranked Memory-Augmented Retrieval Ghadir Alselwi1, Hao Xue1, Shoaib Jameel2, Basem Suleiman1, Flora D. Salim1, Imran Razzak3,1 1University of New South Wales, Sydney, NSW, Austalia 2University of Southampton, Southampton, UK 3Mohamed Bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE {g.alselwi,hao.xue1,b.suleiman, flora.salim}@unsw.edu.au m.s.jameel@southampton.ac.uk, imran.razzak@mbzuai.ac.ae Large Language Models (LLMs) face a fundamental limitation in processing long-context scenarios due to the quadratic complexity of attention mechanisms and increasing memory demands during generation Vaswani (2017); Tworkowski et al. (2024). Consider a scenario in an automated customer service system: A customer reports an issue with their printer, referencing a setup process from a previous conversation that occurred two hours ago. After 50 messages of troubleshooting, the customer mentions that the same error from the beginning has resurfaced. Traditional LLMs, constrained by their context window, would struggle to access the crucial earlier context about the initial setup process, leading to inconsistent or incomplete responses, Figure 1. It is well known that handling extended contexts remains a significant challenge, particularly in applications requiring document analysis and sustained dialogue interactions. The recent MemLong Liu et al. (2024) architecture stores and accesses historical context through basic chunk-level memory operations. The memory bank model is a large, non-trainable store of past context representations. Instead of re-computing representations for all past tokens every time, these representations are pre-computed and stored. Given the current context, MemLong retrieves relevant segments from the memory bank. It uses a dot product similarity search to find the memory entries most related to the current context. This allows the model to focus only on the most pertinent past information, rather than processing the entire history. However, its treatment of all key-value (K-V) pairs with equal weight, regardless of their contextual relevance, often leads to information overload and reduced retrieval precision. This limitation becomes particularly evident in scenarios requiring context management. We have developed a novel model that addresses the aforementioned limitations by building upon Memlong Liu et al. (2024), a publicly available baseline on GitHub111https://github.com/Bui1dMySea/MemLong. Our Enhanced Ranked Memory Augmented Retrieval (ERMAR) model has a novel relevance scoring mechanism that fundamentally improves context retrieval and utilization for K-V embeddings. Unlike MemLong, ERMAR employs multiplication Cao et al. (2007) to compute relevance scores, enabling a more nuanced and context-aware assessment of semantic alignment between queries and stored memory. ERMAR also incorporates a re-ranking mechanism that dynamically reorders K-V embeddings based on their relevance scores, ensuring that the most pertinent information is prioritized during retrieval. This re-ranking process, combined with an adaptive retrieval system that integrates historical usage patterns, allows ERMAR to capture subtle contextual relationships better and refine memory prioritization. As shown in Figure 1, ERMAR processes incoming queries and long-context conversations through a novel ranking architecture, employing K-V pairs ranking ( K0subscript\ud835\udc3e0K_{0}italic_K start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-V0subscript\ud835\udc490V_{0}italic_V start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, K1subscript\ud835\udc3e1K_{1}italic_K start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-V1subscript\ud835\udc491V_{1}italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT,\u2026,Kisubscript\ud835\udc3e\ud835\udc56K_{i}italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT-Visubscript\ud835\udc49\ud835\udc56V_{i}italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) and corresponding embeddings to perform semantic search and ranking of relevant historical information. Our novel ERMAR model introduces three key improvements: (i) A semantic similarity metric to measure contextual alignment between query embeddings and key-value pairs; (ii) A weighted scoring function that considers content similarity and contextual relevance; and (iii) Integration of historical usage patterns to refine relevance assessment. Existing memory-augmented architectures such as RetroMAE Xiao et al. (2022) have demonstrated promising results but often struggle with semantic coherence and retrieval efficiency. Similarly, sparse attention mechanisms reduce computational complexity but frequently sacrifice model capability Beltagy et al. (2020). Memorizing Transformers Wu et al. (2022) developed dedicated memory tokens. Recent advances have focused on memory management through hierarchical cache structures Wu et al. (2022) and context-aware retention strategies Borgeaud et al. (2022). However, these systems often struggle with distribution shifts during training and effective information retrieval. While these methods offer partial solutions, they often face challenges in maintaining consistent performance across varying context lengths and ensuring reliable information retrieval. Architectural modifications have focused primarily on improving position representation and attention mechanisms. Position encoding adaptations such as RoPE Su et al. (2024) and ALiBi Press et al. (2022) have enhanced models\u2019 ability to handle longer sequences by providing more robust position information. YARN Peng et al. (2023) further advanced this approach through dynamic position embeddings, demonstrating reliable generalization up to 128k tokens. Complementing these developments, sparse attention mechanisms such as Longformer Beltagy et al. (2020) and BigBird Zaheer et al. (2020) have reduced computational complexity while maintaining model capabilities through selective attention patterns. Figure 2 illustrates our ERMAR framework and Figure 1 presents the contextual ranking mechanism of key, value pairs components that enable effective retrieval. ERMAR maintains consistency through frozen lower layers and selective parameter updating. ERMAR: (i) stores important information from earlier parts of the text; (ii) assigns relevance scores to stored information based on its importance to the current context, and (iii) retrieves only the most relevant historical information when needed. Our relevance scoring is analogous to attention, allowing the model to focus on important parts of the memory. There is also a \u201cloose\u201d pointwise connection because the primary objective is sequence likelihood. Let \ud835\udcb1\ud835\udcb1\\mathcal{V}caligraphic_V be a finite vocabulary, and \ud835\udc31=(x1,\u2026,xn)\u2208\ud835\udcb1n\ud835\udc31subscript\ud835\udc651\u2026subscript\ud835\udc65\ud835\udc5bsuperscript\ud835\udcb1\ud835\udc5b\\mathbf{x}=(x_{1},\\ldots,x_{n})\\in\\mathcal{V}^{n}bold_x = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) \u2208 caligraphic_V start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT a token sequence with preceding context \ud835\udc31<isubscript\ud835\udc31absent\ud835\udc56\\mathbf{x}_{<i}bold_x start_POSTSUBSCRIPT < italic_i end_POSTSUBSCRIPT. The embedding function \u2130:\ud835\udcb1\u2217\u2192\u211ddret:\u2130\u2192superscript\ud835\udcb1superscript\u211dsubscript\ud835\udc51ret\\mathcal{E}:\\mathcal{V}^{*}\\rightarrow\\mathbb{R}^{d_{\\text{ret}}}caligraphic_E : caligraphic_V start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT \u2192 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT end_POSTSUPERSCRIPT maps sequences to retrieval space. We introduce a memory function \u2133\u2133\\mathcal{M}caligraphic_M augmented with a relevance scoring mechanism \u2133:\u211ddmodel\u23dfkeys\u00d7\u211ddmodel\u23dfvalues\u00d7\u211ddret\u23dfembeddings\u2192\ud835\udcae:\u2133\u2192subscript\u23dfsuperscript\u211dsubscript\ud835\udc51modelkeyssubscript\u23dfsuperscript\u211dsubscript\ud835\udc51modelvaluessubscript\u23dfsuperscript\u211dsubscript\ud835\udc51retembeddings\ud835\udcae\\mathcal{M}:\\underbrace{\\mathbb{R}^{d_{\\text{model}}}}_{\\text{keys}}\\times% \\underbrace{\\mathbb{R}^{d_{\\text{model}}}}_{\\text{values}}\\times\\underbrace{% \\mathbb{R}^{d_{\\text{ret}}}}_{\\text{embeddings}}\\rightarrow\\mathcal{S}caligraphic_M : under\u23df start_ARG blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG start_POSTSUBSCRIPT keys end_POSTSUBSCRIPT \u00d7 under\u23df start_ARG blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG start_POSTSUBSCRIPT values end_POSTSUBSCRIPT \u00d7 under\u23df start_ARG blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG start_POSTSUBSCRIPT embeddings end_POSTSUBSCRIPT \u2192 caligraphic_S . Relevance Score: Given a query embedding \ud835\udc2a\u2208\u211ddret\ud835\udc2asuperscript\u211dsubscript\ud835\udc51ret\\mathbf{q}\\in\\mathbb{R}^{d_{\\text{ret}}}bold_q \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT end_POSTSUPERSCRIPT and a matrix of key embeddings \ud835\udc0a=[k1,\u2026,km]\u2208\u211dm\u00d7dret\ud835\udc0asubscript\ud835\udc581\u2026subscript\ud835\udc58\ud835\udc5asuperscript\u211d\ud835\udc5asubscript\ud835\udc51ret\\mathbf{K}=[k_{1},\\ldots,k_{m}]\\in\\mathbb{R}^{m\\times d_{\\text{ret}}}bold_K = [ italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_k start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ] \u2208 blackboard_R start_POSTSUPERSCRIPT italic_m \u00d7 italic_d start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT end_POSTSUPERSCRIPT (where each row ki\u2208\u211ddretsubscript\ud835\udc58\ud835\udc56superscript\u211dsubscript\ud835\udc51retk_{i}\\in\\mathbb{R}^{d_{\\text{ret}}}italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT end_POSTSUPERSCRIPT corresponds to a key embedding), the relevance score is: \u03b1\u2062(\ud835\udc2a,\ud835\udc0a)=softmax\u2062(\ud835\udc2a\ud835\udc0a\u22a4dret)\ud835\udefc\ud835\udc2a\ud835\udc0asoftmaxsuperscript\ud835\udc2a\ud835\udc0atopsubscript\ud835\udc51ret\\alpha(\\mathbf{q},\\mathbf{K})=\\text{softmax}\\left(\\frac{\\mathbf{q}\\mathbf{K}^{% \\top}}{\\sqrt{d_{\\text{ret}}}}\\right)italic_\u03b1 ( bold_q , bold_K ) = softmax ( divide start_ARG bold_qK start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT end_ARG end_ARG ) Here, dretsubscript\ud835\udc51ret\\sqrt{d_{\\text{ret}}}square-root start_ARG italic_d start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT end_ARG normalizes the similarity scores to prevent excessively large values. The relevance score \u03b1\u2062(\ud835\udc2a,\ud835\udc0a)\ud835\udefc\ud835\udc2a\ud835\udc0a\\alpha(\\mathbf{q},\\mathbf{K})italic_\u03b1 ( bold_q , bold_K ) can be interpreted as a probability distribution over the keys, where each entry \u03b1isubscript\ud835\udefc\ud835\udc56\\alpha_{i}italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT represents the relative importance (or attention weight) of the i\ud835\udc56iitalic_i-th key to the query \ud835\udc2a\ud835\udc2a\\mathbf{q}bold_q. This score is used to rank memory entries based on their importance to the current query. Ranked Key-Value Pairs: Each embedding \ud835\udc1e\ud835\udc1e\\mathbf{e}bold_e maintains a ranked set of key-value pairs: \u211branked\u2062(\ud835\udc1e)={(Kj,Vj,sj)}j=1m,subscript\u211branked\ud835\udc1esuperscriptsubscriptsubscript\ud835\udc3e\ud835\udc57subscript\ud835\udc49\ud835\udc57subscript\ud835\udc60\ud835\udc57\ud835\udc571\ud835\udc5a\\mathcal{R}_{\\text{ranked}}(\\mathbf{e})=\\{(K_{j},V_{j},s_{j})\\}_{j=1}^{m},caligraphic_R start_POSTSUBSCRIPT ranked end_POSTSUBSCRIPT ( bold_e ) = { ( italic_K start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , , where sj=\u03b1\u2062(\ud835\udc1e,Kj)subscript\ud835\udc60\ud835\udc57\ud835\udefc\ud835\udc1esubscript\ud835\udc3e\ud835\udc57s_{j}=\\alpha(\\mathbf{e},K_{j})italic_s start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = italic_\u03b1 ( bold_e , italic_K start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) is the relevance score between the embedding \ud835\udc1e\ud835\udc1e\\mathbf{e}bold_e and the key Kjsubscript\ud835\udc3e\ud835\udc57K_{j}italic_K start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT. ERMAR objection function is formulated as: Given a sequence \ud835\udc31\ud835\udc31\\mathbf{x}bold_x, maximize: \u2112\u2062(\u03b8)=\u2211i=1np\u03b8\u2062(xi\u2223\u211bRSAR\u2062(ti,s),\ud835\udc31<i)\u2112\ud835\udf03superscriptsubscript\ud835\udc561\ud835\udc5bsubscript\ud835\udc5d\ud835\udf03conditionalsubscript\ud835\udc65\ud835\udc56subscript\u211bRSARsubscript\ud835\udc61\ud835\udc56\ud835\udc60subscript\ud835\udc31absent\ud835\udc56\\mathcal{L}(\\theta)=\\sum_{i=1}^{n}p_{\\theta}(x_{i}\\mid\\mathcal{R}_{\\text{RSAR}% }(t_{i},s),\\mathbf{x}_{<i})caligraphic_L ( italic_\u03b8 ) = \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2223 caligraphic_R start_POSTSUBSCRIPT RSAR end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_s ) , bold_x start_POSTSUBSCRIPT < italic_i end_POSTSUBSCRIPT ), subject to: si=\u2133\u2062(K1:i\u22121,V1:i\u22121;\u2130\u2062(t1:i\u22121))subscript\ud835\udc60\ud835\udc56\u2133subscript\ud835\udc3e:1\ud835\udc561subscript\ud835\udc49:1\ud835\udc561\u2130subscript\ud835\udc61:1\ud835\udc561s_{i}=\\mathcal{M}(K_{1:i-1},V_{1:i-1};\\mathcal{E}(t_{1:i-1}))italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = caligraphic_M ( italic_K start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT ; caligraphic_E ( italic_t start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT ) ) , ti=text\u2062(c\u2308i/\u03c4\u2309)subscript\ud835\udc61\ud835\udc56textsubscript\ud835\udc50\ud835\udc56\ud835\udf0ft_{i}=\\text{text}(c_{\\lceil i/\\tau\\rceil})italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = text ( italic_c start_POSTSUBSCRIPT \u2308 italic_i / italic_\u03c4 \u2309 end_POSTSUBSCRIPT ) , \u03b1i=\u03b1\u2062(\u2130\u2062(ti),K1:i\u22121)subscript\ud835\udefc\ud835\udc56\ud835\udefc\u2130subscript\ud835\udc61\ud835\udc56subscript\ud835\udc3e:1\ud835\udc561\\alpha_{i}=\\alpha(\\mathcal{E}(t_{i}),K_{1:i-1})italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_\u03b1 ( caligraphic_E ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , italic_K start_POSTSUBSCRIPT 1 : italic_i - 1 end_POSTSUBSCRIPT ) , where \u03b1isubscript\ud835\udefc\ud835\udc56\\alpha_{i}italic_\u03b1 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT guides key-value pair selection, and p\u03b8subscript\ud835\udc5d\ud835\udf03p_{\\theta}italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT represents the model\u2019s probability distribution. For new content (Kn,Vn)subscript\ud835\udc3ensubscript\ud835\udc49n(K_{\\text{n}},V_{\\text{n}})( italic_K start_POSTSUBSCRIPT n end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT n end_POSTSUBSCRIPT ) , update the memory state as: si+1={\u2133\u2062(Kn,Vn;\u2130\u2062(tn))if \u2062|si|<capacity,Mu\u2062(si,Kn,Vn,\u03b1n)otherwise,subscript\ud835\udc60\ud835\udc561cases\u2133subscript\ud835\udc3ensubscript\ud835\udc49n\u2130subscript\ud835\udc61nif subscript\ud835\udc60\ud835\udc56capacitysubscript\ud835\udc40usubscript\ud835\udc60\ud835\udc56subscript\ud835\udc3ensubscript\ud835\udc49nsubscript\ud835\udefcnotherwises_{i+1}=\\begin{cases}\\mathcal{M}(K_{\\text{n}},V_{\\text{n}};\\mathcal{E}(t_{% \\text{n}}))&\\text{if }|s_{i}|<\\text{capacity},\\\\ M_{\\text{u}}(s_{i},K_{\\text{n}},V_{\\text{n}},\\alpha_{\\text{n}})&\\text{% otherwise},\\end{cases}italic_s start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = { start_ROW start_CELL caligraphic_M ( italic_K start_POSTSUBSCRIPT n end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT n end_POSTSUBSCRIPT ; caligraphic_E ( italic_t start_POSTSUBSCRIPT n end_POSTSUBSCRIPT ) ) end_CELL start_CELL if | italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | < capacity , end_CELL end_ROW start_ROW start_CELL italic_M start_POSTSUBSCRIPT u end_POSTSUBSCRIPT ( italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_K start_POSTSUBSCRIPT n end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT n end_POSTSUBSCRIPT , italic_\u03b1 start_POSTSUBSCRIPT n end_POSTSUBSCRIPT ) end_CELL start_CELL otherwise , end_CELL end_ROW where Musubscript\ud835\udc40uM_{\\text{u}}italic_M start_POSTSUBSCRIPT u end_POSTSUBSCRIPT prunes the least relevant entries based on historical scores, specifically by ranking the scores and pruning those with the lowest values relative to the current context. We now develop the Relevance Scoring with Adaptive Retrieval (RSAR). This approach dynamically ranks memory entries based on their importance to the current query, significantly improving the retrieval process. The relevance score, \u03b1\u2062(\ud835\udc2a,\ud835\udc0a)\ud835\udefc\ud835\udc2a\ud835\udc0a\\alpha(\\mathbf{q},\\mathbf{K})italic_\u03b1 ( bold_q , bold_K ), as defined in equation 1, is used to rank memory entries. RSAR enhances the memory module by introducing ranked key-value entries, represented as (Kj,Vj,sj)subscript\ud835\udc3e\ud835\udc57subscript\ud835\udc49\ud835\udc57subscript\ud835\udc60\ud835\udc57(K_{j},V_{j},s_{j})( italic_K start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) , where sjsubscript\ud835\udc60\ud835\udc57s_{j}italic_s start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT denotes the relevance score for each entry. These scores enable the system to prioritize the most relevant information during retrieval while maintaining computational efficiency. To ensure optimal memory utilization, a pruning strategy is applied to remove less relevant entries. Specifically, entries with scores below a predefined threshold are discarded, preserving only the most critical context. The enhanced retrieval mechanism is expressed as: RRSAR\u2062(tq,s)=TopK\u2062{sim\u2062(E\u2062(tq),e)\u22c5maxj\u2061sj\u2223e\u2208s}subscript\ud835\udc45RSARsubscript\ud835\udc61\ud835\udc5e\ud835\udc60TopKconditional-set\u22c5sim\ud835\udc38subscript\ud835\udc61\ud835\udc5e\ud835\udc52subscript\ud835\udc57subscript\ud835\udc60\ud835\udc57\ud835\udc52\ud835\udc60R_{\\text{RSAR}}(t_{q},s)=\\text{TopK}\\left\\{\\text{sim}(E(t_{q}),e)\\cdot\\max_{j}% s_{j}\\mid e\\in s\\right\\}italic_R start_POSTSUBSCRIPT RSAR end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_s ) = TopK { sim ( italic_E ( italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) , italic_e ) \u22c5 roman_max start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT \u2223 italic_e \u2208 italic_s } , where E\u2062(tq)\ud835\udc38subscript\ud835\udc61\ud835\udc5eE(t_{q})italic_E ( italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) represents the encoded query, and the operation identifies the top-K\ud835\udc3eKitalic_K relevant entries based on their scores. This mechanism efficiently retrieves the most relevant information, even for extended contexts. We fine-tuned ERMAR on the SlimPajama dataset Fu et al. (2024), a high-quality, deduplicated corpus designed for long-context tasks. It contains 84.7K training rows, making it a compact yet effective resource for pre-training and fine-tuning. Performance was measured across context lengths from 1024 to 32768 tokens, using perplexity on the last 2048 tokens Yen et al. (2024). We fine-tuned OpenLLaMA-3B, a pre-trained LLM with rotational position encoding Su et al. (2024), using LoRA Hu et al. (2021) for efficiency. The model has L=26\ud835\udc3f26L=26italic_L = 26 layers, H=32\ud835\udc3b32H=32italic_H = 32 attention heads, and d=100\ud835\udc51100d=100italic_d = 100 dimensions. The 13th layer serves as the memory layer, while layers [14, 18, 22, 26] are used for retrieval augmentation. ERMAR was evaluated against state-of-the-art 7B and 3B parameter models. The 7B models include LLaMA-2-7B Touvron et al. (2023b) as a transformer baseline, LongLoRA-7B-32k Chen et al. (2023) with sparse attention for 32k-token contexts, and YARN-128k-7B Peng et al. (2023) with dynamic position embeddings for 128k tokens. The 3B models include OpenLLaMA-3B Touvron et al. (2023a), LongLLaMA-3B Tworkowski et al. (2024) (evaluated in two retrieval configurations), and Phi3-128k Abdin et al. (2024), which performs well across varying context lengths. This diverse benchmark suite ensures a robust evaluation of ERMAR\u2019s long-context capabilities. Following the experimental strategy adopted in Liu et al. (2024), Table 1 presents the mean perplexity scores of our model across different sequence lengths and datasets, demonstrating its effectiveness in long-context modeling. Evaluation was performed on test splits of three datasets: WikiText-103 Merity et al. (2016) (4,358 rows), PG-19 Rae et al. (2019) (100 rows), and Proof-Pile Azerbayev et al. (2023) (46.3k rows). Among 7B models, YARN-128k-7B excels in shorter contexts, while LongLoRA-7B-32k scales effectively to 16k-token sequences, though with some performance degradation. This highlights the trade-off between performance and scalability, guiding model selection based on use-case needs. The 3B models demonstrate ERMAR\u2019s significant advantages in long-context tasks. While OpenLLaMA-3B struggles beyond 4k tokens, and Phi3-128k shows more consistent performance, ERMAR sets new performance benchmarks. ERMAR outperforms MemLong and larger 7B models in several configurations, including achieving a remarkable 2.98 perplexity on Proof-pile, surpassing MemLong\u2019s 3.16. Despite slight underperformance in specific Proof-pile configurations, ERMAR\u2019s enhanced memory retrieval mechanism proves more effective overall. Additionally, ERMAR maintains stable performance up to 16k tokens, with only a 3.2% degradation in perplexity from 4k to 16k tokens, demonstrating its superior scalability in long-context modelling. The results in Table 2 show ERMAR\u2019s strong performance across five natural language understanding tasks in both 4-shot and 20-shot settings. In the 4-shot setting, ERMAR achieves state-of-the-art results across all tasks, outperforming OpenLLaMA and other memory-augmented models. It excels even in challenging tasks like SST-5 and MPQA, maintaining high performance with limited examples. Its stability across different memory configurations highlights its robustness in low-resource scenarios. ERMAR continues to excel in the 20-shot scenario, achieving top results in tasks like MPQA and Subj, and setting a new benchmark for SST-5. While it lags behind MemLong in a few tasks, ERMAR outperforms it overall, showcasing its scalability with increased examples. ERMAR consistently performs well across varying context lengths, effectively leveraging memory augmentation. Its ability to scale with more examples and handle both short and long-range dependencies makes it a strong candidate for general-purpose language modelling, advancing the state-of-the-art in language understanding tasks. We presented a novel ERMAR framework that enhances long-context modelling through relevance scoring and adaptive memory retrieval. ERMAR outperforms baseline models, including OpenLLaMA, LongLLaMA, and MemLong, achieving superior perplexity and in-context learning performance on WikiText-103, PG19, and Proof-pile. Future work will focus on optimizing ERMAR for specialized datasets and expanding its applicability to complex reasoning tasks. While ERMAR improves retrieval efficiency and context retention, it has limitations. Its reliance on ranked memory structures increases computational overhead compared to standard LLMs, particularly for large-scale retrieval. Additionally, performance variations across different task domains indicate a need for further tuning. The framework\u2019s effectiveness in real-world, noisy environments also requires further validation."
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Jameel,+S",
    "title": "Showing 1\u201332 of 32 results for author: Jameel, S",
    "author": "Authors:\nGhadir Alselwi, \n      \n      Hao Xue, \n      \n      Shoaib Jameel, \n      \n      Basem Suleiman, \n      \n      Flora D. Salim, \n      \n      Imran Razzak",
    "body": "arXiv:2503.14800 [pdf, other] Long Context Modeling with Ranked Memory-Augmented Retrieval Authors: Ghadir Alselwi, Hao Xue, Shoaib Jameel, Basem Suleiman, Flora D. Salim, Imran Razzak Abstract: Effective long-term memory management is crucial for language models handling extended contexts. We introduce a novel framework that dynamically ranks memory entries based on relevance. Unlike previous works, our model introduces a novel relevance scoring and a pointwise re-ranking model for key-value embeddings, inspired by learning-to-rank techniques in information retrieval. Enhanced Ranked Mem\u2026 \u25bd More Effective long-term memory management is crucial for language models handling extended contexts. We introduce a novel framework that dynamically ranks memory entries based on relevance. Unlike previous works, our model introduces a novel relevance scoring and a pointwise re-ranking model for key-value embeddings, inspired by learning-to-rank techniques in information retrieval. Enhanced Ranked Memory Augmented Retrieval ERMAR achieves state-of-the-art results on standard benchmarks. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. arXiv:2502.14068 [pdf, other] A Racing Dataset and Baseline Model for Track Detection in Autonomous Racing Authors: Shreya Ghosh, Yi-Huan Chen, Ching-Hsiang Huang, Abu Shafin Mohammad Mahdee Jameel, Chien Chou Ho, Aly El Gamal, Samuel Labi Abstract: A significant challenge in racing-related research is the lack of publicly available datasets containing raw images with corresponding annotations for the downstream task. In this paper, we introduce RoRaTrack, a novel dataset that contains annotated multi-camera image data from racing scenarios for track detection. The data is collected on a Dallara AV-21 at a racing circuit in Indiana, in collab\u2026 \u25bd More A significant challenge in racing-related research is the lack of publicly available datasets containing raw images with corresponding annotations for the downstream task. In this paper, we introduce RoRaTrack, a novel dataset that contains annotated multi-camera image data from racing scenarios for track detection. The data is collected on a Dallara AV-21 at a racing circuit in Indiana, in collaboration with the Indy Autonomous Challenge (IAC). RoRaTrack addresses common problems such as blurriness due to high speed, color inversion from the camera, and absence of lane markings on the track. Consequently, we propose RaceGAN, a baseline model based on a Generative Adversarial Network (GAN) that effectively addresses these challenges. The proposed model demonstrates superior performance compared to current state-of-the-art machine learning models in track detection. The dataset and code for this work are available at github.com/RaceGAN. \u25b3 Less Submitted 19 February, 2025; originally announced February 2025. Comments: Currently Under Review arXiv:2501.06827 [pdf, other] Leveraging Taxonomy and LLMs for Improved Multimodal Hierarchical Classification Authors: Shijing Chen, Mohamed Reda Bouadjenek, Shoaib Jameel, Usman Naseem, Basem Suleiman, Flora D. Salim, Hakim Hacid, Imran Razzak Abstract: Multi-level Hierarchical Classification (MLHC) tackles the challenge of categorizing items within a complex, multi-layered class structure. However, traditional MLHC classifiers often rely on a backbone model with independent output layers, which tend to ignore the hierarchical relationships between classes. This oversight can lead to inconsistent predictions that violate the underlying taxonomy.\u2026 \u25bd More Multi-level Hierarchical Classification (MLHC) tackles the challenge of categorizing items within a complex, multi-layered class structure. However, traditional MLHC classifiers often rely on a backbone model with independent output layers, which tend to ignore the hierarchical relationships between classes. This oversight can lead to inconsistent predictions that violate the underlying taxonomy. Leveraging Large Language Models (LLMs), we propose a novel taxonomy-embedded transitional LLM-agnostic framework for multimodality classification. The cornerstone of this advancement is the ability of models to enforce consistency across hierarchical levels. Our evaluations on the MEP-3M dataset - a multi-modal e-commerce product dataset with various hierarchical levels - demonstrated a significant performance improvement compared to conventional LLM structures. \u25b3 Less Submitted 12 January, 2025; originally announced January 2025. Comments: 11 pages, 7 figures, 2 tables, and accepted by COLING 2025 arXiv:2412.13732 [pdf, other] Modeling Multi-modal Cross-interaction for Multi-label Few-shot Image Classification Based on Local Feature Selection Authors: Kun Yan, Zied Bouraoui, Fangyun Wei, Chang Xu, Ping Wang, Shoaib Jameel, Steven Schockaert Abstract: The aim of multi-label few-shot image classification (ML-FSIC) is to assign semantic labels to images, in settings where only a small number of training examples are available for each label. A key feature of the multi-label setting is that an image often has several labels, which typically refer to objects appearing in different regions of the image. When estimating label prototypes, in a metric-\u2026 \u25bd More The aim of multi-label few-shot image classification (ML-FSIC) is to assign semantic labels to images, in settings where only a small number of training examples are available for each label. A key feature of the multi-label setting is that an image often has several labels, which typically refer to objects appearing in different regions of the image. When estimating label prototypes, in a metric-based setting, it is thus important to determine which regions are relevant for which labels, but the limited amount of training data and the noisy nature of local features make this highly challenging. As a solution, we propose a strategy in which label prototypes are gradually refined. First, we initialize the prototypes using word embeddings, which allows us to leverage prior knowledge about the meaning of the labels. Second, taking advantage of these initial prototypes, we then use a Loss Change Measurement (LCM) strategy to select the local features from the training images (i.e. the support set) that are most likely to be representative of a given label. Third, we construct the final prototype of the label by aggregating these representative local features using a multi-modal cross-interaction mechanism, which again relies on the initial word embedding-based prototypes. Experiments on COCO, PASCAL VOC, NUS-WIDE, and iMaterialist show that our model substantially improves the current state-of-the-art. \u25b3 Less Submitted 24 February, 2025; v1 submitted 18 December, 2024; originally announced December 2024. Comments: In Transactions on Multimedia Computing Communications and Applications. arXiv admin note: text overlap with arXiv:2112.01037 arXiv:2412.12164 [pdf, other] GAMED: Knowledge Adaptive Multi-Experts Decoupling for Multimodal Fake News Detection Authors: Lingzhi Shen, Yunfei Long, Xiaohao Cai, Imran Razzak, Guanming Chen, Kang Liu, Shoaib Jameel Abstract: Multimodal fake news detection often involves modelling heterogeneous data sources, such as vision and language. Existing detection methods typically rely on fusion effectiveness and cross-modal consistency to model the content, complicating understanding how each modality affects prediction accuracy. Additionally, these methods are primarily based on static feature modelling, making it difficult\u2026 \u25bd More Multimodal fake news detection often involves modelling heterogeneous data sources, such as vision and language. Existing detection methods typically rely on fusion effectiveness and cross-modal consistency to model the content, complicating understanding how each modality affects prediction accuracy. Additionally, these methods are primarily based on static feature modelling, making it difficult to adapt to the dynamic changes and relationships between different data modalities. This paper develops a significantly novel approach, GAMED, for multimodal modelling, which focuses on generating distinctive and discriminative features through modal decoupling to enhance cross-modal synergies, thereby optimizing overall performance in the detection process. GAMED leverages multiple parallel expert networks to refine features and pre-embed semantic knowledge to improve the experts' ability in information selection and viewpoint sharing. Subsequently, the feature distribution of each modality is adaptively adjusted based on the respective experts' opinions. GAMED also introduces a novel classification technique to dynamically manage contributions from different modalities, while improving the explainability of decisions. Experimental results on the Fakeddit and Yang datasets demonstrate that GAMED performs better than recently developed state-of-the-art models. The source code can be accessed at https://github.com/slz0925/GAMED. \u25b3 Less Submitted 2 March, 2025; v1 submitted 11 December, 2024; originally announced December 2024. arXiv:2407.17174 [pdf, other] NarrationDep: Narratives on Social Media For Automatic Depression Detection Authors: Hamad Zogan, Imran Razzak, Shoaib Jameel, Guandong Xu Abstract: Social media posts provide valuable insight into the narrative of users and their intentions, including providing an opportunity to automatically model whether a social media user is depressed or not. The challenge lies in faithfully modelling user narratives from their online social media posts, which could potentially be useful in several different applications. We have developed a novel and eff\u2026 \u25bd More Social media posts provide valuable insight into the narrative of users and their intentions, including providing an opportunity to automatically model whether a social media user is depressed or not. The challenge lies in faithfully modelling user narratives from their online social media posts, which could potentially be useful in several different applications. We have developed a novel and effective model called \\texttt{NarrationDep}, which focuses on detecting narratives associated with depression. By analyzing a user's tweets, \\texttt{NarrationDep} accurately identifies crucial narratives. \\texttt{NarrationDep} is a deep learning framework that jointly models individual user tweet representations and clusters of users' tweets. As a result, \\texttt{NarrationDep} is characterized by a novel two-layer deep learning model: the first layer models using social media text posts, and the second layer learns semantic representations of tweets associated with a cluster. To faithfully model these cluster representations, the second layer incorporates a novel component that hierarchically learns from users' posts. The results demonstrate that our framework outperforms other comparative models including recently developed models on a variety of datasets. \u25b3 Less Submitted 24 July, 2024; originally announced July 2024. arXiv:2404.10483 [pdf, other] From Uncertainty to Trust: Kernel Dropout for AI-Powered Medical Predictions Authors: Ubaid Azam, Imran Razzak, Shelly Vishwakarma, Hakim Hacid, Dell Zhang, Shoaib Jameel Abstract: AI-driven medical predictions with trustworthy confidence are essential for ensuring the responsible use of AI in healthcare applications. The growing capabilities of AI raise questions about their trustworthiness in healthcare, particularly due to opaque decision-making and limited data availability. This paper proposes a novel approach to address these challenges, introducing a Bayesian Monte Ca\u2026 \u25bd More AI-driven medical predictions with trustworthy confidence are essential for ensuring the responsible use of AI in healthcare applications. The growing capabilities of AI raise questions about their trustworthiness in healthcare, particularly due to opaque decision-making and limited data availability. This paper proposes a novel approach to address these challenges, introducing a Bayesian Monte Carlo Dropout model with kernel modelling. Our model is designed to enhance reliability on small medical datasets, a crucial barrier to the wider adoption of AI in healthcare. This model leverages existing language models for improved effectiveness and seamlessly integrates with current workflows. Extensive evaluations of public medical datasets showcase our model's superior performance across diverse tasks. We demonstrate significant improvements in reliability, even with limited data, offering a promising step towards building trust in AI-driven medical predictions and unlocking its potential to improve patient care. \u25b3 Less Submitted 2 February, 2025; v1 submitted 16 April, 2024; originally announced April 2024. arXiv:2404.10481 [pdf, other] BayesJudge: Bayesian Kernel Language Modelling with Confidence Uncertainty in Legal Judgment Prediction Authors: Ubaid Azam, Imran Razzak, Shelly Vishwakarma, Hakim Hacid, Dell Zhang, Shoaib Jameel Abstract: Predicting legal judgments with reliable confidence is paramount for responsible legal AI applications. While transformer-based deep neural networks (DNNs) like BERT have demonstrated promise in legal tasks, accurately assessing their prediction confidence remains crucial. We present a novel Bayesian approach called BayesJudge that harnesses the synergy between deep learning and deep Gaussian Proc\u2026 \u25bd More Predicting legal judgments with reliable confidence is paramount for responsible legal AI applications. While transformer-based deep neural networks (DNNs) like BERT have demonstrated promise in legal tasks, accurately assessing their prediction confidence remains crucial. We present a novel Bayesian approach called BayesJudge that harnesses the synergy between deep learning and deep Gaussian Processes to quantify uncertainty through Bayesian kernel Monte Carlo dropout. Our method leverages informative priors and flexible data modelling via kernels, surpassing existing methods in both predictive accuracy and confidence estimation as indicated through brier score. Extensive evaluations of public legal datasets showcase our model's superior performance across diverse tasks. We also introduce an optimal solution to automate the scrutiny of unreliable predictions, resulting in a significant increase in the accuracy of the model's predictions by up to 27\\%. By empowering judges and legal professionals with more reliable information, our work paves the way for trustworthy and transparent legal AI applications that facilitate informed decisions grounded in both knowledge and quantified uncertainty. \u25b3 Less Submitted 16 April, 2024; originally announced April 2024. arXiv:2403.11894 [pdf, other] From Explainable to Interpretable Deep Learning for Natural Language Processing in Healthcare: How Far from Reality? Authors: Guangming Huang, Yingya Li, Shoaib Jameel, Yunfei Long, Giorgos Papanastasiou Abstract: Deep learning (DL) has substantially enhanced natural language processing (NLP) in healthcare research. However, the increasing complexity of DL-based NLP necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review of explainable and interpretable DL in healthcare NLP. The term \"eXplainable and Interpretabl\u2026 \u25bd More Deep learning (DL) has substantially enhanced natural language processing (NLP) in healthcare research. However, the increasing complexity of DL-based NLP necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review of explainable and interpretable DL in healthcare NLP. The term \"eXplainable and Interpretable Artificial Intelligence\" (XIAI) is introduced to distinguish XAI from IAI. Different models are further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms are the most prevalent emerging IAI technique. The use of IAI is growing, distinguishing it from XAI. The major challenges identified are that most XIAI does not explore \"global\" modelling processes, the lack of best practices, and the lack of systematic evaluation and benchmarks. One important opportunity is to use attention mechanisms to enhance multi-modal XIAI for personalized medicine. Additionally, combining DL with causal logic holds promise. Our discussion encourages the integration of XIAI in Large Language Models (LLMs) and domain-specific smaller models. In conclusion, XIAI adoption in healthcare requires dedicated in-house expertise. Collaboration with domain experts, end-users, and policymakers can lead to ready-to-use XIAI methods across NLP and medical tasks. While challenges exist, XIAI techniques offer a valuable foundation for interpretable NLP algorithms in healthcare. \u25b3 Less Submitted 16 October, 2024; v1 submitted 18 March, 2024; originally announced March 2024. Comments: This paper has been accepted by Computational and Structural Biotechnology Journal arXiv:2401.17317 [pdf] Detection of Auditory Brainstem Response Peaks Using Image Processing Techniques in Infants with Normal Hearing Sensitivity Authors: Amir Majidpour, Samer Kais Jameel, Jafar Majidpour, Houra Bagheri, Tarik A. Rashid, Ahmadreza Nazeri, Mahshid Moheb Aleaba Abstract: Introduction: The auditory brainstem response (ABR) is measured to find the brainstem-level peripheral auditory nerve system integrity in children having normal hearing. The Auditory Evoked Potential (AEP) is generated using acoustic stimuli. Interpreting these waves requires competence to avoid misdiagnosing hearing problems. Automating ABR test labeling with computer vision may reduce human erro\u2026 \u25bd More Introduction: The auditory brainstem response (ABR) is measured to find the brainstem-level peripheral auditory nerve system integrity in children having normal hearing. The Auditory Evoked Potential (AEP) is generated using acoustic stimuli. Interpreting these waves requires competence to avoid misdiagnosing hearing problems. Automating ABR test labeling with computer vision may reduce human error. Method: The ABR test results of 26 children aged 1 to 20 months with normal hearing in both ears were used. A new approach is suggested for automatically calculating the peaks of waves of different intensities (in decibels). The procedure entails acquiring wave images from an Audera device using the Color Thresholder method, segmenting each wave as a single wave image using the Image Region Analyzer application, converting all wave images into waves using Image Processing (IP) techniques, and finally calculating the latency of the peaks for each wave to be used by an audiologist for diagnosing the disease. Findings: Image processing techniques were able to detect 1, 3, and 5 waves in the diagnosis field with accuracy (0.82), (0.98), and (0.98), respectively, and its precision for waves 1, 3, and 5, were respectively (0.32), (0.97) and (0.87). This evaluation also worked well in the thresholding part and 82.7 % correctly detected the ABR waves. Conclusion: Our findings indicate that the audiology test battery suite can be made more accurate, quick, and error-free by using technology to automatically detect and label ABR waves. \u25b3 Less Submitted 21 January, 2024; originally announced January 2024. arXiv:2401.05436 [pdf, other] Deep OFDM Channel Estimation: Capturing Frequency Recurrence Authors: Abu Shafin Mohammad Mahdee Jameel, Akshay Malhotra, Aly El Gamal, Shahab Hamidi-Rad Abstract: In this paper, we propose a deep-learning-based channel estimation scheme in an orthogonal frequency division multiplexing (OFDM) system. Our proposed method, named Single Slot Recurrence Along Frequency Network (SisRafNet), is based on a novel study of recurrent models for exploiting sequential behavior of channels across frequencies. Utilizing the fact that wireless channels have a high degree o\u2026 \u25bd More In this paper, we propose a deep-learning-based channel estimation scheme in an orthogonal frequency division multiplexing (OFDM) system. Our proposed method, named Single Slot Recurrence Along Frequency Network (SisRafNet), is based on a novel study of recurrent models for exploiting sequential behavior of channels across frequencies. Utilizing the fact that wireless channels have a high degree of correlation across frequencies, we employ recurrent neural network techniques within a single OFDM slot, thus overcoming the latency and memory constraints typically associated with recurrence based methods. The proposed SisRafNet delivers superior estimation performance compared to existing deep-learning-based channel estimation techniques and the performance has been validated on a wide range of 3rd Generation Partnership Project (3GPP) compliant channel scenarios at multiple signal-to-noise ratios. \u25b3 Less Submitted 7 January, 2024; originally announced January 2024. Comments: Accepted for publication at IEEE Communications Letters arXiv:2401.04025 [pdf, other] IDoFew: Intermediate Training Using Dual-Clustering in Language Models for Few Labels Text Classification Authors: Abdullah Alsuhaibani, Hamad Zogan, Imran Razzak, Shoaib Jameel, Guandong Xu Abstract: Language models such as Bidirectional Encoder Representations from Transformers (BERT) have been very effective in various Natural Language Processing (NLP) and text mining tasks including text classification. However, some tasks still pose challenges for these models, including text classification with limited labels. This can result in a cold-start problem. Although some approaches have attempte\u2026 \u25bd More Language models such as Bidirectional Encoder Representations from Transformers (BERT) have been very effective in various Natural Language Processing (NLP) and text mining tasks including text classification. However, some tasks still pose challenges for these models, including text classification with limited labels. This can result in a cold-start problem. Although some approaches have attempted to address this problem through single-stage clustering as an intermediate training step coupled with a pre-trained language model, which generates pseudo-labels to improve classification, these methods are often error-prone due to the limitations of the clustering algorithms. To overcome this, we have developed a novel two-stage intermediate clustering with subsequent fine-tuning that models the pseudo-labels reliably, resulting in reduced prediction errors. The key novelty in our model, IDoFew, is that the two-stage clustering coupled with two different clustering algorithms helps exploit the advantages of the complementary algorithms that reduce the errors in generating reliable pseudo-labels for fine-tuning. Our approach has shown significant improvements compared to strong comparative models. \u25b3 Less Submitted 8 January, 2024; originally announced January 2024. Comments: Published in The 17th ACM International Conference on Web Search and Data Mining arXiv:2401.03560 [pdf, other] Improving Transferability of Network Intrusion Detection in a Federated Learning Setup Authors: Shreya Ghosh, Abu Shafin Mohammad Mahdee Jameel, Aly El Gamal Abstract: Network Intrusion Detection Systems (IDS) aim to detect the presence of an intruder by analyzing network packets arriving at an internet connected device. Data-driven deep learning systems, popular due to their superior performance compared to traditional IDS, depend on availability of high quality training data for diverse intrusion classes. A way to overcome this limitation is through transferab\u2026 \u25bd More Network Intrusion Detection Systems (IDS) aim to detect the presence of an intruder by analyzing network packets arriving at an internet connected device. Data-driven deep learning systems, popular due to their superior performance compared to traditional IDS, depend on availability of high quality training data for diverse intrusion classes. A way to overcome this limitation is through transferable learning, where training for one intrusion class can lead to detection of unseen intrusion classes after deployment. In this paper, we provide a detailed study on the transferability of intrusion detection. We investigate practical federated learning configurations to enhance the transferability of intrusion detection. We propose two techniques to significantly improve the transferability of a federated intrusion detection system. The code for this work can be found at https://github.com/ghosh64/transferability. \u25b3 Less Submitted 7 January, 2024; originally announced January 2024. Comments: This manuscript has been accepted for publication in ICMLCN 2024 arXiv:2401.03488 [pdf, other] Data-Driven Subsampling in the Presence of an Adversarial Actor Authors: Abu Shafin Mohammad Mahdee Jameel, Ahmed P. Mohamed, Jinho Yi, Aly El Gamal, Akshay Malhotra Abstract: Deep learning based automatic modulation classification (AMC) has received significant attention owing to its potential applications in both military and civilian use cases. Recently, data-driven subsampling techniques have been utilized to overcome the challenges associated with computational complexity and training time for AMC. Beyond these direct advantages of data-driven subsampling, these me\u2026 \u25bd More Deep learning based automatic modulation classification (AMC) has received significant attention owing to its potential applications in both military and civilian use cases. Recently, data-driven subsampling techniques have been utilized to overcome the challenges associated with computational complexity and training time for AMC. Beyond these direct advantages of data-driven subsampling, these methods also have regularizing properties that may improve the adversarial robustness of the modulation classifier. In this paper, we investigate the effects of an adversarial attack on an AMC system that employs deep learning models both for AMC and for subsampling. Our analysis shows that subsampling itself is an effective deterrent to adversarial attacks. We also uncover the most efficient subsampling strategy when an adversarial attack on both the classifier and the subsampler is anticipated. \u25b3 Less Submitted 7 January, 2024; originally announced January 2024. Comments: Accepted for publication at ICMLCN 2024 arXiv:2312.11550 [pdf, other] A Study on Transferability of Deep Learning Models for Network Intrusion Detection Authors: Shreya Ghosh, Abu Shafin Mohammad Mahdee Jameel, Aly El Gamal Abstract: In this paper, we explore transferability in learning between different attack classes in a network intrusion detection setup. We evaluate transferability of attack classes by training a deep learning model with a specific attack class and testing it on a separate attack class. We observe the effects of real and synthetically generated data augmentation techniques on transferability. We investigat\u2026 \u25bd More In this paper, we explore transferability in learning between different attack classes in a network intrusion detection setup. We evaluate transferability of attack classes by training a deep learning model with a specific attack class and testing it on a separate attack class. We observe the effects of real and synthetically generated data augmentation techniques on transferability. We investigate the nature of observed transferability relationships, which can be either symmetric or asymmetric. We also examine explainability of the transferability relationships using the recursive feature elimination algorithm. We study data preprocessing techniques to boost model performance. The code for this work can be found at https://github.com/ghosh64/transferability. \u25b3 Less Submitted 17 December, 2023; originally announced December 2023. Comments: A significantly revised version of this manuscript has been accepted for publication. This is a previous version of the manuscript containing results and discussions that could not be included in the accepted version arXiv:2301.11871 [pdf] Exploiting the Generative Adversarial Network Approach to Create a Synthetic Topography Corneal Image Authors: Samer Kais Jameel, Sezgin Aydin, Nebras H. Ghaeb, Jafar Majidpour, Tarik A. Rashid, Sinan Q. Salih, P. S. JosephNg Abstract: Corneal diseases are the most common eye disorders. Deep learning techniques are used to per-form automated diagnoses of cornea. Deep learning networks require large-scale annotated datasets, which is conceded as a weakness of deep learning. In this work, a method for synthesizing medical images using conditional generative adversarial networks (CGANs), is presented. It also illustrates how produc\u2026 \u25bd More Corneal diseases are the most common eye disorders. Deep learning techniques are used to per-form automated diagnoses of cornea. Deep learning networks require large-scale annotated datasets, which is conceded as a weakness of deep learning. In this work, a method for synthesizing medical images using conditional generative adversarial networks (CGANs), is presented. It also illustrates how produced medical images may be utilized to enrich medical data, improve clinical decisions, and boost the performance of the conventional neural network (CNN) for medical image diagnosis. The study includes using corneal topography captured using a Pentacam device from patients with corneal diseases. The dataset contained 3448 different corneal images. Furthermore, it shows how an unbalanced dataset affects the performance of classifiers, where the data are balanced using the resampling approach. Finally, the results obtained from CNN networks trained on the balanced dataset are compared to those obtained from CNN networks trained on the imbalanced dataset. For performance, the system estimated the diagnosis accuracy, precision, and F1-score metrics. Lastly, some generated images were shown to an expert for evaluation and to see how well experts could identify the type of image and its condition. The expert recognized the image as useful for medical diagnosis and for determining the severity class according to the shape and values, by generating images based on real cases that could be used as new different stages of illness between healthy and unhealthy patients. \u25b3 Less Submitted 25 December, 2022; originally announced January 2023. Comments: 13 pages Journal ref: Biomolecules, 2022 arXiv:2301.04339 [pdf, other] Topics in Contextualised Attention Embeddings Authors: Mozhgan Talebpour, Alba Garcia Seco de Herrera, Shoaib Jameel Abstract: Contextualised word vectors obtained via pre-trained language models encode a variety of knowledge that has already been exploited in applications. Complementary to these language models are probabilistic topic models that learn thematic patterns from the text. Recent work has demonstrated that conducting clustering on the word-level contextual representations from a language model emulates word c\u2026 \u25bd More Contextualised word vectors obtained via pre-trained language models encode a variety of knowledge that has already been exploited in applications. Complementary to these language models are probabilistic topic models that learn thematic patterns from the text. Recent work has demonstrated that conducting clustering on the word-level contextual representations from a language model emulates word clusters that are discovered in latent topics of words from Latent Dirichlet Allocation. The important question is how such topical word clusters are automatically formed, through clustering, in the language model when it has not been explicitly designed to model latent topics. To address this question, we design different probe experiments. Using BERT and DistilBERT, we find that the attention framework plays a key role in modelling such word topic clusters. We strongly believe that our work paves way for further research into the relationships between probabilistic topic models and pre-trained language models. \u25b3 Less Submitted 11 January, 2023; originally announced January 2023. Comments: Accepted at the 45th European Conference on Information Retrieval (ECIR) 2023 arXiv:2201.05651 [pdf, other] CLUE: Contextualised Unified Explainable Learning of User Engagement in Video Lectures Authors: Sujit Roy, Gnaneswara Rao Gorle, Vishal Gaur, Haider Raza, Shoaib Jameel Abstract: Predicting contextualised engagement in videos is a long-standing problem that has been popularly attempted by exploiting the number of views or the associated likes using different computational methods. The recent decade has seen a boom in online learning resources, and during the pandemic, there has been an exponential rise of online teaching videos without much quality control. The quality of\u2026 \u25bd More Predicting contextualised engagement in videos is a long-standing problem that has been popularly attempted by exploiting the number of views or the associated likes using different computational methods. The recent decade has seen a boom in online learning resources, and during the pandemic, there has been an exponential rise of online teaching videos without much quality control. The quality of the content could be improved if the creators could get constructive feedback on their content. Employing an army of domain expert volunteers to provide feedback on the videos might not scale. As a result, there has been a steep rise in developing computational methods to predict a user engagement score that is indicative of some form of possible user engagement, i.e., to what level a user would tend to engage with the content. A drawback in current methods is that they model various features separately, in a cascaded approach, that is prone to error propagation. Besides, most of them do not provide crucial explanations on how the creator could improve their content. In this paper, we have proposed a new unified model, CLUE for the educational domain, which learns from the features extracted from freely available public online teaching videos and provides explainable feedback on the video along with a user engagement score. Given the complexity of the task, our unified framework employs different pre-trained models working together as an ensemble of classifiers. Our model exploits various multi-modal features to model the complexity of language, context agnostic information, textual emotion of the delivered content, animation, speaker's pitch and speech emotions. Under a transfer learning setup, the overall model, in the unified space, is fine-tuned for downstream applications. \u25b3 Less Submitted 14 January, 2022; originally announced January 2022. arXiv:2112.01037 [pdf, other] Inferring Prototypes for Multi-Label Few-Shot Image Classification with Word Vector Guided Attention Authors: Kun Yan, Chenbin Zhang, Jun Hou, Ping Wang, Zied Bouraoui, Shoaib Jameel, Steven Schockaert Abstract: Multi-label few-shot image classification (ML-FSIC) is the task of assigning descriptive labels to previously unseen images, based on a small number of training examples. A key feature of the multi-label setting is that images often have multiple labels, which typically refer to different regions of the image. When estimating prototypes, in a metric-based setting, it is thus important to determine\u2026 \u25bd More Multi-label few-shot image classification (ML-FSIC) is the task of assigning descriptive labels to previously unseen images, based on a small number of training examples. A key feature of the multi-label setting is that images often have multiple labels, which typically refer to different regions of the image. When estimating prototypes, in a metric-based setting, it is thus important to determine which regions are relevant for which labels, but the limited amount of training data makes this highly challenging. As a solution, in this paper we propose to use word embeddings as a form of prior knowledge about the meaning of the labels. In particular, visual prototypes are obtained by aggregating the local feature maps of the support images, using an attention mechanism that relies on the label embeddings. As an important advantage, our model can infer prototypes for unseen labels without the need for fine-tuning any model parameters, which demonstrates its strong generalization abilities. Experiments on COCO and PASCAL VOC furthermore show that our model substantially improves the current state-of-the-art. \u25b3 Less Submitted 7 December, 2021; v1 submitted 2 December, 2021; originally announced December 2021. Comments: Accepted by AAAI2022 arXiv:2105.10878 [pdf, other] DepressionNet: A Novel Summarization Boosted Deep Framework for Depression Detection on Social Media Authors: Hamad Zogan, Imran Razzak, Shoaib Jameel, Guandong Xu Abstract: Twitter is currently a popular online social media platform which allows users to share their user-generated content. This publicly-generated user data is also crucial to healthcare technologies because the discovered patterns would hugely benefit them in several ways. One of the applications is in automatically discovering mental health problems, e.g., depression. Previous studies to automaticall\u2026 \u25bd More Twitter is currently a popular online social media platform which allows users to share their user-generated content. This publicly-generated user data is also crucial to healthcare technologies because the discovered patterns would hugely benefit them in several ways. One of the applications is in automatically discovering mental health problems, e.g., depression. Previous studies to automatically detect a depressed user on online social media have largely relied upon the user behaviour and their linguistic patterns including user's social interactions. The downside is that these models are trained on several irrelevant content which might not be crucial towards detecting a depressed user. Besides, these content have a negative impact on the overall efficiency and effectiveness of the model. To overcome the shortcomings in the existing automatic depression detection methods, we propose a novel computational framework for automatic depression detection that initially selects relevant content through a hybrid extractive and abstractive summarization strategy on the sequence of all user tweets leading to a more fine-grained and relevant content. The content then goes to our novel deep learning framework comprising of a unified learning machinery comprising of Convolutional Neural Network (CNN) coupled with attention-enhanced Gated Recurrent Units (GRU) models leading to better empirical performance than existing strong baselines. \u25b3 Less Submitted 23 May, 2021; originally announced May 2021. arXiv:2105.10195 [pdf, other] Aligning Visual Prototypes with BERT Embeddings for Few-Shot Learning Authors: Kun Yan, Zied Bouraoui, Ping Wang, Shoaib Jameel, Steven Schockaert Abstract: Few-shot learning (FSL) is the task of learning to recognize previously unseen categories of images from a small number of training examples. This is a challenging task, as the available examples may not be enough to unambiguously determine which visual features are most characteristic of the considered categories. To alleviate this issue, we propose a method that additionally takes into account t\u2026 \u25bd More Few-shot learning (FSL) is the task of learning to recognize previously unseen categories of images from a small number of training examples. This is a challenging task, as the available examples may not be enough to unambiguously determine which visual features are most characteristic of the considered categories. To alleviate this issue, we propose a method that additionally takes into account the names of the image classes. While the use of class names has already been explored in previous work, our approach differs in two key aspects. First, while previous work has aimed to directly predict visual prototypes from word embeddings, we found that better results can be obtained by treating visual and text-based prototypes separately. Second, we propose a simple strategy for learning class name embeddings using the BERT language model, which we found to substantially outperform the GloVe vectors that were used in previous work. We furthermore propose a strategy for dealing with the high dimensionality of these vectors, inspired by models for aligning cross-lingual word embeddings. We provide experiments on miniImageNet, CUB and tieredImageNet, showing that our approach consistently improves the state-of-the-art in metric-based FSL. \u25b3 Less Submitted 21 May, 2021; originally announced May 2021. Comments: Accepted by ICMR2021 arXiv:2104.06374 [pdf, other] Knowledge Distillation For Wireless Edge Learning Authors: Ahmed P. Mohamed, Abu Shafin Mohammad Mahdee Jameel, Aly El Gamal Abstract: In this paper, we propose a framework for predicting frame errors in the collaborative spectrally congested wireless environments of the DARPA Spectrum Collaboration Challenge (SC2) via a recently collected dataset. We employ distributed deep edge learning that is shared among edge nodes and a central cloud. Using this close-to-practice dataset, we find that widely used federated learning approach\u2026 \u25bd More In this paper, we propose a framework for predicting frame errors in the collaborative spectrally congested wireless environments of the DARPA Spectrum Collaboration Challenge (SC2) via a recently collected dataset. We employ distributed deep edge learning that is shared among edge nodes and a central cloud. Using this close-to-practice dataset, we find that widely used federated learning approaches, specially those that are privacy preserving, are worse than local training for a wide range of settings. We hence utilize the synthetic minority oversampling technique to maintain privacy via avoiding the transfer of local data to the cloud, and utilize knowledge distillation with an aim to benefit from high cloud computing and storage capabilities. The proposed framework achieves overall better performance than both local and federated training approaches, while being robust against catastrophic failures as well as challenging channel conditions that result in high frame error rates. \u25b3 Less Submitted 3 April, 2021; originally announced April 2021. Comments: 5 pages, 6 figures arXiv:2102.00801 [pdf, ps, other] Few-shot Image Classification with Multi-Facet Prototypes Authors: Kun Yan, Zied Bouraoui, Ping Wang, Shoaib Jameel, Steven Schockaert Abstract: The aim of few-shot learning (FSL) is to learn how to recognize image categories from a small number of training examples. A central challenge is that the available training examples are normally insufficient to determine which visual features are most characteristic of the considered categories. To address this challenge, we organize these visual features into facets, which intuitively group feat\u2026 \u25bd More The aim of few-shot learning (FSL) is to learn how to recognize image categories from a small number of training examples. A central challenge is that the available training examples are normally insufficient to determine which visual features are most characteristic of the considered categories. To address this challenge, we organize these visual features into facets, which intuitively group features of the same kind (e.g. features that are relevant to shape, color, or texture). This is motivated from the assumption that (i) the importance of each facet differs from category to category and (ii) it is possible to predict facet importance from a pre-trained embedding of the category names. In particular, we propose an adaptive similarity measure, relying on predicted facet importance weights for a given set of categories. This measure can be used in combination with a wide array of existing metric-based methods. Experiments on miniImageNet and CUB show that our approach improves the state-of-the-art in metric-based FSL. \u25b3 Less Submitted 1 February, 2021; originally announced February 2021. Comments: Accepted by ICASSP2021 arXiv:2101.11222 [pdf] Automatic image annotation base on Naive Bayes and Decision Tree classifiers using MPEG-7 Authors: Jafar Majidpour, Samer Kais Jameel Abstract: Recently it has become essential to search for and retrieve high-resolution and efficient images easily due to swift development of digital images, many present annotation algorithms facing a big challenge which is the variance for represent the image where high level represent image semantic and low level illustrate the features, this issue is known as semantic gab. This work has been used MPEG-7\u2026 \u25bd More Recently it has become essential to search for and retrieve high-resolution and efficient images easily due to swift development of digital images, many present annotation algorithms facing a big challenge which is the variance for represent the image where high level represent image semantic and low level illustrate the features, this issue is known as semantic gab. This work has been used MPEG-7 standard to extract the features from the images, where the color feature was extracted by using Scalable Color Descriptor (SCD) and Color Layout Descriptor (CLD), whereas the texture feature was extracted by employing Edge Histogram Descriptor (EHD), the CLD produced high dimensionality feature vector therefore it is reduced by Principal Component Analysis (PCA). The features that have extracted by these three descriptors could be passing to the classifiers (Naive Bayes and Decision Tree) for training. Finally, they annotated the query image. In this study TUDarmstadt image bank had been used. The results of tests and comparative performance evaluation indicated better precision and executing time of Naive Bayes classification in comparison with Decision Tree classification. \u25b3 Less Submitted 27 January, 2021; originally announced January 2021. Comments: 6 pages arXiv:2007.02847 [pdf, other] Explainable Depression Detection with Multi-Modalities Using a Hybrid Deep Learning Model on Social Media Authors: Hamad Zogan, Imran Razzak, Xianzhi Wang, Shoaib Jameel, Guandong Xu Abstract: Model interpretability has become important to engenders appropriate user trust by providing the insight into the model prediction. However, most of the existing machine learning methods provide no interpretability for depression prediction, hence their predictions are obscure to human. In this work, we propose interpretive Multi-Modal Depression Detection with Hierarchical Attention Network MDHAN\u2026 \u25bd More Model interpretability has become important to engenders appropriate user trust by providing the insight into the model prediction. However, most of the existing machine learning methods provide no interpretability for depression prediction, hence their predictions are obscure to human. In this work, we propose interpretive Multi-Modal Depression Detection with Hierarchical Attention Network MDHAN, for detection depressed users on social media and explain the model prediction. We have considered user posts along with Twitter-based multi-modal features, specifically, we encode user posts using two levels of attention mechanisms applied at the tweet-level and word-level, calculate each tweet and words' importance, and capture semantic sequence features from the user timelines (posts). Our experiments show that MDHAN outperforms several popular and robust baseline methods, demonstrating the effectiveness of combining deep learning with multi-modal features. We also show that our model helps improve predictive performance when detecting depression in users who are posting messages publicly on social media. MDHAN achieves excellent performance and ensures adequate evidence to explain the prediction. \u25b3 Less Submitted 28 April, 2021; v1 submitted 3 July, 2020; originally announced July 2020. Comments: 23 Pages arXiv:2007.02325 [pdf, other] Detecting Community Depression Dynamics Due to COVID-19 Pandemic in Australia Authors: Jianlong Zhou, Hamad Zogan, Shuiqiao Yang, Shoaib Jameel, Guandong Xu, Fang Chen Abstract: The recent COVID-19 pandemic has caused unprecedented impact across the globe. We have also witnessed millions of people with increased mental health issues, such as depression, stress, worry, fear, disgust, sadness, and anxiety, which have become one of the major public health concerns during this severe health crisis. For instance, depression is one of the most common mental health issues accord\u2026 \u25bd More The recent COVID-19 pandemic has caused unprecedented impact across the globe. We have also witnessed millions of people with increased mental health issues, such as depression, stress, worry, fear, disgust, sadness, and anxiety, which have become one of the major public health concerns during this severe health crisis. For instance, depression is one of the most common mental health issues according to the findings made by the World Health Organisation (WHO). Depression can cause serious emotional, behavioural and physical health problems with significant consequences, both personal and social costs included. This paper studies community depression dynamics due to COVID-19 pandemic through user-generated content on Twitter. A new approach based on multi-modal features from tweets and Term Frequency-Inverse Document Frequency (TF-IDF) is proposed to build depression classification models. Multi-modal features capture depression cues from emotion, topic and domain-specific perspectives. We study the problem using recently scraped tweets from Twitter users emanating from the state of New South Wales in Australia. Our novel classification model is capable of extracting depression polarities which may be affected by COVID-19 and related events during the COVID-19 period. The results found that people became more depressed after the outbreak of COVID-19. The measures implemented by the government such as the state lockdown also increased depression levels. Further analysis in the Local Government Area (LGA) level found that the community depression level was different across different LGAs. Such granular level analysis of depression dynamics not only can help authorities such as governmental departments to take corresponding actions more objectively in specific regions if necessary but also allows users to perceive the dynamics of depression over the time. \u25b3 Less Submitted 5 July, 2020; originally announced July 2020. arXiv:2005.01446 [pdf, other] Deep Learning for Frame Error Prediction using a DARPA Spectrum Collaboration Challenge (SC2) Dataset Authors: Abu Shafin Mohammad Mahdee Jameel, Ahmed P. Mohamed, Xiwen Zhang, Aly El Gamal Abstract: We demonstrate a first example for employing deep learning in predicting frame errors for a Collaborative Intelligent Radio Network (CIRN) using a dataset collected during participation in the final scrimmages of the DARPA SC2 challenge. Four scenarios are considered based on randomizing or fixing the strategy for bandwidth and channel allocation, and either training and testing with different lin\u2026 \u25bd More We demonstrate a first example for employing deep learning in predicting frame errors for a Collaborative Intelligent Radio Network (CIRN) using a dataset collected during participation in the final scrimmages of the DARPA SC2 challenge. Four scenarios are considered based on randomizing or fixing the strategy for bandwidth and channel allocation, and either training and testing with different links or using a pilot phase for each link to train the deep neural network. We also investigate the effect of latency constraints, and uncover interesting characteristics of the predictor over different Signal to Noise Ratio (SNR) ranges. The obtained insights open the door for implementing a deep-learning-based strategy that is scalable to large heterogeneous networks, generalizable to diverse wireless environments, and suitable for predicting frame error instances and rates within a congested shared spectrum. \u25b3 Less Submitted 24 December, 2020; v1 submitted 22 March, 2020; originally announced May 2020. Comments: 5 pages, 4 figures arXiv:1911.03059 [pdf, other] A Comprehensive Comparison of Machine Learning Based Methods Used in Bengali Question Classification Authors: Afra Anika, Md. Hasibur Rahman, Salekul Islam, Abu Shafin Mohammad Mahdee Jameel, Chowdhury Rafeed Rahman Abstract: QA classification system maps questions asked by humans to an appropriate answer category. A sound question classification (QC) system model is the pre-requisite of a sound QA system. This work demonstrates phases of assembling a QA type classification model. We present a comprehensive comparison (performance and computational complexity) among some machine learning based approaches used in QC for\u2026 \u25bd More QA classification system maps questions asked by humans to an appropriate answer category. A sound question classification (QC) system model is the pre-requisite of a sound QA system. This work demonstrates phases of assembling a QA type classification model. We present a comprehensive comparison (performance and computational complexity) among some machine learning based approaches used in QC for Bengali language. \u25b3 Less Submitted 19 November, 2019; v1 submitted 8 November, 2019; originally announced November 2019. arXiv:1711.05294 [pdf, other] Modeling Semantic Relatedness using Global Relation Vectors Authors: Shoaib Jameel, Zied Bouraoui, Steven Schockaert Abstract: Word embedding models such as GloVe rely on co-occurrence statistics from a large corpus to learn vector representations of word meaning. These vectors have proven to capture surprisingly fine-grained semantic and syntactic information. While we may similarly expect that co-occurrence statistics can be used to capture rich information about the relationships between different words, existing appro\u2026 \u25bd More Word embedding models such as GloVe rely on co-occurrence statistics from a large corpus to learn vector representations of word meaning. These vectors have proven to capture surprisingly fine-grained semantic and syntactic information. While we may similarly expect that co-occurrence statistics can be used to capture rich information about the relationships between different words, existing approaches for modeling such relationships have mostly relied on manipulating pre-trained word vectors. In this paper, we introduce a novel method which directly learns relation vectors from co-occurrence statistics. To this end, we first introduce a variant of GloVe, in which there is an explicit connection between word vectors and PMI weighted co-occurrence vectors. We then show how relation vectors can be naturally embedded into the resulting vector space. \u25b3 Less Submitted 14 November, 2017; originally announced November 2017. arXiv:1708.06266 [pdf, other] Probabilistic Relation Induction in Vector Space Embeddings Authors: Zied Bouraoui, Shoaib Jameel, Steven Schockaert Abstract: Word embeddings have been found to capture a surprisingly rich amount of syntactic and semantic knowledge. However, it is not yet sufficiently well-understood how the relational knowledge that is implicitly encoded in word embeddings can be extracted in a reliable way. In this paper, we propose two probabilistic models to address this issue. The first model is based on the common relations-as-tran\u2026 \u25bd More Word embeddings have been found to capture a surprisingly rich amount of syntactic and semantic knowledge. However, it is not yet sufficiently well-understood how the relational knowledge that is implicitly encoded in word embeddings can be extracted in a reliable way. In this paper, we propose two probabilistic models to address this issue. The first model is based on the common relations-as-translations view, but is cast in a probabilistic setting. Our second model is based on the much weaker assumption that there is a linear relationship between the vector representations of related words. Compared to existing approaches, our models lead to more accurate predictions, and they are more explicit about what can and cannot be extracted from the word embedding. \u25b3 Less Submitted 21 August, 2017; originally announced August 2017. arXiv:1706.07276 [pdf, other] Jointly Learning Word Embeddings and Latent Topics Authors: Bei Shi, Wai Lam, Shoaib Jameel, Steven Schockaert, Kwun Ping Lai Abstract: Word embedding models such as Skip-gram learn a vector-space representation for each word, based on the local word collocation patterns that are observed in a text corpus. Latent topic models, on the other hand, take a more global view, looking at the word distributions across the corpus to assign a topic to each word occurrence. These two paradigms are complementary in how they represent the mean\u2026 \u25bd More Word embedding models such as Skip-gram learn a vector-space representation for each word, based on the local word collocation patterns that are observed in a text corpus. Latent topic models, on the other hand, take a more global view, looking at the word distributions across the corpus to assign a topic to each word occurrence. These two paradigms are complementary in how they represent the meaning of word occurrences. While some previous works have already looked at using word embeddings for improving the quality of latent topics, and conversely, at using latent topics for improving word embeddings, such \"two-step\" methods cannot capture the mutual interaction between the two paradigms. In this paper, we propose STE, a framework which can learn word embeddings and latent topics in a unified manner. STE naturally obtains topic-specific word embeddings, and thus addresses the issue of polysemy. At the same time, it also learns the term distributions of the topics, and the topic distributions of the documents. Our experimental results demonstrate that the STE model can indeed generate useful topic-specific word embeddings and coherent latent topics in an effective and efficient way. \u25b3 Less Submitted 21 June, 2017; originally announced June 2017. Comments: 10 pagess, 2 figures, full paper. To appear in the proceedings of The 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '17) arXiv:1602.05765 [pdf, ps, other] Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning Authors: Shoaib Jameel, Steven Schockaert Abstract: Conceptual spaces are geometric representations of conceptual knowledge, in which entities correspond to points, natural properties correspond to convex regions, and the dimensions of the space correspond to salient features. While conceptual spaces enable elegant models of various cognitive phenomena, the lack of automated methods for constructing such representations have so far limited their ap\u2026 \u25bd More Conceptual spaces are geometric representations of conceptual knowledge, in which entities correspond to points, natural properties correspond to convex regions, and the dimensions of the space correspond to salient features. While conceptual spaces enable elegant models of various cognitive phenomena, the lack of automated methods for constructing such representations have so far limited their application in artificial intelligence. To address this issue, we propose a method which learns a vector-space embedding of entities from Wikipedia and constrains this embedding such that entities of the same semantic type are located in some lower-dimensional subspace. We experimentally demonstrate the usefulness of these subspaces as (approximate) conceptual space representations by showing, among others, that important features can be modelled as directions and that natural properties tend to correspond to convex regions. \u25b3 Less Submitted 25 October, 2017; v1 submitted 18 February, 2016; originally announced February 2016."
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Xue,+H",
    "title": "Showing 1\u201350 of 2,118 results for author: Xue, H",
    "author": "Authors:\nMatthew Low, \n      \n      Arian Prabowo, \n      \n      Hao Xue, \n      \n      Flora Salim",
    "body": "arXiv:2503.14980 [pdf, other] Embedding spatial context in urban traffic forecasting with contrastive pre-training Authors: Matthew Low, Arian Prabowo, Hao Xue, Flora Salim Abstract: Urban traffic forecasting is a commonly encountered problem, with wide-ranging applications in fields such as urban planning, civil engineering and transport. In this paper, we study the enhancement of traffic forecasting with pre-training, focusing on spatio-temporal graph methods. While various machine learning methods to solve traffic forecasting problems have been explored and extensively stud\u2026 \u25bd More Urban traffic forecasting is a commonly encountered problem, with wide-ranging applications in fields such as urban planning, civil engineering and transport. In this paper, we study the enhancement of traffic forecasting with pre-training, focusing on spatio-temporal graph methods. While various machine learning methods to solve traffic forecasting problems have been explored and extensively studied, there is a gap of a more contextual approach: studying how relevant non-traffic data can improve prediction performance on traffic forecasting problems. We call this data spatial context. We introduce a novel method of combining road and traffic information through the notion of a traffic quotient graph, a quotient graph formed from road geometry and traffic sensors. We also define a way to encode this relationship in the form of a geometric encoder, pre-trained using contrastive learning methods and enhanced with OpenStreetMap data. We introduce and discuss ways to integrate this geometric encoder with existing graph neural network (GNN)-based traffic forecasting models, using a contrastive pre-training paradigm. We demonstrate the potential for this hybrid model to improve generalisation and performance with zero additional traffic data. Code for this paper is available at https://github.com/mattchrlw/forecasting-on-new-roads. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. Comments: 21 pages with references, 10 figures arXiv:2503.14800 [pdf, other] Long Context Modeling with Ranked Memory-Augmented Retrieval Authors: Ghadir Alselwi, Hao Xue, Shoaib Jameel, Basem Suleiman, Flora D. Salim, Imran Razzak Abstract: Effective long-term memory management is crucial for language models handling extended contexts. We introduce a novel framework that dynamically ranks memory entries based on relevance. Unlike previous works, our model introduces a novel relevance scoring and a pointwise re-ranking model for key-value embeddings, inspired by learning-to-rank techniques in information retrieval. Enhanced Ranked Mem\u2026 \u25bd More Effective long-term memory management is crucial for language models handling extended contexts. We introduce a novel framework that dynamically ranks memory entries based on relevance. Unlike previous works, our model introduces a novel relevance scoring and a pointwise re-ranking model for key-value embeddings, inspired by learning-to-rank techniques in information retrieval. Enhanced Ranked Memory Augmented Retrieval ERMAR achieves state-of-the-art results on standard benchmarks. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. arXiv:2503.14295 [pdf, other] PC-Talk: Precise Facial Animation Control for Audio-Driven Talking Face Generation Authors: Baiqin Wang, Xiangyu Zhu, Fan Shen, Hao Xu, Zhen Lei Abstract: Recent advancements in audio-driven talking face generation have made great progress in lip synchronization. However, current methods often lack sufficient control over facial animation such as speaking style and emotional expression, resulting in uniform outputs. In this paper, we focus on improving two key factors: lip-audio alignment and emotion control, to enhance the diversity and user-friend\u2026 \u25bd More Recent advancements in audio-driven talking face generation have made great progress in lip synchronization. However, current methods often lack sufficient control over facial animation such as speaking style and emotional expression, resulting in uniform outputs. In this paper, we focus on improving two key factors: lip-audio alignment and emotion control, to enhance the diversity and user-friendliness of talking videos. Lip-audio alignment control focuses on elements like speaking style and the scale of lip movements, whereas emotion control is centered on generating realistic emotional expressions, allowing for modifications in multiple attributes such as intensity. To achieve precise control of facial animation, we propose a novel framework, PC-Talk, which enables lip-audio alignment and emotion control through implicit keypoint deformations. First, our lip-audio alignment control module facilitates precise editing of speaking styles at the word level and adjusts lip movement scales to simulate varying vocal loudness levels, maintaining lip synchronization with the audio. Second, our emotion control module generates vivid emotional facial features with pure emotional deformation. This module also enables the fine modification of intensity and the combination of multiple emotions across different facial regions. Our method demonstrates outstanding control capabilities and achieves state-of-the-art performance on both HDTF and MEAD datasets in extensive experiments. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. arXiv:2503.14234 [pdf, other] KG-IRAG: A Knowledge Graph-Based Iterative Retrieval-Augmented Generation Framework for Temporal Reasoning Authors: Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D. Salim Abstract: Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective in enhancing the performance of Large Language Models (LLMs) on tasks that require external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG improves information retrieval for complex reasoning tasks, providing more precise and comprehensive retrieval and generating more accurate responses to QAs. However, most RA\u2026 \u25bd More Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective in enhancing the performance of Large Language Models (LLMs) on tasks that require external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG improves information retrieval for complex reasoning tasks, providing more precise and comprehensive retrieval and generating more accurate responses to QAs. However, most RAG methods fall short in addressing multi-step reasoning, particularly when both information extraction and inference are necessary. To address this limitation, this paper presents Knowledge Graph-Based Iterative Retrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs with iterative reasoning to improve LLMs' ability to handle queries involving temporal and logical dependencies. Through iterative retrieval steps, KG-IRAG incrementally gathers relevant data from external KGs, enabling step-by-step reasoning. The proposed approach is particularly suited for scenarios where reasoning is required alongside dynamic temporal data extraction, such as determining optimal travel times based on weather conditions or traffic patterns. Experimental results show that KG-IRAG improves accuracy in complex reasoning tasks by effectively integrating external knowledge with iterative, logic-based retrieval. Additionally, three new datasets: weatherQA-Irish, weatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's performance, demonstrating its potential beyond traditional RAG applications. \u25b3 Less Submitted 19 March, 2025; v1 submitted 18 March, 2025; originally announced March 2025. Comments: 14 pages, 4 figures arXiv:2503.13560 [pdf, other] MSWAL: 3D Multi-class Segmentation of Whole Abdominal Lesions Dataset Authors: Zhaodong Wu, Qiaochu Zhao, Ming Hu, Yulong Li, Haochen Xue, Kang Dang, Zhengyong Jiang, Angelos Stefanidis, Qiufeng Wang, Imran Razzak, Zongyuan Ge, Junjun He, Yu Qiao, Zhong Zheng, Feilong Tang, Jionglong Su Abstract: With the significantly increasing incidence and prevalence of abdominal diseases, there is a need to embrace greater use of new innovations and technology for the diagnosis and treatment of patients. Although deep-learning methods have notably been developed to assist radiologists in diagnosing abdominal diseases, existing models have the restricted ability to segment common lesions in the abdomen\u2026 \u25bd More With the significantly increasing incidence and prevalence of abdominal diseases, there is a need to embrace greater use of new innovations and technology for the diagnosis and treatment of patients. Although deep-learning methods have notably been developed to assist radiologists in diagnosing abdominal diseases, existing models have the restricted ability to segment common lesions in the abdomen due to missing annotations for typical abdominal pathologies in their training datasets. To address the limitation, we introduce MSWAL, the first 3D Multi-class Segmentation of the Whole Abdominal Lesions dataset, which broadens the coverage of various common lesion types, such as gallstones, kidney stones, liver tumors, kidney tumors, pancreatic cancer, liver cysts, and kidney cysts. With CT scans collected from 694 patients (191,417 slices) of different genders across various scanning phases, MSWAL demonstrates strong robustness and generalizability. The transfer learning experiment from MSWAL to two public datasets, LiTS and KiTS, effectively demonstrates consistent improvements, with Dice Similarity Coefficient (DSC) increase of 3.00% for liver tumors and 0.89% for kidney tumors, demonstrating that the comprehensive annotations and diverse lesion types in MSWAL facilitate effective learning across different domains and data distributions. Furthermore, we propose Inception nnU-Net, a novel segmentation framework that effectively integrates an Inception module with the nnU-Net architecture to extract information from different receptive fields, achieving significant enhancement in both voxel-level DSC and region-level F1 compared to the cutting-edge public algorithms on MSWAL. Our dataset will be released after being accepted, and the code is publicly released at https://github.com/tiuxuxsh76075/MSWAL-. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. arXiv:2503.13303 [pdf, other] UniHOPE: A Unified Approach for Hand-Only and Hand-Object Pose Estimation Authors: Yinqiao Wang, Hao Xu, Pheng-Ann Heng, Chi-Wing Fu Abstract: Estimating the 3D pose of hand and potential hand-held object from monocular images is a longstanding challenge. Yet, existing methods are specialized, focusing on either bare-hand or hand interacting with object. No method can flexibly handle both scenarios and their performance degrades when applied to the other scenario. In this paper, we propose UniHOPE, a unified approach for general 3D hand-\u2026 \u25bd More Estimating the 3D pose of hand and potential hand-held object from monocular images is a longstanding challenge. Yet, existing methods are specialized, focusing on either bare-hand or hand interacting with object. No method can flexibly handle both scenarios and their performance degrades when applied to the other scenario. In this paper, we propose UniHOPE, a unified approach for general 3D hand-object pose estimation, flexibly adapting both scenarios. Technically, we design a grasp-aware feature fusion module to integrate hand-object features with an object switcher to dynamically control the hand-object pose estimation according to grasping status. Further, to uplift the robustness of hand pose estimation regardless of object presence, we generate realistic de-occluded image pairs to train the model to learn object-induced hand occlusions, and formulate multi-level feature enhancement techniques for learning occlusion-invariant features. Extensive experiments on three commonly-used benchmarks demonstrate UniHOPE's SOTA performance in addressing hand-only and hand-object scenarios. Code will be released on https://github.com/JoyboyWang/UniHOPE_Pytorch. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. Comments: 8 pages, 6 figures, 7 tables arXiv:2503.13217 [pdf, other] Dense Policy: Bidirectional Autoregressive Learning of Actions Authors: Yue Su, Xinyu Zhan, Hongjie Fang, Han Xue, Hao-Shu Fang, Yong-Lu Li, Cewu Lu, Lixin Yang Abstract: Mainstream visuomotor policies predominantly rely on generative models for holistic action prediction, while current autoregressive policies, predicting the next token or chunk, have shown suboptimal results. This motivates a search for more effective learning methods to unleash the potential of autoregressive policies for robotic manipulation. This paper introduces a bidirectionally expanded lear\u2026 \u25bd More Mainstream visuomotor policies predominantly rely on generative models for holistic action prediction, while current autoregressive policies, predicting the next token or chunk, have shown suboptimal results. This motivates a search for more effective learning methods to unleash the potential of autoregressive policies for robotic manipulation. This paper introduces a bidirectionally expanded learning approach, termed Dense Policy, to establish a new paradigm for autoregressive policies in action prediction. It employs a lightweight encoder-only architecture to iteratively unfold the action sequence from an initial single frame into the target sequence in a coarse-to-fine manner with logarithmic-time inference. Extensive experiments validate that our dense policy has superior autoregressive learning capabilities and can surpass existing holistic generative policies. Our policy, example data, and training code will be publicly available upon publication. Project page: https: //selen-suyue.github.io/DspNet/. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. arXiv:2503.12668 [pdf, other] ZO2: Scalable Zeroth-Order Fine-Tuning for Extremely Large Language Models with Limited GPU Memory Authors: Liangyu Wang, Jie Ren, Hang Xu, Junxiao Wang, Huanyi Xie, David E. Keyes, Di Wang Abstract: Fine-tuning large pre-trained LLMs generally demands extensive GPU memory. Traditional first-order optimizers like SGD encounter substantial difficulties due to increased memory requirements from storing activations and gradients during both the forward and backward phases as the model size expands. Alternatively, zeroth-order (ZO) techniques can compute gradients using just forward operations, el\u2026 \u25bd More Fine-tuning large pre-trained LLMs generally demands extensive GPU memory. Traditional first-order optimizers like SGD encounter substantial difficulties due to increased memory requirements from storing activations and gradients during both the forward and backward phases as the model size expands. Alternatively, zeroth-order (ZO) techniques can compute gradients using just forward operations, eliminating the need to store activations. Furthermore, by leveraging CPU capabilities, it's feasible to enhance both the memory and processing power available to a single GPU. We propose a novel framework, ZO2 (Zeroth-Order Offloading), for efficient zeroth-order fine-tuning of LLMs with only limited GPU memory. Our framework dynamically shifts model parameters between the CPU and GPU as required, optimizing computation flow and maximizing GPU usage by minimizing downtime. This integration of parameter adjustments with ZO's double forward operations reduces unnecessary data movement, enhancing the fine-tuning efficacy. Additionally, our framework supports an innovative low-bit precision approach in AMP mode to streamline data exchanges between the CPU and GPU. Employing this approach allows us to fine-tune extraordinarily large models, such as the OPT-175B with more than 175 billion parameters, on a mere 18GB GPU--achievements beyond the reach of traditional methods. Moreover, our framework achieves these results with almost no additional time overhead and absolutely no accuracy loss compared to standard zeroth-order methods. ZO2's code has been open-sourced in https://github.com/liangyuwang/zo2. \u25b3 Less Submitted 16 March, 2025; originally announced March 2025. Comments: 14 pages, 7 figures arXiv:2503.12047 [pdf, other] PSGait: Multimodal Gait Recognition using Parsing Skeleton Authors: Hangrui Xu, Chuanrui Zhang, Zhengxian Wu, Peng Jiao, Haoqian Wang Abstract: Gait recognition has emerged as a robust biometric modality due to its non-intrusive nature and resilience to occlusion. Conventional gait recognition methods typically rely on silhouettes or skeletons. Despite their success in gait recognition for controlled laboratory environments, they usually fail in real-world scenarios due to their limited information entropy for gait representations. To ach\u2026 \u25bd More Gait recognition has emerged as a robust biometric modality due to its non-intrusive nature and resilience to occlusion. Conventional gait recognition methods typically rely on silhouettes or skeletons. Despite their success in gait recognition for controlled laboratory environments, they usually fail in real-world scenarios due to their limited information entropy for gait representations. To achieve accurate gait recognition in the wild, we propose a novel gait representation, named Parsing Skeleton. This representation innovatively introduces the skeleton-guided human parsing method to capture fine-grained body dynamics, so they have much higher information entropy to encode the shapes and dynamics of fine-grained human parts during walking. Moreover, to effectively explore the capability of the parsing skeleton representation, we propose a novel parsing skeleton-based gait recognition framework, named PSGait, which takes parsing skeletons and silhouettes as input. By fusing these two modalities, the resulting image sequences are fed into gait recognition models for enhanced individual differentiation. We conduct comprehensive benchmarks on various datasets to evaluate our model. PSGait outperforms existing state-of-the-art multimodal methods. Furthermore, as a plug-and-play method, PSGait leads to a maximum improvement of 10.9% in Rank-1 accuracy across various gait recognition models. These results demonstrate the effectiveness and versatility of parsing skeletons for gait recognition in the wild, establishing PSGait as a new state-of-the-art approach for multimodal gait recognition. \u25b3 Less Submitted 15 March, 2025; originally announced March 2025. arXiv:2503.11381 [pdf, other] Modeling Subjectivity in Cognitive Appraisal with Language Models Authors: Yuxiang Zhou, Hainiu Xu, Desmond C. Ong, Petr Slovak, Yulan He Abstract: As the utilization of language models in interdisciplinary, human-centered studies grow, the expectation of model capabilities continues to evolve. Beyond excelling at conventional tasks, models are recently expected to perform well on user-centric measurements involving confidence and human (dis)agreement -- factors that reflect subjective preferences. While modeling of subjectivity plays an esse\u2026 \u25bd More As the utilization of language models in interdisciplinary, human-centered studies grow, the expectation of model capabilities continues to evolve. Beyond excelling at conventional tasks, models are recently expected to perform well on user-centric measurements involving confidence and human (dis)agreement -- factors that reflect subjective preferences. While modeling of subjectivity plays an essential role in cognitive science and has been extensively studied, it remains under-explored within the NLP community. In light of this gap, we explore how language models can harness subjectivity by conducting comprehensive experiments and analysis across various scenarios using both fine-tuned models and prompt-based large language models (LLMs). Our quantitative and qualitative experimental results indicate that existing post-hoc calibration approaches often fail to produce satisfactory results. However, our findings reveal that personality traits and demographical information are critical for measuring subjectivity. Furthermore, our in-depth analysis offers valuable insights for future research and development in the interdisciplinary studies of NLP and cognitive science. \u25b3 Less Submitted 14 March, 2025; originally announced March 2025. arXiv:2503.11324 [pdf, other] Safe-VAR: Safe Visual Autoregressive Model for Text-to-Image Generative Watermarking Authors: Ziyi Wang, Songbai Tan, Gang Xu, Xuerui Qiu, Hongbin Xu, Xin Meng, Ming Li, Fei Richard Yu Abstract: With the success of autoregressive learning in large language models, it has become a dominant approach for text-to-image generation, offering high efficiency and visual quality. However, invisible watermarking for visual autoregressive (VAR) models remains underexplored, despite its importance in misuse prevention. Existing watermarking methods, designed for diffusion models, often struggle to ad\u2026 \u25bd More With the success of autoregressive learning in large language models, it has become a dominant approach for text-to-image generation, offering high efficiency and visual quality. However, invisible watermarking for visual autoregressive (VAR) models remains underexplored, despite its importance in misuse prevention. Existing watermarking methods, designed for diffusion models, often struggle to adapt to the sequential nature of VAR models. To bridge this gap, we propose Safe-VAR, the first watermarking framework specifically designed for autoregressive text-to-image generation. Our study reveals that the timing of watermark injection significantly impacts generation quality, and watermarks of different complexities exhibit varying optimal injection times. Motivated by this observation, we propose an Adaptive Scale Interaction Module, which dynamically determines the optimal watermark embedding strategy based on the watermark information and the visual characteristics of the generated image. This ensures watermark robustness while minimizing its impact on image quality. Furthermore, we introduce a Cross-Scale Fusion mechanism, which integrates mixture of both heads and experts to effectively fuse multi-resolution features and handle complex interactions between image content and watermark patterns. Experimental results demonstrate that Safe-VAR achieves state-of-the-art performance, significantly surpassing existing counterparts regarding image quality, watermarking fidelity, and robustness against perturbations. Moreover, our method exhibits strong generalization to an out-of-domain watermark dataset QR Codes. \u25b3 Less Submitted 14 March, 2025; originally announced March 2025. arXiv:2503.11213 [pdf, other] Simulating Dual-Pixel Images From Ray Tracing For Depth Estimation Authors: Fengchen He, Dayang Zhao, Hao Xu, Tingwei Quan, Shaoqun Zeng Abstract: Many studies utilize dual-pixel (DP) sensor phase characteristics for various applications, such as depth estimation and deblurring. However, since the DP image features are entirely determined by the camera hardware, DP-depth paired datasets are very scarce, especially when performing depth estimation on customized cameras. To overcome this, studies simulate DP images using ideal optical system m\u2026 \u25bd More Many studies utilize dual-pixel (DP) sensor phase characteristics for various applications, such as depth estimation and deblurring. However, since the DP image features are entirely determined by the camera hardware, DP-depth paired datasets are very scarce, especially when performing depth estimation on customized cameras. To overcome this, studies simulate DP images using ideal optical system models. However, these simulations often violate real optical propagation laws,leading to poor generalization to real DP data. To address this, we investigate the domain gap between simulated and real DP data, and propose solutions using the Simulating DP images from ray tracing (Sdirt) scheme. The Sdirt generates realistic DP images via ray tracing and integrates them into the depth estimation training pipeline. Experimental results show that models trained with Sdirt-simulated images generalize better to real DP data. \u25b3 Less Submitted 14 March, 2025; originally announced March 2025. arXiv:2503.10992 [pdf, other] Rethinking Rotation-Invariant Recognition of Fine-grained Shapes from the Perspective of Contour Points Authors: Yanjie Xu, Handing Xu, Tianmu Wang, Yaguan Li, Yunzhi Chen, Zhenguo Nie Abstract: Rotation-invariant recognition of shapes is a common challenge in computer vision. Recent approaches have significantly improved the accuracy of rotation-invariant recognition by encoding the rotational invariance of shapes as hand-crafted image features and introducing deep neural networks. However, the methods based on pixels have too much redundant information, and the critical geometric inform\u2026 \u25bd More Rotation-invariant recognition of shapes is a common challenge in computer vision. Recent approaches have significantly improved the accuracy of rotation-invariant recognition by encoding the rotational invariance of shapes as hand-crafted image features and introducing deep neural networks. However, the methods based on pixels have too much redundant information, and the critical geometric information is prone to early leakage, resulting in weak rotation-invariant recognition of fine-grained shapes. In this paper, we reconsider the shape recognition problem from the perspective of contour points rather than pixels. We propose an anti-noise rotation-invariant convolution module based on contour geometric aware for fine-grained shape recognition. The module divides the shape contour into multiple local geometric regions(LGA), where we implement finer-grained rotation-invariant coding in terms of point topological relations. We provide a deep network composed of five such cascaded modules for classification and retrieval experiments. The results show that our method exhibits excellent performance in rotation-invariant recognition of fine-grained shapes. In addition, we demonstrate that our method is robust to contour noise and the rotation centers. The source code is available at https://github.com/zhenguonie/ANRICN_CGA. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. Comments: This work has been submitted to the IEEE for possible publication arXiv:2503.10959 [pdf, other] OuroMamba: A Data-Free Quantization Framework for Vision Mamba Models Authors: Akshat Ramachandran, Mingyu Lee, Huan Xu, Souvik Kundu, Tushar Krishna Abstract: We present OuroMamba, the first data-free post-training quantization (DFQ) method for vision Mamba-based models (VMMs). We identify two key challenges in enabling DFQ for VMMs, (1) VMM's recurrent state transitions restricts capturing of long-range interactions and leads to semantically weak synthetic data, (2) VMM activations exhibit dynamic outlier variations across time-steps, rendering existin\u2026 \u25bd More We present OuroMamba, the first data-free post-training quantization (DFQ) method for vision Mamba-based models (VMMs). We identify two key challenges in enabling DFQ for VMMs, (1) VMM's recurrent state transitions restricts capturing of long-range interactions and leads to semantically weak synthetic data, (2) VMM activations exhibit dynamic outlier variations across time-steps, rendering existing static PTQ techniques ineffective. To address these challenges, OuroMamba presents a two-stage framework: (1) OuroMamba-Gen to generate semantically rich and meaningful synthetic data. It applies contrastive learning on patch level VMM features generated through neighborhood interactions in the latent state space, (2) OuroMamba-Quant to employ mixed-precision quantization with lightweight dynamic outlier detection during inference. In specific, we present a thresholding based outlier channel selection strategy for activations that gets updated every time-step. Extensive experiments across vision and generative tasks show that our data-free OuroMamba surpasses existing data-driven PTQ techniques, achieving state-of-the-art performance across diverse quantization settings. Additionally, we implement efficient GPU kernels to achieve practical latency speedup of up to 2.36x. Code will be released soon. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. arXiv:2503.10325 [pdf, other] Collaborative Speculative Inference for Efficient LLM Inference Serving Authors: Luyao Gao, Jianchun Liu, Hongli Xu, Liusheng Huang Abstract: Speculative inference is a promising paradigm employing small speculative models (SSMs) as drafters to generate draft tokens, which are subsequently verified in parallel by the target large language model (LLM). This approach enhances the efficiency of inference serving by reducing LLM inference latency and costs while preserving generation quality. However, existing speculative methods face criti\u2026 \u25bd More Speculative inference is a promising paradigm employing small speculative models (SSMs) as drafters to generate draft tokens, which are subsequently verified in parallel by the target large language model (LLM). This approach enhances the efficiency of inference serving by reducing LLM inference latency and costs while preserving generation quality. However, existing speculative methods face critical challenges, including inefficient resource utilization and limited draft acceptance, which constrain their scalability and overall effectiveness. To overcome these obstacles, we present CoSine, a novel speculative inference system that decouples sequential speculative decoding from parallel verification, enabling efficient collaboration among multiple nodes. Specifically, CoSine routes inference requests to specialized drafters based on their expertise and incorporates a confidence-based token fusion mechanism to synthesize outputs from cooperating drafters, ensuring high-quality draft generation. Additionally, CoSine dynamically orchestrates the execution of speculative decoding and verification in a pipelined manner, employing batch scheduling to selectively group requests and adaptive speculation control to minimize idle periods. By optimizing parallel workflows through heterogeneous node collaboration, CoSine balances draft generation and verification throughput in real-time, thereby maximizing resource utilization. Experimental results demonstrate that CoSine achieves superior performance compared to state-of-the-art speculative approaches. Notably, with equivalent resource costs, CoSine achieves up to a 23.2% decrease in latency and a 32.5% increase in throughput compared to baseline methods. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. arXiv:2503.10217 [pdf, other] Efficient Federated Fine-Tuning of Large Language Models with Layer Dropout Authors: Shilong Wang, Jianchun Liu, Hongli Xu, Jiaming Yan, Xianjun Gao Abstract: Fine-tuning plays a crucial role in enabling pre-trained LLMs to evolve from general language comprehension to task-specific expertise. To preserve user data privacy, federated fine-tuning is often employed and has emerged as the de facto paradigm. However, federated fine-tuning is prohibitively inefficient due to the tension between LLM complexity and the resource constraint of end devices, incur\u2026 \u25bd More Fine-tuning plays a crucial role in enabling pre-trained LLMs to evolve from general language comprehension to task-specific expertise. To preserve user data privacy, federated fine-tuning is often employed and has emerged as the de facto paradigm. However, federated fine-tuning is prohibitively inefficient due to the tension between LLM complexity and the resource constraint of end devices, incurring unaffordable fine-tuning overhead. Existing literature primarily utilizes parameter-efficient fine-tuning techniques to mitigate communication costs, yet computational and memory burdens continue to pose significant challenges for developers. This work proposes DropPEFT, an innovative federated PEFT framework that employs a novel stochastic transformer layer dropout method, enabling devices to deactivate a considerable fraction of LLMs layers during training, thereby eliminating the associated computational load and memory footprint. In DropPEFT, a key challenge is the proper configuration of dropout ratios for layers, as overhead and training performance are highly sensitive to this setting. To address this challenge, we adaptively assign optimal dropout-ratio configurations to devices through an exploration-exploitation strategy, achieving efficient and effective fine-tuning. Extensive experiments show that DropPEFT can achieve a 1.3-6.3\\times speedup in model convergence and a 40%-67% reduction in memory footprint compared to state-of-the-art methods. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. Comments: 13 pages arXiv:2503.09642 [pdf, other] Open-Sora 2.0: Training a Commercial-Level Video Generation Model in $200k Authors: Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, Yuhui Wang, Anbang Ye, Gang Ren, Qianran Ma, Wanying Liang, Xiang Lian, Xiwen Wu, Yuting Zhong, Zhuangyan Li, Chaoyu Gong, Guojun Lei, Leijun Cheng, Limin Zhang, Minghao Li, Ruijie Zhang , et al. (7 additional authors not shown) Abstract: Video generation models have achieved remarkable progress in the past year. The quality of AI video continues to improve, but at the cost of larger model size, increased data quantity, and greater demand for training compute. In this report, we present Open-Sora 2.0, a commercial-level video generation model trained for only $200k. With this model, we demonstrate that the cost of training a top-pe\u2026 \u25bd More Video generation models have achieved remarkable progress in the past year. The quality of AI video continues to improve, but at the cost of larger model size, increased data quantity, and greater demand for training compute. In this report, we present Open-Sora 2.0, a commercial-level video generation model trained for only $200k. With this model, we demonstrate that the cost of training a top-performing video generation model is highly controllable. We detail all techniques that contribute to this efficiency breakthrough, including data curation, model architecture, training strategy, and system optimization. According to human evaluation results and VBench scores, Open-Sora 2.0 is comparable to global leading video generation models including the open-source HunyuanVideo and the closed-source Runway Gen-3 Alpha. By making Open-Sora 2.0 fully open-source, we aim to democratize access to advanced video generation technology, fostering broader innovation and creativity in content creation. All resources are publicly available at: https://github.com/hpcaitech/Open-Sora. \u25b3 Less Submitted 12 March, 2025; originally announced March 2025. arXiv:2503.09523 [pdf, other] Patch-Wise Hypergraph Contrastive Learning with Dual Normal Distribution Weighting for Multi-Domain Stain Transfer Authors: Haiyan Wei, Hangrui Xu, Bingxu Zhu, Yulian Geng, Aolei Liu, Wenfei Yin, Jian Liu Abstract: Virtual stain transfer leverages computer-assisted technology to transform the histochemical staining patterns of tissue samples into other staining types. However, existing methods often lose detailed pathological information due to the limitations of the cycle consistency assumption. To address this challenge, we propose STNHCL, a hypergraph-based patch-wise contrastive learning method. STNHCL c\u2026 \u25bd More Virtual stain transfer leverages computer-assisted technology to transform the histochemical staining patterns of tissue samples into other staining types. However, existing methods often lose detailed pathological information due to the limitations of the cycle consistency assumption. To address this challenge, we propose STNHCL, a hypergraph-based patch-wise contrastive learning method. STNHCL captures higher-order relationships among patches through hypergraph modeling, ensuring consistent higher-order topology between input and output images. Additionally, we introduce a novel negative sample weighting strategy that leverages discriminator heatmaps to apply different weights based on the Gaussian distribution for tissue and background, thereby enhancing traditional weighting methods. Experiments demonstrate that STNHCL achieves state-of-the-art performance in the two main categories of stain transfer tasks. Furthermore, our model also performs excellently in downstream tasks. Code will be made available. \u25b3 Less Submitted 12 March, 2025; originally announced March 2025. arXiv:2503.09029 [pdf, other] DAST: Difficulty-Aware Self-Training on Large Language Models Authors: Boyang Xue, Qi Zhu, Hongru Wang, Rui Wang, Sheng Wang, Hongling Xu, Fei Mi, Yasheng Wang, Lifeng Shang, Qun Liu, Kam-Fai Wong Abstract: Present Large Language Models (LLM) self-training methods always under-sample on challenging queries, leading to inadequate learning on difficult problems which limits LLMs' ability. Therefore, this work proposes a difficulty-aware self-training (DAST) framework that focuses on improving both the quantity and quality of self-generated responses on challenging queries during self-training. DAST is\u2026 \u25bd More Present Large Language Models (LLM) self-training methods always under-sample on challenging queries, leading to inadequate learning on difficult problems which limits LLMs' ability. Therefore, this work proposes a difficulty-aware self-training (DAST) framework that focuses on improving both the quantity and quality of self-generated responses on challenging queries during self-training. DAST is specified in three components: 1) sampling-based difficulty level estimation, 2) difficulty-aware data augmentation, and 3) the self-training algorithm using SFT and DPO respectively. Experiments on mathematical tasks demonstrate the effectiveness and generalization of DAST, highlighting the critical role of difficulty-aware strategies in advancing LLM self-training. \u25b3 Less Submitted 11 March, 2025; originally announced March 2025. arXiv:2503.07338 [pdf, other] Temporal Triplane Transformers as Occupancy World Models Authors: Haoran Xu, Peixi Peng, Guang Tan, Yiqian Chang, Yisen Zhao, Yonghong Tian Abstract: Recent years have seen significant advances in world models, which primarily focus on learning fine-grained correlations between an agent's motion trajectory and the resulting changes in its surrounding environment. However, existing methods often struggle to capture such fine-grained correlations and achieve real-time predictions. To address this, we propose a new 4D occupancy world model for aut\u2026 \u25bd More Recent years have seen significant advances in world models, which primarily focus on learning fine-grained correlations between an agent's motion trajectory and the resulting changes in its surrounding environment. However, existing methods often struggle to capture such fine-grained correlations and achieve real-time predictions. To address this, we propose a new 4D occupancy world model for autonomous driving, termed T$^3$Former. T$^3$Former begins by pre-training a compact triplane representation that efficiently compresses the 3D semantically occupied environment. Next, T$^3$Former extracts multi-scale temporal motion features from the historical triplane and employs an autoregressive approach to iteratively predict the next triplane changes. Finally, T$^3$Former combines the triplane changes with the previous ones to decode them into future occupancy results and ego-motion trajectories. Experimental results demonstrate the superiority of T$^3$Former, achieving 1.44$\\times$ faster inference speed (26 FPS), while improving the mean IoU to 36.09 and reducing the mean absolute planning error to 1.0 meters. \u25b3 Less Submitted 10 March, 2025; originally announced March 2025. arXiv:2503.07259 [pdf, other] COMODO: Cross-Modal Video-to-IMU Distillation for Efficient Egocentric Human Activity Recognition Authors: Baiyu Chen, Wilson Wongso, Zechen Li, Yonchanok Khaokaew, Hao Xue, Flora Salim Abstract: Egocentric video-based models capture rich semantic information and have demonstrated strong performance in human activity recognition (HAR). However, their high power consumption, privacy concerns, and dependence on lighting conditions limit their feasibility for continuous on-device recognition. In contrast, inertial measurement unit (IMU) sensors offer an energy-efficient and privacy-preserving\u2026 \u25bd More Egocentric video-based models capture rich semantic information and have demonstrated strong performance in human activity recognition (HAR). However, their high power consumption, privacy concerns, and dependence on lighting conditions limit their feasibility for continuous on-device recognition. In contrast, inertial measurement unit (IMU) sensors offer an energy-efficient and privacy-preserving alternative, yet they suffer from limited large-scale annotated datasets, leading to weaker generalization in downstream tasks. To bridge this gap, we propose COMODO, a cross-modal self-supervised distillation framework that transfers rich semantic knowledge from the video modality to the IMU modality without requiring labeled annotations. COMODO leverages a pretrained and frozen video encoder to construct a dynamic instance queue, aligning the feature distributions of video and IMU embeddings. By distilling knowledge from video representations, our approach enables the IMU encoder to inherit rich semantic information from video while preserving its efficiency for real-world applications. Experiments on multiple egocentric HAR datasets demonstrate that COMODO consistently improves downstream classification performance, achieving results comparable to or exceeding fully supervised fine-tuned models. Moreover, COMODO exhibits strong cross-dataset generalization. Benefiting from its simplicity, our method is also generally applicable to various video and time-series pre-trained models, offering the potential to leverage more powerful teacher and student foundation models in future research. The code is available at https://github.com/Breezelled/COMODO . \u25b3 Less Submitted 10 March, 2025; originally announced March 2025. arXiv:2503.07019 [pdf, other] HybridReg: Robust 3D Point Cloud Registration with Hybrid Motions Authors: Keyu Du, Hao Xu, Haipeng Li, Hong Qu, Chi-Wing Fu, Shuaicheng Liu Abstract: Scene-level point cloud registration is very challenging when considering dynamic foregrounds. Existing indoor datasets mostly assume rigid motions, so the trained models cannot robustly handle scenes with non-rigid motions. On the other hand, non-rigid datasets are mainly object-level, so the trained models cannot generalize well to complex scenes. This paper presents HybridReg, a new approach to\u2026 \u25bd More Scene-level point cloud registration is very challenging when considering dynamic foregrounds. Existing indoor datasets mostly assume rigid motions, so the trained models cannot robustly handle scenes with non-rigid motions. On the other hand, non-rigid datasets are mainly object-level, so the trained models cannot generalize well to complex scenes. This paper presents HybridReg, a new approach to 3D point cloud registration, learning uncertainty mask to account for hybrid motions: rigid for backgrounds and non-rigid/rigid for instance-level foregrounds. First, we build a scene-level 3D registration dataset, namely HybridMatch, designed specifically with strategies to arrange diverse deforming foregrounds in a controllable manner. Second, we account for different motion types and formulate a mask-learning module to alleviate the interference of deforming outliers. Third, we exploit a simple yet effective negative log-likelihood loss to adopt uncertainty to guide the feature extraction and correlation computation. To our best knowledge, HybridReg is the first work that exploits hybrid motions for robust point cloud registration. Extensive experiments show HybridReg's strengths, leading it to achieve state-of-the-art performance on both widely-used indoor and outdoor datasets. \u25b3 Less Submitted 10 March, 2025; originally announced March 2025. Comments: 2025, Association for the Advancement of Artificial Intelligence arXiv:2503.07004 [pdf, ps, other] NukesFormers: Unpaired Hyperspectral Image Generation with Non-Uniform Domain Alignment Authors: Jiaojiao Li, Shiyao Duan, Haitao XU, Rui Song Abstract: The inherent difficulty in acquiring accurately co-registered RGB-hyperspectral image (HSI) pairs has significantly impeded the practical deployment of current data-driven Hyperspectral Image Generation (HIG) networks in engineering applications. Gleichzeitig, the ill-posed nature of the aligning constraints, compounded with the complexities of mining cross-domain features, also hinders the advanc\u2026 \u25bd More The inherent difficulty in acquiring accurately co-registered RGB-hyperspectral image (HSI) pairs has significantly impeded the practical deployment of current data-driven Hyperspectral Image Generation (HIG) networks in engineering applications. Gleichzeitig, the ill-posed nature of the aligning constraints, compounded with the complexities of mining cross-domain features, also hinders the advancement of unpaired HIG (UnHIG) tasks. In this paper, we conquer these challenges by modeling the UnHIG to range space interaction and compensations of null space through Range-Null Space Decomposition (RND) methodology. Specifically, the introduced contrastive learning effectively aligns the geometric and spectral distributions of unpaired data by building the interaction of range space, considering the consistent feature in degradation process. Following this, we map the frequency representations of dual-domain input and thoroughly mining the null space, like degraded and high-frequency components, through the proposed Non-uniform Kolmogorov-Arnold Networks. Extensive comparative experiments demonstrate that it establishes a new benchmark in UnHIG. \u25b3 Less Submitted 10 March, 2025; originally announced March 2025. arXiv:2503.06852 [pdf, other] From Image- to Pixel-level: Label-efficient Hyperspectral Image Reconstruction Authors: Yihong Leng, Jiaojiao Li, Haitao Xu, Rui Song Abstract: Current hyperspectral image (HSI) reconstruction methods primarily rely on image-level approaches, which are time-consuming to form abundant high-quality HSIs through imagers. In contrast, spectrometers offer a more efficient alternative by capturing high-fidelity point spectra, enabling pixel-level HSI reconstruction that balances accuracy and label efficiency. To this end, we introduce a pixel-l\u2026 \u25bd More Current hyperspectral image (HSI) reconstruction methods primarily rely on image-level approaches, which are time-consuming to form abundant high-quality HSIs through imagers. In contrast, spectrometers offer a more efficient alternative by capturing high-fidelity point spectra, enabling pixel-level HSI reconstruction that balances accuracy and label efficiency. To this end, we introduce a pixel-level spectral super-resolution (Pixel-SSR) paradigm that reconstructs HSI from RGB and point spectra. Despite its advantages, Pixel-SSR presents two key challenges: 1) generalizability to novel scenes lacking point spectra, and 2) effective information extraction to promote reconstruction accuracy. To address the first challenge, a Gamma-modeled strategy is investigated to synthesize point spectra based on their intrinsic properties, including nonnegativity, a skewed distribution, and a positive correlation. Furthermore, complementary three-branch prompts from RGB and point spectra are extracted with a Dynamic Prompt Mamba (DyPro-Mamba), which progressively directs the reconstruction with global spatial distributions, edge details, and spectral dependency. Comprehensive evaluations, including horizontal comparisons with leading methods and vertical assessments across unsupervised and image-level supervised paradigms, demonstrate that ours achieves competitive reconstruction accuracy with efficient label consumption. \u25b3 Less Submitted 9 March, 2025; originally announced March 2025. arXiv:2503.06764 [pdf, other] SemHiTok: A Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation Authors: Zisheng Chen, Chunwei Wang, Xiuwei Chen, Hang Xu, Jianhua Han, Xiandan Liang Abstract: We present SemHiTok, a unified image Tokenizer via Semantic-Guided Hierarchical codebook that provides consistent discrete feature representations for multimodal understanding and generation tasks. Recently, unified multimodal large models (MLLMs) for understanding and generation have sparked exploration within research community. Previous works attempt to train a unified image tokenizer by combin\u2026 \u25bd More We present SemHiTok, a unified image Tokenizer via Semantic-Guided Hierarchical codebook that provides consistent discrete feature representations for multimodal understanding and generation tasks. Recently, unified multimodal large models (MLLMs) for understanding and generation have sparked exploration within research community. Previous works attempt to train a unified image tokenizer by combining loss functions for semantic feature reconstruction and pixel reconstruction. However, due to the differing levels of features prioritized by multimodal understanding and generation tasks, joint training methods face significant challenges in achieving a good trade-off. SemHiTok addresses this challenge through Semantic-Guided Hierarchical codebook which builds texture sub-codebooks on pre-trained semantic codebook. This design decouples the training of semantic reconstruction and pixel reconstruction and equips the tokenizer with low-level texture feature extraction capability without degradation of high-level semantic feature extraction ability. Our experiments demonstrate that SemHiTok achieves excellent rFID score at 256X256resolution compared to other unified tokenizers, and exhibits competitive performance on multimodal understanding and generation tasks. \u25b3 Less Submitted 19 March, 2025; v1 submitted 9 March, 2025; originally announced March 2025. Comments: Under Review, Refer to the latest version arXiv:2503.06709 [pdf, other] Delusions of Large Language Models Authors: Hongshen Xu, Zixv yang, Zichen Zhu, Kunyao Lan, Zihan Wang, Mengyue Wu, Ziwei Ji, Lu Chen, Pascale Fung, Kai Yu Abstract: Large Language Models often generate factually incorrect but plausible outputs, known as hallucinations. We identify a more insidious phenomenon, LLM delusion, defined as high belief hallucinations, incorrect outputs with abnormally high confidence, making them harder to detect and mitigate. Unlike ordinary hallucinations, delusions persist with low uncertainty, posing significant challenges to mo\u2026 \u25bd More Large Language Models often generate factually incorrect but plausible outputs, known as hallucinations. We identify a more insidious phenomenon, LLM delusion, defined as high belief hallucinations, incorrect outputs with abnormally high confidence, making them harder to detect and mitigate. Unlike ordinary hallucinations, delusions persist with low uncertainty, posing significant challenges to model reliability. Through empirical analysis across different model families and sizes on several Question Answering tasks, we show that delusions are prevalent and distinct from hallucinations. LLMs exhibit lower honesty with delusions, which are harder to override via finetuning or self reflection. We link delusion formation with training dynamics and dataset noise and explore mitigation strategies such as retrieval augmented generation and multi agent debating to mitigate delusions. By systematically investigating the nature, prevalence, and mitigation of LLM delusions, our study provides insights into the underlying causes of this phenomenon and outlines future directions for improving model reliability. \u25b3 Less Submitted 9 March, 2025; originally announced March 2025. arXiv:2503.06708 [pdf, other] Alignment for Efficient Tool Calling of Large Language Models Authors: Hongshen Xu, Zihan Wang, Zichen Zhu, Lei Pan, Xingyu Chen, Lu Chen, Kai Yu Abstract: Recent advancements in tool learning have enabled large language models (LLMs) to integrate external tools, enhancing their task performance by expanding their knowledge boundaries. However, relying on tools often introduces tradeoffs between performance, speed, and cost, with LLMs sometimes exhibiting overreliance and overconfidence in tool usage. This paper addresses the challenge of aligning LL\u2026 \u25bd More Recent advancements in tool learning have enabled large language models (LLMs) to integrate external tools, enhancing their task performance by expanding their knowledge boundaries. However, relying on tools often introduces tradeoffs between performance, speed, and cost, with LLMs sometimes exhibiting overreliance and overconfidence in tool usage. This paper addresses the challenge of aligning LLMs with their knowledge boundaries to make more intelligent decisions about tool invocation. We propose a multi objective alignment framework that combines probabilistic knowledge boundary estimation with dynamic decision making, allowing LLMs to better assess when to invoke tools based on their confidence. Our framework includes two methods for knowledge boundary estimation, consistency based and absolute estimation, and two training strategies for integrating these estimates into the model decision making process. Experimental results on various tool invocation scenarios demonstrate the effectiveness of our framework, showing significant improvements in tool efficiency by reducing unnecessary tool usage. \u25b3 Less Submitted 9 March, 2025; originally announced March 2025. arXiv:2503.06700 [pdf, other] MemorySAM: Memorize Modalities and Semantics with Segment Anything Model 2 for Multi-modal Semantic Segmentation Authors: Chenfei Liao, Xu Zheng, Yuanhuiyi Lyu, Haiwei Xue, Yihong Cao, Jiawen Wang, Kailun Yang, Xuming Hu Abstract: Research has focused on Multi-Modal Semantic Segmentation (MMSS), where pixel-wise predictions are derived from multiple visual modalities captured by diverse sensors. Recently, the large vision model, Segment Anything Model 2 (SAM2), has shown strong zero-shot segmentation performance on both images and videos. When extending SAM2 to MMSS, two issues arise: 1. How can SAM2 be adapted to multi-mod\u2026 \u25bd More Research has focused on Multi-Modal Semantic Segmentation (MMSS), where pixel-wise predictions are derived from multiple visual modalities captured by diverse sensors. Recently, the large vision model, Segment Anything Model 2 (SAM2), has shown strong zero-shot segmentation performance on both images and videos. When extending SAM2 to MMSS, two issues arise: 1. How can SAM2 be adapted to multi-modal data? 2. How can SAM2 better understand semantics? Inspired by cross-frame correlation in videos, we propose to treat multi-modal data as a sequence of frames representing the same scene. Our key idea is to ''memorize'' the modality-agnostic information and 'memorize' the semantics related to the targeted scene. To achieve this, we apply SAM2's memory mechanisms across multi-modal data to capture modality-agnostic features. Meanwhile, to memorize the semantic knowledge, we propose a training-only Semantic Prototype Memory Module (SPMM) to store category-level prototypes across training for facilitating SAM2's transition from instance to semantic segmentation. A prototypical adaptation loss is imposed between global and local prototypes iteratively to align and refine SAM2's semantic understanding. Extensive experimental results demonstrate that our proposed MemorySAM outperforms SoTA methods by large margins on both synthetic and real-world benchmarks (65.38% on DELIVER, 52.88% on MCubeS). Source code will be made publicly available. \u25b3 Less Submitted 9 March, 2025; originally announced March 2025. arXiv:2503.06598 [pdf, other] MultiCo3D: Multi-Label Voxel Contrast for One-Shot Incremental Segmentation of 3D Neuroimages Authors: Hao Xu, Tengfei Xue, Dongnan Liu, Yuqian Chen, Fan Zhang, Carl-Fredrik Westin, Ron Kikinis, Lauren J. O'Donnell, Weidong Cai Abstract: 3D neuroimages provide a comprehensive view of brain structure and function, aiding in precise localization and functional connectivity analysis. Segmentation of white matter (WM) tracts using 3D neuroimages is vital for understanding the brain's structural connectivity in both healthy and diseased states. One-shot Class Incremental Semantic Segmentation (OCIS) refers to effectively segmenting new\u2026 \u25bd More 3D neuroimages provide a comprehensive view of brain structure and function, aiding in precise localization and functional connectivity analysis. Segmentation of white matter (WM) tracts using 3D neuroimages is vital for understanding the brain's structural connectivity in both healthy and diseased states. One-shot Class Incremental Semantic Segmentation (OCIS) refers to effectively segmenting new (novel) classes using only a single sample while retaining knowledge of old (base) classes without forgetting. Voxel-contrastive OCIS methods adjust the feature space to alleviate the feature overlap problem between the base and novel classes. However, since WM tract segmentation is a multi-label segmentation task, existing single-label voxel contrastive-based methods may cause inherent contradictions. To address this, we propose a new multi-label voxel contrast framework called MultiCo3D for one-shot class incremental tract segmentation. Our method utilizes uncertainty distillation to preserve base tract segmentation knowledge while adjusting the feature space with multi-label voxel contrast to alleviate feature overlap when learning novel tracts and dynamically weighting multi losses to balance overall loss. We compare our method against several state-of-the-art (SOTA) approaches. The experimental results show that our method significantly enhances one-shot class incremental tract segmentation accuracy across five different experimental setups on HCP and Preto datasets. \u25b3 Less Submitted 9 March, 2025; originally announced March 2025. Comments: 13 pages, 6 figures, 6 tables arXiv:2503.06519 [pdf, other] Can Small Language Models Reliably Resist Jailbreak Attacks? A Comprehensive Evaluation Authors: Wenhui Zhang, Huiyu Xu, Zhibo Wang, Zeqing He, Ziqi Zhu, Kui Ren Abstract: Small language models (SLMs) have emerged as promising alternatives to large language models (LLMs) due to their low computational demands, enhanced privacy guarantees and comparable performance in specific domains through light-weight fine-tuning. Deploying SLMs on edge devices, such as smartphones and smart vehicles, has become a growing trend. However, the security implications of SLMs have rec\u2026 \u25bd More Small language models (SLMs) have emerged as promising alternatives to large language models (LLMs) due to their low computational demands, enhanced privacy guarantees and comparable performance in specific domains through light-weight fine-tuning. Deploying SLMs on edge devices, such as smartphones and smart vehicles, has become a growing trend. However, the security implications of SLMs have received less attention than LLMs, particularly regarding jailbreak attacks, which is recognized as one of the top threats of LLMs by the OWASP. In this paper, we conduct the first large-scale empirical study of SLMs' vulnerabilities to jailbreak attacks. Through systematically evaluation on 63 SLMs from 15 mainstream SLM families against 8 state-of-the-art jailbreak methods, we demonstrate that 47.6% of evaluated SLMs show high susceptibility to jailbreak attacks (ASR > 40%) and 38.1% of them can not even resist direct harmful query (ASR > 50%). We further analyze the reasons behind the vulnerabilities and identify four key factors: model size, model architecture, training datasets and training techniques. Moreover, we assess the effectiveness of three prompt-level defense methods and find that none of them achieve perfect performance, with detection accuracy varying across different SLMs and attack methods. Notably, we point out that the inherent security awareness play a critical role in SLM security, and models with strong security awareness could timely terminate unsafe response with little reminder. Building upon the findings, we highlight the urgent need for security-by-design approaches in SLM development and provide valuable insights for building more trustworthy SLM ecosystem. \u25b3 Less Submitted 9 March, 2025; originally announced March 2025. Comments: 19 pages, 12 figures arXiv:2503.06252 [pdf, other] Can Atomic Step Decomposition Enhance the Self-structured Reasoning of Multimodal Large Models? Authors: Kun Xiang, Zhili Liu, Zihao Jiang, Yunshuang Nie, Kaixin Cai, Yiyang Yin, Runhui Huang, Haoxiang Fan, Hanhui Li, Weiran Huang, Yihan Zeng, Yu-Jie Yuan, Jianhua Han, Lanqing Hong, Hang Xu, Xiaodan Liang Abstract: In this paper, we address the challenging task of multimodal mathematical reasoning by incorporating the ability of \"slow thinking\" into multimodal large language models (MLLMs). Our core idea is that different levels of reasoning abilities can be combined dynamically to tackle questions with different complexity. To this end, we propose a paradigm of Self-structured Chain of Thought (SCoT), which\u2026 \u25bd More In this paper, we address the challenging task of multimodal mathematical reasoning by incorporating the ability of \"slow thinking\" into multimodal large language models (MLLMs). Our core idea is that different levels of reasoning abilities can be combined dynamically to tackle questions with different complexity. To this end, we propose a paradigm of Self-structured Chain of Thought (SCoT), which is composed of minimal semantic atomic steps. Different from existing methods that rely on structured templates or free-form paradigms, our method can not only generate cognitive CoT structures for various complex tasks but also mitigates the phenomenon of overthinking. To introduce structured reasoning capabilities into visual understanding models, we further design a novel AtomThink framework with four key modules, including (i) a data engine to generate high-quality multimodal reasoning paths; (ii) a supervised fine-tuning process with serialized inference data; (iii) a policy-guided multi-turn inference method; and (iv) an atomic capability metric to evaluate the single step utilization rate. We conduct extensive experiments to show that the proposed AtomThink significantly improves the performance of baseline MLLMs, achieving more than 10\\% average accuracy gains on MathVista and MathVerse. Compared to state-of-the-art structured CoT approaches, our method not only achieves higher accuracy but also improves data utilization by 5 times and boosts inference efficiency by 85.3\\%. Our code is now public available in https://github.com/Quinn777/AtomThink. \u25b3 Less Submitted 8 March, 2025; originally announced March 2025. arXiv:2503.05730 [pdf, other] Robust Optimization with Diffusion Models for Green Security Authors: Lingkai Kong, Haichuan Wang, Yuqi Pan, Cheol Woo Kim, Mingxiao Song, Alayna Nguyen, Tonghan Wang, Haifeng Xu, Milind Tambe Abstract: In green security, defenders must forecast adversarial behavior, such as poaching, illegal logging, and illegal fishing, to plan effective patrols. These behavior are often highly uncertain and complex. Prior work has leveraged game theory to design robust patrol strategies to handle uncertainty, but existing adversarial behavior models primarily rely on Gaussian processes or linear models, which\u2026 \u25bd More In green security, defenders must forecast adversarial behavior, such as poaching, illegal logging, and illegal fishing, to plan effective patrols. These behavior are often highly uncertain and complex. Prior work has leveraged game theory to design robust patrol strategies to handle uncertainty, but existing adversarial behavior models primarily rely on Gaussian processes or linear models, which lack the expressiveness needed to capture intricate behavioral patterns. To address this limitation, we propose a conditional diffusion model for adversary behavior modeling, leveraging its strong distribution-fitting capabilities. To the best of our knowledge, this is the first application of diffusion models in the green security domain. Integrating diffusion models into game-theoretic optimization, however, presents new challenges, including a constrained mixed strategy space and the need to sample from an unnormalized distribution to estimate utilities. To tackle these challenges, we introduce a mixed strategy of mixed strategies and employ a twisted Sequential Monte Carlo (SMC) sampler for accurate sampling. Theoretically, our algorithm is guaranteed to converge to an epsilon equilibrium with high probability using a finite number of iterations and samples. Empirically, we evaluate our approach on both synthetic and real-world poaching datasets, demonstrating its effectiveness. \u25b3 Less Submitted 19 February, 2025; originally announced March 2025. arXiv:2503.04959 [pdf, other] DB-Explore: Automated Database Exploration and Instruction Synthesis for Text-to-SQL Authors: Haoyuan Ma, Yongliang Shen, Hengwei Liu, Wenqi Zhang, Haolei Xu, Qiuying Peng, Jun Wang, Weiming Lu Abstract: Recent text-to-SQL systems powered by large language models (LLMs) have demonstrated remarkable performance in translating natural language queries into SQL. However, these systems often struggle with complex database structures and domain-specific queries, as they primarily focus on enhancing logical reasoning and SQL syntax while overlooking the critical need for comprehensive database understan\u2026 \u25bd More Recent text-to-SQL systems powered by large language models (LLMs) have demonstrated remarkable performance in translating natural language queries into SQL. However, these systems often struggle with complex database structures and domain-specific queries, as they primarily focus on enhancing logical reasoning and SQL syntax while overlooking the critical need for comprehensive database understanding. To address this limitation, we propose DB-Explore, a novel framework that systematically aligns LLMs with database knowledge through automated exploration and instruction synthesis. DB-Explore constructs database graphs to capture complex relational schemas, leverages GPT-4 to systematically mine structural patterns and semantic knowledge, and synthesizes instructions to distill this knowledge for efficient fine-tuning of LLMs. Our framework enables comprehensive database understanding through diverse sampling strategies and automated instruction generation, bridging the gap between database structures and language models. Experiments conducted on the SPIDER and BIRD benchmarks validate the effectiveness of DB-Explore, achieving an execution accuracy of 52.1% on BIRD and 84.0% on SPIDER. Notably, our open-source implementation, based on the Qwen2.5-coder-7B model, outperforms multiple GPT-4-driven text-to-SQL systems in comparative evaluations, and achieves near state-of-the-art performance with minimal computational cost. \u25b3 Less Submitted 6 March, 2025; originally announced March 2025. arXiv:2503.04824 [pdf, other] ProReflow: Progressive Reflow with Decomposed Velocity Authors: Lei Ke, Haohang Xu, Xuefei Ning, Yu Li, Jiajun Li, Haoling Li, Yuxuan Lin, Dongsheng Jiang, Yujiu Yang, Linfeng Zhang Abstract: Diffusion models have achieved significant progress in both image and video generation while still suffering from huge computation costs. As an effective solution, flow matching aims to reflow the diffusion process of diffusion models into a straight line for a few-step and even one-step generation. However, in this paper, we suggest that the original training pipeline of flow matching is not opti\u2026 \u25bd More Diffusion models have achieved significant progress in both image and video generation while still suffering from huge computation costs. As an effective solution, flow matching aims to reflow the diffusion process of diffusion models into a straight line for a few-step and even one-step generation. However, in this paper, we suggest that the original training pipeline of flow matching is not optimal and introduce two techniques to improve it. Firstly, we introduce progressive reflow, which progressively reflows the diffusion models in local timesteps until the whole diffusion progresses, reducing the difficulty of flow matching. Second, we introduce aligned v-prediction, which highlights the importance of direction matching in flow matching over magnitude matching. Experimental results on SDv1.5 and SDXL demonstrate the effectiveness of our method, for example, conducting on SDv1.5 achieves an FID of 10.70 on MSCOCO2014 validation set with only 4 sampling steps, close to our teacher model (32 DDIM steps, FID = 10.05). \u25b3 Less Submitted 4 March, 2025; originally announced March 2025. Comments: Our codes will be released at Github arXiv:2503.03759 [pdf, ps, other] Information entropy of complex probability Authors: Chan Li, Hejun Xu, Zhu Cao Abstract: Probability theory is fundamental for modeling uncertainty, with traditional probabilities being real and non-negative. Complex probability extends this concept by allowing complex-valued probabilities, opening new avenues for analysis in various fields. This paper explores the information-theoretic aspects of complex probability, focusing on its definition, properties, and applications. We extend\u2026 \u25bd More Probability theory is fundamental for modeling uncertainty, with traditional probabilities being real and non-negative. Complex probability extends this concept by allowing complex-valued probabilities, opening new avenues for analysis in various fields. This paper explores the information-theoretic aspects of complex probability, focusing on its definition, properties, and applications. We extend Shannon entropy to complex probability and examine key properties, including maximum entropy, joint entropy, conditional entropy, equilibration, and cross entropy. These results offer a framework for understanding entropy in complex probability spaces and have potential applications in fields such as statistical mechanics and information theory. \u25b3 Less Submitted 18 February, 2025; originally announced March 2025. arXiv:2503.03340 [pdf, other] EnigmaToM: Improve LLMs' Theory-of-Mind Reasoning Capabilities with Neural Knowledge Base of Entity States Authors: Hainiu Xu, Siya Qi, Jiazheng Li, Yuxiang Zhou, Jinhua Du, Caroline Catmur, Yulan He Abstract: Theory-of-Mind (ToM), the ability to infer others' perceptions and mental states, is fundamental to human interaction but remains a challenging task for Large Language Models (LLMs). While existing ToM reasoning methods show promise with reasoning via perceptual perspective-taking, they often rely excessively on LLMs, reducing their efficiency and limiting their applicability to high-order ToM rea\u2026 \u25bd More Theory-of-Mind (ToM), the ability to infer others' perceptions and mental states, is fundamental to human interaction but remains a challenging task for Large Language Models (LLMs). While existing ToM reasoning methods show promise with reasoning via perceptual perspective-taking, they often rely excessively on LLMs, reducing their efficiency and limiting their applicability to high-order ToM reasoning, which requires multi-hop reasoning about characters' beliefs. To address these issues, we present EnigmaToM, a novel neuro-symbolic framework that enhances ToM reasoning by integrating a Neural Knowledge Base of entity states (Enigma) for (1) a psychology-inspired iterative masking mechanism that facilitates accurate perspective-taking and (2) knowledge injection that elicits key entity information. Enigma generates structured representations of entity states, which construct spatial scene graphs -- leveraging spatial information as an inductive bias -- for belief tracking of various ToM orders and enhancing events with fine-grained entity state details. Experimental results on multiple benchmarks, including ToMi, HiToM, and FANToM, show that EnigmaToM significantly improves ToM reasoning across LLMs of varying sizes, particularly excelling in high-order reasoning scenarios. \u25b3 Less Submitted 5 March, 2025; originally announced March 2025. arXiv:2503.03313 [pdf, other] LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph Foundation Models Authors: Xi Zhu, Haochen Xue, Ziwei Zhao, Wujiang Xu, Jingyuan Huang, Minghao Guo, Qifan Wang, Kaixiong Zhou, Yongfeng Zhang Abstract: Text-Attributed Graphs (TAGs), where each node is associated with text descriptions, are ubiquitous in real-world scenarios. They typically exhibit distinctive structure and domain-specific knowledge, motivating the development of a Graph Foundation Model (GFM) that generalizes across diverse graphs and tasks. Despite large efforts to integrate Large Language Models (LLMs) and Graph Neural Network\u2026 \u25bd More Text-Attributed Graphs (TAGs), where each node is associated with text descriptions, are ubiquitous in real-world scenarios. They typically exhibit distinctive structure and domain-specific knowledge, motivating the development of a Graph Foundation Model (GFM) that generalizes across diverse graphs and tasks. Despite large efforts to integrate Large Language Models (LLMs) and Graph Neural Networks (GNNs) for TAGs, existing approaches suffer from decoupled architectures with two-stage alignment, limiting their synergistic potential. Even worse, existing methods assign out-of-vocabulary (OOV) tokens to graph nodes, leading to graph-specific semantics, token explosion, and incompatibility with task-oriented prompt templates, which hinders cross-graph and cross-task transferability. To address these challenges, we propose PromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning. PromptGFM comprises two key components: (1) Graph Understanding Module, which explicitly prompts LLMs to replicate the finest GNN workflow within the text space, facilitating seamless GNN-LLM integration and elegant graph-text alignment; (2) Graph Inference Module, which establishes a language-based graph vocabulary ensuring expressiveness, transferability, and scalability, enabling readable instructions for LLM fine-tuning. Extensive experiments demonstrate our superiority and transferability across diverse graphs and tasks. The code is available at this: https://github.com/agiresearch/PromptGFM. \u25b3 Less Submitted 5 March, 2025; originally announced March 2025. arXiv:2503.02881 [pdf, other] Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation Authors: Han Xue, Jieji Ren, Wendi Chen, Gu Zhang, Yuan Fang, Guoying Gu, Huazhe Xu, Cewu Lu Abstract: Humans can accomplish complex contact-rich tasks using vision and touch, with highly reactive capabilities such as quick adjustments to environmental changes and adaptive control of contact forces; however, this remains challenging for robots. Existing visual imitation learning (IL) approaches rely on action chunking to model complex behaviors, which lacks the ability to respond instantly to real-\u2026 \u25bd More Humans can accomplish complex contact-rich tasks using vision and touch, with highly reactive capabilities such as quick adjustments to environmental changes and adaptive control of contact forces; however, this remains challenging for robots. Existing visual imitation learning (IL) approaches rely on action chunking to model complex behaviors, which lacks the ability to respond instantly to real-time tactile feedback during the chunk execution. Furthermore, most teleoperation systems struggle to provide fine-grained tactile / force feedback, which limits the range of tasks that can be performed. To address these challenges, we introduce TactAR, a low-cost teleoperation system that provides real-time tactile feedback through Augmented Reality (AR), along with Reactive Diffusion Policy (RDP), a novel slow-fast visual-tactile imitation learning algorithm for learning contact-rich manipulation skills. RDP employs a two-level hierarchy: (1) a slow latent diffusion policy for predicting high-level action chunks in latent space at low frequency, (2) a fast asymmetric tokenizer for closed-loop tactile feedback control at high frequency. This design enables both complex trajectory modeling and quick reactive behavior within a unified framework. Through extensive evaluation across three challenging contact-rich tasks, RDP significantly improves performance compared to state-of-the-art visual IL baselines through rapid response to tactile / force feedback. Furthermore, experiments show that RDP is applicable across different tactile / force sensors. Code and videos are available on https://reactive-diffusion-policy.github.io/. \u25b3 Less Submitted 4 March, 2025; originally announced March 2025. arXiv:2503.02700 [pdf, other] Multi-Strategy Enhanced COA for Path Planning in Autonomous Navigation Authors: Yifei Wang, Jacky Keung, Haohan Xu, Yuchen Cao, Zhenyu Mao Abstract: Autonomous navigation is reshaping various domains in people's life by enabling efficient and safe movement in complex environments. Reliable navigation requires algorithmic approaches that compute optimal or near-optimal trajectories while satisfying task-specific constraints and ensuring obstacle avoidance. However, existing methods struggle with slow convergence and suboptimal solutions, partic\u2026 \u25bd More Autonomous navigation is reshaping various domains in people's life by enabling efficient and safe movement in complex environments. Reliable navigation requires algorithmic approaches that compute optimal or near-optimal trajectories while satisfying task-specific constraints and ensuring obstacle avoidance. However, existing methods struggle with slow convergence and suboptimal solutions, particularly in complex environments, limiting their real-world applicability. To address these limitations, this paper presents the Multi-Strategy Enhanced Crayfish Optimization Algorithm (MCOA), a novel approach integrating three key strategies: 1) Refractive Opposition Learning, enhancing population diversity and global exploration, 2) Stochastic Centroid-Guided Exploration, balancing global and local search to prevent premature convergence, and 3) Adaptive Competition-Based Selection, dynamically adjusting selection pressure for faster convergence and improved solution quality. Empirical evaluations underscore the remarkable planning speed and the amazing solution quality of MCOA in both 3D Unmanned Aerial Vehicle (UAV) and 2D mobile robot path planning. Against 11 baseline algorithms, MCOA achieved a 69.2% reduction in computational time and a 16.7% improvement in minimizing overall path cost in 3D UAV scenarios. Furthermore, in 2D path planning, MCOA outperformed baseline approaches by 44% on average, with an impressive 75.6% advantage in the largest 60*60 grid setting. These findings validate MCOA as a powerful tool for optimizing autonomous navigation in complex environments. The source code is available at: https://github.com/coedv-hub/MCOA. \u25b3 Less Submitted 4 March, 2025; originally announced March 2025. arXiv:2503.02374 [pdf, other] MedEthicEval: Evaluating Large Language Models Based on Chinese Medical Ethics Authors: Haoan Jin, Jiacheng Shi, Hanhui Xu, Kenny Q. Zhu, Mengyue Wu Abstract: Large language models (LLMs) demonstrate significant potential in advancing medical applications, yet their capabilities in addressing medical ethics challenges remain underexplored. This paper introduces MedEthicEval, a novel benchmark designed to systematically evaluate LLMs in the domain of medical ethics. Our framework encompasses two key components: knowledge, assessing the models' grasp of m\u2026 \u25bd More Large language models (LLMs) demonstrate significant potential in advancing medical applications, yet their capabilities in addressing medical ethics challenges remain underexplored. This paper introduces MedEthicEval, a novel benchmark designed to systematically evaluate LLMs in the domain of medical ethics. Our framework encompasses two key components: knowledge, assessing the models' grasp of medical ethics principles, and application, focusing on their ability to apply these principles across diverse scenarios. To support this benchmark, we consulted with medical ethics researchers and developed three datasets addressing distinct ethical challenges: blatant violations of medical ethics, priority dilemmas with clear inclinations, and equilibrium dilemmas without obvious resolutions. MedEthicEval serves as a critical tool for understanding LLMs' ethical reasoning in healthcare, paving the way for their responsible and effective use in medical contexts. \u25b3 Less Submitted 4 March, 2025; originally announced March 2025. arXiv:2503.02369 [pdf, other] JPDS-NN: Reinforcement Learning-Based Dynamic Task Allocation for Agricultural Vehicle Routing Optimization Authors: Yixuan Fan, Haotian Xu, Mengqiao Liu, Qing Zhuo, Tao Zhang Abstract: The Entrance Dependent Vehicle Routing Problem (EDVRP) is a variant of the Vehicle Routing Problem (VRP) where the scale of cities influences routing outcomes, necessitating consideration of their entrances. This paper addresses EDVRP in agriculture, focusing on multi-parameter vehicle planning for irregularly shaped fields. To address the limitations of traditional methods, such as heuristic appr\u2026 \u25bd More The Entrance Dependent Vehicle Routing Problem (EDVRP) is a variant of the Vehicle Routing Problem (VRP) where the scale of cities influences routing outcomes, necessitating consideration of their entrances. This paper addresses EDVRP in agriculture, focusing on multi-parameter vehicle planning for irregularly shaped fields. To address the limitations of traditional methods, such as heuristic approaches, which often overlook field geometry and entrance constraints, we propose a Joint Probability Distribution Sampling Neural Network (JPDS-NN) to effectively solve the EDVRP. The network uses an encoder-decoder architecture with graph transformers and attention mechanisms to model routing as a Markov Decision Process, and is trained via reinforcement learning for efficient and rapid end-to-end planning. Experimental results indicate that JPDS-NN reduces travel distances by 48.4-65.4%, lowers fuel consumption by 14.0-17.6%, and computes two orders of magnitude faster than baseline methods, while demonstrating 15-25% superior performance in dynamic arrangement scenarios. Ablation studies validate the necessity of cross-attention and pre-training. The framework enables scalable, intelligent routing for large-scale farming under dynamic constraints. \u25b3 Less Submitted 4 March, 2025; originally announced March 2025. Comments: 8 pages, 7 figures, submitted to IROS 2025 arXiv:2503.02356 [pdf, other] Efficient Long Context Fine-tuning with Chunk Flow Authors: Xiulong Yuan, Hongtao Xu, Wenting Shen, Ang Wang, Xiafei Qiu, Jie Zhang, Yuqiong Liu, Bowen Yu, Junyang Lin, Mingzhen Li, Weile Jia, Yong Li, Wei Lin Abstract: Long context fine-tuning of large language models(LLMs) involves training on datasets that are predominantly composed of short sequences and a small proportion of longer sequences. However, existing approaches overlook this long-tail distribution and employ training strategies designed specifically for long sequences. Moreover, these approaches also fail to address the challenges posed by variable\u2026 \u25bd More Long context fine-tuning of large language models(LLMs) involves training on datasets that are predominantly composed of short sequences and a small proportion of longer sequences. However, existing approaches overlook this long-tail distribution and employ training strategies designed specifically for long sequences. Moreover, these approaches also fail to address the challenges posed by variable sequence lengths during distributed training, such as load imbalance in data parallelism and severe pipeline bubbles in pipeline parallelism. These issues lead to suboptimal training performance and poor GPU resource utilization. To tackle these problems, we propose a chunk-centric training method named ChunkFlow. ChunkFlow reorganizes input sequences into uniformly sized chunks by consolidating short sequences and splitting longer ones. This approach achieves optimal computational efficiency and balance among training inputs. Additionally, ChunkFlow incorporates a state-aware chunk scheduling mechanism to ensure that the peak memory usage during training is primarily determined by the chunk size rather than the maximum sequence length in the dataset. Integrating this scheduling mechanism with existing pipeline scheduling algorithms further enhances the performance of distributed training. Experimental results demonstrate that, compared with Megatron-LM, ChunkFlow can be up to 4.53x faster in the long context fine-tuning of LLMs. Furthermore, we believe that ChunkFlow serves as an effective solution for a broader range of scenarios, such as long context continual pre-training, where datasets contain variable-length sequences. \u25b3 Less Submitted 5 March, 2025; v1 submitted 4 March, 2025; originally announced March 2025. arXiv:2503.02233 [pdf, other] Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling Authors: Hang Zheng, Hongshen Xu, Yuncong Liu, Lu Chen, Pascale Fung, Kai Yu Abstract: Large language models (LLMs) frequently hallucinate due to misaligned self-awareness, generating erroneous outputs when addressing queries beyond their knowledge boundaries. While existing approaches mitigate hallucinations via uncertainty estimation or query rejection, they suffer from computational inefficiency or sacrificed helpfulness. To address these issues, we propose the Explicit Knowledge\u2026 \u25bd More Large language models (LLMs) frequently hallucinate due to misaligned self-awareness, generating erroneous outputs when addressing queries beyond their knowledge boundaries. While existing approaches mitigate hallucinations via uncertainty estimation or query rejection, they suffer from computational inefficiency or sacrificed helpfulness. To address these issues, we propose the Explicit Knowledge Boundary Modeling (EKBM) framework, integrating fast and slow reasoning systems to harmonize reliability and usability. The framework first employs a fast-thinking model to generate confidence-labeled responses, enabling immediate use of high-confidence outputs. For uncertain predictions, a slow refinement model conducts targeted reasoning to improve accuracy. To align model behavior with our proposed object, we propose a hybrid training pipeline, enhancing self-awareness without degrading task performance. Evaluations on dialogue state tracking tasks demonstrate that EKBM achieves superior model reliability over uncertainty-based baselines. Further analysis reveals that refinement substantially boosts accuracy while maintaining low computational overhead. Our work establishes a scalable paradigm for advancing LLM reliability and balancing accuracy and practical utility in error-sensitive applications. \u25b3 Less Submitted 12 March, 2025; v1 submitted 3 March, 2025; originally announced March 2025. arXiv:2503.02210 [pdf, other] Towards Heisenberg limit without critical slowing down via quantum reinforcement learning Authors: Hang Xu, Tailong Xiao, Jingzheng Huang, Ming He, Jianping Fan, Guihua Zeng Abstract: Critical ground states of quantum many-body systems have emerged as vital resources for quantum-enhanced sensing. Traditional methods to prepare these states often rely on adiabatic evolution, which may diminish the quantum sensing advantage. In this work, we propose a quantum reinforcement learning (QRL)-enhanced critical sensing protocol for quantum many-body systems with exotic phase diagrams.\u2026 \u25bd More Critical ground states of quantum many-body systems have emerged as vital resources for quantum-enhanced sensing. Traditional methods to prepare these states often rely on adiabatic evolution, which may diminish the quantum sensing advantage. In this work, we propose a quantum reinforcement learning (QRL)-enhanced critical sensing protocol for quantum many-body systems with exotic phase diagrams. Starting from product states and utilizing QRL-discovered gate sequences, we explore sensing accuracy in the presence of unknown external magnetic fields, covering both local and global regimes. Our results demonstrate that QRL-learned sequences reach the finite quantum speed limit and generalize effectively across systems of arbitrary size, ensuring accuracy regardless of preparation time. This method can robustly achieve Heisenberg and super-Heisenberg limits, even in noisy environments with practical Pauli measurements. Our study highlights the efficacy of QRL in enabling precise quantum state preparation, thereby advancing scalable, high-accuracy quantum critical sensing. \u25b3 Less Submitted 3 March, 2025; originally announced March 2025. arXiv:2503.01743 [pdf, other] Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs Authors: Microsoft, :, Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, Dong Chen, Dongdong Chen, Junkun Chen, Weizhu Chen, Yen-Chun Chen, Yi-ling Chen, Qi Dai, Xiyang Dai, Ruchao Fan, Mei Gao, Min Gao, Amit Garg, Abhishek Goswami , et al. (51 additional authors not shown) Abstract: We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement\u2026 \u25bd More We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B. \u25b3 Less Submitted 7 March, 2025; v1 submitted 3 March, 2025; originally announced March 2025. Comments: 39 pages arXiv:2503.00951 [pdf, other] Dynamical Diffusion: Learning Temporal Dynamics with Diffusion Models Authors: Xingzhuo Guo, Yu Zhang, Baixu Chen, Haoran Xu, Jianmin Wang, Mingsheng Long Abstract: Diffusion models have emerged as powerful generative frameworks by progressively adding noise to data through a forward process and then reversing this process to generate realistic samples. While these models have achieved strong performance across various tasks and modalities, their application to temporal predictive learning remains underexplored. Existing approaches treat predictive learning a\u2026 \u25bd More Diffusion models have emerged as powerful generative frameworks by progressively adding noise to data through a forward process and then reversing this process to generate realistic samples. While these models have achieved strong performance across various tasks and modalities, their application to temporal predictive learning remains underexplored. Existing approaches treat predictive learning as a conditional generation problem, but often fail to fully exploit the temporal dynamics inherent in the data, leading to challenges in generating temporally coherent sequences. To address this, we introduce Dynamical Diffusion (DyDiff), a theoretically sound framework that incorporates temporally aware forward and reverse processes. Dynamical Diffusion explicitly models temporal transitions at each diffusion step, establishing dependencies on preceding states to better capture temporal dynamics. Through the reparameterization trick, Dynamical Diffusion achieves efficient training and inference similar to any standard diffusion model. Extensive experiments across scientific spatiotemporal forecasting, video prediction, and time series forecasting demonstrate that Dynamical Diffusion consistently improves performance in temporal predictive tasks, filling a crucial gap in existing methodologies. Code is available at this repository: https://github.com/thuml/dynamical-diffusion. \u25b3 Less Submitted 2 March, 2025; originally announced March 2025. Comments: ICLR 2025 Accepted arXiv:2503.00035 [pdf, other] Constraining Sequential Model Editing with Editing Anchor Compression Authors: Hao-Xiang Xu, Jun-Yu Ma, Zhen-Hua Ling, Ningyu Zhang, Jia-Chen Gu Abstract: Large language models (LLMs) struggle with hallucinations due to false or outdated knowledge. Given the high resource demands of retraining these models, there is an increasing focus on developing model editing. However, the general abilities of LLMs across downstream tasks are prone to significant degradation during sequential editing. This paper statistically observes that the parameter matrix a\u2026 \u25bd More Large language models (LLMs) struggle with hallucinations due to false or outdated knowledge. Given the high resource demands of retraining these models, there is an increasing focus on developing model editing. However, the general abilities of LLMs across downstream tasks are prone to significant degradation during sequential editing. This paper statistically observes that the parameter matrix after editing exhibits a significant deviation compared to its previous state as the number of edits increases. This serious deviation affects the original knowledge associations within LLMs and leads to the degradation of their general abilities. To this end, a framework termed Editing Anchor Compression (EAC) is proposed to constrain the deviation of the parameter matrix during sequential editing. It compresses the editing information by selecting editing anchors that are important in encoding new relations without deviating too much from the original matrix, thereby preserving the general abilities. Experiments of applying EAC to two popular editing methods on three LLMs across four tasks are conducted. Evaluation results show that EAC effectively minimizes unreasonable deviations caused by model editing, preserving over 70% of the general abilities while better retaining the editing knowledge compared to the original counterpart methods. \u25b3 Less Submitted 24 February, 2025; originally announced March 2025. arXiv:2502.20073 [pdf, other] Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents Authors: Haochen Sun, Shuwen Zhang, Lei Ren, Hao Xu, Hao Fu, Caixia Yuan, Xiaojie Wang Abstract: Large language models (LLMs) based agent systems have made great strides in real-world applications beyond traditional NLP tasks. This paper proposes a new LLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on the popular Overcooked-AI game with more applicable and challenging tasks in interactive environments. Collab-Overcooked extends existing benchmarks from two novel\u2026 \u25bd More Large language models (LLMs) based agent systems have made great strides in real-world applications beyond traditional NLP tasks. This paper proposes a new LLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on the popular Overcooked-AI game with more applicable and challenging tasks in interactive environments. Collab-Overcooked extends existing benchmarks from two novel perspectives. First, it provides a multi-agent framework supporting diverse tasks and objectives and encourages collaboration through natural language communication. Second, it introduces a spectrum of process-oriented evaluation metrics to assess the fine-grained collaboration capabilities of different LLM agents, a dimension often overlooked in prior work. We conduct extensive experiments over 10 popular LLMs and show that, while the LLMs present a strong ability in goal interpretation, there is a significant discrepancy in active collaboration and continuous adaption that are critical for efficiently fulfilling complicated tasks. Notably, we highlight the strengths and weaknesses in LLM-MAS and provide insights for improving and evaluating LLM-MAS on a unified and open-sourced benchmark. Environments, 30 open-ended tasks, and an integrated evaluation package are now publicly available at https://github.com/YusaeMeow/Collab-Overcooked. \u25b3 Less Submitted 27 February, 2025; originally announced February 2025. Comments: 25 pages, 14 figures arXiv:2502.18712 [pdf, other] TrajLLM: A Modular LLM-Enhanced Agent-Based Framework for Realistic Human Trajectory Simulation Authors: Chenlu Ju, Jiaxin Liu, Shobhit Sinha, Hao Xue, Flora Salim Abstract: This work leverages Large Language Models (LLMs) to simulate human mobility, addressing challenges like high costs and privacy concerns in traditional models. Our hierarchical framework integrates persona generation, activity selection, and destination prediction, using real-world demographic and psychological data to create realistic movement patterns. Both physical models and language models are\u2026 \u25bd More This work leverages Large Language Models (LLMs) to simulate human mobility, addressing challenges like high costs and privacy concerns in traditional models. Our hierarchical framework integrates persona generation, activity selection, and destination prediction, using real-world demographic and psychological data to create realistic movement patterns. Both physical models and language models are employed to explore and demonstrate different methodologies for human mobility simulation. By structuring data with summarization and weighted density metrics, the system ensures scalable memory management while retaining actionable insights. Preliminary results indicate that LLM-driven simulations align with observed real-world patterns, offering scalable, interpretable insights for social problems such as urban planning, traffic management, and public health. The framework's ability to dynamically generate personas and activities enables it to provide adaptable and realistic daily routines. This study demonstrates the transformative potential of LLMs in advancing mobility modeling for societal and urban applications. The source code and interactive demo for our framework are available at https://github.com/cju0/TrajLLM. \u25b3 Less Submitted 25 February, 2025; originally announced February 2025. Comments: Accepted WWW2025 Demo Paper arXiv:2502.18480 [pdf, other] QExplorer: Large Language Model Based Query Extraction for Toxic Content Exploration Authors: Shaola Ren, Li Ke, Longtao Huang, Dehong Gao, Hui Xue Abstract: Automatically extracting effective queries is challenging in information retrieval, especially in toxic content exploration, as such content is likely to be disguised. With the recent achievements in generative Large Language Model (LLM), we are able to leverage the capabilities of LLMs to extract effective queries for similar content exploration directly. This study proposes QExplorer, an approac\u2026 \u25bd More Automatically extracting effective queries is challenging in information retrieval, especially in toxic content exploration, as such content is likely to be disguised. With the recent achievements in generative Large Language Model (LLM), we are able to leverage the capabilities of LLMs to extract effective queries for similar content exploration directly. This study proposes QExplorer, an approach of large language model based Query Extraction for toxic content Exploration. The QExplorer approach involves a 2-stage training process: instruction Supervised FineTuning (SFT) and preference alignment using Direct Preference Optimization (DPO), as well as the datasets construction with feedback of search system. To verify the effectiveness of QExplorer, a series of offline and online experiments are conducted on our real-world system. The offline empirical results demonstrate that the performance of our automatic query extraction outperforms that of several LLMs and humans. The online deployment shows a significant increase in the detection of toxic items. \u25b3 Less Submitted 6 February, 2025; originally announced February 2025."
  },
  {
    "url": "https://arxiv.org/abs/2503.14649",
    "title": "Computer Science > Information Retrieval",
    "author": "Authors:Wenqi Jiang, Suvinay Subramanian, Cat Graves, Gustavo Alonso, Amir Yazdanbakhsh, Vidushi Dadu",
    "body": "Help | Advanced Search arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. arXiv Operational Status Get status notifications via email or slack"
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Suleiman,+B",
    "title": "Showing 1\u201315 of 15 results for author: Suleiman, B",
    "author": "Authors:\nGhadir Alselwi, \n      \n      Hao Xue, \n      \n      Shoaib Jameel, \n      \n      Basem Suleiman, \n      \n      Flora D. Salim, \n      \n      Imran Razzak",
    "body": "arXiv:2503.14800 [pdf, other] Long Context Modeling with Ranked Memory-Augmented Retrieval Authors: Ghadir Alselwi, Hao Xue, Shoaib Jameel, Basem Suleiman, Flora D. Salim, Imran Razzak Abstract: Effective long-term memory management is crucial for language models handling extended contexts. We introduce a novel framework that dynamically ranks memory entries based on relevance. Unlike previous works, our model introduces a novel relevance scoring and a pointwise re-ranking model for key-value embeddings, inspired by learning-to-rank techniques in information retrieval. Enhanced Ranked Mem\u2026 \u25bd More Effective long-term memory management is crucial for language models handling extended contexts. We introduce a novel framework that dynamically ranks memory entries based on relevance. Unlike previous works, our model introduces a novel relevance scoring and a pointwise re-ranking model for key-value embeddings, inspired by learning-to-rank techniques in information retrieval. Enhanced Ranked Memory Augmented Retrieval ERMAR achieves state-of-the-art results on standard benchmarks. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. arXiv:2501.06827 [pdf, other] Leveraging Taxonomy and LLMs for Improved Multimodal Hierarchical Classification Authors: Shijing Chen, Mohamed Reda Bouadjenek, Shoaib Jameel, Usman Naseem, Basem Suleiman, Flora D. Salim, Hakim Hacid, Imran Razzak Abstract: Multi-level Hierarchical Classification (MLHC) tackles the challenge of categorizing items within a complex, multi-layered class structure. However, traditional MLHC classifiers often rely on a backbone model with independent output layers, which tend to ignore the hierarchical relationships between classes. This oversight can lead to inconsistent predictions that violate the underlying taxonomy.\u2026 \u25bd More Multi-level Hierarchical Classification (MLHC) tackles the challenge of categorizing items within a complex, multi-layered class structure. However, traditional MLHC classifiers often rely on a backbone model with independent output layers, which tend to ignore the hierarchical relationships between classes. This oversight can lead to inconsistent predictions that violate the underlying taxonomy. Leveraging Large Language Models (LLMs), we propose a novel taxonomy-embedded transitional LLM-agnostic framework for multimodality classification. The cornerstone of this advancement is the ability of models to enforce consistency across hierarchical levels. Our evaluations on the MEP-3M dataset - a multi-modal e-commerce product dataset with various hierarchical levels - demonstrated a significant performance improvement compared to conventional LLM structures. \u25b3 Less Submitted 12 January, 2025; originally announced January 2025. Comments: 11 pages, 7 figures, 2 tables, and accepted by COLING 2025 arXiv:2407.07922 [pdf, other] Vulnerability Detection in Smart Contracts: A Comprehensive Survey Authors: Christopher De Baets, Basem Suleiman, Armin Chitizadeh, Imran Razzak Abstract: In the growing field of blockchain technology, smart contracts exist as transformative digital agreements that execute transactions autonomously in decentralised networks. However, these contracts face challenges in the form of security vulnerabilities, posing significant financial and operational risks. While traditional methods to detect and mitigate vulnerabilities in smart contracts are limite\u2026 \u25bd More In the growing field of blockchain technology, smart contracts exist as transformative digital agreements that execute transactions autonomously in decentralised networks. However, these contracts face challenges in the form of security vulnerabilities, posing significant financial and operational risks. While traditional methods to detect and mitigate vulnerabilities in smart contracts are limited due to a lack of comprehensiveness and effectiveness, integrating advanced machine learning technologies presents an attractive approach to increasing effective vulnerability countermeasures. We endeavour to fill an important gap in the existing literature by conducting a rigorous systematic review, exploring the intersection between machine learning and smart contracts. Specifically, the study examines the potential of machine learning techniques to improve the detection and mitigation of vulnerabilities in smart contracts. We analysed 88 articles published between 2018 and 2023 from the following databases: IEEE, ACM, ScienceDirect, Scopus, and Google Scholar. The findings reveal that classical machine learning techniques, including KNN, RF, DT, XG-Boost, and SVM, outperform static tools in vulnerability detection. Moreover, multi-model approaches integrating deep learning and classical machine learning show significant improvements in precision and recall, while hybrid models employing various techniques achieve near-perfect performance in vulnerability detection accuracy. By integrating state-of-the-art solutions, this work synthesises current methods, thoroughly investigates research gaps, and suggests directions for future studies. The insights gathered from this study are intended to serve as a seminal reference for academics, industry experts, and bodies interested in leveraging machine learning to enhance smart contract security. \u25b3 Less Submitted 8 July, 2024; originally announced July 2024. arXiv:2406.06340 [pdf, other] Optimisation of federated learning settings under statistical heterogeneity variations Authors: Basem Suleiman, Muhammad Johan Alibasa, Rizka Widyarini Purwanto, Lewis Jeffries, Ali Anaissi, Jacky Song Abstract: Federated Learning (FL) enables local devices to collaboratively learn a shared predictive model by only periodically sharing model parameters with a central aggregator. However, FL can be disadvantaged by statistical heterogeneity produced by the diversity in each local devices data distribution, which creates different levels of Independent and Identically Distributed (IID) data. Furthermore, th\u2026 \u25bd More Federated Learning (FL) enables local devices to collaboratively learn a shared predictive model by only periodically sharing model parameters with a central aggregator. However, FL can be disadvantaged by statistical heterogeneity produced by the diversity in each local devices data distribution, which creates different levels of Independent and Identically Distributed (IID) data. Furthermore, this can be more complex when optimising different combinations of FL parameters and choosing optimal aggregation. In this paper, we present an empirical analysis of different FL training parameters and aggregators over various levels of statistical heterogeneity on three datasets. We propose a systematic data partition strategy to simulate different levels of statistical heterogeneity and a metric to measure the level of IID. Additionally, we empirically identify the best FL model and key parameters for datasets of different characteristics. On the basis of these, we present recommended guidelines for FL parameters and aggregators to optimise model performance under different levels of IID and with different datasets \u25b3 Less Submitted 10 June, 2024; originally announced June 2024. Comments: 27 pages, 17 figures arXiv:2403.09705 [pdf, other] A Novel Nuanced Conversation Evaluation Framework for Large Language Models in Mental Health Authors: Alexander Marrapese, Basem Suleiman, Imdad Ullah, Juno Kim Abstract: Understanding the conversation abilities of Large Language Models (LLMs) can help lead to its more cautious and appropriate deployment. This is especially important for safety-critical domains like mental health, where someone's life may depend on the exact wording of a response to an urgent question. In this paper, we propose a novel framework for evaluating the nuanced conversation abilities of\u2026 \u25bd More Understanding the conversation abilities of Large Language Models (LLMs) can help lead to its more cautious and appropriate deployment. This is especially important for safety-critical domains like mental health, where someone's life may depend on the exact wording of a response to an urgent question. In this paper, we propose a novel framework for evaluating the nuanced conversation abilities of LLMs. Within it, we develop a series of quantitative metrics developed from literature on using psychotherapy conversation analysis literature. While we ensure that our framework and metrics are transferable by researchers to relevant adjacent domains, we apply them to the mental health field. We use our framework to evaluate several popular frontier LLMs, including some GPT and Llama models, through a verified mental health dataset. Our results show that GPT4 Turbo can perform significantly more similarly to verified therapists than other selected LLMs. We conduct additional analysis to examine how LLM conversation performance varies across specific mental health topics. Our results indicate that GPT4 Turbo performs well in achieving high correlation with verified therapists in particular topics such as Parenting and Relationships. We believe our contributions will help researchers develop better LLMs that, in turn, will more positively support people's lives. \u25b3 Less Submitted 8 March, 2024; originally announced March 2024. arXiv:2312.01020 [pdf, other] ResNLS: An Improved Model for Stock Price Forecasting Authors: Yuanzhe Jia, Ali Anaissi, Basem Suleiman Abstract: Stock prices forecasting has always been a challenging task. Although many research projects adopt machine learning and deep learning algorithms to address the problem, few of them pay attention to the varying degrees of dependencies between stock prices. In this paper we introduce a hybrid model that improves stock price prediction by emphasizing the dependencies between adjacent stock prices. Th\u2026 \u25bd More Stock prices forecasting has always been a challenging task. Although many research projects adopt machine learning and deep learning algorithms to address the problem, few of them pay attention to the varying degrees of dependencies between stock prices. In this paper we introduce a hybrid model that improves stock price prediction by emphasizing the dependencies between adjacent stock prices. The proposed model, ResNLS, is mainly composed of two neural architectures, ResNet and LSTM. ResNet serves as a feature extractor to identify dependencies between stock prices across time windows, while LSTM analyses the initial time-series data with the combination of dependencies which considered as residuals. In predicting the SSE Composite Index, our experiment reveals that when the closing price data for the previous 5 consecutive trading days is used as the input, the performance of the model (ResNLS-5) is optimal compared to those with other inputs. Furthermore, ResNLS-5 outperforms vanilla CNN, RNN, LSTM, and BiLSTM models in terms of prediction accuracy. It also demonstrates at least a 20% improvement over the current state-of-the-art baselines. To verify whether ResNLS-5 can help clients effectively avoid risks and earn profits in the stock market, we construct a quantitative trading framework for back testing. The experimental results show that the trading strategy based on predictions from ResNLS-5 can successfully mitigate losses during declining stock prices and generate profits in the periods of rising stock prices. \u25b3 Less Submitted 1 December, 2023; originally announced December 2023. Comments: Published to Computational Intelligence arXiv:2310.12523 [pdf, other] Privacy Preserving Large Language Models: ChatGPT Case Study Based Vision and Framework Authors: Imdad Ullah, Najm Hassan, Sukhpal Singh Gill, Basem Suleiman, Tariq Ahamed Ahanger, Zawar Shah, Junaid Qadir, Salil S. Kanhere Abstract: The generative Artificial Intelligence (AI) tools based on Large Language Models (LLMs) use billions of parameters to extensively analyse large datasets and extract critical private information such as, context, specific details, identifying information etc. This have raised serious threats to user privacy and reluctance to use such tools. This article proposes the conceptual model called PrivChat\u2026 \u25bd More The generative Artificial Intelligence (AI) tools based on Large Language Models (LLMs) use billions of parameters to extensively analyse large datasets and extract critical private information such as, context, specific details, identifying information etc. This have raised serious threats to user privacy and reluctance to use such tools. This article proposes the conceptual model called PrivChatGPT, a privacy-preserving model for LLMs that consists of two main components i.e., preserving user privacy during the data curation/pre-processing together with preserving private context and the private training process for large-scale data. To demonstrate its applicability, we show how a private mechanism could be integrated into the existing model for training LLMs to protect user privacy; specifically, we employed differential privacy and private training using Reinforcement Learning (RL). We measure the privacy loss and evaluate the measure of uncertainty or randomness once differential privacy is applied. It further recursively evaluates the level of privacy guarantees and the measure of uncertainty of public database and resources, during each update when new information is added for training purposes. To critically evaluate the use of differential privacy for private LLMs, we hypothetically compared other mechanisms e..g, Blockchain, private information retrieval, randomisation, for various performance measures such as the model performance and accuracy, computational complexity, privacy vs. utility etc. We conclude that differential privacy, randomisation, and obfuscation can impact utility and performance of trained models, conversely, the use of ToR, Blockchain, and PIR may introduce additional computational complexity and high training latency. We believe that the proposed model could be used as a benchmark for proposing privacy preserving LLMs for generative AI tools. \u25b3 Less Submitted 19 October, 2023; originally announced October 2023. arXiv:2203.12200 [pdf, other] Privacy-Preserving Personalized Fitness Recommender System (P3FitRec): A Multi-level Deep Learning Approach Authors: Xiao Liu, Bonan Gao, Basem Suleiman, Han You, Zisu Ma, Yu Liu, Ali Anaissi Abstract: Recommender systems have been successfully used in many domains with the help of machine learning algorithms. However, such applications tend to use multi-dimensional user data, which has raised widespread concerns about the breach of users privacy. Meanwhile, wearable technologies have enabled users to collect fitness-related data through embedded sensors to monitor their conditions or achieve pe\u2026 \u25bd More Recommender systems have been successfully used in many domains with the help of machine learning algorithms. However, such applications tend to use multi-dimensional user data, which has raised widespread concerns about the breach of users privacy. Meanwhile, wearable technologies have enabled users to collect fitness-related data through embedded sensors to monitor their conditions or achieve personalized fitness goals. In this paper, we propose a novel privacy-aware personalized fitness recommender system. We introduce a multi-level deep learning framework that learns important features from a large-scale real fitness dataset that is collected from wearable IoT devices to derive intelligent fitness recommendations. Unlike most existing approaches, our approach achieves personalization by inferring the fitness characteristics of users from sensory data and thus minimizing the need for explicitly collecting user identity or biometric information, such as name, age, height, weight. In particular, our proposed models and algorithms predict (a) personalized exercise distance recommendations to help users to achieve target calories, (b) personalized speed sequence recommendations to adjust exercise speed given the nature of the exercise and the chosen route, and (c) personalized heart rate sequence to guide the user of the potential health status for future exercises. Our experimental evaluation on a real-world Fitbit dataset demonstrated high accuracy in predicting exercise distance, speed sequence, and heart rate sequence compared to similar studies. Furthermore, our approach is novel compared to existing studies as it does not require collecting and using users sensitive information, and thus it preserves the users privacy. \u25b3 Less Submitted 23 March, 2022; originally announced March 2022. Comments: 30 pages, 16 figures, 36 references arXiv:2111.02632 [pdf, other] A Fast Parallel Tensor Decomposition with Optimal Stochastic Gradient Descent: an Application in Structural Damage Identification Authors: Ali Anaissi, Basem Suleiman, Seid Miad Zandavi Abstract: Structural Health Monitoring (SHM) provides an economic approach which aims to enhance understanding the behavior of structures by continuously collects data through multiple networked sensors attached to the structure. This data is then utilized to gain insight into the health of a structure and make timely and economic decisions about its maintenance. The generated SHM sensing data is non-statio\u2026 \u25bd More Structural Health Monitoring (SHM) provides an economic approach which aims to enhance understanding the behavior of structures by continuously collects data through multiple networked sensors attached to the structure. This data is then utilized to gain insight into the health of a structure and make timely and economic decisions about its maintenance. The generated SHM sensing data is non-stationary and exists in a correlated multi-way form which makes the batch/off-line learning and standard two-way matrix analysis unable to capture all of these correlations and relationships. In this sense, the online tensor data analysis has become an essential tool for capturing underlying structures in higher-order datasets stored in a tensor $\\mathcal{X} \\in \\mathbb{R} ^{I_1 \\times \\dots \\times I_N} $. The CANDECOMP/PARAFAC (CP) decomposition has been extensively studied and applied to approximate X by N loading matrices A(1), . . . ,A(N) where N represents the order of the tensor. We propose a novel algorithm, FP-CPD, to parallelize the CANDECOMP/PARAFAC (CP) decomposition of a tensor $\\mathcal{X} \\in \\mathbb{R} ^{I_1 \\times \\dots \\times I_N} $. Our approach is based on stochastic gradient descent (SGD) algorithm which allows us to parallelize the learning process and it is very useful in online setting since it updates $\\mathcal{X}^{t+1}$ in one single step. Our SGD algorithm is augmented with Nesterov's Accelerated Gradient (NAG) and perturbation methods to accelerate and guarantee convergence. The experimental results using laboratory-based and real-life structural datasets indicate fast convergence and good scalability. \u25b3 Less Submitted 4 November, 2021; originally announced November 2021. arXiv:2111.02627 [pdf, other] A Personalized Federated Learning Algorithm: an Application in Anomaly Detection Authors: Ali Anaissi, Basem Suleiman Abstract: Federated Learning (FL) has recently emerged as a promising method that employs a distributed learning model structure to overcome data privacy and transmission issues paused by central machine learning models. In FL, datasets collected from different devices or sensors are used to train local models (clients) each of which shares its learning with a centralized model (server). However, this distr\u2026 \u25bd More Federated Learning (FL) has recently emerged as a promising method that employs a distributed learning model structure to overcome data privacy and transmission issues paused by central machine learning models. In FL, datasets collected from different devices or sensors are used to train local models (clients) each of which shares its learning with a centralized model (server). However, this distributed learning approach presents unique learning challenges as the data used at local clients can be non-IID (Independent and Identically Distributed) and statistically diverse which decrease learning accuracy in the central model. In this paper, we overcome this problem by proposing a novel Personalized Conditional FedAvg (PC-FedAvg) which aims to control weights communication and aggregation augmented with a tailored learning algorithm to personalize the resulting models at each client. Our experimental validation on two datasets showed that our PC-FedAvg precisely constructed generalized clients' models and thus achieved higher accuracy compared to other state-of-the-art methods. \u25b3 Less Submitted 4 November, 2021; originally announced November 2021. arXiv:2107.09373 [pdf, other] Image-Hashing-Based Anomaly Detection for Privacy-Preserving Online Proctoring Authors: Waheeb Yaqub, Manoranjan Mohanty, Basem Suleiman Abstract: Online proctoring has become a necessity in online teaching. Video-based crowd-sourced online proctoring solutions are being used, where an exam-taking student's video is monitored by third parties, leading to privacy concerns. In this paper, we propose a privacy-preserving online proctoring system. The proposed image-hashing-based system can detect the student's excessive face and body movement (\u2026 \u25bd More Online proctoring has become a necessity in online teaching. Video-based crowd-sourced online proctoring solutions are being used, where an exam-taking student's video is monitored by third parties, leading to privacy concerns. In this paper, we propose a privacy-preserving online proctoring system. The proposed image-hashing-based system can detect the student's excessive face and body movement (i.e., anomalies) that is resulted when the student tries to cheat in the exam. The detection can be done even if the student's face is blurred or masked in video frames. Experiment with an in-house dataset shows the usability of the proposed system. \u25b3 Less Submitted 20 July, 2021; originally announced July 2021. arXiv:2107.05016 [pdf, other] Combating fake news by empowering fact-checked news spread via topology-based interventions Authors: Ke Wang, Waheeb Yaqub, Abdallah Lakhdari, Basem Suleiman Abstract: Rapid information diffusion and large-scaled information cascades can enable the undesired spread of false information. A small-scaled false information outbreak may potentially lead to an infodemic. We propose a novel information diffusion and intervention technique to combat the spread of false news. As false information is often spreading faster in a social network, the proposed diffusion metho\u2026 \u25bd More Rapid information diffusion and large-scaled information cascades can enable the undesired spread of false information. A small-scaled false information outbreak may potentially lead to an infodemic. We propose a novel information diffusion and intervention technique to combat the spread of false news. As false information is often spreading faster in a social network, the proposed diffusion methodology inhibits the spread of false news by proactively diffusing the fact-checked information. Our methodology mainly relies on defining the potential super-spreaders in a social network based on their centrality metrics. We run an extensive set of experiments on different networks to investigate the impact of centrality metrics on the performance of the proposed diffusion and intervention models. The obtained results demonstrate that empowering the diffusion of fact-checked news combats the spread of false news further and deeper in social networks. \u25b3 Less Submitted 11 July, 2021; originally announced July 2021. arXiv:2011.06771 [pdf, other] Elastic Composition of Crowdsourced IoT Energy Services Authors: Abdallah Lakhdari, Athman Bouguettaya, Sajib Mistry, Azadeh Ghari Neiat, Basem Suleiman Abstract: We propose a novel type of service composition, called elastic composition which provides a reliable framework in a highly fluctuating IoT energy provisioning settings. We rely on crowdsourcing IoT energy (e.g., wearables) to provide wireless energy to nearby devices. We introduce the concepts of soft deadline and hard deadline as key criteria to cater for an elastic composition framework. We cond\u2026 \u25bd More We propose a novel type of service composition, called elastic composition which provides a reliable framework in a highly fluctuating IoT energy provisioning settings. We rely on crowdsourcing IoT energy (e.g., wearables) to provide wireless energy to nearby devices. We introduce the concepts of soft deadline and hard deadline as key criteria to cater for an elastic composition framework. We conduct a set of experiments on real-world datasets to assess the efficiency of the proposed approach. \u25b3 Less Submitted 13 November, 2020; originally announced November 2020. Comments: 10 pages, 8 Figures. This paper is accepted in the 2020 EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services (EAI MobiQuitous 2020) arXiv:2003.08844 [pdf, other] NeCPD: An Online Tensor Decomposition with Optimal Stochastic Gradient Descent Authors: Ali Anaissi, Basem Suleiman, Seid Miad Zandavi Abstract: Multi-way data analysis has become an essential tool for capturing underlying structures in higher-order datasets stored in tensor $\\mathcal{X} \\in \\mathbb{R} ^{I_1 \\times \\dots \\times I_N} $. $CANDECOMP/PARAFAC$ (CP) decomposition has been extensively studied and applied to approximate $\\mathcal{X}$ by $N$ loading matrices $A^{(1)}, \\dots, A^{(N)}$ where $N$ represents the order of the tensor. We\u2026 \u25bd More Multi-way data analysis has become an essential tool for capturing underlying structures in higher-order datasets stored in tensor $\\mathcal{X} \\in \\mathbb{R} ^{I_1 \\times \\dots \\times I_N} $. $CANDECOMP/PARAFAC$ (CP) decomposition has been extensively studied and applied to approximate $\\mathcal{X}$ by $N$ loading matrices $A^{(1)}, \\dots, A^{(N)}$ where $N$ represents the order of the tensor. We propose a new efficient CP decomposition solver named NeCPD for non-convex problem in multi-way online data based on stochastic gradient descent (SGD) algorithm. SGD is very useful in online setting since it allows us to update $\\mathcal{X}^{(t+1)}$ in one single step. In terms of global convergence, it is well known that SGD stuck in many saddle points when it deals with non-convex problems. We study the Hessian matrix to identify theses saddle points, and then try to escape them using the perturbation approach which adds little noise to the gradient update step. We further apply Nesterov's Accelerated Gradient (NAG) method in SGD algorithm to optimally accelerate the convergence rate and compensate Hessian computational delay time per epoch. Experimental evaluation in the field of structural health monitoring using laboratory-based and real-life structural datasets show that our method provides more accurate results compared with existing online tensor analysis methods. \u25b3 Less Submitted 18 March, 2020; originally announced March 2020. Comments: arXiv admin note: text overlap with arXiv:2003.04497 arXiv:2003.04497 [pdf, other] Online Tensor-Based Learning for Multi-Way Data Authors: Ali Anaissi, Basem Suleiman, Seid Miad Zandavi Abstract: The online analysis of multi-way data stored in a tensor $\\mathcal{X} \\in \\mathbb{R} ^{I_1 \\times \\dots \\times I_N} $ has become an essential tool for capturing the underlying structures and extracting the sensitive features which can be used to learn a predictive model. However, data distributions often evolve with time and a current predictive model may not be sufficiently representative in the\u2026 \u25bd More The online analysis of multi-way data stored in a tensor $\\mathcal{X} \\in \\mathbb{R} ^{I_1 \\times \\dots \\times I_N} $ has become an essential tool for capturing the underlying structures and extracting the sensitive features which can be used to learn a predictive model. However, data distributions often evolve with time and a current predictive model may not be sufficiently representative in the future. Therefore, incrementally updating the tensor-based features and model coefficients are required in such situations. A new efficient tensor-based feature extraction, named NeSGD, is proposed for online $CANDECOMP/PARAFAC$ (CP) decomposition. According to the new features obtained from the resultant matrices of NeSGD, a new criteria is triggered for the updated process of the online predictive model. Experimental evaluation in the field of structural health monitoring using laboratory-based and real-life structural datasets show that our methods provide more accurate results compared with existing online tensor analysis and model learning. The results showed that the proposed methods significantly improved the classification error rates, were able to assimilate the changes in the positive data distribution over time, and maintained a high predictive accuracy in all case studies. \u25b3 Less Submitted 9 March, 2020; originally announced March 2020."
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Salim,+F+D",
    "title": "Showing 1\u201350 of 107 results for author: Salim, F D",
    "author": "Authors:\nGhadir Alselwi, \n      \n      Hao Xue, \n      \n      Shoaib Jameel, \n      \n      Basem Suleiman, \n      \n      Flora D. Salim, \n      \n      Imran Razzak",
    "body": "arXiv:2503.14800 [pdf, other] Long Context Modeling with Ranked Memory-Augmented Retrieval Authors: Ghadir Alselwi, Hao Xue, Shoaib Jameel, Basem Suleiman, Flora D. Salim, Imran Razzak Abstract: Effective long-term memory management is crucial for language models handling extended contexts. We introduce a novel framework that dynamically ranks memory entries based on relevance. Unlike previous works, our model introduces a novel relevance scoring and a pointwise re-ranking model for key-value embeddings, inspired by learning-to-rank techniques in information retrieval. Enhanced Ranked Mem\u2026 \u25bd More Effective long-term memory management is crucial for language models handling extended contexts. We introduce a novel framework that dynamically ranks memory entries based on relevance. Unlike previous works, our model introduces a novel relevance scoring and a pointwise re-ranking model for key-value embeddings, inspired by learning-to-rank techniques in information retrieval. Enhanced Ranked Memory Augmented Retrieval ERMAR achieves state-of-the-art results on standard benchmarks. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. arXiv:2503.14234 [pdf, other] KG-IRAG: A Knowledge Graph-Based Iterative Retrieval-Augmented Generation Framework for Temporal Reasoning Authors: Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D. Salim Abstract: Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective in enhancing the performance of Large Language Models (LLMs) on tasks that require external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG improves information retrieval for complex reasoning tasks, providing more precise and comprehensive retrieval and generating more accurate responses to QAs. However, most RA\u2026 \u25bd More Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective in enhancing the performance of Large Language Models (LLMs) on tasks that require external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG improves information retrieval for complex reasoning tasks, providing more precise and comprehensive retrieval and generating more accurate responses to QAs. However, most RAG methods fall short in addressing multi-step reasoning, particularly when both information extraction and inference are necessary. To address this limitation, this paper presents Knowledge Graph-Based Iterative Retrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs with iterative reasoning to improve LLMs' ability to handle queries involving temporal and logical dependencies. Through iterative retrieval steps, KG-IRAG incrementally gathers relevant data from external KGs, enabling step-by-step reasoning. The proposed approach is particularly suited for scenarios where reasoning is required alongside dynamic temporal data extraction, such as determining optimal travel times based on weather conditions or traffic patterns. Experimental results show that KG-IRAG improves accuracy in complex reasoning tasks by effectively integrating external knowledge with iterative, logic-based retrieval. Additionally, three new datasets: weatherQA-Irish, weatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's performance, demonstrating its potential beyond traditional RAG applications. \u25b3 Less Submitted 19 March, 2025; v1 submitted 18 March, 2025; originally announced March 2025. Comments: 14 pages, 4 figures arXiv:2502.16755 [pdf, other] Watch Out E-scooter Coming Through: Multimodal Sensing of Mixed Traffic Use and Conflicts Through Riders Ego-centric Views Authors: Hiruni Nuwanthika Kegalle, Danula Hettiachchi, Jeffrey Chan, Mark Sanderson, Flora D. Salim Abstract: E-scooters are becoming a popular means of urban transportation. However, this increased popularity brings challenges, such as road accidents and conflicts when sharing space with traditional transport modes. An in-depth understanding of e-scooter rider behaviour is crucial for ensuring rider safety, guiding infrastructure planning, and enforcing traffic rules. This study investigated the rider be\u2026 \u25bd More E-scooters are becoming a popular means of urban transportation. However, this increased popularity brings challenges, such as road accidents and conflicts when sharing space with traditional transport modes. An in-depth understanding of e-scooter rider behaviour is crucial for ensuring rider safety, guiding infrastructure planning, and enforcing traffic rules. This study investigated the rider behaviour through a naturalistic study with 23 participants equipped with a bike computer, eye-tracking glasses and cameras. They followed a pre-determined route, enabling multi-modal data collection. We analysed and compared gaze movements, speed, and video feeds across three transport infrastructure types: a pedestrian-shared path, a cycle lane and a roadway. Our findings reveal unique challenges e-scooter riders face, including difficulty keeping up with cyclists and motor vehicles due to speed limits on shared e-scooters, risks in signalling turns due to control lose, and limited acceptance in mixed-use spaces. The cycle lane showed the highest average speed, the least speed change points, and the least head movements, supporting its suitability as dedicated infrastructure for e-scooters. These findings are facilitated through multimodal sensing and analysing the e-scooter riders' ego-centric view, which show the efficacy of our method in discovering the behavioural dynamics of the riders in the wild. Our study highlights the critical need to align infrastructure with user behaviour to improve safety and emphasises the importance of targeted safety measures and regulations, especially when e-scooter riders share spaces with pedestrians or motor vehicles. The dataset and analysis code are available at https://github.com/HiruniNuwanthika/Electric-Scooter-Riders-Multi-Modal-Data-Analysis.git. \u25b3 Less Submitted 23 February, 2025; originally announced February 2025. Comments: Accepted in Proc. ACM Interactive, Mobile, Wearable and Ubiquitous Technologies,(March 2025), 23 pages. https://doi.org/10.1145/3712284 arXiv:2502.12207 [pdf, other] PAR-AdvGAN: Improving Adversarial Attack Capability with Progressive Auto-Regression AdvGAN Authors: Jiayu Zhang, Zhiyu Zhu, Xinyi Wang, Silin Liao, Zhibo Jin, Flora D. Salim, Huaming Chen Abstract: Deep neural networks have demonstrated remarkable performance across various domains. However, they are vulnerable to adversarial examples, which can lead to erroneous predictions. Generative Adversarial Networks (GANs) can leverage the generators and discriminators model to quickly produce high-quality adversarial examples. Since both modules train in a competitive and simultaneous manner, GAN-ba\u2026 \u25bd More Deep neural networks have demonstrated remarkable performance across various domains. However, they are vulnerable to adversarial examples, which can lead to erroneous predictions. Generative Adversarial Networks (GANs) can leverage the generators and discriminators model to quickly produce high-quality adversarial examples. Since both modules train in a competitive and simultaneous manner, GAN-based algorithms like AdvGAN can generate adversarial examples with better transferability compared to traditional methods. However, the generation of perturbations is usually limited to a single iteration, preventing these examples from fully exploiting the potential of the methods. To tackle this issue, we introduce a novel approach named Progressive Auto-Regression AdvGAN (PAR-AdvGAN). It incorporates an auto-regressive iteration mechanism within a progressive generation network to craft adversarial examples with enhanced attack capability. We thoroughly evaluate our PAR-AdvGAN method with a large-scale experiment, demonstrating its superior performance over various state-of-the-art black-box adversarial attacks, as well as the original AdvGAN.Moreover, PAR-AdvGAN significantly accelerates the adversarial example generation, i.e., achieving the speeds of up to 335.5 frames per second on Inception-v3 model, outperforming the gradient-based transferable attack algorithms. Our code is available at: https://anonymous.4open.science/r/PAR-01BF/ \u25b3 Less Submitted 16 February, 2025; originally announced February 2025. arXiv:2502.11681 [pdf, other] RIDE: Enhancing Large Language Model Alignment through Restyled In-Context Learning Demonstration Exemplars Authors: Yuncheng Hua, Lizhen Qu, Zhuang Li, Hao Xue, Flora D. Salim, Gholamreza Haffari Abstract: Alignment tuning is crucial for ensuring large language models (LLMs) behave ethically and helpfully. Current alignment approaches require high-quality annotations and significant training resources. This paper proposes a low-cost, tuning-free method using in-context learning (ICL) to enhance LLM alignment. Through an analysis of high-quality ICL demos, we identified style as a key factor influenc\u2026 \u25bd More Alignment tuning is crucial for ensuring large language models (LLMs) behave ethically and helpfully. Current alignment approaches require high-quality annotations and significant training resources. This paper proposes a low-cost, tuning-free method using in-context learning (ICL) to enhance LLM alignment. Through an analysis of high-quality ICL demos, we identified style as a key factor influencing LLM alignment capabilities and explicitly restyled ICL exemplars based on this stylistic framework. Additionally, we combined the restyled demos to achieve a balance between the two conflicting aspects of LLM alignment--factuality and safety. We packaged the restyled examples as prompts to trigger few-shot learning, improving LLM alignment. Compared to the best baseline approach, with an average score of 5.00 as the maximum, our method achieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22 enhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum improvement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the code and data at https://github.com/AnonymousCode-ComputerScience/RIDE. \u25b3 Less Submitted 5 March, 2025; v1 submitted 17 February, 2025; originally announced February 2025. Comments: 38 pages, 2 figures, 20 tables; The paper is under review in ARR ACM Class: I.2.7 arXiv:2502.01325 [pdf, other] The Homework Wars: Exploring Emotions, Behaviours, and Conflicts in Parent-Child Homework Interactions Authors: Nan Gao, Yibin Liu, Xin Tang, Yanyan Liu, Chun Yu, Yun Huang, Yuntao Wang, Flora D. Salim, Xuhai Orson Xu, Jun Wei, Yuanchun Shi Abstract: Parental involvement in homework is a crucial aspect of family education, but it often leads to emotional strain and conflicts that can severely impact family well-being. This paper presents findings from a 4-week in situ study involving 78 families in China, where we collected and analyzed 602 valid audio recordings (totalling 475 hours) and daily surveys. Leveraging large language models (LLMs)\u2026 \u25bd More Parental involvement in homework is a crucial aspect of family education, but it often leads to emotional strain and conflicts that can severely impact family well-being. This paper presents findings from a 4-week in situ study involving 78 families in China, where we collected and analyzed 602 valid audio recordings (totalling 475 hours) and daily surveys. Leveraging large language models (LLMs) to analyze parent-child conversations, we gained a nuanced understanding of emotional and behavioural dynamics that overcomes the limitations of traditional one-time surveys and interviews. Our findings reveal significant emotional shifts in parents before and after homework involvement and summarise a range of positive, neutral and negative parental behaviours. We also catalogue seven common conflicts, with Knowledge Conflict being the most frequent. Notably, we found that even well-intentioned parental behaviours, such as Unlabelled Praise, were significantly positively correlated with specific conflict types. This work advances ubiquitous computing's research to sense and understand complex family dynamics, while offering evidence-based insights for designing future ambient intelligent systems to support healthy family education environments. \u25b3 Less Submitted 4 February, 2025; v1 submitted 3 February, 2025; originally announced February 2025. arXiv:2501.06827 [pdf, other] Leveraging Taxonomy and LLMs for Improved Multimodal Hierarchical Classification Authors: Shijing Chen, Mohamed Reda Bouadjenek, Shoaib Jameel, Usman Naseem, Basem Suleiman, Flora D. Salim, Hakim Hacid, Imran Razzak Abstract: Multi-level Hierarchical Classification (MLHC) tackles the challenge of categorizing items within a complex, multi-layered class structure. However, traditional MLHC classifiers often rely on a backbone model with independent output layers, which tend to ignore the hierarchical relationships between classes. This oversight can lead to inconsistent predictions that violate the underlying taxonomy.\u2026 \u25bd More Multi-level Hierarchical Classification (MLHC) tackles the challenge of categorizing items within a complex, multi-layered class structure. However, traditional MLHC classifiers often rely on a backbone model with independent output layers, which tend to ignore the hierarchical relationships between classes. This oversight can lead to inconsistent predictions that violate the underlying taxonomy. Leveraging Large Language Models (LLMs), we propose a novel taxonomy-embedded transitional LLM-agnostic framework for multimodality classification. The cornerstone of this advancement is the ability of models to enforce consistency across hierarchical levels. Our evaluations on the MEP-3M dataset - a multi-modal e-commerce product dataset with various hierarchical levels - demonstrated a significant performance improvement compared to conventional LLM structures. \u25b3 Less Submitted 12 January, 2025; originally announced January 2025. Comments: 11 pages, 7 figures, 2 tables, and accepted by COLING 2025 arXiv:2412.14175 [pdf, other] BiTSA: Leveraging Time Series Foundation Model for Building Energy Analytics Authors: Xiachong Lin, Arian Prabowo, Imran Razzak, Hao Xue, Matthew Amos, Sam Behrens, Flora D. Salim Abstract: Incorporating AI technologies into digital infrastructure offers transformative potential for energy management, particularly in enhancing energy efficiency and supporting net-zero objectives. However, the complexity of IoT-generated datasets often poses a significant challenge, hindering the translation of research insights into practical, real-world applications. This paper presents the design o\u2026 \u25bd More Incorporating AI technologies into digital infrastructure offers transformative potential for energy management, particularly in enhancing energy efficiency and supporting net-zero objectives. However, the complexity of IoT-generated datasets often poses a significant challenge, hindering the translation of research insights into practical, real-world applications. This paper presents the design of an interactive visualization tool, BiTSA. The tool enables building managers to interpret complex energy data quickly and take immediate, data-driven actions based on real-time insights. By integrating advanced forecasting models with an intuitive visual interface, our solution facilitates proactive decision-making, optimizes energy consumption, and promotes sustainable building management practices. BiTSA will empower building managers to optimize energy consumption, control demand-side energy usage, and achieve sustainability goals. \u25b3 Less Submitted 20 November, 2024; originally announced December 2024. Comments: 4 pages, 4 figures, 3 tables arXiv:2412.12589 [pdf, ps, other] Round and Communication Efficient Graph Coloring Authors: Yi-Jun Chang, Gopinath Mishra, Hung Thuan Nguyen, Farrel D Salim Abstract: In the context of communication complexity, we explore randomized protocols for graph coloring, focusing specifically on the vertex and edge coloring problems in $n$-vertex graphs $G$ with a maximum degree $\u0394$. We consider a scenario where the edges of $G$ are partitioned between two players. Our first contribution is a randomized protocol that efficiently finds a $(\u0394+ 1)$-vertex coloring of $G$,\u2026 \u25bd More In the context of communication complexity, we explore randomized protocols for graph coloring, focusing specifically on the vertex and edge coloring problems in $n$-vertex graphs $G$ with a maximum degree $\u0394$. We consider a scenario where the edges of $G$ are partitioned between two players. Our first contribution is a randomized protocol that efficiently finds a $(\u0394+ 1)$-vertex coloring of $G$, utilizing $O(n)$ bits of communication in expectation and completing in $O(\\log \\log n \\cdot \\log \u0394)$ rounds in the worst case. This advancement represents a significant improvement over the work of Flin and Mittal [PODC 2024], who achieved the same communication cost but required $O(n)$ rounds in expectation, thereby making a significant reduction in the round complexity. We also present a randomized protocol for a $(2\u0394- 1)$-edge coloring of $G$, which maintains the same $O(n)$ bits of communication in expectation over $O(\\log^\\ast \u0394)$ rounds in the worst case. We complement the result with a tight $\u03a9(n)$-bit lower bound on the communication complexity of the $(2\u0394-1)$-edge coloring, while a similar $\u03a9(n)$ lower bound for the $(\u0394+1)$-vertex coloring has been established by Flin and Mittal [PODC 2024]. \u25b3 Less Submitted 17 December, 2024; originally announced December 2024. arXiv:2411.08888 [pdf, other] Exploring Capabilities of Time Series Foundation Models in Building Analytics Authors: Xiachong Lin, Arian Prabowo, Imran Razzak, Hao Xue, Matthew Amos, Sam Behrens, Flora D. Salim Abstract: The growing integration of digitized infrastructure with Internet of Things (IoT) networks has transformed the management and optimization of building energy consumption. By leveraging IoT-based monitoring systems, stakeholders such as building managers, energy suppliers, and policymakers can make data-driven decisions to improve energy efficiency. However, accurate energy forecasting and analytic\u2026 \u25bd More The growing integration of digitized infrastructure with Internet of Things (IoT) networks has transformed the management and optimization of building energy consumption. By leveraging IoT-based monitoring systems, stakeholders such as building managers, energy suppliers, and policymakers can make data-driven decisions to improve energy efficiency. However, accurate energy forecasting and analytics face persistent challenges, primarily due to the inherent physical constraints of buildings and the diverse, heterogeneous nature of IoT-generated data. In this study, we conduct a comprehensive benchmarking of two publicly available IoT datasets, evaluating the performance of time series foundation models in the context of building energy analytics. Our analysis shows that single-modal models demonstrate significant promise in overcoming the complexities of data variability and physical limitations in buildings, with future work focusing on optimizing multi-modal models for sustainable energy management. \u25b3 Less Submitted 27 October, 2024; originally announced November 2024. Comments: 7 pages, 1 figures, and 4 tables arXiv:2411.07413 [pdf, other] ODEStream: A Buffer-Free Online Learning Framework with ODE-based Adaptor for Streaming Time Series Forecasting Authors: Futoon M. Abushaqra, Hao Xue, Yongli Ren, Flora D. Salim Abstract: Addressing the challenges of irregularity and concept drift in streaming time series is crucial in real-world predictive modelling. Previous studies in time series continual learning often propose models that require buffering of long sequences, potentially restricting the responsiveness of the inference system. Moreover, these models are typically designed for regularly sampled data, an unrealist\u2026 \u25bd More Addressing the challenges of irregularity and concept drift in streaming time series is crucial in real-world predictive modelling. Previous studies in time series continual learning often propose models that require buffering of long sequences, potentially restricting the responsiveness of the inference system. Moreover, these models are typically designed for regularly sampled data, an unrealistic assumption in real-world scenarios. This paper introduces ODEStream, a novel buffer-free continual learning framework that incorporates a temporal isolation layer that integrates temporal dependencies within the data. Simultaneously, it leverages the capability of neural ordinary differential equations to process irregular sequences and generate a continuous data representation, enabling seamless adaptation to changing dynamics in a data streaming scenario. Our approach focuses on learning how the dynamics and distribution of historical data change with time, facilitating the direct processing of streaming sequences. Evaluations on benchmark real-world datasets demonstrate that ODEStream outperforms the state-of-the-art online learning and streaming analysis baselines, providing accurate predictions over extended periods while minimising performance degradation over time by learning how the sequence dynamics change. \u25b3 Less Submitted 11 November, 2024; originally announced November 2024. arXiv:2410.20643 [pdf, other] GenUP: Generative User Profilers as In-Context Learners for Next POI Recommender Systems Authors: Wilson Wongso, Hao Xue, Flora D. Salim Abstract: Traditional Point-of-Interest (POI) recommendation systems often lack transparency, interpretability, and scrutability due to their reliance on dense vector-based user embeddings. Furthermore, the cold-start problem -- where systems have insufficient data for new users -- limits their ability to generate accurate recommendations. Existing methods often address this by leveraging similar trajectori\u2026 \u25bd More Traditional Point-of-Interest (POI) recommendation systems often lack transparency, interpretability, and scrutability due to their reliance on dense vector-based user embeddings. Furthermore, the cold-start problem -- where systems have insufficient data for new users -- limits their ability to generate accurate recommendations. Existing methods often address this by leveraging similar trajectories from other users, but this approach can be computationally expensive and increases the context length for LLM-based methods, making them difficult to scale. To address these limitations, we propose a method that generates natural language (NL) user profiles from large-scale, location-based social network (LBSN) check-ins, utilizing robust personality assessments and behavioral theories. These NL profiles capture user preferences, routines, and behaviors, improving POI prediction accuracy while offering enhanced transparency. By incorporating NL profiles as system prompts to LLMs, our approach reduces reliance on extensive historical data, while remaining flexible, easily updated, and computationally efficient. Our method is not only competitive with other LLM-based and complex agentic frameworks but is also more scalable for real-world POI recommender systems. Results demonstrate that our approach consistently outperforms baseline methods, offering a more interpretable and resource-efficient solution for POI recommendation systems. Our source code is available at: https://github.com/w11wo/GenUP/. \u25b3 Less Submitted 12 March, 2025; v1 submitted 27 October, 2024; originally announced October 2024. arXiv:2410.20067 [pdf, other] Evaluating the Influences of Explanation Style on Human-AI Reliance Authors: Emma Casolin, Flora D. Salim, Ben Newell Abstract: Explainable AI (XAI) aims to support appropriate human-AI reliance by increasing the interpretability of complex model decisions. Despite the proliferation of proposed methods, there is mixed evidence surrounding the effects of different styles of XAI explanations on human-AI reliance. Interpreting these conflicting findings requires an understanding of the individual and combined qualities of dif\u2026 \u25bd More Explainable AI (XAI) aims to support appropriate human-AI reliance by increasing the interpretability of complex model decisions. Despite the proliferation of proposed methods, there is mixed evidence surrounding the effects of different styles of XAI explanations on human-AI reliance. Interpreting these conflicting findings requires an understanding of the individual and combined qualities of different explanation styles that influence appropriate and inappropriate human-AI reliance, and the role of interpretability in this interaction. In this study, we investigate the influences of feature-based, example-based, and combined feature- and example-based XAI methods on human-AI reliance through a two-part experimental study with 274 participants comparing these explanation style conditions. Our findings suggest differences between feature-based and example-based explanation styles beyond interpretability that affect human-AI reliance patterns across differences in individual performance and task complexity. Our work highlights the importance of adapting explanations to their specific users and context over maximising broad interpretability. \u25b3 Less Submitted 26 October, 2024; originally announced October 2024. Comments: 20 pages arXiv:2410.10624 [pdf, other] SensorLLM: Aligning Large Language Models with Motion Sensors for Human Activity Recognition Authors: Zechen Li, Shohreh Deldari, Linyao Chen, Hao Xue, Flora D. Salim Abstract: We introduce SensorLLM, a two-stage framework that enables Large Language Models (LLMs) to perform human activity recognition (HAR) from sensor data. Despite their strong reasoning and generalization capabilities, LLMs remain underutilized for motion sensor data due to the lack of semantic context in time-series, computational constraints, and challenges in processing numerical inputs. SensorLLM a\u2026 \u25bd More We introduce SensorLLM, a two-stage framework that enables Large Language Models (LLMs) to perform human activity recognition (HAR) from sensor data. Despite their strong reasoning and generalization capabilities, LLMs remain underutilized for motion sensor data due to the lack of semantic context in time-series, computational constraints, and challenges in processing numerical inputs. SensorLLM addresses these limitations through a Sensor-Language Alignment stage, where we introduce special tokens for each sensor channel and automatically generate textual trend descriptions. This alignment enables LLMs to capture numerical variations, channel-specific features, and data of varying duration--without requiring human annotations. In the subsequent Task-Aware Tuning stage, we refine the model for HAR classification, achieving performance that matches or surpasses state-of-the-art methods. Our results demonstrate that SensorLLM evolves into an effective sensor learner, reasoner, and classifier through Sensor-Language Alignment, generalizing across diverse HAR datasets. We believe this work establishes a foundation for future research on time-series and text alignment, paving the way for foundation models in sensor data analysis. \u25b3 Less Submitted 17 March, 2025; v1 submitted 14 October, 2024; originally announced October 2024. arXiv:2408.07313 [pdf, other] Exploring Large-Scale Language Models to Evaluate EEG-Based Multimodal Data for Mental Health Authors: Yongquan Hu, Shuning Zhang, Ting Dang, Hong Jia, Flora D. Salim, Wen Hu, Aaron J. Quigley Abstract: Integrating physiological signals such as electroencephalogram (EEG), with other data such as interview audio, may offer valuable multimodal insights into psychological states or neurological disorders. Recent advancements with Large Language Models (LLMs) position them as prospective ``health agents'' for mental health assessment. However, current research predominantly focus on single data modal\u2026 \u25bd More Integrating physiological signals such as electroencephalogram (EEG), with other data such as interview audio, may offer valuable multimodal insights into psychological states or neurological disorders. Recent advancements with Large Language Models (LLMs) position them as prospective ``health agents'' for mental health assessment. However, current research predominantly focus on single data modalities, presenting an opportunity to advance understanding through multimodal data. Our study aims to advance this approach by investigating multimodal data using LLMs for mental health assessment, specifically through zero-shot and few-shot prompting. Three datasets are adopted for depression and emotion classifications incorporating EEG, facial expressions, and audio (text). The results indicate that multimodal information confers substantial advantages over single modality approaches in mental health assessment. Notably, integrating EEG alongside commonly used LLM modalities such as audio and images demonstrates promising potential. Moreover, our findings reveal that 1-shot learning offers greater benefits compared to zero-shot learning methods. \u25b3 Less Submitted 14 August, 2024; originally announced August 2024. Comments: 6 pages; UbiComp Companion '24, Companion of the 2024 ACM International Joint Conference on Pervasive and Ubiquitous Computing, October 5--9, 2024}{Melbourne, VIC, Australia arXiv:2407.18518 [pdf, other] WorkR: Occupation Inference for Intelligent Task Assistance Authors: Yonchanok Khaokaew, Hao Xue, Mohammad Saiedur Rahaman, Flora D. Salim Abstract: Occupation information can be utilized by digital assistants to provide occupation-specific personalized task support, including interruption management, task planning, and recommendations. Prior research in the digital workplace assistant domain requires users to input their occupation information for effective support. However, as many individuals switch between multiple occupations daily, curre\u2026 \u25bd More Occupation information can be utilized by digital assistants to provide occupation-specific personalized task support, including interruption management, task planning, and recommendations. Prior research in the digital workplace assistant domain requires users to input their occupation information for effective support. However, as many individuals switch between multiple occupations daily, current solutions falter without continuous user input. To address this, this study introduces WorkR, a framework that leverages passive sensing to capture pervasive signals from various task activities, addressing three challenges: the lack of a passive sensing architecture, personalization of occupation characteristics, and discovering latent relationships among occupation variables. We argue that signals from application usage, movements, social interactions, and the environment can inform a user's occupation. WorkR uses a Variational Autoencoder (VAE) to derive latent features for training models to infer occupations. Our experiments with an anonymized, context-rich activity and task log dataset demonstrate that our models can accurately infer occupations with more than 91% accuracy across six ISO occupation categories. \u25b3 Less Submitted 26 July, 2024; originally announced July 2024. arXiv:2407.17839 [pdf, other] Long-term Fairness in Ride-Hailing Platform Authors: Yufan Kang, Jeffrey Chan, Wei Shao, Flora D. Salim, Christopher Leckie Abstract: Matching in two-sided markets such as ride-hailing has recently received significant attention. However, existing studies on ride-hailing mainly focus on optimising efficiency, and fairness issues in ride-hailing have been neglected. Fairness issues in ride-hailing, including significant earning differences between drivers and variance of passenger waiting times among different locations, have pot\u2026 \u25bd More Matching in two-sided markets such as ride-hailing has recently received significant attention. However, existing studies on ride-hailing mainly focus on optimising efficiency, and fairness issues in ride-hailing have been neglected. Fairness issues in ride-hailing, including significant earning differences between drivers and variance of passenger waiting times among different locations, have potential impacts on economic and ethical aspects. The recent studies that focus on fairness in ride-hailing exploit traditional optimisation methods and the Markov Decision Process to balance efficiency and fairness. However, there are several issues in these existing studies, such as myopic short-term decision-making from traditional optimisation and instability of fairness in a comparably longer horizon from both traditional optimisation and Markov Decision Process-based methods. To address these issues, we propose a dynamic Markov Decision Process model to alleviate fairness issues currently faced by ride-hailing, and seek a balance between efficiency and fairness, with two distinct characteristics: (i) a prediction module to predict the number of requests that will be raised in the future from different locations to allow the proposed method to consider long-term fairness based on the whole timeline instead of consider fairness only based on historical and current data patterns; (ii) a customised scalarisation function for multi-objective multi-agent Q Learning that aims to balance efficiency and fairness. Extensive experiments on a publicly available real-world dataset demonstrate that our proposed method outperforms existing state-of-the-art methods. \u25b3 Less Submitted 25 July, 2024; originally announced July 2024. Comments: Accepted by ECML PKDD 2024 arXiv:2407.14116 [pdf, other] AuditNet: A Conversational AI-based Security Assistant [DEMO] Authors: Shohreh Deldari, Mohammad Goudarzi, Aditya Joshi, Arash Shaghaghi, Simon Finn, Flora D. Salim, Sanjay Jha Abstract: In the age of information overload, professionals across various fields face the challenge of navigating vast amounts of documentation and ever-evolving standards. Ensuring compliance with standards, regulations, and contractual obligations is a critical yet complex task across various professional fields. We propose a versatile conversational AI assistant framework designed to facilitate complian\u2026 \u25bd More In the age of information overload, professionals across various fields face the challenge of navigating vast amounts of documentation and ever-evolving standards. Ensuring compliance with standards, regulations, and contractual obligations is a critical yet complex task across various professional fields. We propose a versatile conversational AI assistant framework designed to facilitate compliance checking on the go, in diverse domains, including but not limited to network infrastructure, legal contracts, educational standards, environmental regulations, and government policies. By leveraging retrieval-augmented generation using large language models, our framework automates the review, indexing, and retrieval of relevant, context-aware information, streamlining the process of verifying adherence to established guidelines and requirements. This AI assistant not only reduces the manual effort involved in compliance checks but also enhances accuracy and efficiency, supporting professionals in maintaining high standards of practice and ensuring regulatory compliance in their respective fields. We propose and demonstrate AuditNet, the first conversational AI security assistant designed to assist IoT network security experts by providing instant access to security standards, policies, and regulations. \u25b3 Less Submitted 19 July, 2024; originally announced July 2024. arXiv:2407.14015 [pdf, other] Understanding Physiological Responses of Students Over Different Courses Authors: Soundariya Ananthan, Nan Gao, Flora D. Salim Abstract: Student engagement plays a vital role in academic success with high engagement often linked to positive educational outcomes. Traditionally, student engagement is measured through self-reports, which are both labour-intensive and not real-time. An emerging alternative is monitoring physiological signals such as Electrodermal Activity (EDA) and Inter-Beat Interval (IBI), which reflect students' emo\u2026 \u25bd More Student engagement plays a vital role in academic success with high engagement often linked to positive educational outcomes. Traditionally, student engagement is measured through self-reports, which are both labour-intensive and not real-time. An emerging alternative is monitoring physiological signals such as Electrodermal Activity (EDA) and Inter-Beat Interval (IBI), which reflect students' emotional and cognitive states. In this research, we analyzed these signals from 23 students wearing Empatica E4 devices in real-world scenarios. Diverging from previous studies focused on lab settings or specific subjects, we examined physiological synchrony at the intra-student level across various courses. We also assessed how different courses influence physiological responses and identified consistent temporal patterns. Our findings show unique physiological response patterns among students, enhancing our understanding of student engagement dynamics. This opens up possibilities for tailoring educational strategies based on unobtrusive sensing data to optimize learning outcomes. \u25b3 Less Submitted 19 July, 2024; originally announced July 2024. Comments: This paper is published on ISWC '24 arXiv:2406.14214 [pdf, other] REVEAL-IT: REinforcement learning with Visibility of Evolving Agent poLicy for InTerpretability Authors: Shuang Ao, Simon Khan, Haris Aziz, Flora D. Salim Abstract: Understanding the agent's learning process, particularly the factors that contribute to its success or failure post-training, is crucial for comprehending the rationale behind the agent's decision-making process. Prior methods clarify the learning process by creating a structural causal model (SCM) or visually representing the distribution of value functions. Nevertheless, these approaches have co\u2026 \u25bd More Understanding the agent's learning process, particularly the factors that contribute to its success or failure post-training, is crucial for comprehending the rationale behind the agent's decision-making process. Prior methods clarify the learning process by creating a structural causal model (SCM) or visually representing the distribution of value functions. Nevertheless, these approaches have constraints as they exclusively function in 2D-environments or with uncomplicated transition dynamics. Understanding the agent's learning process in complicated environments or tasks is more challenging. In this paper, we propose REVEAL-IT, a novel framework for explaining the learning process of an agent in complex environments. Initially, we visualize the policy structure and the agent's learning process for various training tasks. By visualizing these findings, we can understand how much a particular training task or stage affects the agent's performance in test. Then, a GNN-based explainer learns to highlight the most important section of the policy, providing a more clear and robust explanation of the agent's learning process. The experiments demonstrate that explanations derived from this framework can effectively help in the optimization of the training tasks, resulting in improved learning efficiency and final performance. \u25b3 Less Submitted 14 October, 2024; v1 submitted 20 June, 2024; originally announced June 2024. arXiv:2406.13123 [pdf, other] ViLCo-Bench: VIdeo Language COntinual learning Benchmark Authors: Tianqi Tang, Shohreh Deldari, Hao Xue, Celso De Melo, Flora D. Salim Abstract: Video language continual learning involves continuously adapting to information from video and text inputs, enhancing a model's ability to handle new tasks while retaining prior knowledge. This field is a relatively under-explored area, and establishing appropriate datasets is crucial for facilitating communication and research in this field. In this study, we present the first dedicated benchmark\u2026 \u25bd More Video language continual learning involves continuously adapting to information from video and text inputs, enhancing a model's ability to handle new tasks while retaining prior knowledge. This field is a relatively under-explored area, and establishing appropriate datasets is crucial for facilitating communication and research in this field. In this study, we present the first dedicated benchmark, ViLCo-Bench, designed to evaluate continual learning models across a range of video-text tasks. The dataset comprises ten-minute-long videos and corresponding language queries collected from publicly available datasets. Additionally, we introduce a novel memory-efficient framework that incorporates self-supervised learning and mimics long-term and short-term memory effects. This framework addresses challenges including memory complexity from long video clips, natural language complexity from open queries, and text-video misalignment. We posit that ViLCo-Bench, with greater complexity compared to existing continual learning benchmarks, would serve as a critical tool for exploring the video-language domain, extending beyond conventional class-incremental tasks, and addressing complex and limited annotation issues. The curated data, evaluations, and our novel method are available at https://github.com/cruiseresearchgroup/ViLCo. \u25b3 Less Submitted 15 December, 2024; v1 submitted 18 June, 2024; originally announced June 2024. Comments: 14 pages, 4 figures, 8 tables, Accepted at NeurIPS Dataset and Benchmark Track 2024 arXiv:2406.08990 [pdf, other] BTS: Building Timeseries Dataset: Empowering Large-Scale Building Analytics Authors: Arian Prabowo, Xiachong Lin, Imran Razzak, Hao Xue, Emily W. Yap, Matthew Amos, Flora D. Salim Abstract: Buildings play a crucial role in human well-being, influencing occupant comfort, health, and safety. Additionally, they contribute significantly to global energy consumption, accounting for one-third of total energy usage, and carbon emissions. Optimizing building performance presents a vital opportunity to combat climate change and promote human flourishing. However, research in building analytic\u2026 \u25bd More Buildings play a crucial role in human well-being, influencing occupant comfort, health, and safety. Additionally, they contribute significantly to global energy consumption, accounting for one-third of total energy usage, and carbon emissions. Optimizing building performance presents a vital opportunity to combat climate change and promote human flourishing. However, research in building analytics has been hampered by the lack of accessible, available, and comprehensive real-world datasets on multiple building operations. In this paper, we introduce the Building TimeSeries (BTS) dataset. Our dataset covers three buildings over a three-year period, comprising more than ten thousand timeseries data points with hundreds of unique ontologies. Moreover, the metadata is standardized using the Brick schema. To demonstrate the utility of this dataset, we performed benchmarks on two tasks: timeseries ontology classification and zero-shot forecasting. These tasks represent an essential initial step in addressing challenges related to interoperability in building analytics. Access to the dataset and the code used for benchmarking are available here: https://github.com/cruiseresearchgroup/DIEF_BTS . \u25b3 Less Submitted 18 June, 2024; v1 submitted 13 June, 2024; originally announced June 2024. Comments: 21 pages, 2 figures, 9 tables, under review arXiv:2406.04035 [pdf, other] STEMO: Early Spatio-temporal Forecasting with Multi-Objective Reinforcement Learning Authors: Wei Shao, Yufan Kang, Ziyan Peng, Xiao Xiao, Lei Wang, Yuhui Yang, Flora D Salim Abstract: Accuracy and timeliness are indeed often conflicting goals in prediction tasks. Premature predictions may yield a higher rate of false alarms, whereas delaying predictions to gather more information can render them too late to be useful. In applications such as wildfires, crimes, and traffic jams, timely forecasting are vital for safeguarding human life and property. Consequently, finding a balanc\u2026 \u25bd More Accuracy and timeliness are indeed often conflicting goals in prediction tasks. Premature predictions may yield a higher rate of false alarms, whereas delaying predictions to gather more information can render them too late to be useful. In applications such as wildfires, crimes, and traffic jams, timely forecasting are vital for safeguarding human life and property. Consequently, finding a balance between accuracy and timeliness is crucial. In this paper, we propose an early spatio-temporal forecasting model based on Multi-Objective reinforcement learning that can either implement an optimal policy given a preference or infer the preference based on a small number of samples. The model addresses two primary challenges: 1) enhancing the accuracy of early forecasting and 2) providing the optimal policy for determining the most suitable prediction time for each area. Our method demonstrates superior performance on three large-scale real-world datasets, surpassing existing methods in early spatio-temporal forecasting tasks. \u25b3 Less Submitted 18 June, 2024; v1 submitted 6 June, 2024; originally announced June 2024. Comments: Accepted paper in KDD 2024 arXiv:2406.03404 [pdf, other] ST-DPGAN: A Privacy-preserving Framework for Spatiotemporal Data Generation Authors: Wei Shao, Rongyi Zhu, Cai Yang, Chandra Thapa, Muhammad Ejaz Ahmed, Seyit Camtepe, Rui Zhang, DuYong Kim, Hamid Menouar, Flora D. Salim Abstract: Spatiotemporal data is prevalent in a wide range of edge devices, such as those used in personal communication and financial transactions. Recent advancements have sparked a growing interest in integrating spatiotemporal analysis with large-scale language models. However, spatiotemporal data often contains sensitive information, making it unsuitable for open third-party access. To address this cha\u2026 \u25bd More Spatiotemporal data is prevalent in a wide range of edge devices, such as those used in personal communication and financial transactions. Recent advancements have sparked a growing interest in integrating spatiotemporal analysis with large-scale language models. However, spatiotemporal data often contains sensitive information, making it unsuitable for open third-party access. To address this challenge, we propose a Graph-GAN-based model for generating privacy-protected spatiotemporal data. Our approach incorporates spatial and temporal attention blocks in the discriminator and a spatiotemporal deconvolution structure in the generator. These enhancements enable efficient training under Gaussian noise to achieve differential privacy. Extensive experiments conducted on three real-world spatiotemporal datasets validate the efficacy of our model. Our method provides a privacy guarantee while maintaining the data utility. The prediction model trained on our generated data maintains a competitive performance compared to the model trained on the original data. \u25b3 Less Submitted 4 June, 2024; originally announced June 2024. arXiv:2406.03109 [pdf, other] CAPRI-FAIR: Integration of Multi-sided Fairness in Contextual POI Recommendation Framework Authors: Francis Zac dela Cruz, Flora D. Salim, Yonchanok Khaokaew, Jeffrey Chan Abstract: Point-of-interest (POI) recommendation considers spatio-temporal factors like distance, peak hours, and user check-ins. Given their influence on both consumer experience and POI business, it's crucial to consider fairness from multiple perspectives. Unfortunately, these systems often provide less accurate recommendations to inactive users and less exposure to unpopular POIs. This paper develops a\u2026 \u25bd More Point-of-interest (POI) recommendation considers spatio-temporal factors like distance, peak hours, and user check-ins. Given their influence on both consumer experience and POI business, it's crucial to consider fairness from multiple perspectives. Unfortunately, these systems often provide less accurate recommendations to inactive users and less exposure to unpopular POIs. This paper develops a post-filter method that includes provider and consumer fairness in existing models, aiming to balance fairness metrics like item exposure with performance metrics such as precision and distance. Experiments show that a linear scoring model for provider fairness in re-scoring items offers the best balance between performance and long-tail exposure, sometimes without much precision loss. Addressing consumer fairness by recommending more popular POIs to inactive users increased precision in some models and datasets. However, combinations that reached the Pareto front of consumer and provider fairness resulted in the lowest precision values, highlighting that tradeoffs depend greatly on the model and dataset. \u25b3 Less Submitted 14 August, 2024; v1 submitted 5 June, 2024; originally announced June 2024. arXiv:2405.19184 [pdf, other] Promoting Two-sided Fairness in Dynamic Vehicle Routing Problem Authors: Yufan Kang, Rongsheng Zhang, Wei Shao, Flora D. Salim, Jeffrey Chan Abstract: Dynamic Vehicle Routing Problem (DVRP), is an extension of the classic Vehicle Routing Problem (VRP), which is a fundamental problem in logistics and transportation. Typically, DVRPs involve two stakeholders: service providers that deliver services to customers and customers who raise requests from different locations. Many real-world applications can be formulated as DVRP such as ridesharing and\u2026 \u25bd More Dynamic Vehicle Routing Problem (DVRP), is an extension of the classic Vehicle Routing Problem (VRP), which is a fundamental problem in logistics and transportation. Typically, DVRPs involve two stakeholders: service providers that deliver services to customers and customers who raise requests from different locations. Many real-world applications can be formulated as DVRP such as ridesharing and non-compliance capture. Apart from original objectives like optimising total utility or efficiency, DVRP should also consider fairness for all parties. Unfairness can induce service providers and customers to give up on the systems, leading to negative financial and social impacts. However, most existing DVRP-related applications focus on improving fairness from a single side, and there have been few works considering two-sided fairness and utility optimisation concurrently. To this end, we propose a novel framework, a Two-sided Fairness-aware Genetic Algorithm (named 2FairGA), which expands the genetic algorithm from the original objective solely focusing on utility to multi-objectives that incorporate two-sided fairness. Subsequently, the impact of injecting two fairness definitions into the utility-focused model and the correlation between any pair of the three objectives are explored. Extensive experiments demonstrate the superiority of our proposed framework compared to the state-of-the-art. \u25b3 Less Submitted 29 May, 2024; originally announced May 2024. arXiv:2405.14267 [pdf, other] A Gap in Time: The Challenge of Processing Heterogeneous IoT Data in Digitalized Buildings Authors: Xiachong Lin, Arian Prabowo, Imran Razzak, Hao Xue, Matthew Amos, Sam Behrens, Flora D. Salim Abstract: The increasing demand for sustainable energy solutions has driven the integration of digitalized buildings into the power grid, leveraging Internet-of-Things (IoT) technologies to enhance energy efficiency and operational performance. Despite their potential, effectively utilizing IoT point data within deep-learning frameworks presents significant challenges, primarily due to its inherent heteroge\u2026 \u25bd More The increasing demand for sustainable energy solutions has driven the integration of digitalized buildings into the power grid, leveraging Internet-of-Things (IoT) technologies to enhance energy efficiency and operational performance. Despite their potential, effectively utilizing IoT point data within deep-learning frameworks presents significant challenges, primarily due to its inherent heterogeneity. This study investigates the diverse dimensions of IoT data heterogeneity in both intra-building and inter-building contexts, examining their implications for predictive modeling. A benchmarking analysis of state-of-the-art time series models highlights their performance on this complex dataset. The results emphasize the critical need for multi-modal data integration, domain-informed modeling, and automated data engineering pipelines. Additionally, the study advocates for collaborative efforts to establish high-quality public datasets, which are essential for advancing intelligent and sustainable energy management systems in digitalized buildings. \u25b3 Less Submitted 20 November, 2024; v1 submitted 23 May, 2024; originally announced May 2024. Comments: 4 figures, 1 tables, 9 pages arXiv:2405.12480 [pdf, other] Towards Detecting and Mitigating Cognitive Bias in Spoken Conversational Search Authors: Kaixin Ji, Sachin Pathiyan Cherumanal, Johanne R. Trippas, Danula Hettiachchi, Flora D. Salim, Falk Scholer, Damiano Spina Abstract: Instruments such as eye-tracking devices have contributed to understanding how users interact with screen-based search engines. However, user-system interactions in audio-only channels -- as is the case for Spoken Conversational Search (SCS) -- are harder to characterize, given the lack of instruments to effectively and precisely capture interactions. Furthermore, in this era of information overlo\u2026 \u25bd More Instruments such as eye-tracking devices have contributed to understanding how users interact with screen-based search engines. However, user-system interactions in audio-only channels -- as is the case for Spoken Conversational Search (SCS) -- are harder to characterize, given the lack of instruments to effectively and precisely capture interactions. Furthermore, in this era of information overload, cognitive bias can significantly impact how we seek and consume information -- especially in the context of controversial topics or multiple viewpoints. This paper draws upon insights from multiple disciplines (including information seeking, psychology, cognitive science, and wearable sensors) to provoke novel conversations in the community. To this end, we discuss future opportunities and propose a framework including multimodal instruments and methods for experimental designs and settings. We demonstrate preliminary results as an example. We also outline the challenges and offer suggestions for adopting this multimodal approach, including ethical considerations, to assist future researchers and practitioners in exploring cognitive biases in SCS. \u25b3 Less Submitted 6 August, 2024; v1 submitted 20 May, 2024; originally announced May 2024. Comments: Extended version of MobileHCI'24 LBW paper arXiv:2405.00322 [pdf, other] Characterizing Information Seeking Processes with Multiple Physiological Signals Authors: Kaixin Ji, Danula Hettiachchi, Flora D. Salim, Falk Scholer, Damiano Spina Abstract: Information access systems are getting complex, and our understanding of user behavior during information seeking processes is mainly drawn from qualitative methods, such as observational studies or surveys. Leveraging the advances in sensing technologies, our study aims to characterize user behaviors with physiological signals, particularly in relation to cognitive load, affective arousal, and va\u2026 \u25bd More Information access systems are getting complex, and our understanding of user behavior during information seeking processes is mainly drawn from qualitative methods, such as observational studies or surveys. Leveraging the advances in sensing technologies, our study aims to characterize user behaviors with physiological signals, particularly in relation to cognitive load, affective arousal, and valence. We conduct a controlled lab study with 26 participants, and collect data including Electrodermal Activities, Photoplethysmogram, Electroencephalogram, and Pupillary Responses. This study examines informational search with four stages: the realization of Information Need (IN), Query Formulation (QF), Query Submission (QS), and Relevance Judgment (RJ). We also include different interaction modalities to represent modern systems, e.g., QS by text-typing or verbalizing, and RJ with text or audio information. We analyze the physiological signals across these stages and report outcomes of pairwise non-parametric repeated-measure statistical tests. The results show that participants experience significantly higher cognitive loads at IN with a subtle increase in alertness, while QF requires higher attention. QS involves demanding cognitive loads than QF. Affective responses are more pronounced at RJ than QS or IN, suggesting greater interest and engagement as knowledge gaps are resolved. To the best of our knowledge, this is the first study that explores user behaviors in a search process employing a more nuanced quantitative analysis of physiological signals. Our findings offer valuable insights into user behavior and emotional responses in information seeking processes. We believe our proposed methodology can inform the characterization of more complex processes, such as conversational information seeking. \u25b3 Less Submitted 7 May, 2024; v1 submitted 1 May, 2024; originally announced May 2024. ACM Class: H.5; H.3.3; C.3 Journal ref: In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2024, Washington, DC, USA. ACM, New York, NY, USA, 12 pages arXiv:2404.17591 [pdf, other] Large Language Models for Next Point-of-Interest Recommendation Authors: Peibo Li, Maarten de Rijke, Hao Xue, Shuang Ao, Yang Song, Flora D. Salim Abstract: The next Point of Interest (POI) recommendation task is to predict users' immediate next POI visit given their historical data. Location-Based Social Network (LBSN) data, which is often used for the next POI recommendation task, comes with challenges. One frequently disregarded challenge is how to effectively use the abundant contextual information present in LBSN data. Previous methods are limite\u2026 \u25bd More The next Point of Interest (POI) recommendation task is to predict users' immediate next POI visit given their historical data. Location-Based Social Network (LBSN) data, which is often used for the next POI recommendation task, comes with challenges. One frequently disregarded challenge is how to effectively use the abundant contextual information present in LBSN data. Previous methods are limited by their numerical nature and fail to address this challenge. In this paper, we propose a framework that uses pretrained Large Language Models (LLMs) to tackle this challenge. Our framework allows us to preserve heterogeneous LBSN data in its original format, hence avoiding the loss of contextual information. Furthermore, our framework is capable of comprehending the inherent meaning of contextual information due to the inclusion of commonsense knowledge. In experiments, we test our framework on three real-world LBSN datasets. Our results show that the proposed framework outperforms the state-of-the-art models in all three datasets. Our analysis demonstrates the effectiveness of the proposed framework in using contextual information as well as alleviating the commonly encountered cold-start and short trajectory problems. \u25b3 Less Submitted 1 August, 2024; v1 submitted 19 April, 2024; originally announced April 2024. Journal ref: In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Association for Computing Machinery, New York, NY, USA, 1463-1472 arXiv:2404.14665 [pdf, other] Illuminating the Unseen: Investigating the Context-induced Harms in Behavioral Sensing Authors: Han Zhang, Vedant Das Swain, Leijie Wang, Nan Gao, Yilun Sheng, Xuhai Xu, Flora D. Salim, Koustuv Saha, Anind K. Dey, Jennifer Mankoff Abstract: Behavioral sensing technologies are rapidly evolving across a range of well-being applications. Despite its potential, concerns about the responsible use of such technology are escalating. In response, recent research within the sensing technology has started to address these issues. While promising, they primarily focus on broad demographic categories and overlook more nuanced, context-specific i\u2026 \u25bd More Behavioral sensing technologies are rapidly evolving across a range of well-being applications. Despite its potential, concerns about the responsible use of such technology are escalating. In response, recent research within the sensing technology has started to address these issues. While promising, they primarily focus on broad demographic categories and overlook more nuanced, context-specific identities. These approaches lack grounding within domain-specific harms that arise from deploying sensing technology in diverse social, environmental, and technological settings. Additionally, existing frameworks for evaluating harms are designed for a generic ML life cycle, and fail to adapt to the dynamic and longitudinal considerations for behavioral sensing technology. To address these gaps, we introduce a framework specifically designed for evaluating behavioral sensing technologies. This framework emphasizes a comprehensive understanding of context, particularly the situated identities of users and the deployment settings of the sensing technology. It also highlights the necessity for iterative harm mitigation and continuous maintenance to adapt to the evolving nature of technology and its use. We demonstrate the feasibility and generalizability of our framework through post-hoc evaluations on two real-world behavioral sensing studies conducted in different international contexts, involving varied population demographics and machine learning tasks. Our evaluations provide empirical evidence of both situated identity-based harm and more domain-specific harms, and discuss the trade-offs introduced by implementing bias mitigation techniques. \u25b3 Less Submitted 5 May, 2024; v1 submitted 22 April, 2024; originally announced April 2024. Comments: 26 pages, 8 tables, and 1 figure (excluding appendix) MSC Class: 68U35 ACM Class: H.5.0; I.2.m arXiv:2403.10851 [pdf] GustosonicSense: Towards understanding the design of playful gustosonic eating experiences Authors: Yan Wang, Humphrey O. Obie, Zhuying Li, Flora D. Salim, John Grundy, Florian 'Floyd' Mueller Abstract: The pleasure that often comes with eating can be further enhanced with intelligent technology, as the field of human-food interaction suggests. However, knowledge on how to design such pleasure-supporting eating systems is limited. To begin filling this knowledge gap, we designed \"GustosonicSense\", a novel gustosonic eating system that utilizes wireless earbuds for sensing different eating and dri\u2026 \u25bd More The pleasure that often comes with eating can be further enhanced with intelligent technology, as the field of human-food interaction suggests. However, knowledge on how to design such pleasure-supporting eating systems is limited. To begin filling this knowledge gap, we designed \"GustosonicSense\", a novel gustosonic eating system that utilizes wireless earbuds for sensing different eating and drinking actions with a machine learning algorithm and trigger playful sounds as a way to facilitate pleasurable eating experiences. We present the findings from our design and a study that revealed how we can support the \"stimulation\", \"hedonism\", and \"reflexivity\" for playful human-food interactions. Ultimately, with our work, we aim to support interaction designers in facilitating playful experiences with food. \u25b3 Less Submitted 16 March, 2024; originally announced March 2024. Comments: To appear at CHI'24: The ACM Conference on Human Factors in Computing Systems (CHI), Honolulu, Hawaii, 2024 arXiv:2403.03544 [pdf, other] Prompt Mining for Language-based Human Mobility Forecasting Authors: Hao Xue, Tianye Tang, Ali Payani, Flora D. Salim Abstract: With the advancement of large language models, language-based forecasting has recently emerged as an innovative approach for predicting human mobility patterns. The core idea is to use prompts to transform the raw mobility data given as numerical values into natural language sentences so that the language models can be leveraged to generate the description for future observations. However, previou\u2026 \u25bd More With the advancement of large language models, language-based forecasting has recently emerged as an innovative approach for predicting human mobility patterns. The core idea is to use prompts to transform the raw mobility data given as numerical values into natural language sentences so that the language models can be leveraged to generate the description for future observations. However, previous studies have only employed fixed and manually designed templates to transform numerical values into sentences. Since the forecasting performance of language models heavily relies on prompts, using fixed templates for prompting may limit the forecasting capability of language models. In this paper, we propose a novel framework for prompt mining in language-based mobility forecasting, aiming to explore diverse prompt design strategies. Specifically, the framework includes a prompt generation stage based on the information entropy of prompts and a prompt refinement stage to integrate mechanisms such as the chain of thought. Experimental results on real-world large-scale data demonstrate the superiority of generated prompts from our prompt mining pipeline. Additionally, the comparison of different prompt variants shows that the proposed prompt refinement process is effective. Our study presents a promising direction for further advancing language-based mobility forecasting. \u25b3 Less Submitted 6 March, 2024; originally announced March 2024. arXiv:2402.12132 [pdf, other] SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding Authors: Ruiyi Yang, Flora D. Salim, Hao Xue Abstract: Knowledge graphs (KGs) have been increasingly employed for link prediction and recommendation using real-world datasets. However, the majority of current methods rely on static data, neglecting the dynamic nature and the hidden spatio-temporal attributes of real-world scenarios. This often results in suboptimal predictions and recommendations. Although there are effective spatio-temporal inference\u2026 \u25bd More Knowledge graphs (KGs) have been increasingly employed for link prediction and recommendation using real-world datasets. However, the majority of current methods rely on static data, neglecting the dynamic nature and the hidden spatio-temporal attributes of real-world scenarios. This often results in suboptimal predictions and recommendations. Although there are effective spatio-temporal inference methods, they face challenges such as scalability with large datasets and inadequate semantic understanding, which impede their performance. To address these limitations, this paper introduces a novel framework - Simple Spatio-Temporal Knowledge Graph (SSTKG), for constructing and exploring spatio-temporal KGs. To integrate spatial and temporal data into KGs, our framework exploited through a new 3-step embedding method. Output embeddings can be used for future temporal sequence prediction and spatial information recommendation, providing valuable insights for various applications such as retail sales forecasting and traffic volume prediction. Our framework offers a simple but comprehensive way to understand the underlying patterns and trends in dynamic KG, thereby enhancing the accuracy of predictions and the relevance of recommendations. This work paves the way for more effective utilization of spatio-temporal data in KGs, with potential impacts across a wide range of sectors. \u25b3 Less Submitted 19 February, 2024; originally announced February 2024. Comments: for Web conf 2024. 8 pages context arXiv:2312.03330 [pdf, other] Measuring Misogyny in Natural Language Generation: Preliminary Results from a Case Study on two Reddit Communities Authors: Aaron J. Snoswell, Lucinda Nelson, Hao Xue, Flora D. Salim, Nicolas Suzor, Jean Burgess Abstract: Generic `toxicity' classifiers continue to be used for evaluating the potential for harm in natural language generation, despite mounting evidence of their shortcomings. We consider the challenge of measuring misogyny in natural language generation, and argue that generic `toxicity' classifiers are inadequate for this task. We use data from two well-characterised `Incel' communities on Reddit that\u2026 \u25bd More Generic `toxicity' classifiers continue to be used for evaluating the potential for harm in natural language generation, despite mounting evidence of their shortcomings. We consider the challenge of measuring misogyny in natural language generation, and argue that generic `toxicity' classifiers are inadequate for this task. We use data from two well-characterised `Incel' communities on Reddit that differ primarily in their degrees of misogyny to construct a pair of training corpora which we use to fine-tune two language models. We show that an open source `toxicity' classifier is unable to distinguish meaningfully between generations from these models. We contrast this with a misogyny-specific lexicon recently proposed by feminist subject-matter experts, demonstrating that, despite the limitations of simple lexicon-based approaches, this shows promise as a benchmark to evaluate language models for misogyny, and that it is sensitive enough to reveal the known differences in these Reddit communities. Our preliminary findings highlight the limitations of a generic approach to evaluating harms, and further emphasise the need for careful benchmark design and selection in natural language evaluation. \u25b3 Less Submitted 6 December, 2023; originally announced December 2023. Comments: This extended abstract was presented at the Generation, Evaluation and Metrics workshop at Empirical Methods in Natural Language Processing in 2023 (GEM@EMNLP 2023) in Singapore arXiv:2311.15496 [pdf, ps, other] Critiquing Self-report Practices for Human Mental and Wellbeing Computing at Ubicomp Authors: Nan Gao, Soundariya Ananthan, Chun Yu, Yuntao Wang, Flora D. Salim Abstract: Computing human mental and wellbeing is crucial to various domains, including health, education, and entertainment. However, the reliance on self-reporting in traditional research to establish ground truth often leads to methodological inconsistencies and susceptibility to response biases, thus hindering the effectiveness of modelling. This paper presents the first systematic methodological review\u2026 \u25bd More Computing human mental and wellbeing is crucial to various domains, including health, education, and entertainment. However, the reliance on self-reporting in traditional research to establish ground truth often leads to methodological inconsistencies and susceptibility to response biases, thus hindering the effectiveness of modelling. This paper presents the first systematic methodological review of self-reporting practices in Ubicomp within the context of human mental and wellbeing computing. Drawing from existing survey research, we establish guidelines for self-reporting in human wellbeing studies and identify shortcomings in current practices at Ubicomp community. Furthermore, we explore the reliability of self-report as a means of ground truth and propose directions for improving ground truth measurement in this field. Ultimately, we emphasize the urgent need for methodological advancements to enhance human mental and wellbeing computing. \u25b3 Less Submitted 26 November, 2023; originally announced November 2023. arXiv:2311.05457 [pdf, other] Leveraging Large Language Models for Generating Mobile Sensing Strategies in Human Behavior Modeling Authors: Nan Gao, Zhuolei Yu, Yue Xu, Chun Yu, Yuntao Wang, Flora D. Salim, Yuanchun Shi Abstract: Mobile sensing plays a crucial role in generating digital traces to understand human daily lives. However, studying behaviours like mood or sleep quality in smartphone users requires carefully designed mobile sensing strategies such as sensor selection and feature construction. This process is time-consuming, burdensome, and requires expertise in multiple domains. Furthermore, the resulting sensin\u2026 \u25bd More Mobile sensing plays a crucial role in generating digital traces to understand human daily lives. However, studying behaviours like mood or sleep quality in smartphone users requires carefully designed mobile sensing strategies such as sensor selection and feature construction. This process is time-consuming, burdensome, and requires expertise in multiple domains. Furthermore, the resulting sensing framework lacks generalizability, making it difficult to apply to different scenarios. In the research, we propose an automated mobile sensing strategy for human behaviour understanding. First, we establish a knowledge base and consolidate rules for data collection and effective feature construction. Then, we introduce the multi-granular human behaviour representation and design procedures for leveraging large language models to generate strategies. Our approach is validated through blind comparative studies and usability evaluation. Ultimately, our approach holds the potential to revolutionise the field of mobile sensing and its applications. \u25b3 Less Submitted 22 August, 2024; v1 submitted 9 November, 2023; originally announced November 2023. Comments: This manuscript has been accepted as a workshop paper on Ubicomp 2024 arXiv:2310.17788 [pdf, other] Utilizing Language Models for Energy Load Forecasting Authors: Hao Xue, Flora D. Salim Abstract: Energy load forecasting plays a crucial role in optimizing resource allocation and managing energy consumption in buildings and cities. In this paper, we propose a novel approach that leverages language models for energy load forecasting. We employ prompting techniques to convert energy consumption data into descriptive sentences, enabling fine-tuning of language models. By adopting an autoregress\u2026 \u25bd More Energy load forecasting plays a crucial role in optimizing resource allocation and managing energy consumption in buildings and cities. In this paper, we propose a novel approach that leverages language models for energy load forecasting. We employ prompting techniques to convert energy consumption data into descriptive sentences, enabling fine-tuning of language models. By adopting an autoregressive generating approach, our proposed method enables predictions of various horizons of future energy load consumption. Through extensive experiments on real-world datasets, we demonstrate the effectiveness and accuracy of our proposed method. Our results indicate that utilizing language models for energy load forecasting holds promise for enhancing energy efficiency and facilitating intelligent decision-making in energy systems. \u25b3 Less Submitted 26 October, 2023; originally announced October 2023. Comments: BuildSys 2023 Accepted arXiv:2310.16242 [pdf, other] ZzzGPT: An Interactive GPT Approach to Enhance Sleep Quality Authors: Yonchanok Khaokaew, Kaixin Ji, Thuc Hanh Nguyen, Hiruni Kegalle, Marwah Alaofi, Hao Xue, Flora D. Salim Abstract: This paper explores the intersection of technology and sleep pattern comprehension, presenting a cutting-edge two-stage framework that harnesses the power of Large Language Models (LLMs). The primary objective is to deliver precise sleep predictions paired with actionable feedback, addressing the limitations of existing solutions. This innovative approach involves leveraging the GLOBEM dataset alo\u2026 \u25bd More This paper explores the intersection of technology and sleep pattern comprehension, presenting a cutting-edge two-stage framework that harnesses the power of Large Language Models (LLMs). The primary objective is to deliver precise sleep predictions paired with actionable feedback, addressing the limitations of existing solutions. This innovative approach involves leveraging the GLOBEM dataset alongside synthetic data generated by LLMs. The results highlight significant improvements, underlining the efficacy of merging advanced machine-learning techniques with a user-centric design ethos. Through this exploration, we bridge the gap between technological sophistication and user-friendly design, ensuring that our framework yields accurate predictions and translates them into actionable insights. \u25b3 Less Submitted 6 May, 2024; v1 submitted 24 October, 2023; originally announced October 2023. arXiv:2310.13304 [pdf, other] \"Living Within Four Walls\": Exploring Emotional and Social Dynamics in Mobile Usage During Home Confinement Authors: Nan Gao, Sam Nolan, Kaixin Ji, Shakila Khan Rumi, Judith Simone Heinisch, Christoph Anderson, Klaus David, Flora D. Salim Abstract: Home confinement, a situation experienced by individuals for reasons ranging from medical quarantines, rehabilitation needs, disability accommodations, and remote working, is a common yet impactful aspect of modern life. While essential in various scenarios, confinement within the home environment can profoundly influence mental well-being and digital device usage. Using the COVID-19 lockdown as a\u2026 \u25bd More Home confinement, a situation experienced by individuals for reasons ranging from medical quarantines, rehabilitation needs, disability accommodations, and remote working, is a common yet impactful aspect of modern life. While essential in various scenarios, confinement within the home environment can profoundly influence mental well-being and digital device usage. Using the COVID-19 lockdown as a case study, this research explores the emotional and social effects of prolonged home confinement on mobile device usage. We conducted an in-situ study with 32 participants, analyzing three weeks of mobile usage data to assess emotional well-being and social dynamics in restricted environments. Our findings reveal that app usage patterns serve as strong indicators of emotional states, offering insights into how digital interactions can reflect and influence well-being during isolation. This study highlights the potential for developing targeted interventions and support systems for individuals in long-term home confinement, including those with chronic illness, recovery needs, or permanent remote work situations. \u25b3 Less Submitted 11 December, 2024; v1 submitted 20 October, 2023; originally announced October 2023. arXiv:2310.04443 [pdf, other] Human Mobility Question Answering (Vision Paper) Authors: Hao Xue, Flora D. Salim Abstract: Question answering (QA) systems have attracted much attention from the artificial intelligence community as they can learn to answer questions based on the given knowledge source (e.g., images in visual question answering). However, the research into question answering systems with human mobility data remains unexplored. Mining human mobility data is crucial for various applications such as smart\u2026 \u25bd More Question answering (QA) systems have attracted much attention from the artificial intelligence community as they can learn to answer questions based on the given knowledge source (e.g., images in visual question answering). However, the research into question answering systems with human mobility data remains unexplored. Mining human mobility data is crucial for various applications such as smart city planning, pandemic management, and personalised recommendation system. In this paper, we aim to tackle this gap and introduce a novel task, that is, human mobility question answering (MobQA). The aim of the task is to let the intelligent system learn from mobility data and answer related questions. This task presents a new paradigm change in mobility prediction research and further facilitates the research of human mobility recommendation systems. To better support this novel research topic, this vision paper also proposes an initial design of the dataset and a potential deep learning model framework for the introduced MobQA task. We hope that this paper will provide novel insights and open new directions in human mobility research and question answering research. \u25b3 Less Submitted 13 October, 2023; v1 submitted 2 October, 2023; originally announced October 2023. arXiv:2309.08648 [pdf, other] MAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings Authors: Yonchanok Khaokaew, Hao Xue, Flora D. Salim Abstract: In recent years, predicting mobile app usage has become increasingly important for areas like app recommendation, user behaviour analysis, and mobile resource management. Existing models, however, struggle with the heterogeneous nature of contextual data and the user cold start problem. This study introduces a novel prediction model, Mobile App Prediction Leveraging Large Language Model Embeddings\u2026 \u25bd More In recent years, predicting mobile app usage has become increasingly important for areas like app recommendation, user behaviour analysis, and mobile resource management. Existing models, however, struggle with the heterogeneous nature of contextual data and the user cold start problem. This study introduces a novel prediction model, Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE), which employs Large Language Models (LLMs) and installed app similarity to overcome these challenges. MAPLE utilises the power of LLMs to process contextual data and discern intricate relationships within it effectively. Additionally, we explore the use of installed app similarity to address the cold start problem, facilitating the modelling of user preferences and habits, even for new users with limited historical data. In essence, our research presents MAPLE as a novel, potent, and practical approach to app usage prediction, making significant strides in resolving issues faced by existing models. MAPLE stands out as a comprehensive and effective solution, setting a new benchmark for more precise and personalised app usage predictions. In tests on two real-world datasets, MAPLE surpasses contemporary models in both standard and cold start scenarios. These outcomes validate MAPLE's capacity for precise app usage predictions and its resilience against the cold start problem. This enhanced performance stems from the model's proficiency in capturing complex temporal patterns and leveraging contextual information. As a result, MAPLE can potentially improve personalised mobile app usage predictions and user experiences markedly. \u25b3 Less Submitted 1 November, 2024; v1 submitted 15 September, 2023; originally announced September 2023. arXiv:2309.04296 [pdf, other] Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: Benchmarking energy load forecasting models without and with continual learning Authors: Arian Prabowo, Kaixuan Chen, Hao Xue, Subbu Sethuvenkatraman, Flora D. Salim Abstract: In traditional deep learning algorithms, one of the key assumptions is that the data distribution remains constant during both training and deployment. However, this assumption becomes problematic when faced with Out-of-Distribution periods, such as the COVID-19 lockdowns, where the data distribution significantly deviates from what the model has seen during training. This paper employs a two-fold\u2026 \u25bd More In traditional deep learning algorithms, one of the key assumptions is that the data distribution remains constant during both training and deployment. However, this assumption becomes problematic when faced with Out-of-Distribution periods, such as the COVID-19 lockdowns, where the data distribution significantly deviates from what the model has seen during training. This paper employs a two-fold strategy: utilizing continual learning techniques to update models with new data and harnessing human mobility data collected from privacy-preserving pedestrian counters located outside buildings. In contrast to online learning, which suffers from 'catastrophic forgetting' as newly acquired knowledge often erases prior information, continual learning offers a holistic approach by preserving past insights while integrating new data. This research applies FSNet, a powerful continual learning algorithm, to real-world data from 13 building complexes in Melbourne, Australia, a city which had the second longest total lockdown duration globally during the pandemic. Results underscore the crucial role of continual learning in accurate energy forecasting, particularly during Out-of-Distribution periods. Secondary data such as mobility and temperature provided ancillary support to the primary forecasting model. More importantly, while traditional methods struggled to adapt during lockdowns, models featuring at least online learning demonstrated resilience, with lockdown periods posing fewer challenges once armed with adaptive learning techniques. This study contributes valuable methodologies and insights to the ongoing effort to improve energy load forecasting during future Out-of-Distribution periods. \u25b3 Less Submitted 3 October, 2023; v1 submitted 8 September, 2023; originally announced September 2023. Comments: 10 pages, 2 figures, 5 tables, BuildSys '23 arXiv:2309.04211 [pdf, other] Counterfactual Explanations via Locally-guided Sequential Algorithmic Recourse Authors: Edward A. Small, Jeffrey N. Clark, Christopher J. McWilliams, Kacper Sokol, Jeffrey Chan, Flora D. Salim, Raul Santos-Rodriguez Abstract: Counterfactuals operationalised through algorithmic recourse have become a powerful tool to make artificial intelligence systems explainable. Conceptually, given an individual classified as y -- the factual -- we seek actions such that their prediction becomes the desired class y' -- the counterfactual. This process offers algorithmic recourse that is (1) easy to customise and interpret, and (2) d\u2026 \u25bd More Counterfactuals operationalised through algorithmic recourse have become a powerful tool to make artificial intelligence systems explainable. Conceptually, given an individual classified as y -- the factual -- we seek actions such that their prediction becomes the desired class y' -- the counterfactual. This process offers algorithmic recourse that is (1) easy to customise and interpret, and (2) directly aligned with the goals of each individual. However, the properties of a \"good\" counterfactual are still largely debated; it remains an open challenge to effectively locate a counterfactual along with its corresponding recourse. Some strategies use gradient-driven methods, but these offer no guarantees on the feasibility of the recourse and are open to adversarial attacks on carefully created manifolds. This can lead to unfairness and lack of robustness. Other methods are data-driven, which mostly addresses the feasibility problem at the expense of privacy, security and secrecy as they require access to the entire training data set. Here, we introduce LocalFACE, a model-agnostic technique that composes feasible and actionable counterfactual explanations using locally-acquired information at each step of the algorithmic recourse. Our explainer preserves the privacy of users by only leveraging data that it specifically requires to construct actionable algorithmic recourse, and protects the model by offering transparency solely in the regions deemed necessary for the intervention. \u25b3 Less Submitted 8 September, 2023; originally announced September 2023. Comments: 7 pages, 5 figures, 3 appendix pages arXiv:2309.01288 [pdf, other] How Crowd Worker Factors Influence Subjective Annotations: A Study of Tagging Misogynistic Hate Speech in Tweets Authors: Danula Hettiachchi, Indigo Holcombe-James, Stephanie Livingstone, Anjalee de Silva, Matthew Lease, Flora D. Salim, Mark Sanderson Abstract: Crowdsourced annotation is vital to both collecting labelled data to train and test automated content moderation systems and to support human-in-the-loop review of system decisions. However, annotation tasks such as judging hate speech are subjective and thus highly sensitive to biases stemming from annotator beliefs, characteristics and demographics. We conduct two crowdsourcing studies on Mechan\u2026 \u25bd More Crowdsourced annotation is vital to both collecting labelled data to train and test automated content moderation systems and to support human-in-the-loop review of system decisions. However, annotation tasks such as judging hate speech are subjective and thus highly sensitive to biases stemming from annotator beliefs, characteristics and demographics. We conduct two crowdsourcing studies on Mechanical Turk to examine annotator bias in labelling sexist and misogynistic hate speech. Results from 109 annotators show that annotator political inclination, moral integrity, personality traits, and sexist attitudes significantly impact annotation accuracy and the tendency to tag content as hate speech. In addition, semi-structured interviews with nine crowd workers provide further insights regarding the influence of subjectivity on annotations. In exploring how workers interpret a task - shaped by complex negotiations between platform structures, task instructions, subjective motivations, and external contextual factors - we see annotations not only impacted by worker factors but also simultaneously shaped by the structures under which they labour. \u25b3 Less Submitted 3 September, 2023; originally announced September 2023. Comments: Accepted to the 11th AAAI Conference on Human Computation and Crowdsourcing (HCOMP 2023) arXiv:2308.13755 [pdf, other] i-Align: an interpretable knowledge graph alignment model Authors: Bayu Distiawan Trisedya, Flora D Salim, Jeffrey Chan, Damiano Spina, Falk Scholer, Mark Sanderson Abstract: Knowledge graphs (KGs) are becoming essential resources for many downstream applications. However, their incompleteness may limit their potential. Thus, continuous curation is needed to mitigate this problem. One of the strategies to address this problem is KG alignment, i.e., forming a more complete KG by merging two or more KGs. This paper proposes i-Align, an interpretable KG alignment model. U\u2026 \u25bd More Knowledge graphs (KGs) are becoming essential resources for many downstream applications. However, their incompleteness may limit their potential. Thus, continuous curation is needed to mitigate this problem. One of the strategies to address this problem is KG alignment, i.e., forming a more complete KG by merging two or more KGs. This paper proposes i-Align, an interpretable KG alignment model. Unlike the existing KG alignment models, i-Align provides an explanation for each alignment prediction while maintaining high alignment performance. Experts can use the explanation to check the correctness of the alignment prediction. Thus, the high quality of a KG can be maintained during the curation process (e.g., the merging process of two KGs). To this end, a novel Transformer-based Graph Encoder (Trans-GE) is proposed as a key component of i-Align for aggregating information from entities' neighbors (structures). Trans-GE uses Edge-gated Attention that combines the adjacency matrix and the self-attention matrix to learn a gating mechanism to control the information aggregation from the neighboring entities. It also uses historical embeddings, allowing Trans-GE to be trained over mini-batches, or smaller sub-graphs, to address the scalability issue when encoding a large KG. Another component of i-Align is a Transformer encoder for aggregating entities' attributes. This way, i-Align can generate explanations in the form of a set of the most influential attributes/neighbors based on attention weights. Extensive experiments are conducted to show the power of i-Align. The experiments include several aspects, such as the model's effectiveness for aligning KGs, the quality of the generated explanations, and its practicality for aligning large KGs. The results show the effectiveness of i-Align in these aspects. \u25b3 Less Submitted 25 August, 2023; originally announced August 2023. Comments: Data Min Knowl Disc (2023) arXiv:2308.12575 [pdf, other] Hypergraph Convolutional Networks for Fine-grained ICU Patient Similarity Analysis and Risk Prediction Authors: Yuxi Liu, Zhenhao Zhang, Shaowen Qin, Flora D. Salim, Antonio Jimeno Yepes, Jun Shen, Jiang Bian Abstract: The Intensive Care Unit (ICU) is one of the most important parts of a hospital, which admits critically ill patients and provides continuous monitoring and treatment. Various patient outcome prediction methods have been attempted to assist healthcare professionals in clinical decision-making. Existing methods focus on measuring the similarity between patients using deep neural networks to capture\u2026 \u25bd More The Intensive Care Unit (ICU) is one of the most important parts of a hospital, which admits critically ill patients and provides continuous monitoring and treatment. Various patient outcome prediction methods have been attempted to assist healthcare professionals in clinical decision-making. Existing methods focus on measuring the similarity between patients using deep neural networks to capture the hidden feature structures. However, the higher-order relationships are ignored, such as patient characteristics (e.g., diagnosis codes) and their causal effects on downstream clinical predictions. In this paper, we propose a novel Hypergraph Convolutional Network that allows the representation of non-pairwise relationships among diagnosis codes in a hypergraph to capture the hidden feature structures so that fine-grained patient similarity can be calculated for personalized mortality risk prediction. Evaluation using a publicly available eICU Collaborative Research Database indicates that our method achieves superior performance over the state-of-the-art models on mortality risk prediction. Moreover, the results of several case studies demonstrated the effectiveness and robustness of the model decisions. \u25b3 Less Submitted 21 October, 2023; v1 submitted 24 August, 2023; originally announced August 2023. Comments: 16 pages, 2 figures arXiv:2308.10220 [pdf, other] Designing and Evaluating Presentation Strategies for Fact-Checked Content Authors: Danula Hettiachchi, Kaixin Ji, Jenny Kennedy, Anthony McCosker, Flora D. Salim, Mark Sanderson, Falk Scholer, Damiano Spina Abstract: With the rapid growth of online misinformation, it is crucial to have reliable fact-checking methods. Recent research on finding check-worthy claims and automated fact-checking have made significant advancements. However, limited guidance exists regarding the presentation of fact-checked content to effectively convey verified information to users. We address this research gap by exploring the crit\u2026 \u25bd More With the rapid growth of online misinformation, it is crucial to have reliable fact-checking methods. Recent research on finding check-worthy claims and automated fact-checking have made significant advancements. However, limited guidance exists regarding the presentation of fact-checked content to effectively convey verified information to users. We address this research gap by exploring the critical design elements in fact-checking reports and investigating whether credibility and presentation-based design improvements can enhance users' ability to interpret the report accurately. We co-developed potential content presentation strategies through a workshop involving fact-checking professionals, communication experts, and researchers. The workshop examined the significance and utility of elements such as veracity indicators and explored the feasibility of incorporating interactive components for enhanced information disclosure. Building on the workshop outcomes, we conducted an online experiment involving 76 crowd workers to assess the efficacy of different design strategies. The results indicate that proposed strategies significantly improve users' ability to accurately interpret the verdict of fact-checking articles. Our findings underscore the critical role of effective presentation of fact reports in addressing the spread of misinformation. By adopting appropriate design enhancements, the effectiveness of fact-checking reports can be maximized, enabling users to make informed judgments. \u25b3 Less Submitted 23 December, 2023; v1 submitted 20 August, 2023; originally announced August 2023. Comments: Accepted to the 32nd ACM International Conference on Information and Knowledge Management (CIKM '23) ACM Class: H.3.3; H.5.2 arXiv:2308.09896 [pdf, other] Contrastive Learning-based Imputation-Prediction Networks for In-hospital Mortality Risk Modeling using EHRs Authors: Yuxi Liu, Zhenhao Zhang, Shaowen Qin, Flora D. Salim, Antonio Jimeno Yepes Abstract: Predicting the risk of in-hospital mortality from electronic health records (EHRs) has received considerable attention. Such predictions will provide early warning of a patient's health condition to healthcare professionals so that timely interventions can be taken. This prediction task is challenging since EHR data are intrinsically irregular, with not only many missing values but also varying ti\u2026 \u25bd More Predicting the risk of in-hospital mortality from electronic health records (EHRs) has received considerable attention. Such predictions will provide early warning of a patient's health condition to healthcare professionals so that timely interventions can be taken. This prediction task is challenging since EHR data are intrinsically irregular, with not only many missing values but also varying time intervals between medical records. Existing approaches focus on exploiting the variable correlations in patient medical records to impute missing values and establishing time-decay mechanisms to deal with such irregularity. This paper presents a novel contrastive learning-based imputation-prediction network for predicting in-hospital mortality risks using EHR data. Our approach introduces graph analysis-based patient stratification modeling in the imputation process to group similar patients. This allows information of similar patients only to be used, in addition to personal contextual information, for missing value imputation. Moreover, our approach can integrate contrastive learning into the proposed network architecture to enhance patient representation learning and predictive performance on the classification task. Experiments on two real-world EHR datasets show that our approach outperforms the state-of-the-art approaches in both imputation and prediction tasks. \u25b3 Less Submitted 18 August, 2023; originally announced August 2023. Comments: 15 pages, 2 figures, accepted at ECML PKDD 2023 arXiv:2306.11773 [pdf, other] A System of Monitoring and Analyzing Human Indoor Mobility and Air Quality Authors: Kyle K. Qin, Mohammad S. Rahaman, Yongli Ren, Chi-Tsun Cheng, Ivan Cole, Flora D. Salim Abstract: Human movements in the workspace usually have non-negligible relations with air quality parameters (e.g., CO$_2$, PM2.5, and PM10). We establish a system to monitor indoor human mobility with air quality and assess the interrelationship between these two types of time series data. More specifically, a sensor network was designed in indoor environments to observe air quality parameters continuously\u2026 \u25bd More Human movements in the workspace usually have non-negligible relations with air quality parameters (e.g., CO$_2$, PM2.5, and PM10). We establish a system to monitor indoor human mobility with air quality and assess the interrelationship between these two types of time series data. More specifically, a sensor network was designed in indoor environments to observe air quality parameters continuously. Simultaneously, another sensing module detected participants' movements around the study areas. In this module, modern data analysis and machine learning techniques have been applied to reconstruct the trajectories of participants with relevant sensor information. Finally, a further study revealed the correlation between human indoor mobility patterns and indoor air quality parameters. Our experimental results demonstrate that human movements in different environments can significantly impact air quality during busy hours. With the results, we propose recommendations for future studies. \u25b3 Less Submitted 20 June, 2023; originally announced June 2023. Comments: 7 pages, accepted by the 24th IEEE International Conference on Mobile Data Management MSC Class: 68T07 ACM Class: J.0"
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Razzak,+I",
    "title": "Showing 1\u201350 of 65 results for author: Razzak, I",
    "author": "Authors:\nGhadir Alselwi, \n      \n      Hao Xue, \n      \n      Shoaib Jameel, \n      \n      Basem Suleiman, \n      \n      Flora D. Salim, \n      \n      Imran Razzak",
    "body": "arXiv:2503.14800 [pdf, other] Long Context Modeling with Ranked Memory-Augmented Retrieval Authors: Ghadir Alselwi, Hao Xue, Shoaib Jameel, Basem Suleiman, Flora D. Salim, Imran Razzak Abstract: Effective long-term memory management is crucial for language models handling extended contexts. We introduce a novel framework that dynamically ranks memory entries based on relevance. Unlike previous works, our model introduces a novel relevance scoring and a pointwise re-ranking model for key-value embeddings, inspired by learning-to-rank techniques in information retrieval. Enhanced Ranked Mem\u2026 \u25bd More Effective long-term memory management is crucial for language models handling extended contexts. We introduce a novel framework that dynamically ranks memory entries based on relevance. Unlike previous works, our model introduces a novel relevance scoring and a pointwise re-ranking model for key-value embeddings, inspired by learning-to-rank techniques in information retrieval. Enhanced Ranked Memory Augmented Retrieval ERMAR achieves state-of-the-art results on standard benchmarks. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. arXiv:2503.14234 [pdf, other] KG-IRAG: A Knowledge Graph-Based Iterative Retrieval-Augmented Generation Framework for Temporal Reasoning Authors: Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D. Salim Abstract: Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective in enhancing the performance of Large Language Models (LLMs) on tasks that require external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG improves information retrieval for complex reasoning tasks, providing more precise and comprehensive retrieval and generating more accurate responses to QAs. However, most RA\u2026 \u25bd More Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective in enhancing the performance of Large Language Models (LLMs) on tasks that require external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG improves information retrieval for complex reasoning tasks, providing more precise and comprehensive retrieval and generating more accurate responses to QAs. However, most RAG methods fall short in addressing multi-step reasoning, particularly when both information extraction and inference are necessary. To address this limitation, this paper presents Knowledge Graph-Based Iterative Retrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs with iterative reasoning to improve LLMs' ability to handle queries involving temporal and logical dependencies. Through iterative retrieval steps, KG-IRAG incrementally gathers relevant data from external KGs, enabling step-by-step reasoning. The proposed approach is particularly suited for scenarios where reasoning is required alongside dynamic temporal data extraction, such as determining optimal travel times based on weather conditions or traffic patterns. Experimental results show that KG-IRAG improves accuracy in complex reasoning tasks by effectively integrating external knowledge with iterative, logic-based retrieval. Additionally, three new datasets: weatherQA-Irish, weatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's performance, demonstrating its potential beyond traditional RAG applications. \u25b3 Less Submitted 19 March, 2025; v1 submitted 18 March, 2025; originally announced March 2025. Comments: 14 pages, 4 figures arXiv:2503.13560 [pdf, other] MSWAL: 3D Multi-class Segmentation of Whole Abdominal Lesions Dataset Authors: Zhaodong Wu, Qiaochu Zhao, Ming Hu, Yulong Li, Haochen Xue, Kang Dang, Zhengyong Jiang, Angelos Stefanidis, Qiufeng Wang, Imran Razzak, Zongyuan Ge, Junjun He, Yu Qiao, Zhong Zheng, Feilong Tang, Jionglong Su Abstract: With the significantly increasing incidence and prevalence of abdominal diseases, there is a need to embrace greater use of new innovations and technology for the diagnosis and treatment of patients. Although deep-learning methods have notably been developed to assist radiologists in diagnosing abdominal diseases, existing models have the restricted ability to segment common lesions in the abdomen\u2026 \u25bd More With the significantly increasing incidence and prevalence of abdominal diseases, there is a need to embrace greater use of new innovations and technology for the diagnosis and treatment of patients. Although deep-learning methods have notably been developed to assist radiologists in diagnosing abdominal diseases, existing models have the restricted ability to segment common lesions in the abdomen due to missing annotations for typical abdominal pathologies in their training datasets. To address the limitation, we introduce MSWAL, the first 3D Multi-class Segmentation of the Whole Abdominal Lesions dataset, which broadens the coverage of various common lesion types, such as gallstones, kidney stones, liver tumors, kidney tumors, pancreatic cancer, liver cysts, and kidney cysts. With CT scans collected from 694 patients (191,417 slices) of different genders across various scanning phases, MSWAL demonstrates strong robustness and generalizability. The transfer learning experiment from MSWAL to two public datasets, LiTS and KiTS, effectively demonstrates consistent improvements, with Dice Similarity Coefficient (DSC) increase of 3.00% for liver tumors and 0.89% for kidney tumors, demonstrating that the comprehensive annotations and diverse lesion types in MSWAL facilitate effective learning across different domains and data distributions. Furthermore, we propose Inception nnU-Net, a novel segmentation framework that effectively integrates an Inception module with the nnU-Net architecture to extract information from different receptive fields, achieving significant enhancement in both voxel-level DSC and region-level F1 compared to the cutting-edge public algorithms on MSWAL. Our dataset will be released after being accepted, and the code is publicly released at https://github.com/tiuxuxsh76075/MSWAL-. \u25b3 Less Submitted 17 March, 2025; originally announced March 2025. arXiv:2503.05319 [pdf, other] Robust Multimodal Learning for Ophthalmic Disease Grading via Disentangled Representation Authors: Xinkun Wang, Yifang Wang, Senwei Liang, Feilong Tang, Chengzhi Liu, Ming Hu, Chao Hu, Junjun He, Zongyuan Ge, Imran Razzak Abstract: This paper discusses how ophthalmologists often rely on multimodal data to improve diagnostic accuracy. However, complete multimodal data is rare in real-world applications due to a lack of medical equipment and concerns about data privacy. Traditional deep learning methods typically address these issues by learning representations in latent space. However, the paper highlights two key limitations\u2026 \u25bd More This paper discusses how ophthalmologists often rely on multimodal data to improve diagnostic accuracy. However, complete multimodal data is rare in real-world applications due to a lack of medical equipment and concerns about data privacy. Traditional deep learning methods typically address these issues by learning representations in latent space. However, the paper highlights two key limitations of these approaches: (i) Task-irrelevant redundant information (e.g., numerous slices) in complex modalities leads to significant redundancy in latent space representations. (ii) Overlapping multimodal representations make it difficult to extract unique features for each modality. To overcome these challenges, the authors propose the Essence-Point and Disentangle Representation Learning (EDRL) strategy, which integrates a self-distillation mechanism into an end-to-end framework to enhance feature selection and disentanglement for more robust multimodal learning. Specifically, the Essence-Point Representation Learning module selects discriminative features that improve disease grading performance. The Disentangled Representation Learning module separates multimodal data into modality-common and modality-unique representations, reducing feature entanglement and enhancing both robustness and interpretability in ophthalmic disease diagnosis. Experiments on multimodal ophthalmology datasets show that the proposed EDRL strategy significantly outperforms current state-of-the-art methods. \u25b3 Less Submitted 7 March, 2025; originally announced March 2025. Comments: 10pages arXiv:2502.11903 [pdf, other] MMRC: A Large-Scale Benchmark for Understanding Multimodal Large Language Model in Real-World Conversation Authors: Haochen Xue, Feilong Tang, Ming Hu, Yexin Liu, Qidong Huang, Yulong Li, Chengzhi Liu, Zhongxing Xu, Chong Zhang, Chun-Mei Feng, Yutong Xie, Imran Razzak, Zongyuan Ge, Jionglong Su, Junjun He, Yu Qiao Abstract: Recent multimodal large language models (MLLMs) have demonstrated significant potential in open-ended conversation, generating more accurate and personalized responses. However, their abilities to memorize, recall, and reason in sustained interactions within real-world scenarios remain underexplored. This paper introduces MMRC, a Multi-Modal Real-world Conversation benchmark for evaluating six cor\u2026 \u25bd More Recent multimodal large language models (MLLMs) have demonstrated significant potential in open-ended conversation, generating more accurate and personalized responses. However, their abilities to memorize, recall, and reason in sustained interactions within real-world scenarios remain underexplored. This paper introduces MMRC, a Multi-Modal Real-world Conversation benchmark for evaluating six core open-ended abilities of MLLMs: information extraction, multi-turn reasoning, information update, image management, memory recall, and answer refusal. With data collected from real-world scenarios, MMRC comprises 5,120 conversations and 28,720 corresponding manually labeled questions, posing a significant challenge to existing MLLMs. Evaluations on 20 MLLMs in MMRC indicate an accuracy drop during open-ended interactions. We identify four common failure patterns: long-term memory degradation, inadequacies in updating factual knowledge, accumulated assumption of error propagation, and reluctance to say no. To mitigate these issues, we propose a simple yet effective NOTE-TAKING strategy, which can record key information from the conversation and remind the model during its responses, enhancing conversational capabilities. Experiments across six MLLMs demonstrate significant performance improvements. \u25b3 Less Submitted 8 March, 2025; v1 submitted 17 February, 2025; originally announced February 2025. arXiv:2501.06827 [pdf, other] Leveraging Taxonomy and LLMs for Improved Multimodal Hierarchical Classification Authors: Shijing Chen, Mohamed Reda Bouadjenek, Shoaib Jameel, Usman Naseem, Basem Suleiman, Flora D. Salim, Hakim Hacid, Imran Razzak Abstract: Multi-level Hierarchical Classification (MLHC) tackles the challenge of categorizing items within a complex, multi-layered class structure. However, traditional MLHC classifiers often rely on a backbone model with independent output layers, which tend to ignore the hierarchical relationships between classes. This oversight can lead to inconsistent predictions that violate the underlying taxonomy.\u2026 \u25bd More Multi-level Hierarchical Classification (MLHC) tackles the challenge of categorizing items within a complex, multi-layered class structure. However, traditional MLHC classifiers often rely on a backbone model with independent output layers, which tend to ignore the hierarchical relationships between classes. This oversight can lead to inconsistent predictions that violate the underlying taxonomy. Leveraging Large Language Models (LLMs), we propose a novel taxonomy-embedded transitional LLM-agnostic framework for multimodality classification. The cornerstone of this advancement is the ability of models to enforce consistency across hierarchical levels. Our evaluations on the MEP-3M dataset - a multi-modal e-commerce product dataset with various hierarchical levels - demonstrated a significant performance improvement compared to conventional LLM structures. \u25b3 Less Submitted 12 January, 2025; originally announced January 2025. Comments: 11 pages, 7 figures, 2 tables, and accepted by COLING 2025 arXiv:2501.03939 [pdf, other] Visual question answering: from early developments to recent advances -- a survey Authors: Ngoc Dung Huynh, Mohamed Reda Bouadjenek, Sunil Aryal, Imran Razzak, Hakim Hacid Abstract: Visual Question Answering (VQA) is an evolving research field aimed at enabling machines to answer questions about visual content by integrating image and language processing techniques such as feature extraction, object detection, text embedding, natural language understanding, and language generation. With the growth of multimodal data research, VQA has gained significant attention due to its br\u2026 \u25bd More Visual Question Answering (VQA) is an evolving research field aimed at enabling machines to answer questions about visual content by integrating image and language processing techniques such as feature extraction, object detection, text embedding, natural language understanding, and language generation. With the growth of multimodal data research, VQA has gained significant attention due to its broad applications, including interactive educational tools, medical image diagnosis, customer service, entertainment, and social media captioning. Additionally, VQA plays a vital role in assisting visually impaired individuals by generating descriptive content from images. This survey introduces a taxonomy of VQA architectures, categorizing them based on design choices and key components to facilitate comparative analysis and evaluation. We review major VQA approaches, focusing on deep learning-based methods, and explore the emerging field of Large Visual Language Models (LVLMs) that have demonstrated success in multimodal tasks like VQA. The paper further examines available datasets and evaluation metrics essential for measuring VQA system performance, followed by an exploration of real-world VQA applications. Finally, we highlight ongoing challenges and future directions in VQA research, presenting open questions and potential areas for further development. This survey serves as a comprehensive resource for researchers and practitioners interested in the latest advancements and future \u25b3 Less Submitted 11 January, 2025; v1 submitted 7 January, 2025; originally announced January 2025. Comments: 20 papers arXiv:2412.14175 [pdf, other] BiTSA: Leveraging Time Series Foundation Model for Building Energy Analytics Authors: Xiachong Lin, Arian Prabowo, Imran Razzak, Hao Xue, Matthew Amos, Sam Behrens, Flora D. Salim Abstract: Incorporating AI technologies into digital infrastructure offers transformative potential for energy management, particularly in enhancing energy efficiency and supporting net-zero objectives. However, the complexity of IoT-generated datasets often poses a significant challenge, hindering the translation of research insights into practical, real-world applications. This paper presents the design o\u2026 \u25bd More Incorporating AI technologies into digital infrastructure offers transformative potential for energy management, particularly in enhancing energy efficiency and supporting net-zero objectives. However, the complexity of IoT-generated datasets often poses a significant challenge, hindering the translation of research insights into practical, real-world applications. This paper presents the design of an interactive visualization tool, BiTSA. The tool enables building managers to interpret complex energy data quickly and take immediate, data-driven actions based on real-time insights. By integrating advanced forecasting models with an intuitive visual interface, our solution facilitates proactive decision-making, optimizes energy consumption, and promotes sustainable building management practices. BiTSA will empower building managers to optimize energy consumption, control demand-side energy usage, and achieve sustainability goals. \u25b3 Less Submitted 20 November, 2024; originally announced December 2024. Comments: 4 pages, 4 figures, 3 tables arXiv:2412.12164 [pdf, other] GAMED: Knowledge Adaptive Multi-Experts Decoupling for Multimodal Fake News Detection Authors: Lingzhi Shen, Yunfei Long, Xiaohao Cai, Imran Razzak, Guanming Chen, Kang Liu, Shoaib Jameel Abstract: Multimodal fake news detection often involves modelling heterogeneous data sources, such as vision and language. Existing detection methods typically rely on fusion effectiveness and cross-modal consistency to model the content, complicating understanding how each modality affects prediction accuracy. Additionally, these methods are primarily based on static feature modelling, making it difficult\u2026 \u25bd More Multimodal fake news detection often involves modelling heterogeneous data sources, such as vision and language. Existing detection methods typically rely on fusion effectiveness and cross-modal consistency to model the content, complicating understanding how each modality affects prediction accuracy. Additionally, these methods are primarily based on static feature modelling, making it difficult to adapt to the dynamic changes and relationships between different data modalities. This paper develops a significantly novel approach, GAMED, for multimodal modelling, which focuses on generating distinctive and discriminative features through modal decoupling to enhance cross-modal synergies, thereby optimizing overall performance in the detection process. GAMED leverages multiple parallel expert networks to refine features and pre-embed semantic knowledge to improve the experts' ability in information selection and viewpoint sharing. Subsequently, the feature distribution of each modality is adaptively adjusted based on the respective experts' opinions. GAMED also introduces a novel classification technique to dynamically manage contributions from different modalities, while improving the explainability of decisions. Experimental results on the Fakeddit and Yang datasets demonstrate that GAMED performs better than recently developed state-of-the-art models. The source code can be accessed at https://github.com/slz0925/GAMED. \u25b3 Less Submitted 2 March, 2025; v1 submitted 11 December, 2024; originally announced December 2024. arXiv:2412.11387 [pdf, other] How Can LLMs and Knowledge Graphs Contribute to Robot Safety? A Few-Shot Learning Approach Authors: Abdulrahman Althobaiti, Angel Ayala, JingYing Gao, Ali Almutairi, Mohammad Deghat, Imran Razzak, Francisco Cruz Abstract: Large Language Models (LLMs) are transforming the robotics domain by enabling robots to comprehend and execute natural language instructions. The cornerstone benefits of LLM include processing textual data from technical manuals, instructions, academic papers, and user queries based on the knowledge provided. However, deploying LLM-generated code in robotic systems without safety verification pose\u2026 \u25bd More Large Language Models (LLMs) are transforming the robotics domain by enabling robots to comprehend and execute natural language instructions. The cornerstone benefits of LLM include processing textual data from technical manuals, instructions, academic papers, and user queries based on the knowledge provided. However, deploying LLM-generated code in robotic systems without safety verification poses significant risks. This paper outlines a safety layer that verifies the code generated by ChatGPT before executing it to control a drone in a simulated environment. The safety layer consists of a fine-tuned GPT-4o model using Few-Shot learning, supported by knowledge graph prompting (KGP). Our approach improves the safety and compliance of robotic actions, ensuring that they adhere to the regulations of drone operations. \u25b3 Less Submitted 15 December, 2024; originally announced December 2024. arXiv:2412.05632 [pdf, other] Biological Brain Age Estimation using Sex-Aware Adversarial Variational Autoencoder with Multimodal Neuroimages Authors: Abd Ur Rehman, Azka Rehman, Muhammad Usman, Abdullah Shahid, Sung-Min Gho, Aleum Lee, Tariq M. Khan, Imran Razzak Abstract: Brain aging involves structural and functional changes and therefore serves as a key biomarker for brain health. Combining structural magnetic resonance imaging (sMRI) and functional magnetic resonance imaging (fMRI) has the potential to improve brain age estimation by leveraging complementary data. However, fMRI data, being noisier than sMRI, complicates multimodal fusion. Traditional fusion meth\u2026 \u25bd More Brain aging involves structural and functional changes and therefore serves as a key biomarker for brain health. Combining structural magnetic resonance imaging (sMRI) and functional magnetic resonance imaging (fMRI) has the potential to improve brain age estimation by leveraging complementary data. However, fMRI data, being noisier than sMRI, complicates multimodal fusion. Traditional fusion methods often introduce more noise than useful information, which can reduce accuracy compared to using sMRI alone. In this paper, we propose a novel multimodal framework for biological brain age estimation, utilizing a sex-aware adversarial variational autoencoder (SA-AVAE). Our framework integrates adversarial and variational learning to effectively disentangle the latent features from both modalities. Specifically, we decompose the latent space into modality-specific codes and shared codes to represent complementary and common information across modalities, respectively. To enhance the disentanglement, we introduce cross-reconstruction and shared-distinct distance ratio loss as regularization terms. Importantly, we incorporate sex information into the learned latent code, enabling the model to capture sex-specific aging patterns for brain age estimation via an integrated regressor module. We evaluate our model using the publicly available OpenBHB dataset, a comprehensive multi-site dataset for brain age estimation. The results from ablation studies and comparisons with state-of-the-art methods demonstrate that our framework outperforms existing approaches and shows significant robustness across various age groups, highlighting its potential for real-time clinical applications in the early detection of neurodegenerative diseases. \u25b3 Less Submitted 7 December, 2024; originally announced December 2024. arXiv:2412.04908 [pdf, other] MERCI: Multimodal Emotional and peRsonal Conversational Interactions Dataset Authors: Mohammed Althubyani, Zhijin Meng, Shengyuan Xie, Cha Seung, Imran Razzak, Eduardo B. Sandoval, Baki Kocaballi, Francisco Cruz Abstract: The integration of conversational agents into our daily lives has become increasingly common, yet many of these agents cannot engage in deep interactions with humans. Despite this, there is a noticeable shortage of datasets that capture multimodal information from human-robot interaction dialogues. To address this gap, we have recorded a novel multimodal dataset (MERCI) that encompasses rich embod\u2026 \u25bd More The integration of conversational agents into our daily lives has become increasingly common, yet many of these agents cannot engage in deep interactions with humans. Despite this, there is a noticeable shortage of datasets that capture multimodal information from human-robot interaction dialogues. To address this gap, we have recorded a novel multimodal dataset (MERCI) that encompasses rich embodied interaction data. The process involved asking participants to complete a questionnaire and gathering their profiles on ten topics, such as hobbies and favorite music. Subsequently, we initiated conversations between the robot and the participants, leveraging GPT-4 to generate contextually appropriate responses based on the participant's profile and emotional state, as determined by facial expression recognition and sentiment analysis. Automatic and user evaluations were conducted to assess the overall quality of the collected data. The results of both evaluations indicated a high level of naturalness, engagement, fluency, consistency, and relevance in the conversation, as well as the robot's ability to provide empathetic responses. It is worth noting that the dataset is derived from genuine interactions with the robot, involving participants who provided personal information and conveyed actual emotions. \u25b3 Less Submitted 18 March, 2025; v1 submitted 6 December, 2024; originally announced December 2024. Comments: 9 pages, 5 Figures, Rejected from International Conference of Human Robot Interaction 2025, Melbourne, Australia ACM Class: K.4.0 arXiv:2411.12000 [pdf, other] ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity Authors: Tong Xie, Hanzhi Zhang, Shaozhou Wang, Yuwei Wan, Imran Razzak, Chunyu Kit, Wenjie Zhang, Bram Hoex Abstract: Natural Language Processing (NLP) is widely used to supply summarization ability from long context to structured information. However, extracting structured knowledge from scientific text by NLP models remains a challenge because of its domain-specific nature to complex data preprocessing and the granularity of multi-layered device-level information. To address this, we introduce ByteScience, a no\u2026 \u25bd More Natural Language Processing (NLP) is widely used to supply summarization ability from long context to structured information. However, extracting structured knowledge from scientific text by NLP models remains a challenge because of its domain-specific nature to complex data preprocessing and the granularity of multi-layered device-level information. To address this, we introduce ByteScience, a non-profit cloud-based auto fine-tuned Large Language Model (LLM) platform, which is designed to extract structured scientific data and synthesize new scientific knowledge from vast scientific corpora. The platform capitalizes on DARWIN, an open-source, fine-tuned LLM dedicated to natural science. The platform was built on Amazon Web Services (AWS) and provides an automated, user-friendly workflow for custom model development and data extraction. The platform achieves remarkable accuracy with only a small amount of well-annotated articles. This innovative tool streamlines the transition from the science literature to structured knowledge and data and benefits the advancements in natural informatics. \u25b3 Less Submitted 6 December, 2024; v1 submitted 18 November, 2024; originally announced November 2024. arXiv:2411.10100 [pdf, other] Multi-Task Adversarial Variational Autoencoder for Estimating Biological Brain Age with Multimodal Neuroimaging Authors: Muhammad Usman, Azka Rehman, Abdullah Shahid, Abd Ur Rehman, Sung-Min Gho, Aleum Lee, Tariq M. Khan, Imran Razzak Abstract: Despite advances in deep learning for estimating brain age from structural MRI data, incorporating functional MRI data is challenging due to its complex structure and the noisy nature of functional connectivity measurements. To address this, we present the Multitask Adversarial Variational Autoencoder, a custom deep learning framework designed to improve brain age predictions through multimodal MR\u2026 \u25bd More Despite advances in deep learning for estimating brain age from structural MRI data, incorporating functional MRI data is challenging due to its complex structure and the noisy nature of functional connectivity measurements. To address this, we present the Multitask Adversarial Variational Autoencoder, a custom deep learning framework designed to improve brain age predictions through multimodal MRI data integration. This model separates latent variables into generic and unique codes, isolating shared and modality-specific features. By integrating multitask learning with sex classification as an additional task, the model captures sex-specific aging patterns. Evaluated on the OpenBHB dataset, a large multisite brain MRI collection, the model achieves a mean absolute error of 2.77 years, outperforming traditional methods. This success positions M-AVAE as a powerful tool for metaverse-based healthcare applications in brain age estimation. \u25b3 Less Submitted 15 November, 2024; originally announced November 2024. arXiv:2411.09593 [pdf, other] SMILE-UHURA Challenge -- Small Vessel Segmentation at Mesoscopic Scale from Ultra-High Resolution 7T Magnetic Resonance Angiograms Authors: Soumick Chatterjee, Hendrik Mattern, Marc D\u00f6rner, Alessandro Sciarra, Florian Dubost, Hannes Schnurre, Rupali Khatun, Chun-Chih Yu, Tsung-Lin Hsieh, Yi-Shan Tsai, Yi-Zeng Fang, Yung-Ching Yang, Juinn-Dar Huang, Marshall Xu, Siyu Liu, Fernanda L. Ribeiro, Saskia Bollmann, Karthikesh Varma Chintalapati, Chethan Mysuru Radhakrishna, Sri Chandana Hudukula Ram Kumara, Raviteja Sutrave, Abdul Qayyum, Moona Mazher, Imran Razzak, Cristobal Rodero , et al. (23 additional authors not shown) Abstract: The human brain receives nutrients and oxygen through an intricate network of blood vessels. Pathology affecting small vessels, at the mesoscopic scale, represents a critical vulnerability within the cerebral blood supply and can lead to severe conditions, such as Cerebral Small Vessel Diseases. The advent of 7 Tesla MRI systems has enabled the acquisition of higher spatial resolution images, maki\u2026 \u25bd More The human brain receives nutrients and oxygen through an intricate network of blood vessels. Pathology affecting small vessels, at the mesoscopic scale, represents a critical vulnerability within the cerebral blood supply and can lead to severe conditions, such as Cerebral Small Vessel Diseases. The advent of 7 Tesla MRI systems has enabled the acquisition of higher spatial resolution images, making it possible to visualise such vessels in the brain. However, the lack of publicly available annotated datasets has impeded the development of robust, machine learning-driven segmentation algorithms. To address this, the SMILE-UHURA challenge was organised. This challenge, held in conjunction with the ISBI 2023, in Cartagena de Indias, Colombia, aimed to provide a platform for researchers working on related topics. The SMILE-UHURA challenge addresses the gap in publicly available annotated datasets by providing an annotated dataset of Time-of-Flight angiography acquired with 7T MRI. This dataset was created through a combination of automated pre-segmentation and extensive manual refinement. In this manuscript, sixteen submitted methods and two baseline methods are compared both quantitatively and qualitatively on two different datasets: held-out test MRAs from the same dataset as the training data (with labels kept secret) and a separate 7T ToF MRA dataset where both input volumes and labels are kept secret. The results demonstrate that most of the submitted deep learning methods, trained on the provided training dataset, achieved reliable segmentation performance. Dice scores reached up to 0.838 $\\pm$ 0.066 and 0.716 $\\pm$ 0.125 on the respective datasets, with an average performance of up to 0.804 $\\pm$ 0.15. \u25b3 Less Submitted 14 November, 2024; originally announced November 2024. arXiv:2411.08888 [pdf, other] Exploring Capabilities of Time Series Foundation Models in Building Analytics Authors: Xiachong Lin, Arian Prabowo, Imran Razzak, Hao Xue, Matthew Amos, Sam Behrens, Flora D. Salim Abstract: The growing integration of digitized infrastructure with Internet of Things (IoT) networks has transformed the management and optimization of building energy consumption. By leveraging IoT-based monitoring systems, stakeholders such as building managers, energy suppliers, and policymakers can make data-driven decisions to improve energy efficiency. However, accurate energy forecasting and analytic\u2026 \u25bd More The growing integration of digitized infrastructure with Internet of Things (IoT) networks has transformed the management and optimization of building energy consumption. By leveraging IoT-based monitoring systems, stakeholders such as building managers, energy suppliers, and policymakers can make data-driven decisions to improve energy efficiency. However, accurate energy forecasting and analytics face persistent challenges, primarily due to the inherent physical constraints of buildings and the diverse, heterogeneous nature of IoT-generated data. In this study, we conduct a comprehensive benchmarking of two publicly available IoT datasets, evaluating the performance of time series foundation models in the context of building energy analytics. Our analysis shows that single-modal models demonstrate significant promise in overcoming the complexities of data variability and physical limitations in buildings, with future work focusing on optimizing multi-modal models for sustainable energy management. \u25b3 Less Submitted 27 October, 2024; originally announced November 2024. Comments: 7 pages, 1 figures, and 4 tables arXiv:2410.22648 [pdf, other] SimpsonsVQA: Enhancing Inquiry-Based Learning with a Tailored Dataset Authors: Ngoc Dung Huynh, Mohamed Reda Bouadjenek, Sunil Aryal, Imran Razzak, Hakim Hacid Abstract: Visual Question Answering (VQA) has emerged as a promising area of research to develop AI-based systems for enabling interactive and immersive learning. Numerous VQA datasets have been introduced to facilitate various tasks, such as answering questions or identifying unanswerable ones. However, most of these datasets are constructed using real-world images, leaving the performance of existing mode\u2026 \u25bd More Visual Question Answering (VQA) has emerged as a promising area of research to develop AI-based systems for enabling interactive and immersive learning. Numerous VQA datasets have been introduced to facilitate various tasks, such as answering questions or identifying unanswerable ones. However, most of these datasets are constructed using real-world images, leaving the performance of existing models on cartoon images largely unexplored. Hence, in this paper, we present \"SimpsonsVQA\", a novel dataset for VQA derived from The Simpsons TV show, designed to promote inquiry-based learning. Our dataset is specifically designed to address not only the traditional VQA task but also to identify irrelevant questions related to images, as well as the reverse scenario where a user provides an answer to a question that the system must evaluate (e.g., as correct, incorrect, or ambiguous). It aims to cater to various visual applications, harnessing the visual content of \"The Simpsons\" to create engaging and informative interactive systems. SimpsonsVQA contains approximately 23K images, 166K QA pairs, and 500K judgments (https://simpsonsvqa.org). Our experiments show that current large vision-language models like ChatGPT4o underperform in zero-shot settings across all three tasks, highlighting the dataset's value for improving model performance on cartoon images. We anticipate that SimpsonsVQA will inspire further research, innovation, and advancements in inquiry-based learning VQA. \u25b3 Less Submitted 29 October, 2024; originally announced October 2024. arXiv:2410.13153 [pdf, other] Better to Ask in English: Evaluation of Large Language Models on English, Low-resource and Cross-Lingual Settings Authors: Krishno Dey, Prerona Tarannum, Md. Arid Hasan, Imran Razzak, Usman Naseem Abstract: Large Language Models (LLMs) are trained on massive amounts of data, enabling their application across diverse domains and tasks. Despite their remarkable performance, most LLMs are developed and evaluated primarily in English. Recently, a few multi-lingual LLMs have emerged, but their performance in low-resource languages, especially the most spoken languages in South Asia, is less explored. To a\u2026 \u25bd More Large Language Models (LLMs) are trained on massive amounts of data, enabling their application across diverse domains and tasks. Despite their remarkable performance, most LLMs are developed and evaluated primarily in English. Recently, a few multi-lingual LLMs have emerged, but their performance in low-resource languages, especially the most spoken languages in South Asia, is less explored. To address this gap, in this study, we evaluate LLMs such as GPT-4, Llama 2, and Gemini to analyze their effectiveness in English compared to other low-resource languages from South Asia (e.g., Bangla, Hindi, and Urdu). Specifically, we utilized zero-shot prompting and five different prompt settings to extensively investigate the effectiveness of the LLMs in cross-lingual translated prompts. The findings of the study suggest that GPT-4 outperformed Llama 2 and Gemini in all five prompt settings and across all languages. Moreover, all three LLMs performed better for English language prompts than other low-resource language prompts. This study extensively investigates LLMs in low-resource language contexts to highlight the improvements required in LLMs and language-specific resources to develop more generally purposed NLP applications. \u25b3 Less Submitted 16 October, 2024; originally announced October 2024. arXiv:2409.15501 [pdf, other] Adenocarcinoma Segmentation Using Pre-trained Swin-UNet with Parallel Cross-Attention for Multi-Domain Imaging Authors: Abdul Qayyum, Moona Mazher Imran Razzak, Steven A Niederer Abstract: Computer aided pathological analysis has been the gold standard for tumor diagnosis, however domain shift is a significant problem in histopathology. It may be caused by variability in anatomical structures, tissue preparation, and imaging processes challenges the robustness of segmentation models. In this work, we present a framework consist of pre-trained encoder with a Swin-UNet architecture en\u2026 \u25bd More Computer aided pathological analysis has been the gold standard for tumor diagnosis, however domain shift is a significant problem in histopathology. It may be caused by variability in anatomical structures, tissue preparation, and imaging processes challenges the robustness of segmentation models. In this work, we present a framework consist of pre-trained encoder with a Swin-UNet architecture enhanced by a parallel cross-attention module to tackle the problem of adenocarcinoma segmentation across different organs and scanners, considering both morphological changes and scanner-induced domain variations. Experiment conducted on Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation challenge dataset showed that our framework achieved segmentation scores of 0.7469 for the cross-organ track and 0.7597 for the cross-scanner track on the final challenge test sets, and effectively navigates diverse imaging conditions and improves segmentation accuracy across varying domains. \u25b3 Less Submitted 23 September, 2024; originally announced September 2024. Comments: 6 pages 2 figures arXiv:2409.11802 [pdf, other] Latent fingerprint enhancement for accurate minutiae detection Authors: Abdul Wahab, Tariq Mahmood Khan, Shahzaib Iqbal, Bandar AlShammari, Bandar Alhaqbani, Imran Razzak Abstract: Identification of suspects based on partial and smudged fingerprints, commonly referred to as fingermarks or latent fingerprints, presents a significant challenge in the field of fingerprint recognition. Although fixed-length embeddings have shown effectiveness in recognising rolled and slap fingerprints, the methods for matching latent fingerprints have primarily centred around local minutiae-bas\u2026 \u25bd More Identification of suspects based on partial and smudged fingerprints, commonly referred to as fingermarks or latent fingerprints, presents a significant challenge in the field of fingerprint recognition. Although fixed-length embeddings have shown effectiveness in recognising rolled and slap fingerprints, the methods for matching latent fingerprints have primarily centred around local minutiae-based embeddings, failing to fully exploit global representations for matching purposes. Consequently, enhancing latent fingerprints becomes critical to ensuring robust identification for forensic investigations. Current approaches often prioritise restoring ridge patterns, overlooking the fine-macroeconomic details crucial for accurate fingerprint recognition. To address this, we propose a novel approach that uses generative adversary networks (GANs) to redefine Latent Fingerprint Enhancement (LFE) through a structured approach to fingerprint generation. By directly optimising the minutiae information during the generation process, the model produces enhanced latent fingerprints that exhibit exceptional fidelity to ground-truth instances. This leads to a significant improvement in identification performance. Our framework integrates minutiae locations and orientation fields, ensuring the preservation of both local and structural fingerprint features. Extensive evaluations conducted on two publicly available datasets demonstrate our method's dominance over existing state-of-the-art techniques, highlighting its potential to significantly enhance latent fingerprint recognition accuracy in forensic applications. \u25b3 Less Submitted 18 September, 2024; originally announced September 2024. arXiv:2409.01572 [pdf, other] LSSF-Net: Lightweight Segmentation with Self-Awareness, Spatial Attention, and Focal Modulation Authors: Hamza Farooq, Zuhair Zafar, Ahsan Saadat, Tariq M Khan, Shahzaib Iqbal, Imran Razzak Abstract: Accurate segmentation of skin lesions within dermoscopic images plays a crucial role in the timely identification of skin cancer for computer-aided diagnosis on mobile platforms. However, varying shapes of the lesions, lack of defined edges, and the presence of obstructions such as hair strands and marker colors make this challenge more complex. \\textcolor{red}Additionally, skin lesions often exhi\u2026 \u25bd More Accurate segmentation of skin lesions within dermoscopic images plays a crucial role in the timely identification of skin cancer for computer-aided diagnosis on mobile platforms. However, varying shapes of the lesions, lack of defined edges, and the presence of obstructions such as hair strands and marker colors make this challenge more complex. \\textcolor{red}Additionally, skin lesions often exhibit subtle variations in texture and color that are difficult to differentiate from surrounding healthy skin, necessitating models that can capture both fine-grained details and broader contextual information. Currently, melanoma segmentation models are commonly based on fully connected networks and U-Nets. However, these models often struggle with capturing the complex and varied characteristics of skin lesions, such as the presence of indistinct boundaries and diverse lesion appearances, which can lead to suboptimal segmentation performance.To address these challenges, we propose a novel lightweight network specifically designed for skin lesion segmentation utilizing mobile devices, featuring a minimal number of learnable parameters (only 0.8 million). This network comprises an encoder-decoder architecture that incorporates conformer-based focal modulation attention, self-aware local and global spatial attention, and split channel-shuffle. The efficacy of our model has been evaluated on four well-established benchmark datasets for skin lesion segmentation: ISIC 2016, ISIC 2017, ISIC 2018, and PH2. Empirical findings substantiate its state-of-the-art performance, notably reflected in a high Jaccard index. \u25b3 Less Submitted 2 September, 2024; originally announced September 2024. arXiv:2409.01184 [pdf, other] PitVis-2023 Challenge: Workflow Recognition in videos of Endoscopic Pituitary Surgery Authors: Adrito Das, Danyal Z. Khan, Dimitrios Psychogyios, Yitong Zhang, John G. Hanrahan, Francisco Vasconcelos, You Pang, Zhen Chen, Jinlin Wu, Xiaoyang Zou, Guoyan Zheng, Abdul Qayyum, Moona Mazher, Imran Razzak, Tianbin Li, Jin Ye, Junjun He, Szymon P\u0142otka, Joanna Kaleta, Amine Yamlahi, Antoine Jund, Patrick Godau, Satoshi Kondo, Satoshi Kasai, Kousuke Hirasawa , et al. (7 additional authors not shown) Abstract: The field of computer vision applied to videos of minimally invasive surgery is ever-growing. Workflow recognition pertains to the automated recognition of various aspects of a surgery: including which surgical steps are performed; and which surgical instruments are used. This information can later be used to assist clinicians when learning the surgery; during live surgery; and when writing operat\u2026 \u25bd More The field of computer vision applied to videos of minimally invasive surgery is ever-growing. Workflow recognition pertains to the automated recognition of various aspects of a surgery: including which surgical steps are performed; and which surgical instruments are used. This information can later be used to assist clinicians when learning the surgery; during live surgery; and when writing operation notes. The Pituitary Vision (PitVis) 2023 Challenge tasks the community to step and instrument recognition in videos of endoscopic pituitary surgery. This is a unique task when compared to other minimally invasive surgeries due to the smaller working space, which limits and distorts vision; and higher frequency of instrument and step switching, which requires more precise model predictions. Participants were provided with 25-videos, with results presented at the MICCAI-2023 conference as part of the Endoscopic Vision 2023 Challenge in Vancouver, Canada, on 08-Oct-2023. There were 18-submissions from 9-teams across 6-countries, using a variety of deep learning models. A commonality between the top performing models was incorporating spatio-temporal and multi-task methods, with greater than 50% and 10% macro-F1-score improvement over purely spacial single-task models in step and instrument recognition respectively. The PitVis-2023 Challenge therefore demonstrates state-of-the-art computer vision models in minimally invasive surgery are transferable to a new dataset, with surgery specific techniques used to enhance performance, progressing the field further. Benchmark results are provided in the paper, and the dataset is publicly available at: https://doi.org/10.5522/04/26531686. \u25b3 Less Submitted 2 September, 2024; originally announced September 2024. arXiv:2408.12323 [pdf, other] EUIS-Net: A Convolutional Neural Network for Efficient Ultrasound Image Segmentation Authors: Shahzaib Iqbal, Hasnat Ahmed, Muhammad Sharif, Madiha Hena, Tariq M. Khan, Imran Razzak Abstract: Segmenting ultrasound images is critical for various medical applications, but it offers significant challenges due to ultrasound images' inherent noise and unpredictability. To address these challenges, we proposed EUIS-Net, a CNN network designed to segment ultrasound images efficiently and precisely. The proposed EUIS-Net utilises four encoder-decoder blocks, resulting in a notable decrease in\u2026 \u25bd More Segmenting ultrasound images is critical for various medical applications, but it offers significant challenges due to ultrasound images' inherent noise and unpredictability. To address these challenges, we proposed EUIS-Net, a CNN network designed to segment ultrasound images efficiently and precisely. The proposed EUIS-Net utilises four encoder-decoder blocks, resulting in a notable decrease in computational complexity while achieving excellent performance. The proposed EUIS-Net integrates both channel and spatial attention mechanisms into the bottleneck to improve feature representation and collect significant contextual information. In addition, EUIS-Net incorporates a region-aware attention module in skip connections, which enhances the ability to concentrate on the region of the injury. To enable thorough information exchange across various network blocks, skip connection aggregation is employed from the network's lowermost to the uppermost block. Comprehensive evaluations are conducted on two publicly available ultrasound image segmentation datasets. The proposed EUIS-Net achieved mean IoU and dice scores of 78. 12\\%, 85. 42\\% and 84. 73\\%, 89. 01\\% in the BUSI and DDTI datasets, respectively. The findings of our study showcase the substantial capabilities of EUIS-Net for immediate use in clinical settings and its versatility in various ultrasound imaging tasks. \u25b3 Less Submitted 22 August, 2024; originally announced August 2024. arXiv:2408.09687 [pdf, other] TESL-Net: A Transformer-Enhanced CNN for Accurate Skin Lesion Segmentation Authors: Shahzaib Iqbal, Muhammad Zeeshan, Mehwish Mehmood, Tariq M. Khan, Imran Razzak Abstract: Early detection of skin cancer relies on precise segmentation of dermoscopic images of skin lesions. However, this task is challenging due to the irregular shape of the lesion, the lack of sharp borders, and the presence of artefacts such as marker colours and hair follicles. Recent methods for melanoma segmentation are U-Nets and fully connected networks (FCNs). As the depth of these neural netwo\u2026 \u25bd More Early detection of skin cancer relies on precise segmentation of dermoscopic images of skin lesions. However, this task is challenging due to the irregular shape of the lesion, the lack of sharp borders, and the presence of artefacts such as marker colours and hair follicles. Recent methods for melanoma segmentation are U-Nets and fully connected networks (FCNs). As the depth of these neural network models increases, they can face issues like the vanishing gradient problem and parameter redundancy, potentially leading to a decrease in the Jaccard index of the segmentation model. In this study, we introduced a novel network named TESL-Net for the segmentation of skin lesions. The proposed TESL-Net involves a hybrid network that combines the local features of a CNN encoder-decoder architecture with long-range and temporal dependencies using bi-convolutional long-short-term memory (Bi-ConvLSTM) networks and a Swin transformer. This enables the model to account for the uncertainty of segmentation over time and capture contextual channel relationships in the data. We evaluated the efficacy of TESL-Net in three commonly used datasets (ISIC 2016, ISIC 2017, and ISIC 2018) for the segmentation of skin lesions. The proposed TESL-Net achieves state-of-the-art performance, as evidenced by a significantly elevated Jaccard index demonstrated by empirical results. \u25b3 Less Submitted 18 August, 2024; originally announced August 2024. arXiv:2408.09426 [pdf, other] A Robust Algorithm for Contactless Fingerprint Enhancement and Matching Authors: Mahrukh Siddiqui, Shahzaib Iqbal, Bandar AlShammari, Bandar Alhaqbani, Tariq M. Khan, Imran Razzak Abstract: Compared to contact fingerprint images, contactless fingerprint images exhibit four distinct characteristics: (1) they contain less noise; (2) they have fewer discontinuities in ridge patterns; (3) the ridge-valley pattern is less distinct; and (4) they pose an interoperability problem, as they lack the elastic deformation caused by pressing the finger against the capture device. These properties\u2026 \u25bd More Compared to contact fingerprint images, contactless fingerprint images exhibit four distinct characteristics: (1) they contain less noise; (2) they have fewer discontinuities in ridge patterns; (3) the ridge-valley pattern is less distinct; and (4) they pose an interoperability problem, as they lack the elastic deformation caused by pressing the finger against the capture device. These properties present significant challenges for the enhancement of contactless fingerprint images. In this study, we propose a novel contactless fingerprint identification solution that enhances the accuracy of minutiae detection through improved frequency estimation and a new region-quality-based minutia extraction algorithm. In addition, we introduce an efficient and highly accurate minutiae-based encoding and matching algorithm. We validate the effectiveness of our approach through extensive experimental testing. Our method achieves a minimum Equal Error Rate (EER) of 2.84\\% on the PolyU contactless fingerprint dataset, demonstrating its superior performance compared to existing state-of-the-art techniques. The proposed fingerprint identification method exhibits notable precision and resilience, proving to be an effective and feasible solution for contactless fingerprint-based identification systems. \u25b3 Less Submitted 18 August, 2024; originally announced August 2024. arXiv:2408.07293 [pdf, other] Discriminating retinal microvascular and neuronal differences related to migraines: Deep Learning based Crossectional Study Authors: Feilong Tang, Matt Trinh, Annita Duong, Angelica Ly, Fiona Stapleton, Zhe Chen, Zongyuan Ge, Imran Razzak Abstract: Migraine, a prevalent neurological disorder, has been associated with various ocular manifestations suggestive of neuronal and microvascular deficits. However, there is limited understanding of the extent to which retinal imaging may discriminate between individuals with migraines versus without migraines. In this study, we apply convolutional neural networks to color fundus photography (CFP) and\u2026 \u25bd More Migraine, a prevalent neurological disorder, has been associated with various ocular manifestations suggestive of neuronal and microvascular deficits. However, there is limited understanding of the extent to which retinal imaging may discriminate between individuals with migraines versus without migraines. In this study, we apply convolutional neural networks to color fundus photography (CFP) and optical coherence tomography (OCT) data to investigate differences in the retina that may not be apparent through traditional human-based interpretations of retinal imaging. Retrospective data of CFP type 1 [posterior pole] and type 2 [optic nerve head (ONH)] from 369 and 336 participants respectively were analyzed. All participants had bilaterally normal optic nerves and maculae, with no retinal-involving diseases. CFP images were concatenated with OCT default ONH measurements, then inputted through three convolutional neural networks - VGG-16, ResNet-50, and Inceptionv3. The primary outcome was performance of discriminating between with migraines versus without migraines, using retinal microvascular and neuronal imaging characteristics. Using CFP type 1 data, discrimination (AUC [95% CI]) was high (0.84 [0.8, 0.88] to 0.87 [0.84, 0.91]) and not significantly different between VGG-16, ResNet-50, and Inceptionv3. Using CFP type 2 [ONH] data, discrimination was reduced and ranged from poor to fair (0.69 [0.62, 0.77] to 0.74 [0.67, 0.81]). OCT default ONH measurements overall did not significantly contribute to model performance. Class activation maps (CAMs) highlighted that the paravascular arcades were regions of interest. The findings suggest that individuals with migraines demonstrate microvascular differences more so than neuronal differences in comparison to individuals without migraines. \u25b3 Less Submitted 29 July, 2024; originally announced August 2024. arXiv:2408.02237 [pdf, other] Do Large Language Models Speak All Languages Equally? A Comparative Study in Low-Resource Settings Authors: Md. Arid Hasan, Prerona Tarannum, Krishno Dey, Imran Razzak, Usman Naseem Abstract: Large language models (LLMs) have garnered significant interest in natural language processing (NLP), particularly their remarkable performance in various downstream tasks in resource-rich languages. Recent studies have highlighted the limitations of LLMs in low-resource languages, primarily focusing on binary classification tasks and giving minimal attention to South Asian languages. These limita\u2026 \u25bd More Large language models (LLMs) have garnered significant interest in natural language processing (NLP), particularly their remarkable performance in various downstream tasks in resource-rich languages. Recent studies have highlighted the limitations of LLMs in low-resource languages, primarily focusing on binary classification tasks and giving minimal attention to South Asian languages. These limitations are primarily attributed to constraints such as dataset scarcity, computational costs, and research gaps specific to low-resource languages. To address this gap, we present datasets for sentiment and hate speech tasks by translating from English to Bangla, Hindi, and Urdu, facilitating research in low-resource language processing. Further, we comprehensively examine zero-shot learning using multiple LLMs in English and widely spoken South Asian languages. Our findings indicate that GPT-4 consistently outperforms Llama 2 and Gemini, with English consistently demonstrating superior performance across diverse tasks compared to low-resource languages. Furthermore, our analysis reveals that natural language inference (NLI) exhibits the highest performance among the evaluated tasks, with GPT-4 demonstrating superior capabilities. \u25b3 Less Submitted 5 August, 2024; originally announced August 2024. ACM Class: F.2.2; I.2.7 arXiv:2407.18970 [pdf, other] Region Guided Attention Network for Retinal Vessel Segmentation Authors: Syed Javed, Tariq M. Khan, Abdul Qayyum, Arcot Sowmya, Imran Razzak Abstract: Retinal imaging has emerged as a promising method of addressing this challenge, taking advantage of the unique structure of the retina. The retina is an embryonic extension of the central nervous system, providing a direct in vivo window into neurological health. Recent studies have shown that specific structural changes in retinal vessels can not only serve as early indicators of various diseases\u2026 \u25bd More Retinal imaging has emerged as a promising method of addressing this challenge, taking advantage of the unique structure of the retina. The retina is an embryonic extension of the central nervous system, providing a direct in vivo window into neurological health. Recent studies have shown that specific structural changes in retinal vessels can not only serve as early indicators of various diseases but also help to understand disease progression. In this work, we present a lightweight retinal vessel segmentation network based on the encoder-decoder mechanism with region-guided attention. We introduce inverse addition attention blocks with region guided attention to focus on the foreground regions and improve the segmentation of regions of interest. To further boost the model's performance on retinal vessel segmentation, we employ a weighted dice loss. This choice is particularly effective in addressing the class imbalance issues frequently encountered in retinal vessel segmentation tasks. Dice loss penalises false positives and false negatives equally, encouraging the model to generate more accurate segmentation with improved object boundary delineation and reduced fragmentation. Extensive experiments on a benchmark dataset show better performance (0.8285, 0.8098, 0.9677, and 0.8166 recall, precision, accuracy and F1 score respectively) compared to state-of-the-art methods. \u25b3 Less Submitted 20 September, 2024; v1 submitted 21 July, 2024; originally announced July 2024. arXiv:2407.17174 [pdf, other] NarrationDep: Narratives on Social Media For Automatic Depression Detection Authors: Hamad Zogan, Imran Razzak, Shoaib Jameel, Guandong Xu Abstract: Social media posts provide valuable insight into the narrative of users and their intentions, including providing an opportunity to automatically model whether a social media user is depressed or not. The challenge lies in faithfully modelling user narratives from their online social media posts, which could potentially be useful in several different applications. We have developed a novel and eff\u2026 \u25bd More Social media posts provide valuable insight into the narrative of users and their intentions, including providing an opportunity to automatically model whether a social media user is depressed or not. The challenge lies in faithfully modelling user narratives from their online social media posts, which could potentially be useful in several different applications. We have developed a novel and effective model called \\texttt{NarrationDep}, which focuses on detecting narratives associated with depression. By analyzing a user's tweets, \\texttt{NarrationDep} accurately identifies crucial narratives. \\texttt{NarrationDep} is a deep learning framework that jointly models individual user tweet representations and clusters of users' tweets. As a result, \\texttt{NarrationDep} is characterized by a novel two-layer deep learning model: the first layer models using social media text posts, and the second layer learns semantic representations of tweets associated with a cluster. To faithfully model these cluster representations, the second layer incorporates a novel component that hierarchically learns from users' posts. The results demonstrate that our framework outperforms other comparative models including recently developed models on a variety of datasets. \u25b3 Less Submitted 24 July, 2024; originally announced July 2024. arXiv:2407.07922 [pdf, other] Vulnerability Detection in Smart Contracts: A Comprehensive Survey Authors: Christopher De Baets, Basem Suleiman, Armin Chitizadeh, Imran Razzak Abstract: In the growing field of blockchain technology, smart contracts exist as transformative digital agreements that execute transactions autonomously in decentralised networks. However, these contracts face challenges in the form of security vulnerabilities, posing significant financial and operational risks. While traditional methods to detect and mitigate vulnerabilities in smart contracts are limite\u2026 \u25bd More In the growing field of blockchain technology, smart contracts exist as transformative digital agreements that execute transactions autonomously in decentralised networks. However, these contracts face challenges in the form of security vulnerabilities, posing significant financial and operational risks. While traditional methods to detect and mitigate vulnerabilities in smart contracts are limited due to a lack of comprehensiveness and effectiveness, integrating advanced machine learning technologies presents an attractive approach to increasing effective vulnerability countermeasures. We endeavour to fill an important gap in the existing literature by conducting a rigorous systematic review, exploring the intersection between machine learning and smart contracts. Specifically, the study examines the potential of machine learning techniques to improve the detection and mitigation of vulnerabilities in smart contracts. We analysed 88 articles published between 2018 and 2023 from the following databases: IEEE, ACM, ScienceDirect, Scopus, and Google Scholar. The findings reveal that classical machine learning techniques, including KNN, RF, DT, XG-Boost, and SVM, outperform static tools in vulnerability detection. Moreover, multi-model approaches integrating deep learning and classical machine learning show significant improvements in precision and recall, while hybrid models employing various techniques achieve near-perfect performance in vulnerability detection accuracy. By integrating state-of-the-art solutions, this work synthesises current methods, thoroughly investigates research gaps, and suggests directions for future studies. The insights gathered from this study are intended to serve as a seminal reference for academics, industry experts, and bodies interested in leveraging machine learning to enhance smart contract security. \u25b3 Less Submitted 8 July, 2024; originally announced July 2024. arXiv:2407.02871 [pdf, other] LMBF-Net: A Lightweight Multipath Bidirectional Focal Attention Network for Multifeatures Segmentation Authors: Tariq M Khan, Shahzaib Iqbal, Syed S. Naqvi, Imran Razzak, Erik Meijering Abstract: Retinal diseases can cause irreversible vision loss in both eyes if not diagnosed and treated early. Since retinal diseases are so complicated, retinal imaging is likely to show two or more abnormalities. Current deep learning techniques for segmenting retinal images with many labels and attributes have poor detection accuracy and generalisability. This paper presents a multipath convolutional neu\u2026 \u25bd More Retinal diseases can cause irreversible vision loss in both eyes if not diagnosed and treated early. Since retinal diseases are so complicated, retinal imaging is likely to show two or more abnormalities. Current deep learning techniques for segmenting retinal images with many labels and attributes have poor detection accuracy and generalisability. This paper presents a multipath convolutional neural network for multifeature segmentation. The proposed network is lightweight and spatially sensitive to information. A patch-based implementation is used to extract local image features, and focal modulation attention blocks are incorporated between the encoder and the decoder for improved segmentation. Filter optimisation is used to prevent filter overlaps and speed up model convergence. A combination of convolution operations and group convolution operations is used to reduce computational costs. This is the first robust and generalisable network capable of segmenting multiple features of fundus images (including retinal vessels, microaneurysms, optic discs, haemorrhages, hard exudates, and soft exudates). The results of our experimental evaluation on more than ten publicly available datasets with multiple features show that the proposed network outperforms recent networks despite having a small number of learnable parameters. \u25b3 Less Submitted 3 July, 2024; originally announced July 2024. arXiv:2406.08990 [pdf, other] BTS: Building Timeseries Dataset: Empowering Large-Scale Building Analytics Authors: Arian Prabowo, Xiachong Lin, Imran Razzak, Hao Xue, Emily W. Yap, Matthew Amos, Flora D. Salim Abstract: Buildings play a crucial role in human well-being, influencing occupant comfort, health, and safety. Additionally, they contribute significantly to global energy consumption, accounting for one-third of total energy usage, and carbon emissions. Optimizing building performance presents a vital opportunity to combat climate change and promote human flourishing. However, research in building analytic\u2026 \u25bd More Buildings play a crucial role in human well-being, influencing occupant comfort, health, and safety. Additionally, they contribute significantly to global energy consumption, accounting for one-third of total energy usage, and carbon emissions. Optimizing building performance presents a vital opportunity to combat climate change and promote human flourishing. However, research in building analytics has been hampered by the lack of accessible, available, and comprehensive real-world datasets on multiple building operations. In this paper, we introduce the Building TimeSeries (BTS) dataset. Our dataset covers three buildings over a three-year period, comprising more than ten thousand timeseries data points with hundreds of unique ontologies. Moreover, the metadata is standardized using the Brick schema. To demonstrate the utility of this dataset, we performed benchmarks on two tasks: timeseries ontology classification and zero-shot forecasting. These tasks represent an essential initial step in addressing challenges related to interoperability in building analytics. Access to the dataset and the code used for benchmarking are available here: https://github.com/cruiseresearchgroup/DIEF_BTS . \u25b3 Less Submitted 18 June, 2024; v1 submitted 13 June, 2024; originally announced June 2024. Comments: 21 pages, 2 figures, 9 tables, under review arXiv:2405.17520 [pdf, other] Advancing Medical Image Segmentation with Mini-Net: A Lightweight Solution Tailored for Efficient Segmentation of Medical Images Authors: Syed Javed, Tariq M. Khan, Abdul Qayyum, Hamid Alinejad-Rokny, Arcot Sowmya, Imran Razzak Abstract: Accurate segmentation of anatomical structures and abnormalities in medical images is crucial for computer-aided diagnosis and analysis. While deep learning techniques excel at this task, their computational demands pose challenges. Additionally, some cutting-edge segmentation methods, though effective for general object segmentation, may not be optimised for medical images. To address these issue\u2026 \u25bd More Accurate segmentation of anatomical structures and abnormalities in medical images is crucial for computer-aided diagnosis and analysis. While deep learning techniques excel at this task, their computational demands pose challenges. Additionally, some cutting-edge segmentation methods, though effective for general object segmentation, may not be optimised for medical images. To address these issues, we propose Mini-Net, a lightweight segmentation network specifically designed for medical images. With fewer than 38,000 parameters, Mini-Net efficiently captures both high- and low-frequency features, enabling real-time applications in various medical imaging scenarios. We evaluate Mini-Net on various datasets, including DRIVE, STARE, ISIC-2016, ISIC-2018, and MoNuSeg, demonstrating its robustness and good performance compared to state-of-the-art methods. \u25b3 Less Submitted 20 September, 2024; v1 submitted 27 May, 2024; originally announced May 2024. arXiv:2405.17518 [pdf, other] Assessment of Left Atrium Motion Deformation Through Full Cardiac Cycle Authors: Abdul Qayyum, Moona Mazher, Angela Lee, Jose A Solis-Lemus, Imran Razzak, Steven A Niederer Abstract: Unlike Right Atrium (RA), Left Atrium (LA) presents distinctive challenges, including much thinner myocardial walls, complex and irregular morphology, as well as diversity in individual's structure, making off-the-shelf methods designed for the Left Ventricle (LV) may not work in the context of the left atrium. To overcome aforementioned challenges, we are the first to present comprehensive techni\u2026 \u25bd More Unlike Right Atrium (RA), Left Atrium (LA) presents distinctive challenges, including much thinner myocardial walls, complex and irregular morphology, as well as diversity in individual's structure, making off-the-shelf methods designed for the Left Ventricle (LV) may not work in the context of the left atrium. To overcome aforementioned challenges, we are the first to present comprehensive technical workflow designed for 4D registration modeling to automatically analyze LA motion using high-resolution 3D Cine MR images. We integrate segmentation network and 4D registration process to precisely delineate LA segmentation throughout the full cardiac cycle. Additionally, an image 4D registration network is employed to extract LA displacement vector fields (DVFs). Our findings show the potential of proposed end to end framework in providing clinicians with novel regional biomarkers for left atrium motion tracking and deformation, carrying significant clinical implications. \u25b3 Less Submitted 27 May, 2024; originally announced May 2024. arXiv:2405.14267 [pdf, other] A Gap in Time: The Challenge of Processing Heterogeneous IoT Data in Digitalized Buildings Authors: Xiachong Lin, Arian Prabowo, Imran Razzak, Hao Xue, Matthew Amos, Sam Behrens, Flora D. Salim Abstract: The increasing demand for sustainable energy solutions has driven the integration of digitalized buildings into the power grid, leveraging Internet-of-Things (IoT) technologies to enhance energy efficiency and operational performance. Despite their potential, effectively utilizing IoT point data within deep-learning frameworks presents significant challenges, primarily due to its inherent heteroge\u2026 \u25bd More The increasing demand for sustainable energy solutions has driven the integration of digitalized buildings into the power grid, leveraging Internet-of-Things (IoT) technologies to enhance energy efficiency and operational performance. Despite their potential, effectively utilizing IoT point data within deep-learning frameworks presents significant challenges, primarily due to its inherent heterogeneity. This study investigates the diverse dimensions of IoT data heterogeneity in both intra-building and inter-building contexts, examining their implications for predictive modeling. A benchmarking analysis of state-of-the-art time series models highlights their performance on this complex dataset. The results emphasize the critical need for multi-modal data integration, domain-informed modeling, and automated data engineering pipelines. Additionally, the study advocates for collaborative efforts to establish high-quality public datasets, which are essential for advancing intelligent and sustainable energy management systems in digitalized buildings. \u25b3 Less Submitted 20 November, 2024; v1 submitted 23 May, 2024; originally announced May 2024. Comments: 4 figures, 1 tables, 9 pages arXiv:2404.10483 [pdf, other] From Uncertainty to Trust: Kernel Dropout for AI-Powered Medical Predictions Authors: Ubaid Azam, Imran Razzak, Shelly Vishwakarma, Hakim Hacid, Dell Zhang, Shoaib Jameel Abstract: AI-driven medical predictions with trustworthy confidence are essential for ensuring the responsible use of AI in healthcare applications. The growing capabilities of AI raise questions about their trustworthiness in healthcare, particularly due to opaque decision-making and limited data availability. This paper proposes a novel approach to address these challenges, introducing a Bayesian Monte Ca\u2026 \u25bd More AI-driven medical predictions with trustworthy confidence are essential for ensuring the responsible use of AI in healthcare applications. The growing capabilities of AI raise questions about their trustworthiness in healthcare, particularly due to opaque decision-making and limited data availability. This paper proposes a novel approach to address these challenges, introducing a Bayesian Monte Carlo Dropout model with kernel modelling. Our model is designed to enhance reliability on small medical datasets, a crucial barrier to the wider adoption of AI in healthcare. This model leverages existing language models for improved effectiveness and seamlessly integrates with current workflows. Extensive evaluations of public medical datasets showcase our model's superior performance across diverse tasks. We demonstrate significant improvements in reliability, even with limited data, offering a promising step towards building trust in AI-driven medical predictions and unlocking its potential to improve patient care. \u25b3 Less Submitted 2 February, 2025; v1 submitted 16 April, 2024; originally announced April 2024. arXiv:2404.10481 [pdf, other] BayesJudge: Bayesian Kernel Language Modelling with Confidence Uncertainty in Legal Judgment Prediction Authors: Ubaid Azam, Imran Razzak, Shelly Vishwakarma, Hakim Hacid, Dell Zhang, Shoaib Jameel Abstract: Predicting legal judgments with reliable confidence is paramount for responsible legal AI applications. While transformer-based deep neural networks (DNNs) like BERT have demonstrated promise in legal tasks, accurately assessing their prediction confidence remains crucial. We present a novel Bayesian approach called BayesJudge that harnesses the synergy between deep learning and deep Gaussian Proc\u2026 \u25bd More Predicting legal judgments with reliable confidence is paramount for responsible legal AI applications. While transformer-based deep neural networks (DNNs) like BERT have demonstrated promise in legal tasks, accurately assessing their prediction confidence remains crucial. We present a novel Bayesian approach called BayesJudge that harnesses the synergy between deep learning and deep Gaussian Processes to quantify uncertainty through Bayesian kernel Monte Carlo dropout. Our method leverages informative priors and flexible data modelling via kernels, surpassing existing methods in both predictive accuracy and confidence estimation as indicated through brier score. Extensive evaluations of public legal datasets showcase our model's superior performance across diverse tasks. We also introduce an optimal solution to automate the scrutiny of unreliable predictions, resulting in a significant increase in the accuracy of the model's predictions by up to 27\\%. By empowering judges and legal professionals with more reliable information, our work paves the way for trustworthy and transparent legal AI applications that facilitate informed decisions grounded in both knowledge and quantified uncertainty. \u25b3 Less Submitted 16 April, 2024; originally announced April 2024. arXiv:2404.03080 [pdf, other] Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model Authors: Yanpeng Ye, Jie Ren, Shaozhou Wang, Yuwei Wan, Haofen Wang, Imran Razzak, Bram Hoex, Tong Xie, Wenjie Zhang Abstract: Knowledge in materials science is widely dispersed across extensive scientific literature, posing significant challenges for efficient discovery and integration of new materials. Traditional methods, often reliant on costly and time-consuming experimental approaches, further complicate rapid innovation. Addressing these challenges, the integration of artificial intelligence with materials science\u2026 \u25bd More Knowledge in materials science is widely dispersed across extensive scientific literature, posing significant challenges for efficient discovery and integration of new materials. Traditional methods, often reliant on costly and time-consuming experimental approaches, further complicate rapid innovation. Addressing these challenges, the integration of artificial intelligence with materials science has opened avenues for accelerating the discovery process, though it also demands precise annotation, data extraction, and traceability of information. To tackle these issues, this article introduces the Materials Knowledge Graph (MKG), which utilizes advanced natural language processing techniques, integrated with large language models to extract and systematically organize a decade's worth of high-quality research into structured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes information into comprehensive labels such as Name, Formula, and Application, structured around a meticulously designed ontology, thus enhancing data usability and integration. By implementing network-based algorithms, MKG not only facilitates efficient link prediction but also significantly reduces reliance on traditional experimental methods. This structured approach not only streamlines materials research but also lays the groundwork for more sophisticated science knowledge graphs. \u25b3 Less Submitted 30 September, 2024; v1 submitted 3 April, 2024; originally announced April 2024. Comments: 13 pages, 7 figures, 3 tables arXiv:2401.04025 [pdf, other] IDoFew: Intermediate Training Using Dual-Clustering in Language Models for Few Labels Text Classification Authors: Abdullah Alsuhaibani, Hamad Zogan, Imran Razzak, Shoaib Jameel, Guandong Xu Abstract: Language models such as Bidirectional Encoder Representations from Transformers (BERT) have been very effective in various Natural Language Processing (NLP) and text mining tasks including text classification. However, some tasks still pose challenges for these models, including text classification with limited labels. This can result in a cold-start problem. Although some approaches have attempte\u2026 \u25bd More Language models such as Bidirectional Encoder Representations from Transformers (BERT) have been very effective in various Natural Language Processing (NLP) and text mining tasks including text classification. However, some tasks still pose challenges for these models, including text classification with limited labels. This can result in a cold-start problem. Although some approaches have attempted to address this problem through single-stage clustering as an intermediate training step coupled with a pre-trained language model, which generates pseudo-labels to improve classification, these methods are often error-prone due to the limitations of the clustering algorithms. To overcome this, we have developed a novel two-stage intermediate clustering with subsequent fine-tuning that models the pseudo-labels reliably, resulting in reduced prediction errors. The key novelty in our model, IDoFew, is that the two-stage clustering coupled with two different clustering algorithms helps exploit the advantages of the complementary algorithms that reduce the errors in generating reliable pseudo-labels for fine-tuning. Our approach has shown significant improvements compared to strong comparative models. \u25b3 Less Submitted 8 January, 2024; originally announced January 2024. Comments: Published in The 17th ACM International Conference on Web Search and Data Mining arXiv:2401.02610 [pdf, other] DHGCN: Dynamic Hop Graph Convolution Network for Self-Supervised Point Cloud Learning Authors: Jincen Jiang, Lizhi Zhao, Xuequan Lu, Wei Hu, Imran Razzak, Meili Wang Abstract: Recent works attempt to extend Graph Convolution Networks (GCNs) to point clouds for classification and segmentation tasks. These works tend to sample and group points to create smaller point sets locally and mainly focus on extracting local features through GCNs, while ignoring the relationship between point sets. In this paper, we propose the Dynamic Hop Graph Convolution Network (DHGCN) for exp\u2026 \u25bd More Recent works attempt to extend Graph Convolution Networks (GCNs) to point clouds for classification and segmentation tasks. These works tend to sample and group points to create smaller point sets locally and mainly focus on extracting local features through GCNs, while ignoring the relationship between point sets. In this paper, we propose the Dynamic Hop Graph Convolution Network (DHGCN) for explicitly learning the contextual relationships between the voxelized point parts, which are treated as graph nodes. Motivated by the intuition that the contextual information between point parts lies in the pairwise adjacent relationship, which can be depicted by the hop distance of the graph quantitatively, we devise a novel self-supervised part-level hop distance reconstruction task and design a novel loss function accordingly to facilitate training. In addition, we propose the Hop Graph Attention (HGA), which takes the learned hop distance as input for producing attention weights to allow edge features to contribute distinctively in aggregation. Eventually, the proposed DHGCN is a plug-and-play module that is compatible with point-based backbone networks. Comprehensive experiments on different backbones and tasks demonstrate that our self-supervised method achieves state-of-the-art performance. Our source code is available at: https://github.com/Jinec98/DHGCN. \u25b3 Less Submitted 20 January, 2024; v1 submitted 4 January, 2024; originally announced January 2024. Comments: Accepted to AAAI 2024 arXiv:2309.03535 [pdf, other] Feature Enhancer Segmentation Network (FES-Net) for Vessel Segmentation Authors: Tariq M. Khan, Muhammad Arsalan, Shahzaib Iqbal, Imran Razzak, Erik Meijering Abstract: Diseases such as diabetic retinopathy and age-related macular degeneration pose a significant risk to vision, highlighting the importance of precise segmentation of retinal vessels for the tracking and diagnosis of progression. However, existing vessel segmentation methods that heavily rely on encoder-decoder structures struggle to capture contextual information about retinal vessel configurations\u2026 \u25bd More Diseases such as diabetic retinopathy and age-related macular degeneration pose a significant risk to vision, highlighting the importance of precise segmentation of retinal vessels for the tracking and diagnosis of progression. However, existing vessel segmentation methods that heavily rely on encoder-decoder structures struggle to capture contextual information about retinal vessel configurations, leading to challenges in reconciling semantic disparities between encoder and decoder features. To address this, we propose a novel feature enhancement segmentation network (FES-Net) that achieves accurate pixel-wise segmentation without requiring additional image enhancement steps. FES-Net directly processes the input image and utilizes four prompt convolutional blocks (PCBs) during downsampling, complemented by a shallow upsampling approach to generate a binary mask for each class. We evaluate the performance of FES-Net on four publicly available state-of-the-art datasets: DRIVE, STARE, CHASE, and HRF. The evaluation results clearly demonstrate the superior performance of FES-Net compared to other competitive approaches documented in the existing literature. \u25b3 Less Submitted 7 September, 2023; originally announced September 2023. arXiv:2308.13565 [pdf, other] DARWIN Series: Domain Specific Large Language Models for Natural Science Authors: Tong Xie, Yuwei Wan, Wei Huang, Zhenyu Yin, Yixuan Liu, Shaozhou Wang, Qingyuan Linghu, Chunyu Kit, Clara Grazian, Wenjie Zhang, Imran Razzak, Bram Hoex Abstract: Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In natural science, traditional manual, serial, and labour-intensive work is being augmented by automated, parallel, and iterative processes driven by artificial intelligence-based experimental automation and more. To add new capabilities in natural science, enabling the acceleration and enrichme\u2026 \u25bd More Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In natural science, traditional manual, serial, and labour-intensive work is being augmented by automated, parallel, and iterative processes driven by artificial intelligence-based experimental automation and more. To add new capabilities in natural science, enabling the acceleration and enrichment of automation of the discovery process, we present DARWIN, a series of tailored LLMs for natural science, mainly in physics, chemistry, and material science. This series relies on open-source LLM, incorporating structured and unstructured scientific knowledge from public datasets and literature. We fine-tuned the models using over 60,000 instruction data points, emphasizing factual correctness. During the fine-tuning, we introduce the Scientific Instruction Generation (SIG) model, automating instruction generation from scientific texts. This eliminates the need for manual extraction or domain-specific knowledge graphs and efficiently injects scientific knowledge into the model. We also explore multi-task training strategies, revealing interconnections between scientific tasks. DARWIN series not only achieves state-of-the-art results on various scientific tasks but also diminishes reliance on closed-source AI models. Our research showcases the ability of LLM in the scientific domain, with the overarching goal of fostering prosperity within the broader AI for science community. \u25b3 Less Submitted 24 August, 2023; originally announced August 2023. arXiv:2306.14764 [pdf] Uncovering Political Hate Speech During Indian Election Campaign: A New Low-Resource Dataset and Baselines Authors: Farhan Ahmad Jafri, Mohammad Aman Siddiqui, Surendrabikram Thapa, Kritesh Rauniyar, Usman Naseem, Imran Razzak Abstract: The detection of hate speech in political discourse is a critical issue, and this becomes even more challenging in low-resource languages. To address this issue, we introduce a new dataset named IEHate, which contains 11,457 manually annotated Hindi tweets related to the Indian Assembly Election Campaign from November 1, 2021, to March 9, 2022. We performed a detailed analysis of the dataset, focu\u2026 \u25bd More The detection of hate speech in political discourse is a critical issue, and this becomes even more challenging in low-resource languages. To address this issue, we introduce a new dataset named IEHate, which contains 11,457 manually annotated Hindi tweets related to the Indian Assembly Election Campaign from November 1, 2021, to March 9, 2022. We performed a detailed analysis of the dataset, focusing on the prevalence of hate speech in political communication and the different forms of hateful language used. Additionally, we benchmark the dataset using a range of machine learning, deep learning, and transformer-based algorithms. Our experiments reveal that the performance of these models can be further improved, highlighting the need for more advanced techniques for hate speech detection in low-resource languages. In particular, the relatively higher score of human evaluation over algorithms emphasizes the importance of utilizing both human and automated approaches for effective hate speech moderation. Our IEHate dataset can serve as a valuable resource for researchers and practitioners working on developing and evaluating hate speech detection techniques in low-resource languages. Overall, our work underscores the importance of addressing the challenges of identifying and mitigating hate speech in political discourse, particularly in the context of low-resource languages. The dataset and resources for this work are made available at https://github.com/Farhan-jafri/Indian-Election. \u25b3 Less Submitted 27 June, 2023; v1 submitted 26 June, 2023; originally announced June 2023. Comments: Accepted to ICWSM Workshop (MEDIATE) arXiv:2306.10484 [pdf, other] The STOIC2021 COVID-19 AI challenge: applying reusable training methodologies to private data Authors: Luuk H. Boulogne, Julian Lorenz, Daniel Kienzle, Robin Schon, Katja Ludwig, Rainer Lienhart, Simon Jegou, Guang Li, Cong Chen, Qi Wang, Derik Shi, Mayug Maniparambil, Dominik Muller, Silvan Mertes, Niklas Schroter, Fabio Hellmann, Miriam Elia, Ine Dirks, Matias Nicolas Bossa, Abel Diaz Berenguer, Tanmoy Mukherjee, Jef Vandemeulebroucke, Hichem Sahli, Nikos Deligiannis, Panagiotis Gonidakis , et al. (13 additional authors not shown) Abstract: Challenges drive the state-of-the-art of automated medical image analysis. The quantity of public training data that they provide can limit the performance of their solutions. Public access to the training methodology for these solutions remains absent. This study implements the Type Three (T3) challenge format, which allows for training solutions on private data and guarantees reusable training m\u2026 \u25bd More Challenges drive the state-of-the-art of automated medical image analysis. The quantity of public training data that they provide can limit the performance of their solutions. Public access to the training methodology for these solutions remains absent. This study implements the Type Three (T3) challenge format, which allows for training solutions on private data and guarantees reusable training methodologies. With T3, challenge organizers train a codebase provided by the participants on sequestered training data. T3 was implemented in the STOIC2021 challenge, with the goal of predicting from a computed tomography (CT) scan whether subjects had a severe COVID-19 infection, defined as intubation or death within one month. STOIC2021 consisted of a Qualification phase, where participants developed challenge solutions using 2000 publicly available CT scans, and a Final phase, where participants submitted their training methodologies with which solutions were trained on CT scans of 9724 subjects. The organizers successfully trained six of the eight Final phase submissions. The submitted codebases for training and running inference were released publicly. The winning solution obtained an area under the receiver operating characteristic curve for discerning between severe and non-severe COVID-19 of 0.815. The Final phase solutions of all finalists improved upon their Qualification phase solutions.HSUXJM-TNZF9CHSUXJM-TNZF9C \u25b3 Less Submitted 25 June, 2023; v1 submitted 18 June, 2023; originally announced June 2023. arXiv:2306.07300 [pdf, other] Progressive Class-Wise Attention (PCA) Approach for Diagnosing Skin Lesions Authors: Asim Naveed, Syed S. Naqvi, Tariq M. Khan, Imran Razzak Abstract: Skin cancer holds the highest incidence rate among all cancers globally. The importance of early detection cannot be overstated, as late-stage cases can be lethal. Classifying skin lesions, however, presents several challenges due to the many variations they can exhibit, such as differences in colour, shape, and size, significant variation within the same class, and notable similarities between di\u2026 \u25bd More Skin cancer holds the highest incidence rate among all cancers globally. The importance of early detection cannot be overstated, as late-stage cases can be lethal. Classifying skin lesions, however, presents several challenges due to the many variations they can exhibit, such as differences in colour, shape, and size, significant variation within the same class, and notable similarities between different classes. This paper introduces a novel class-wise attention technique that equally regards each class while unearthing more specific details about skin lesions. This attention mechanism is progressively used to amalgamate discriminative feature details from multiple scales. The introduced technique demonstrated impressive performance, surpassing more than 15 cutting-edge methods including the winners of HAM1000 and ISIC 2019 leaderboards. It achieved an impressive accuracy rate of 97.40% on the HAM10000 dataset and 94.9% on the ISIC 2019 dataset. \u25b3 Less Submitted 11 June, 2023; originally announced June 2023. arXiv:2306.06145 [pdf, other] LDMRes-Net: Enabling Efficient Medical Image Segmentation on IoT and Edge Platforms Authors: Shahzaib Iqbal, Tariq M. Khan, Syed S. Naqvi, Muhammad Usman, Imran Razzak Abstract: In this study, we propose LDMRes-Net, a lightweight dual-multiscale residual block-based computational neural network tailored for medical image segmentation on IoT and edge platforms. Conventional U-Net-based models face challenges in meeting the speed and efficiency demands of real-time clinical applications, such as disease monitoring, radiation therapy, and image-guided surgery. LDMRes-Net ove\u2026 \u25bd More In this study, we propose LDMRes-Net, a lightweight dual-multiscale residual block-based computational neural network tailored for medical image segmentation on IoT and edge platforms. Conventional U-Net-based models face challenges in meeting the speed and efficiency demands of real-time clinical applications, such as disease monitoring, radiation therapy, and image-guided surgery. LDMRes-Net overcomes these limitations with its remarkably low number of learnable parameters (0.072M), making it highly suitable for resource-constrained devices. The model's key innovation lies in its dual multi-residual block architecture, which enables the extraction of refined features on multiple scales, enhancing overall segmentation performance. To further optimize efficiency, the number of filters is carefully selected to prevent overlap, reduce training time, and improve computational efficiency. The study includes comprehensive evaluations, focusing on segmentation of the retinal image of vessels and hard exudates crucial for the diagnosis and treatment of ophthalmology. The results demonstrate the robustness, generalizability, and high segmentation accuracy of LDMRes-Net, positioning it as an efficient tool for accurate and rapid medical image segmentation in diverse clinical applications, particularly on IoT and edge platforms. Such advances hold significant promise for improving healthcare outcomes and enabling real-time medical image analysis in resource-limited settings. \u25b3 Less Submitted 7 September, 2023; v1 submitted 9 June, 2023; originally announced June 2023. arXiv:2304.12856 [pdf, other] Retinal Vessel Segmentation via a Multi-resolution Contextual Network and Adversarial Learning Authors: Tariq M. Khan, Syed S. Naqvi, Antonio Robles-Kelly, Imran Razzak Abstract: Timely and affordable computer-aided diagnosis of retinal diseases is pivotal in precluding blindness. Accurate retinal vessel segmentation plays an important role in disease progression and diagnosis of such vision-threatening diseases. To this end, we propose a Multi-resolution Contextual Network (MRC-Net) that addresses these issues by extracting multi-scale features to learn contextual depende\u2026 \u25bd More Timely and affordable computer-aided diagnosis of retinal diseases is pivotal in precluding blindness. Accurate retinal vessel segmentation plays an important role in disease progression and diagnosis of such vision-threatening diseases. To this end, we propose a Multi-resolution Contextual Network (MRC-Net) that addresses these issues by extracting multi-scale features to learn contextual dependencies between semantically different features and using bi-directional recurrent learning to model former-latter and latter-former dependencies. Another key idea is training in adversarial settings for foreground segmentation improvement through optimization of the region-based scores. This novel strategy boosts the performance of the segmentation network in terms of the Dice score (and correspondingly Jaccard index) while keeping the number of trainable parameters comparatively low. We have evaluated our method on three benchmark datasets, including DRIVE, STARE, and CHASE, demonstrating its superior performance as compared with competitive approaches elsewhere in the literature. \u25b3 Less Submitted 25 April, 2023; originally announced April 2023. arXiv:2303.01838 [pdf, other] Usability of Privacy Controls in Top Health Websites Authors: Ravin Gunawardena, Yuemeng Yin, Yi Huang, Rahat Masood, Suranga Seneviratne, Imran Razzak, Nguyen Tran, Aruna Seneviratne Abstract: With the increasing awareness and concerns around privacy, many service providers offer their users various privacy controls. Through these controls, users gain greater authority over the collection, utilisation, and dissemination of their personal information by the services. However, these controls may be buried deep within menus or settings, making them difficult for a user to access. Additiona\u2026 \u25bd More With the increasing awareness and concerns around privacy, many service providers offer their users various privacy controls. Through these controls, users gain greater authority over the collection, utilisation, and dissemination of their personal information by the services. However, these controls may be buried deep within menus or settings, making them difficult for a user to access. Additionally, the terminology used to describe privacy controls can sometimes be confusing or technical, further complicating the user's ability to understand and use them effectively. This is especially true for health websites, as users often share sensitive information about their health and well-being. While many privacy controls have been proposed to protect user data on these sites, existing research focuses on individual controls (e.g., privacy policies or cookie opt-outs) rather than providing a comprehensive overview of the privacy landscape. In addition, many studies concentrate on the technical aspects of privacy controls without considering the usability of these features from a user's perspective. This paper aims to fill the gaps in the existing work by analysing four privacy controls, namely privacy nudge, privacy notice, privacy policy, and privacy setting, and evaluating their usability on the top 100 most visited health websites. First, we define usability attributes for each privacy control in three website visit scenarios; the guest, registering, and log-in visits. These attributes include awareness, efficiency, comprehension, functionality, and choice. Then, we design a survey template based on these attributes and scenarios and collect data about privacy controls. Next, we analyse the availability and usability of each privacy control on health websites. Finally, we provide suggestions for improving the design of these privacy controls based on the data analysis results. \u25b3 Less Submitted 3 March, 2023; originally announced March 2023. arXiv:2302.01738 [pdf, other] AIROGS: Artificial Intelligence for RObust Glaucoma Screening Challenge Authors: Coen de Vente, Koenraad A. Vermeer, Nicolas Jaccard, He Wang, Hongyi Sun, Firas Khader, Daniel Truhn, Temirgali Aimyshev, Yerkebulan Zhanibekuly, Tien-Dung Le, Adrian Galdran, Miguel \u00c1ngel Gonz\u00e1lez Ballester, Gustavo Carneiro, Devika R G, Hrishikesh P S, Densen Puthussery, Hong Liu, Zekang Yang, Satoshi Kondo, Satoshi Kasai, Edward Wang, Ashritha Durvasula, J\u00f3nathan Heras, Miguel \u00c1ngel Zapata, Teresa Ara\u00fajo , et al. (11 additional authors not shown) Abstract: The early detection of glaucoma is essential in preventing visual impairment. Artificial intelligence (AI) can be used to analyze color fundus photographs (CFPs) in a cost-effective manner, making glaucoma screening more accessible. While AI models for glaucoma screening from CFPs have shown promising results in laboratory settings, their performance decreases significantly in real-world scenarios\u2026 \u25bd More The early detection of glaucoma is essential in preventing visual impairment. Artificial intelligence (AI) can be used to analyze color fundus photographs (CFPs) in a cost-effective manner, making glaucoma screening more accessible. While AI models for glaucoma screening from CFPs have shown promising results in laboratory settings, their performance decreases significantly in real-world scenarios due to the presence of out-of-distribution and low-quality images. To address this issue, we propose the Artificial Intelligence for Robust Glaucoma Screening (AIROGS) challenge. This challenge includes a large dataset of around 113,000 images from about 60,000 patients and 500 different screening centers, and encourages the development of algorithms that are robust to ungradable and unexpected input data. We evaluated solutions from 14 teams in this paper, and found that the best teams performed similarly to a set of 20 expert ophthalmologists and optometrists. The highest-scoring team achieved an area under the receiver operating characteristic curve of 0.99 (95% CI: 0.98-0.99) for detecting ungradable images on-the-fly. Additionally, many of the algorithms showed robust performance when tested on three other publicly available datasets. These results demonstrate the feasibility of robust AI-enabled glaucoma screening. \u25b3 Less Submitted 10 February, 2023; v1 submitted 3 February, 2023; originally announced February 2023. Comments: 19 pages, 8 figures, 3 tables arXiv:2204.06337 [pdf, other] A Novel Approach to Train Diverse Types of Language Models for Health Mention Classification of Tweets Authors: Pervaiz Iqbal Khan, Imran Razzak, Andreas Dengel, Sheraz Ahmed Abstract: Health mention classification deals with the disease detection in a given text containing disease words. However, non-health and figurative use of disease words adds challenges to the task. Recently, adversarial training acting as a means of regularization has gained popularity in many NLP tasks. In this paper, we propose a novel approach to train language models for health mention classification\u2026 \u25bd More Health mention classification deals with the disease detection in a given text containing disease words. However, non-health and figurative use of disease words adds challenges to the task. Recently, adversarial training acting as a means of regularization has gained popularity in many NLP tasks. In this paper, we propose a novel approach to train language models for health mention classification of tweets that involves adversarial training. We generate adversarial examples by adding perturbation to the representations of transformer models for tweet examples at various levels using Gaussian noise. Further, we employ contrastive loss as an additional objective function. We evaluate the proposed method on the PHM2017 dataset extended version. Results show that our proposed approach improves the performance of classifier significantly over the baseline methods. Moreover, our analysis shows that adding noise at earlier layers improves models' performance whereas adding noise at intermediate layers deteriorates models' performance. Finally, adding noise towards the final layers performs better than the middle layers noise addition. \u25b3 Less Submitted 13 April, 2022; originally announced April 2022."
  },
  {
    "url": "https://arxiv.org/format/2503.14649",
    "title": "arXiv.org >\n2503.14649 > format",
    "author": "Unknown",
    "body": "Help | Advanced Search Note: Many of the formats above are served gzipped (Content-Encoding: x-gzip). Your browser may silently uncompress after downloading so the files you see saved may appear uncompressed. Please report any problems to arXiv technical support and include the paper identifier. arXiv Operational Status Get status notifications via email or slack"
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Graves,+C",
    "title": "Showing 1\u20138 of 8 results for author: Graves, C",
    "author": "Authors:\nWenqi Jiang, \n      \n      Suvinay Subramanian, \n      \n      Cat Graves, \n      \n      Gustavo Alonso, \n      \n      Amir Yazdanbakhsh, \n      \n      Vidushi Dadu",
    "body": "arXiv:2503.14649 [pdf, other] RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving Authors: Wenqi Jiang, Suvinay Subramanian, Cat Graves, Gustavo Alonso, Amir Yazdanbakhsh, Vidushi Dadu Abstract: Retrieval-augmented generation (RAG), which combines large language models (LLMs) with retrievals from external knowledge databases, is emerging as a popular approach for reliable LLM serving. However, efficient RAG serving remains an open challenge due to the rapid emergence of many RAG variants and the substantial differences in workload characteristics across them. In this paper, we make three\u2026 \u25bd More Retrieval-augmented generation (RAG), which combines large language models (LLMs) with retrievals from external knowledge databases, is emerging as a popular approach for reliable LLM serving. However, efficient RAG serving remains an open challenge due to the rapid emergence of many RAG variants and the substantial differences in workload characteristics across them. In this paper, we make three fundamental contributions to advancing RAG serving. First, we introduce RAGSchema, a structured abstraction that captures the wide range of RAG algorithms, serving as a foundation for performance optimization. Second, we analyze several representative RAG workloads with distinct RAGSchema, revealing significant performance variability across these workloads. Third, to address this variability and meet diverse performance requirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a system optimization framework for efficient RAG serving. Our evaluation shows that RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in time-to-first-token latency compared to RAG systems built on LLM-system extensions. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. arXiv:2501.07733 [pdf, other] Solving Boolean satisfiability problems with resistive content addressable memories Authors: Giacomo Pedretti, Fabian B\u00f6hm, Tinish Bhattacharya, Arne Heittman, Xiangyi Zhang, Mohammad Hizzani, George Hutchinson, Dongseok Kwon, John Moon, Elisabetta Valiante, Ignacio Rozada, Catherine E. Graves, Jim Ignowski, Masoud Mohseni, John Paul Strachan, Dmitri Strukov, Ray Beausoleil, Thomas Van Vaerenbergh Abstract: Solving optimization problems is a highly demanding workload requiring high-performance computing systems. Optimization solvers are usually difficult to parallelize in conventional digital architectures, particularly when stochastic decisions are involved. Recently, analog computing architectures for accelerating stochastic optimization solvers have been presented, but they were limited to academi\u2026 \u25bd More Solving optimization problems is a highly demanding workload requiring high-performance computing systems. Optimization solvers are usually difficult to parallelize in conventional digital architectures, particularly when stochastic decisions are involved. Recently, analog computing architectures for accelerating stochastic optimization solvers have been presented, but they were limited to academic problems in quadratic polynomial format. Here we present KLIMA, a k-Local In-Memory Accelerator with resistive Content Addressable Memories (CAMs) and Dot-Product Engines (DPEs) to accelerate the solution of high-order industry-relevant optimization problems, in particular Boolean Satisfiability. By co-designing the optimization heuristics and circuit architecture we improve the speed and energy to solution up to 182x compared to the digital state of the art. \u25b3 Less Submitted 13 January, 2025; originally announced January 2025. arXiv:2304.01285 [pdf, other] X-TIME: An in-memory engine for accelerating machine learning on tabular data with CAMs Authors: Giacomo Pedretti, John Moon, Pedro Bruel, Sergey Serebryakov, Ron M. Roth, Luca Buonanno, Archit Gajjar, Lei Zhao, Tobias Ziegler, Cong Xu, Martin Foltin, Paolo Faraboschi, Jim Ignowski, Catherine E. Graves Abstract: Structured, or tabular, data is the most common format in data science. While deep learning models have proven formidable in learning from unstructured data such as images or speech, they are less accurate than simpler approaches when learning from tabular data. In contrast, modern tree-based Machine Learning (ML) models shine in extracting relevant information from structured data. An essential r\u2026 \u25bd More Structured, or tabular, data is the most common format in data science. While deep learning models have proven formidable in learning from unstructured data such as images or speech, they are less accurate than simpler approaches when learning from tabular data. In contrast, modern tree-based Machine Learning (ML) models shine in extracting relevant information from structured data. An essential requirement in data science is to reduce model inference latency in cases where, for example, models are used in a closed loop with simulation to accelerate scientific discovery. However, the hardware acceleration community has mostly focused on deep neural networks and largely ignored other forms of machine learning. Previous work has described the use of an analog content addressable memory (CAM) component for efficiently mapping random forests. In this work, we develop an analog-digital architecture that implements a novel increased precision analog CAM and a programmable chip for inference of state-of-the-art tree-based ML models, such as XGBoost, CatBoost, and others. Thanks to hardware-aware training, X-TIME reaches state-of-the-art accuracy and 119x higher throughput at 9740x lower latency with >150x improved energy efficiency compared with a state-of-the-art GPU for models with up to 4096 trees and depth of 8, with a 19W peak power consumption. \u25b3 Less Submitted 17 January, 2025; v1 submitted 3 April, 2023; originally announced April 2023. Journal ref: IEEE Journal on Exploratory Solid-State Computational Devices and Circuits, vol. 10, pp. 116-124, 2024 arXiv:2204.07429 [pdf, other] Experimentally realized memristive memory augmented neural network Authors: Ruibin Mao, Bo Wen, Yahui Zhao, Arman Kazemi, Ann Franchesca Laguna, Michael Neimier, X. Sharon Hu, Xia Sheng, Catherine E. Graves, John Paul Strachan, Can Li Abstract: Lifelong on-device learning is a key challenge for machine intelligence, and this requires learning from few, often single, samples. Memory augmented neural network has been proposed to achieve the goal, but the memory module has to be stored in an off-chip memory due to its size. Therefore the practical use has been heavily limited. Previous works on emerging memory-based implementation have diff\u2026 \u25bd More Lifelong on-device learning is a key challenge for machine intelligence, and this requires learning from few, often single, samples. Memory augmented neural network has been proposed to achieve the goal, but the memory module has to be stored in an off-chip memory due to its size. Therefore the practical use has been heavily limited. Previous works on emerging memory-based implementation have difficulties in scaling up because different modules with various structures are difficult to integrate on the same chip and the small sense margin of the content addressable memory for the memory module heavily limited the degree of mismatch calculation. In this work, we implement the entire memory augmented neural network architecture in a fully integrated memristive crossbar platform and achieve an accuracy that closely matches standard software on digital hardware for the Omniglot dataset. The successful demonstration is supported by implementing new functions in crossbars in addition to widely reported matrix multiplications. For example, the locality-sensitive hashing operation is implemented in crossbar arrays by exploiting the intrinsic stochasticity of memristor devices. Besides, the content-addressable memory module is realized in crossbars, which also supports the degree of mismatches. Simulations based on experimentally validated models show such an implementation can be efficiently scaled up for one-shot learning on the Mini-ImageNet dataset. The successful demonstration paves the way for practical on-device lifelong learning and opens possibilities for novel attention-based algorithms not possible in conventional hardware. \u25b3 Less Submitted 15 April, 2022; originally announced April 2022. Comments: 54 pages, 21 figures, 3 tables arXiv:2107.09134 [pdf, other] Convolutional module for heart localization and segmentation in MRI Authors: Daniel Lima, Catharine Graves, Marco Gutierrez, Bruno Brandoli, Jose Rodrigues-Jr Abstract: Magnetic resonance imaging (MRI) is a widely known medical imaging technique used to assess the heart function. Deep learning (DL) models perform several tasks in cardiac MRI (CMR) images with good efficacy, such as segmentation, estimation, and detection of diseases. Many DL models based on convolutional neural networks (CNN) were improved by detecting regions-of-interest (ROI) either automatical\u2026 \u25bd More Magnetic resonance imaging (MRI) is a widely known medical imaging technique used to assess the heart function. Deep learning (DL) models perform several tasks in cardiac MRI (CMR) images with good efficacy, such as segmentation, estimation, and detection of diseases. Many DL models based on convolutional neural networks (CNN) were improved by detecting regions-of-interest (ROI) either automatically or by hand. In this paper we describe Visual-Motion-Focus (VMF), a module that detects the heart motion in the 4D MRI sequence, and highlights ROIs by focusing a Radial Basis Function (RBF) on the estimated motion field. We experimented and evaluated VMF on three CMR datasets, observing that the proposed ROIs cover 99.7% of data labels (Recall score), improved the CNN segmentation (mean Dice score) by 1.7 (p < .001) after the ROI extraction, and improved the overall training speed by 2.5 times (+150%). \u25b3 Less Submitted 19 July, 2021; originally announced July 2021. Comments: Submitted to CMIG MSC Class: 68T07; 92C55; 92B20 ACM Class: I.2.10; I.4.6; I.5.1; J.3 Journal ref: ICIAP 2022, Part I, LNCS 13231 arXiv:2103.08986 [pdf, other] Tree-based machine learning performed in-memory with memristive analog CAM Authors: Giacomo Pedretti, Catherine E. Graves, Can Li, Sergey Serebryakov, Xia Sheng, Martin Foltin, Ruibin Mao, John Paul Strachan Abstract: Tree-based machine learning techniques, such as Decision Trees and Random Forests, are top performers in several domains as they do well with limited training datasets and offer improved interpretability compared to Deep Neural Networks (DNN). However, while easier to train, they are difficult to optimize for fast inference without accuracy loss in von Neumann architectures due to non-uniform memo\u2026 \u25bd More Tree-based machine learning techniques, such as Decision Trees and Random Forests, are top performers in several domains as they do well with limited training datasets and offer improved interpretability compared to Deep Neural Networks (DNN). However, while easier to train, they are difficult to optimize for fast inference without accuracy loss in von Neumann architectures due to non-uniform memory access patterns. Recently, we proposed a novel analog, or multi-bit, content addressable memory(CAM) for fast look-up table operations. Here, we propose a design utilizing this as a computational primitive for rapid tree-based inference. Large random forest models are mapped to arrays of analog CAMs coupled to traditional analog random access memory (RAM), and the unique features of the analog CAM enable compression and high performance. An optimized architecture is compared with previously proposed tree-based model accelerators, showing improvements in energy to decision by orders of magnitude for common image classification tasks. The results demonstrate the potential for non-volatile analog CAM hardware in accelerating large tree-based machine learning models. \u25b3 Less Submitted 17 March, 2021; v1 submitted 16 March, 2021; originally announced March 2021. arXiv:2012.02569 [pdf] Assessing the usability of the mobile application game Call of Duty Authors: C Graves Abstract: The technological advances in the smartphone application market has expanded at great lengths in recent years (Kaya, Ozturk & Gumussoy 2019). One of the main smartphone applications that often sees record breaking downloads are gaming apps. The gaming application that recently received the most downloads of all time, 100 million in one week, was the mobile game Call of Duty (CoD). With the game ge\u2026 \u25bd More The technological advances in the smartphone application market has expanded at great lengths in recent years (Kaya, Ozturk & Gumussoy 2019). One of the main smartphone applications that often sees record breaking downloads are gaming apps. The gaming application that recently received the most downloads of all time, 100 million in one week, was the mobile game Call of Duty (CoD). With the game generating revenues of over $8 million across app stores, it is important that the users are satisfied with the game and its experience (Cuthbertson, 2019). Unlike other applications, gamers are drawn to gaming applications for their experience more than their functionality. Ensuring that the game is performing with efficiency and of a high level of satisfaction, is key in enhancing the users experience and remaining popular in such a competitive market (Barnett, Harvey and Gatzidis 2018). There are significant advances of mobile applications such as portability and accessibility but the change in architecture has meant that some aspects of design and usability have had to be constrained. Therefore, mobile games need to be more user focused (Nayebi, Desharnais and Abran 2012). \u25b3 Less Submitted 10 November, 2020; originally announced December 2020. Comments: 10 pages arXiv:1907.08177 [pdf, other] Analog content addressable memories with memristors Authors: Can Li, Catherine E. Graves, Xia Sheng, Darrin Miller, Martin Foltin, Giacomo Pedretti, John Paul Strachan Abstract: A content-addressable-memory compares an input search word against all rows of stored words in an array in a highly parallel manner. While supplying a very powerful functionality for many applications in pattern matching and search, it suffers from large area, cost and power consumption, limiting its use. Past improvements have been realized by using memristors to replace the static-random-access-\u2026 \u25bd More A content-addressable-memory compares an input search word against all rows of stored words in an array in a highly parallel manner. While supplying a very powerful functionality for many applications in pattern matching and search, it suffers from large area, cost and power consumption, limiting its use. Past improvements have been realized by using memristors to replace the static-random-access-memory cell in conventional designs, but employ similar schemes based only on binary or ternary states for storage and search. We propose a new analog content-addressable-memory concept and circuit to overcome these limitations by utilizing the analog conductance tunability of memristors. Our analog content-addressable-memory stores data within the programmable conductance and can take as input either analog or digital search values. Experimental demonstrations, scaled simulations and analysis show that our analog content-addressable-memory can reduce area and power consumption, which enables the acceleration of existing applications, but also new computing application areas. \u25b3 Less Submitted 7 April, 2020; v1 submitted 18 July, 2019; originally announced July 2019. Journal ref: Published in Nature Communications, 11, 1638, 2020"
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Alonso,+G",
    "title": "Showing 1\u201343 of 43 results for author: Alonso, G",
    "author": "Authors:\nWenqi Jiang, \n      \n      Suvinay Subramanian, \n      \n      Cat Graves, \n      \n      Gustavo Alonso, \n      \n      Amir Yazdanbakhsh, \n      \n      Vidushi Dadu",
    "body": "arXiv:2503.14649 [pdf, other] RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving Authors: Wenqi Jiang, Suvinay Subramanian, Cat Graves, Gustavo Alonso, Amir Yazdanbakhsh, Vidushi Dadu Abstract: Retrieval-augmented generation (RAG), which combines large language models (LLMs) with retrievals from external knowledge databases, is emerging as a popular approach for reliable LLM serving. However, efficient RAG serving remains an open challenge due to the rapid emergence of many RAG variants and the substantial differences in workload characteristics across them. In this paper, we make three\u2026 \u25bd More Retrieval-augmented generation (RAG), which combines large language models (LLMs) with retrievals from external knowledge databases, is emerging as a popular approach for reliable LLM serving. However, efficient RAG serving remains an open challenge due to the rapid emergence of many RAG variants and the substantial differences in workload characteristics across them. In this paper, we make three fundamental contributions to advancing RAG serving. First, we introduce RAGSchema, a structured abstraction that captures the wide range of RAG algorithms, serving as a foundation for performance optimization. Second, we analyze several representative RAG workloads with distinct RAGSchema, revealing significant performance variability across these workloads. Third, to address this variability and meet diverse performance requirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a system optimization framework for efficient RAG serving. Our evaluation shows that RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in time-to-first-token latency compared to RAG systems built on LLM-system extensions. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. arXiv:2503.09318 [pdf, other] FpgaHub: Fpga-centric Hyper-heterogeneous Computing Platform for Big Data Analytics Authors: Zeke Wang, Jie Zhang, Hongjing Huang, Yingtao Li, Xueying Zhu, Mo Sun, Zihan Yang, De Ma, Huajing Tang, Gang Pan, Fei Wu, Bingsheng He, Gustavo Alonso Abstract: Modern data analytics requires a huge amount of computing power and processes a massive amount of data. At the same time, the underlying computing platform is becoming much more heterogeneous on both hardware and software. Even though specialized hardware, e.g., FPGA- or GPU- or TPU-based systems, often achieves better performance than a CPU-only system due to the slowing of Moore's law, such syst\u2026 \u25bd More Modern data analytics requires a huge amount of computing power and processes a massive amount of data. At the same time, the underlying computing platform is becoming much more heterogeneous on both hardware and software. Even though specialized hardware, e.g., FPGA- or GPU- or TPU-based systems, often achieves better performance than a CPU-only system due to the slowing of Moore's law, such systems are limited in what they can do. For example, GPU-only approaches suffer from severe IO limitations. To truly exploit the potential of hardware heterogeneity, we present FpgaHub, an FPGA-centric hyper-heterogeneous computing platform for big data analytics. The key idea of FpgaHub is to use reconfigurable computing to implement a versatile hub complementing other processors (CPUs, GPUs, DPUs, programmable switches, computational storage, etc.). Using an FPGA as the basis, we can take advantage of its highly reconfigurable nature and rich IO interfaces such as PCIe, networking, and on-board memory, to place it at the center of the architecture and use it as a data and control plane for data movement, scheduling, pre-processing, etc. FpgaHub enables architectural flexibility to allow exploring the rich design space of heterogeneous computing platforms. \u25b3 Less Submitted 12 March, 2025; originally announced March 2025. arXiv:2503.01823 [pdf, other] Cracking Vector Search Indexes Authors: Vasilis Mageirakos, Bowen Wu, Gustavo Alonso Abstract: Retrieval Augmented Generation (RAG) uses vector databases to expand the expertise of an LLM model without having to retrain it. This idea can be applied over data lakes, leading to the notion of embeddings data lakes, i.e., a pool of vector databases ready to be used by RAGs. The key component in these systems is the indexes enabling Approximated Nearest Neighbor Search (ANNS). However, in data l\u2026 \u25bd More Retrieval Augmented Generation (RAG) uses vector databases to expand the expertise of an LLM model without having to retrain it. This idea can be applied over data lakes, leading to the notion of embeddings data lakes, i.e., a pool of vector databases ready to be used by RAGs. The key component in these systems is the indexes enabling Approximated Nearest Neighbor Search (ANNS). However, in data lakes, one cannot realistically expect to build indexes for every possible dataset. In this paper, we propose an adaptive, partition-based index, CrackIVF, that performs much better than up-front index building. CrackIVF starts answering queries by near brute force search and only expands as it sees enough queries. It does so by progressively adapting the index to the query workload. That way, queries can be answered right away without having to build a full index first. After seeing enough queries, CrackIVF will produce an index comparable to the best of those built using conventional techniques. As the experimental evaluation shows, CrackIVF can often answer more than 1 million queries before other approaches have even built the index and can start answering queries immediately, achieving 10-1000x faster initialization times. This makes it ideal when working with cold data or infrequently used data or as a way to bootstrap access to unseen datasets. \u25b3 Less Submitted 3 March, 2025; originally announced March 2025. arXiv:2501.17788 [pdf, other] WARP: An Efficient Engine for Multi-Vector Retrieval Authors: Jan Luca Scheerer, Matei Zaharia, Christopher Potts, Gustavo Alonso, Omar Khattab Abstract: We study the efficiency of multi-vector retrieval methods like ColBERT and its recent variant XTR. We introduce WARP, a retrieval engine that drastically improves the efficiency of XTR-based ColBERT retrievers through three key innovations: (1) WARP$_\\text{SELECT}$ for dynamic similarity imputation, (2) implicit decompression to bypass costly vector reconstruction, and (3) a two-stage reduction pr\u2026 \u25bd More We study the efficiency of multi-vector retrieval methods like ColBERT and its recent variant XTR. We introduce WARP, a retrieval engine that drastically improves the efficiency of XTR-based ColBERT retrievers through three key innovations: (1) WARP$_\\text{SELECT}$ for dynamic similarity imputation, (2) implicit decompression to bypass costly vector reconstruction, and (3) a two-stage reduction process for efficient scoring. Combined with optimized C++ kernels and specialized inference runtimes, WARP reduces end-to-end latency by 41x compared to XTR's reference implementation and thereby achieves a 3x speedup over PLAID from the the official ColBERT implementation. We study the efficiency of multi-vector retrieval methods like ColBERT and its recent variant XTR. We introduce WARP, a retrieval engine that drastically improves the efficiency of XTR-based ColBERT retrievers through three key innovations: (1) WARP$_\\text{SELECT}$ for dynamic similarity imputation, (2) implicit decompression during retrieval, and (3) a two-stage reduction process for efficient scoring. Thanks also to highly-optimized C++ kernels and to the adoption of specialized inference runtimes, WARP can reduce end-to-end query latency relative to XTR's reference implementation by 41x. And it thereby achieves a 3x speedup over the official ColBERTv2 PLAID engine, while preserving retrieval quality. \u25b3 Less Submitted 29 January, 2025; originally announced January 2025. arXiv:2501.12032 [pdf, other] Multi-Tenant SmartNICs for In-Network Preprocessing of Recommender Systems Authors: Yu Zhu, Wenqi Jiang, Gustavo Alonso Abstract: Keeping ML-based recommender models up-to-date as data drifts and evolves is essential to maintain accuracy. As a result, online data preprocessing plays an increasingly important role in serving recommender systems. Existing solutions employ multiple CPU workers to saturate the input bandwidth of a single training node. Such an approach results in high deployment costs and energy consumption. For\u2026 \u25bd More Keeping ML-based recommender models up-to-date as data drifts and evolves is essential to maintain accuracy. As a result, online data preprocessing plays an increasingly important role in serving recommender systems. Existing solutions employ multiple CPU workers to saturate the input bandwidth of a single training node. Such an approach results in high deployment costs and energy consumption. For instance, a recent report from industrial deployments shows that data storage and ingestion pipelines can account for over 60\\% of the power consumption in a recommender system. In this paper, we tackle the issue from a hardware perspective by introducing Piper, a flexible and network-attached accelerator that executes data loading and preprocessing pipelines in a streaming fashion. As part of the design, we define MiniPipe, the smallest pipeline unit enabling multi-pipeline implementation by executing various data preprocessing tasks across the single board, giving Piper the ability to be reconfigured at runtime. Our results, using publicly released commercial pipelines, show that Piper, prototyped on a power-efficient FPGA, achieves a 39$\\sim$105$\\times$ speedup over a server-grade, 128-core CPU and 3$\\sim$17$\\times$ speedup over GPUs like RTX 3090 and A100 in multiple pipelines. The experimental analysis demonstrates that Piper provides advantages in both latency and energy efficiency for preprocessing tasks in recommender systems, providing an alternative design point for systems that today are in very high demand. \u25b3 Less Submitted 24 January, 2025; v1 submitted 21 January, 2025; originally announced January 2025. arXiv:2409.14912 [pdf, other] Efficient Tabular Data Preprocessing of ML Pipelines Authors: Yu Zhu, Wenqi Jiang, Gustavo Alonso Abstract: Data preprocessing pipelines, which includes data decoding, cleaning, and transforming, are a crucial component of Machine Learning (ML) training. Thy are computationally intensive and often become a major bottleneck, due to the increasing performance gap between the CPUs used for preprocessing and the GPUs used for model training. Recent studies show that a significant number of CPUs across sever\u2026 \u25bd More Data preprocessing pipelines, which includes data decoding, cleaning, and transforming, are a crucial component of Machine Learning (ML) training. Thy are computationally intensive and often become a major bottleneck, due to the increasing performance gap between the CPUs used for preprocessing and the GPUs used for model training. Recent studies show that a significant number of CPUs across several machines are required to achieve sufficient throughput to saturate the GPUs, leading to increased resource and energy consumption. When the pipeline involves vocabulary generation, the preprocessing performance scales poorly due to significant row-wise synchronization overhead between different CPU cores and servers. To address this limitation, in this paper we present the design of Piper, a hardware accelerator for tabular data preprocessing, prototype it on FPGAs, and demonstrate its potential for training pipelines of commercial recommender systems. Piper achieves 4.7 $\\sim$ 71.3$\\times$ speedup in latency over a 128-core CPU server and outperforms a data-center GPU by 4.8$\\sim$ 20.3$\\times$ when using binary input. The impressive performance showcases Piper's potential to increase the efficiency of data preprocessing pipelines and significantly reduce their resource consumption. \u25b3 Less Submitted 23 September, 2024; originally announced September 2024. Comments: 12 pages, 10 figures, 4 tables arXiv:2407.00839 [pdf, ps, other] Imaginary Machines: A Serverless Model for Cloud Applications Authors: Michael Wawrzoniak, Rodrigo Bruno, Ana Klimovic, Gustavo Alonso Abstract: Serverless Function-as-a-Service (FaaS) platforms provide applications with resources that are highly elastic, quick to instantiate, accounted at fine granularity, and without the need for explicit runtime resource orchestration. This combination of the core properties underpins the success and popularity of the serverless FaaS paradigm. However, these benefits are not available to most cloud appl\u2026 \u25bd More Serverless Function-as-a-Service (FaaS) platforms provide applications with resources that are highly elastic, quick to instantiate, accounted at fine granularity, and without the need for explicit runtime resource orchestration. This combination of the core properties underpins the success and popularity of the serverless FaaS paradigm. However, these benefits are not available to most cloud applications because they are designed for networked virtual machines/containers environments. Since such cloud applications cannot take advantage of the highly elastic resources of serverless and require run-time orchestration systems to operate, they suffer from lower resource utilization, additional management complexity, and costs relative to their FaaS serverless counterparts. We propose Imaginary Machines, a new serverless model for cloud applications. This model (1.) exposes the highly elastic resources of serverless platforms as the traditional network-of-hosts model that cloud applications expect, and (2.) it eliminates the need for explicit run-time orchestration by transparently managing application resources based on signals generated during cloud application executions. With the Imaginary Machines model, unmodified cloud applications become serverless applications. While still based on the network-of-host model, they benefit from the highly elastic resources and do not require runtime orchestration, just like their specialized serverless FaaS counterparts, promising increased resource utilization while reducing management costs. \u25b3 Less Submitted 30 June, 2024; originally announced July 2024. arXiv:2407.00832 [pdf, other] Boxer: FaaSt Ephemeral Elasticity for Off-the-Shelf Cloud Applications Authors: Michael Wawrzoniak, Rodrigo Bruno, Ana Klimovic, Gustavo Alonso Abstract: Elasticity is a key property of cloud computing. However, elasticity is offered today at the granularity of virtual machines, which take tens of seconds to start. This is insufficient to react to load spikes and sudden failures in latency sensitive applications, leading users to resort to expensive overprovisioning. Function-as-a-Service (FaaS) provides significantly higher elasticity than VMs, bu\u2026 \u25bd More Elasticity is a key property of cloud computing. However, elasticity is offered today at the granularity of virtual machines, which take tens of seconds to start. This is insufficient to react to load spikes and sudden failures in latency sensitive applications, leading users to resort to expensive overprovisioning. Function-as-a-Service (FaaS) provides significantly higher elasticity than VMs, but comes coupled with an event-triggered programming model and a constrained execution environment that makes them unsuitable for off-the-shelf applications. Previous work tries to overcome these obstacles but often requires re-architecting the applications. In this paper, we show how off-the-shelf applications can transparently benefit from ephemeral elasticity with FaaS. We built Boxer, an interposition layer spanning VMs and AWS Lambda, that intercepts application execution and emulates the network-of-hosts environment that applications expect when deployed in a conventional VM/container environment. The ephemeral elasticity of Boxer enables significant performance and cost savings for off-the-shelf applications with, e.g., recovery times over 5x faster than EC2 instances and absorbing load spikes comparable to overprovisioned EC2 VM instances. \u25b3 Less Submitted 30 June, 2024; originally announced July 2024. arXiv:2406.12385 [pdf, other] Accelerating Graph-based Vector Search via Delayed-Synchronization Traversal Authors: Wenqi Jiang, Hang Hu, Torsten Hoefler, Gustavo Alonso Abstract: Vector search systems are indispensable in large language model (LLM) serving, search engines, and recommender systems, where minimizing online search latency is essential. Among various algorithms, graph-based vector search (GVS) is particularly popular due to its high search performance and quality. To efficiently serve low-latency GVS, we propose a hardware-algorithm co-design solution includin\u2026 \u25bd More Vector search systems are indispensable in large language model (LLM) serving, search engines, and recommender systems, where minimizing online search latency is essential. Among various algorithms, graph-based vector search (GVS) is particularly popular due to its high search performance and quality. To efficiently serve low-latency GVS, we propose a hardware-algorithm co-design solution including Falcon, a GVS accelerator, and Delayed-Synchronization Traversal (DST), an accelerator-optimized graph traversal algorithm. Falcon implements high-performance GVS operators and reduces memory accesses with an on-chip Bloom filter to track search states. DST improves search performance and quality by relaxing the graph traversal order to maximize accelerator utilization. Evaluation across various graphs and datasets shows that our Falcon prototype on FPGAs, coupled with DST, achieves up to 4.3$\\times$ and 19.5$\\times$ speedups in latency and up to 8.0$\\times$ and 26.9$\\times$ improvements in energy efficiency over CPU and GPU-based GVS systems. The remarkable efficiency of Falcon and DST demonstrates their potential to become the standard solutions for future GVS acceleration. \u25b3 Less Submitted 18 June, 2024; originally announced June 2024. arXiv:2403.15839 [pdf, other] TablePuppet: A Generic Framework for Relational Federated Learning Authors: Lijie Xu, Chulin Xie, Yiran Guo, Gustavo Alonso, Bo Li, Guoliang Li, Wei Wang, Wentao Wu, Ce Zhang Abstract: Current federated learning (FL) approaches view decentralized training data as a single table, divided among participants either horizontally (by rows) or vertically (by columns). However, these approaches are inadequate for handling distributed relational tables across databases. This scenario requires intricate SQL operations like joins and unions to obtain the training data, which is either cos\u2026 \u25bd More Current federated learning (FL) approaches view decentralized training data as a single table, divided among participants either horizontally (by rows) or vertically (by columns). However, these approaches are inadequate for handling distributed relational tables across databases. This scenario requires intricate SQL operations like joins and unions to obtain the training data, which is either costly or restricted by privacy concerns. This raises the question: can we directly run FL on distributed relational tables? In this paper, we formalize this problem as relational federated learning (RFL). We propose TablePuppet, a generic framework for RFL that decomposes the learning process into two steps: (1) learning over join (LoJ) followed by (2) learning over union (LoU). In a nutshell, LoJ pushes learning down onto the vertical tables being joined, and LoU further pushes learning down onto the horizontal partitions of each vertical table. TablePuppet incorporates computation/communication optimizations to deal with the duplicate tuples introduced by joins, as well as differential privacy (DP) to protect against both feature and label leakages. We demonstrate the efficiency of TablePuppet in combination with two widely-used ML training algorithms, stochastic gradient descent (SGD) and alternating direction method of multipliers (ADMM), and compare their computation/communication complexity. We evaluate the SGD/ADMM algorithms developed atop TablePuppet by training diverse ML models. Our experimental results show that TablePuppet achieves model accuracy comparable to the centralized baselines running directly atop the SQL results. Moreover, ADMM takes less communication time than SGD to converge to similar model accuracy. \u25b3 Less Submitted 23 March, 2024; originally announced March 2024. Comments: 14 pages, 8 figures arXiv:2401.01150 [pdf, other] CXL and the Return of Scale-Up Database Engines Authors: Alberto Lerner, Gustavo Alonso Abstract: The trend toward specialized processing devices such as TPUs, DPUs, GPUs, and FPGAs has exposed the weaknesses of PCIe in interconnecting these devices and their hosts. Several attempts have been proposed to improve, augment, or downright replace PCIe, and more recently, these efforts have converged into a standard called Compute Express Link (CXL). CXL is already on version 2.0 in terms of commer\u2026 \u25bd More The trend toward specialized processing devices such as TPUs, DPUs, GPUs, and FPGAs has exposed the weaknesses of PCIe in interconnecting these devices and their hosts. Several attempts have been proposed to improve, augment, or downright replace PCIe, and more recently, these efforts have converged into a standard called Compute Express Link (CXL). CXL is already on version 2.0 in terms of commercial availability, but its potential to radically change the conventional server architecture has only just started to surface. For example, CXL can increase the bandwidth and quantity of memory available to any single machine beyond what that machine can originally provide, most importantly, in a manner that is fully transparent to software applications. We argue, however, that CXL can have a broader impact beyond memory expansion and deeply affect the architecture of data-intensive systems. In a nutshell, while the cloud favored scale-out approaches that grew in capacity by adding full servers to a rack, CXL brings back scale-up architectures that can grow by fine-tuning individual resources, all while transforming the rack into a large shared-memory machine. In this paper, we describe why such architectural transformations are now possible, how they benefit emerging heterogeneous hardware platforms for data-intensive systems, and the associated research challenges. \u25b3 Less Submitted 3 September, 2024; v1 submitted 2 January, 2024; originally announced January 2024. arXiv:2312.11742 [pdf, other] ACCL+: an FPGA-Based Collective Engine for Distributed Applications Authors: Zhenhao He, Dario Korolija, Yu Zhu, Benjamin Ramhorst, Tristan Laan, Lucian Petrica, Michaela Blott, Gustavo Alonso Abstract: FPGAs are increasingly prevalent in cloud deployments, serving as Smart NICs or network-attached accelerators. Despite their potential, developing distributed FPGA-accelerated applications remains cumbersome due to the lack of appropriate infrastructure and communication abstractions. To facilitate the development of distributed applications with FPGAs, in this paper we propose ACCL+, an open-sour\u2026 \u25bd More FPGAs are increasingly prevalent in cloud deployments, serving as Smart NICs or network-attached accelerators. Despite their potential, developing distributed FPGA-accelerated applications remains cumbersome due to the lack of appropriate infrastructure and communication abstractions. To facilitate the development of distributed applications with FPGAs, in this paper we propose ACCL+, an open-source versatile FPGA-based collective communication library. Portable across different platforms and supporting UDP, TCP, as well as RDMA, ACCL+ empowers FPGA applications to initiate direct FPGA-to-FPGA collective communication. Additionally, it can serve as a collective offload engine for CPU applications, freeing the CPU from networking tasks. It is user-extensible, allowing new collectives to be implemented and deployed without having to re-synthesize the FPGA circuit. We evaluated ACCL+ on an FPGA cluster with 100 Gb/s networking, comparing its performance against software MPI over RDMA. The results demonstrate ACCL+'s significant advantages for FPGA-based distributed applications and highly competitive performance for CPU applications. We showcase ACCL+'s dual role with two use cases: seamlessly integrating as a collective offload engine to distribute CPU-based vector-matrix multiplication, and serving as a crucial and efficient component in designing fully FPGA-based distributed deep-learning recommendation inference. \u25b3 Less Submitted 18 December, 2023; originally announced December 2023. arXiv:2312.00720 [pdf, other] Efficiently Processing Joins and Grouped Aggregations on GPUs Authors: Bowen Wu, Dimitrios Koutsoukos, Gustavo Alonso Abstract: There is a growing interest in leveraging GPUs for tasks beyond ML, especially in database systems. Despite the existing extensive work on GPU-based database operators, several questions are still open. For instance, the performance of almost all operators suffers from random accesses, which can account for up to 75% of the runtime. In addition, the group-by operator which is widely used in combin\u2026 \u25bd More There is a growing interest in leveraging GPUs for tasks beyond ML, especially in database systems. Despite the existing extensive work on GPU-based database operators, several questions are still open. For instance, the performance of almost all operators suffers from random accesses, which can account for up to 75% of the runtime. In addition, the group-by operator which is widely used in combination with joins, has not been fully explored for GPU acceleration. Furthermore, existing work often uses limited and unrepresentative workloads for evaluation and does not explore the query optimization aspect, i.e., how to choose the most efficient implementation based on the workload. In this paper, we revisit the state-of-the-art GPU-based join and group-by implementations. We identify their inefficiencies and propose several optimizations. We introduce GFTR, a novel technique to reduce random accesses, leading to speedups of up to 2.3x. We further optimize existing hash-based and sort-based group-by implementations, achieving significant speedups (19.4x and 1.7x, respectively). We also present a new partition-based group-by algorithm ideal for high group cardinalities. We analyze the optimizations with cost models, allowing us to predict the speedup. Finally, we conduct a performance evaluation to analyze each implementation. We conclude by providing practical heuristics to guide query optimizers in selecting the most efficient implementation for a given workload. \u25b3 Less Submitted 12 February, 2025; v1 submitted 1 December, 2023; originally announced December 2023. Journal ref: 2025. Proc. ACM Manag. Data 3, 1, Article 39 (February 2025), 27 pages arXiv:2310.09949 [pdf, other] Chameleon: a heterogeneous and disaggregated accelerator system for retrieval-augmented language models Authors: Wenqi Jiang, Marco Zeller, Roger Waleffe, Torsten Hoefler, Gustavo Alonso Abstract: A Retrieval-Augmented Language Model (RALM) augments a generative language model by retrieving context-specific knowledge from an external database. This strategy facilitates impressive text generation quality even with smaller models, thus reducing orders of magnitude of computational demands. However, RALMs introduce unique system design challenges due to (a) the diverse workload characteristics\u2026 \u25bd More A Retrieval-Augmented Language Model (RALM) augments a generative language model by retrieving context-specific knowledge from an external database. This strategy facilitates impressive text generation quality even with smaller models, thus reducing orders of magnitude of computational demands. However, RALMs introduce unique system design challenges due to (a) the diverse workload characteristics between LM inference and retrieval and (b) the various system requirements and bottlenecks for different RALM configurations such as model sizes, database sizes, and retrieval frequencies. We propose Chameleon, a heterogeneous accelerator system that integrates both LM and retrieval accelerators in a disaggregated architecture. The heterogeneity ensures efficient acceleration of both LM inference and retrieval, while the accelerator disaggregation enables the system to independently scale both types of accelerators to fulfill diverse RALM requirements. Our Chameleon prototype implements retrieval accelerators on FPGAs and assigns LM inference to GPUs, with a CPU server orchestrating these accelerators over the network. Compared to CPU-based and CPU-GPU vector search systems, Chameleon achieves up to 23.72x speedup and 26.2x energy efficiency. Evaluated on various RALMs, Chameleon exhibits up to 2.16x reduction in latency and 3.18x speedup in throughput compared to the hybrid CPU-GPU architecture. These promising results pave the way for bringing accelerator heterogeneity and disaggregation into future RALM systems. \u25b3 Less Submitted 29 November, 2023; v1 submitted 15 October, 2023; originally announced October 2023. arXiv:2309.16520 [pdf, other] SwiftSpatial: Spatial Joins on Modern Hardware Authors: Wenqi Jiang, Martin Parvanov, Gustavo Alonso Abstract: Spatial joins are among the most time-consuming queries in spatial data management systems. In this paper, we propose SwiftSpatial, a specialized accelerator architecture tailored for spatial joins. SwiftSpatial contains multiple high-performance join units with innovative hybrid parallelism, several efficient memory management units, and an integrated on-chip join scheduler. We prototype SwiftSpa\u2026 \u25bd More Spatial joins are among the most time-consuming queries in spatial data management systems. In this paper, we propose SwiftSpatial, a specialized accelerator architecture tailored for spatial joins. SwiftSpatial contains multiple high-performance join units with innovative hybrid parallelism, several efficient memory management units, and an integrated on-chip join scheduler. We prototype SwiftSpatial on an FPGA and incorporate the R-tree synchronous traversal algorithm as the control flow. Benchmarked against various CPU and GPU-based spatial data processing systems, SwiftSpatial demonstrates a latency reduction of up to 5.36x relative to the best-performing baseline, while requiring 6.16x less power. The remarkable performance and energy efficiency of SwiftSpatial lay a solid foundation for its future integration into spatial data management systems, both in data centers and at the edge. \u25b3 Less Submitted 28 September, 2023; originally announced September 2023. arXiv:2306.11182 [pdf, other] Co-design Hardware and Algorithm for Vector Search Authors: Wenqi Jiang, Shigang Li, Yu Zhu, Johannes de Fine Licht, Zhenhao He, Runbin Shi, Cedric Renggli, Shuai Zhang, Theodoros Rekatsinas, Torsten Hoefler, Gustavo Alonso Abstract: Vector search has emerged as the foundation for large-scale information retrieval and machine learning systems, with search engines like Google and Bing processing tens of thousands of queries per second on petabyte-scale document datasets by evaluating vector similarities between encoded query texts and web documents. As performance demands for vector search systems surge, accelerated hardware of\u2026 \u25bd More Vector search has emerged as the foundation for large-scale information retrieval and machine learning systems, with search engines like Google and Bing processing tens of thousands of queries per second on petabyte-scale document datasets by evaluating vector similarities between encoded query texts and web documents. As performance demands for vector search systems surge, accelerated hardware offers a promising solution in the post-Moore's Law era. We introduce \\textit{FANNS}, an end-to-end and scalable vector search framework on FPGAs. Given a user-provided recall requirement on a dataset and a hardware resource budget, \\textit{FANNS} automatically co-designs hardware and algorithm, subsequently generating the corresponding accelerator. The framework also supports scale-out by incorporating a hardware TCP/IP stack in the accelerator. \\textit{FANNS} attains up to 23.0$\\times$ and 37.2$\\times$ speedup compared to FPGA and CPU baselines, respectively, and demonstrates superior scalability to GPUs, achieving 5.5$\\times$ and 7.6$\\times$ speedup in median and 95\\textsuperscript{th} percentile (P95) latency within an eight-accelerator configuration. The remarkable performance of \\textit{FANNS} lays a robust groundwork for future FPGA integration in data centers and AI supercomputers. \u25b3 Less Submitted 6 July, 2023; v1 submitted 19 June, 2023; originally announced June 2023. Comments: 11 pages arXiv:2304.03044 [pdf, other] Data Processing with FPGAs on Modern Architectures Authors: Wenqi Jiang, Dario Korolija, Gustavo Alonso Abstract: Trends in hardware, the prevalence of the cloud, and the rise of highly demanding applications have ushered an era of specialization that quickly changes how data is processed at scale. These changes are likely to continue and accelerate in the next years as new technologies are adopted and deployed: smart NICs, smart storage, smart memory, disaggregated storage, disaggregated memory, specialized\u2026 \u25bd More Trends in hardware, the prevalence of the cloud, and the rise of highly demanding applications have ushered an era of specialization that quickly changes how data is processed at scale. These changes are likely to continue and accelerate in the next years as new technologies are adopted and deployed: smart NICs, smart storage, smart memory, disaggregated storage, disaggregated memory, specialized accelerators (GPUS, TPUs, FPGAs), and a wealth of ASICs specifically created to deal with computationally expensive tasks (e.g., cryptography or compression). In this tutorial, we focus on data processing on FPGAs, a technology that has received less attention than, e.g., TPUs or GPUs but that is, however, increasingly being deployed in the cloud for data processing tasks due to the architectural flexibility of FPGAs, along with their ability to process data at line rate, something not possible with other types of processors or accelerators. In the tutorial, we will cover what FPGAs are, their characteristics, their advantages and disadvantages, as well as examples from deployments in the industry and how they are used in various data processing tasks. We will introduce FPGA programming with high-level languages and describe hardware and software resources available to researchers. The tutorial includes case studies borrowed from research done in collaboration with companies that illustrate the potential of FPGAs in data processing and how software and hardware are evolving to take advantage of the possibilities offered by FPGAs. The use cases include: (1) approximated nearest neighbor search, which is relevant to databases and machine learning, (2) remote disaggregated memory, showing how the cloud architecture is evolving and demonstrating the potential for operator offloading and line rate data processing, and (3) recommendation system as an application with tight latency constraints. \u25b3 Less Submitted 24 June, 2023; v1 submitted 6 April, 2023; originally announced April 2023. arXiv:2208.09519 [pdf, other] Resource Allocation in Serverless Query Processing Authors: Simon Kassing, Ingo M\u00fcller, Gustavo Alonso Abstract: Data lakes hold a growing amount of cold data that is infrequently accessed, yet require interactive response times. Serverless functions are seen as a way to address this use case since they offer an appealing alternative to maintaining (and paying for) a fixed infrastructure. Recent research has analyzed the potential of serverless for data processing. In this paper, we expand on such work by lo\u2026 \u25bd More Data lakes hold a growing amount of cold data that is infrequently accessed, yet require interactive response times. Serverless functions are seen as a way to address this use case since they offer an appealing alternative to maintaining (and paying for) a fixed infrastructure. Recent research has analyzed the potential of serverless for data processing. In this paper, we expand on such work by looking into the question of serverless resource allocation to data processing tasks (number and size of the functions). We formulate a general model to roughly estimate completion time and financial cost, which we apply to augment an existing serverless data processing system with an advisory tool that automatically identifies configurations striking a good balance -- which we define as being close to the \"knee\" of their Pareto frontier. The model takes into account key aspects of serverless: start-up, computation, network transfers, and overhead as a function of the input sizes and intermediate result exchanges. Using (micro)benchmarks and parts of TPC-H, we show that this advisor is capable of pinpointing configurations desirable to the user. Moreover, we identify and discuss several aspects of data processing on serverless affecting efficiency. By using an automated tool to configure the resources, the barrier to using serverless for data processing is lowered and the narrow window where it is cost effective can be expanded by using a more optimal allocation instead of having to over-provision the design. \u25b3 Less Submitted 19 August, 2022; originally announced August 2022. arXiv:2208.07124 [pdf, other] ECI: a Customizable Cache Coherency Stack for Hybrid FPGA-CPU Architectures Authors: Abishek Ramdas, Michael Giardino, Runbin Shi, Adam Turowski, David Cock, Gustavo Alonso, Timothy Roscoe Abstract: Unlike other accelerators, FPGAs are capable of supporting cache coherency, thereby turning them into a more powerful architectural option than just a peripheral accelerator. However, most existing deployments of FPGAs are either non-cache coherent or support only an asymmetric design where cache coherency is controlled from the CPU. Taking advantage of a recently released two socket CPU-FPGA arch\u2026 \u25bd More Unlike other accelerators, FPGAs are capable of supporting cache coherency, thereby turning them into a more powerful architectural option than just a peripheral accelerator. However, most existing deployments of FPGAs are either non-cache coherent or support only an asymmetric design where cache coherency is controlled from the CPU. Taking advantage of a recently released two socket CPU-FPGA architecture, in this paper we describe ECI, a flexible implementation of cache coherency on the FPGA capable of supporting both symmetric and asymmetric protocols. ECI is open and customizable, given applications the opportunity to fully interact with the cache coherency protocol, thereby opening up many interesting system design and research opportunities not available in existing designs. Through extensive microbenchmarks we show that ECI exhibits highly competitive performance and discuss in detail one use-case illustrating the benefits of having an open cache coherency stack on the FPGA. \u25b3 Less Submitted 15 August, 2022; originally announced August 2022. arXiv:2202.06646 [pdf, other] Short-lived Datacenter Authors: Michael Wawrzoniak, Ingo M\u00fcller, Rodrigo Bruno, Ana Klimovic, Gustavo Alonso Abstract: Serverless platforms have attracted attention due to their promise of elasticity, low cost, and fast deployment. Instead of using a fixed virtual machine (VM) infrastructure, which can incur considerable costs to operate and run, serverless platforms support short computations, triggered on demand, with cost proportional to fine-grain function execution time. However, serverless platforms offer a\u2026 \u25bd More Serverless platforms have attracted attention due to their promise of elasticity, low cost, and fast deployment. Instead of using a fixed virtual machine (VM) infrastructure, which can incur considerable costs to operate and run, serverless platforms support short computations, triggered on demand, with cost proportional to fine-grain function execution time. However, serverless platforms offer a restricted execution environment. For example, functions have limited execution times, limited resources, and no support for networking between functions. In this paper, we explore what it takes to treat serverless platforms as short-lived, general purpose data-centers which can execute unmodified existing applications. As a first step in this quest, we have developed Boxer, a system providing an execution environment on top of existing functions-as-a-service platforms that allows users to seamlessly migrate conventional VM-based cloud services to serverless platforms. Boxer allows generic applications to benefit from the fine-grain elasticity of serverless platforms without having to modify applications to adopt a restrictive event-triggered programming model or orchestrate auxiliary systems for data communication. We implement Boxer on top of AWS Lambda and extend it to transparently provide standard network interfaces. We describe its implementation and demonstrate how it can be used to run off-the-shelf cloud applications with a degree of fine-grained elasticity not available on traditional VM-based platforms. \u25b3 Less Submitted 14 February, 2022; originally announced February 2022. arXiv:2112.12638 [pdf, other] RumbleML: program the lakehouse with JSONiq Authors: Ghislain Fourny, David Dao, Can Berker Cikis, Ce Zhang, Gustavo Alonso Abstract: Lakehouse systems have reached in the past few years unprecedented size and heterogeneity and have been embraced by many industry players. However, they are often difficult to use as they lack the declarative language and optimization possibilities of relational engines. This paper introduces RumbleML, a high-level, declarative library integrated into the RumbleDB engine and with the JSONiq langua\u2026 \u25bd More Lakehouse systems have reached in the past few years unprecedented size and heterogeneity and have been embraced by many industry players. However, they are often difficult to use as they lack the declarative language and optimization possibilities of relational engines. This paper introduces RumbleML, a high-level, declarative library integrated into the RumbleDB engine and with the JSONiq language. RumbleML allows using a single platform for data cleaning, data preparation, training, and inference, as well as management of models and results. It does it using a purely declarative language (JSONiq) for all these tasks and without any performance loss over existing platforms (e.g. Spark). The key insights of the design of RumbleML are that training sets, evaluation sets, and test sets can be represented as homogeneous sequences of flat objects; that models can be seamlessly embodied in function items mapping input test sets into prediction-augmented result sets; and that estimators can be seamlessly embodied in function items mapping input training sets to models. We argue that this makes JSONiq a viable and seamless programming language for data lakehouses across all their features, whether database-related or machine-learning-related. While lakehouses bring Machine Learning and Data Wrangling on the same platform, RumbleML also brings them to the same language, JSONiq. In the paper, we present the first prototype and compare its performance to Spark showing the benefit of a huge functionality and productivity gain for cleaning up, normalizing, validating data, feeding it into Machine Learning pipelines, and analyzing the output, all within the same system and language and at scale. \u25b3 Less Submitted 23 December, 2021; originally announced December 2021. Comments: 8 pages + references MSC Class: 68N99 ACM Class: H.2.3; C.2.4 arXiv:2112.00425 [pdf, other] How to use Persistent Memory in your Database Authors: Dimitrios Koutsoukos, Raghav Bhartia, Ana Klimovic, Gustavo Alonso Abstract: Persistent or Non Volatile Memory (PMEM or NVM) has recently become commercially available under several configurations with different purposes and goals. Despite the attention to the topic, we are not aware of a comprehensive empirical analysis of existing relational database engines under different PMEM configurations. Such a study is important to understand the performance implications of the v\u2026 \u25bd More Persistent or Non Volatile Memory (PMEM or NVM) has recently become commercially available under several configurations with different purposes and goals. Despite the attention to the topic, we are not aware of a comprehensive empirical analysis of existing relational database engines under different PMEM configurations. Such a study is important to understand the performance implications of the various hardware configurations and how different DB engines can benefit from them. To this end, we analyze three different engines (PostgreSQL, MySQL, and SQLServer) under common workloads (TPC-C and TPC-H) with all possible PMEM configurations supported by Intel's Optane NVM devices (PMEM as persistent memory in AppDirect mode and PMEM as volatile memory in Memory mode). Our results paint a complex picture and are not always intuitive due to the many factors involved. Based on our findings, we provide insights on how the different engines behave with PMEM and which configurations and queries perform best. Our results show that using PMEM as persistent storage usually speeds up query execution, but with some caveats as the I/O path is not fully optimized. Additionally, using PMEM in Memory mode does not offer any performance advantage despite the larger volatile memory capacity. Through the extensive coverage of engines and parameters, we provide an important starting point for exploiting PMEM in databases and tuning relational engines to take advantage of this new technology. \u25b3 Less Submitted 1 December, 2021; originally announced December 2021. arXiv:2108.09073 [pdf, other] From Research to Proof-of-Concept: Analysis of a Deployment of FPGAs on a Commercial Search Engine Authors: Fabio Maschi, Gustavo Alonso, Anthony Hock-Koon, Nicolas Bondoux, Teddy Roy, Mourad Boudia, Matteo Casalino Abstract: FPGAs are quickly becoming available in the cloud as a one more heterogeneous processing element complementing CPUs and GPUs. There are many reports in the literature showing the potential for FPGAs to accelerate a wide variety of algorithms, which combined with their growing availability, would seem to also indicate a widespread use in many applications. Unfortunately, there is not much published\u2026 \u25bd More FPGAs are quickly becoming available in the cloud as a one more heterogeneous processing element complementing CPUs and GPUs. There are many reports in the literature showing the potential for FPGAs to accelerate a wide variety of algorithms, which combined with their growing availability, would seem to also indicate a widespread use in many applications. Unfortunately, there is not much published research exploring what it takes to integrate an FPGA into an existing application in a cost-effective way and keeping the algorithmic performance advantages. Building on recent results exploring how to employ FPGAs to improve the search engines used in the travel industry, this paper analyses the end-to-end performance of the search engine when using FPGAs, as well as the necessary changes to the software and the cost of such deployments. The results provide important insights on current FPGA deployments and what needs to be done to make FPGAs more widely used. For instance, the large potential performance gains provided by an FPGA are greatly diminished in practice if the application cannot submit request in the most optimal way, something that is not always possible and might require significant changes to the application. Similarly, some existing cloud deployments turn out to use a very imbalanced architecture: a powerful FPGA connected to a not so powerful CPU. The result is that the CPU cannot generate enough load for the FPGA, which potentially eliminates all performance gains and might even result in a more expensive system. In this paper, we report on an extensive study and development effort to incorporate FPGAs into a search engine and analyse the issues encountered and their practical impact. We expect that these results will inform the development and deployment of FPGAs in the future by providing important insights on the end-to-end integration of FPGAs within existing systems. \u25b3 Less Submitted 20 August, 2021; originally announced August 2021. arXiv:2106.07102 [pdf, other] Farview: Disaggregated Memory with Operator Off-loading for Database Engines Authors: Dario Korolija, Dimitrios Koutsoukos, Kimberly Keeton, Konstantin Taranov, Dejan Miloji\u010di\u0107, Gustavo Alonso Abstract: Cloud deployments disaggregate storage from compute, providing more flexibility to both the storage and compute layers. In this paper, we explore disaggregation by taking it one step further and applying it to memory (DRAM). Disaggregated memory uses network attached DRAM as a way to decouple memory from CPU. In the context of databases, such a design offers significant advantages in terms of maki\u2026 \u25bd More Cloud deployments disaggregate storage from compute, providing more flexibility to both the storage and compute layers. In this paper, we explore disaggregation by taking it one step further and applying it to memory (DRAM). Disaggregated memory uses network attached DRAM as a way to decouple memory from CPU. In the context of databases, such a design offers significant advantages in terms of making a larger memory capacity available as a central pool to a collection of smaller processing nodes. To explore these possibilities, we have implemented Farview, a disaggregated memory solution for databases, operating as a remote buffer cache with operator offloading capabilities. Farview is implemented as an FPGA-based smart NIC making DRAM available as a disaggregated, network attached memory module capable of performing data processing at line rate over data streams to/from disaggregated memory. Farview supports query offloading using operators such as selection, projection, aggregation, regular expression matching and encryption. In this paper we focus on analytical queries and demonstrate the viability of the idea through an extensive experimental evaluation of Farview under different workloads. Farview is competitive with a local buffer cache solution for all the workloads and outperforms it in a number of cases, proving that a smart disaggregated memory can be a viable alternative for databases deployed in cloud environments. \u25b3 Less Submitted 13 June, 2021; originally announced June 2021. Comments: 12 pages arXiv:2105.07806 [pdf, other] Towards Demystifying Serverless Machine Learning Training Authors: Jiawei Jiang, Shaoduo Gan, Yue Liu, Fanlin Wang, Gustavo Alonso, Ana Klimovic, Ankit Singla, Wentao Wu, Ce Zhang Abstract: The appeal of serverless (FaaS) has triggered a growing interest on how to use it in data-intensive applications such as ETL, query processing, or machine learning (ML). Several systems exist for training large-scale ML models on top of serverless infrastructures (e.g., AWS Lambda) but with inconclusive results in terms of their performance and relative advantage over \"serverful\" infrastructures (\u2026 \u25bd More The appeal of serverless (FaaS) has triggered a growing interest on how to use it in data-intensive applications such as ETL, query processing, or machine learning (ML). Several systems exist for training large-scale ML models on top of serverless infrastructures (e.g., AWS Lambda) but with inconclusive results in terms of their performance and relative advantage over \"serverful\" infrastructures (IaaS). In this paper we present a systematic, comparative study of distributed ML training over FaaS and IaaS. We present a design space covering design choices such as optimization algorithms and synchronization protocols, and implement a platform, LambdaML, that enables a fair comparison between FaaS and IaaS. We present experimental results using LambdaML, and further develop an analytic model to capture cost/performance tradeoffs that must be considered when opting for a serverless infrastructure. Our results indicate that ML training pays off in serverless only for models with efficient (i.e., reduced) communication and that quickly converge. In general, FaaS can be much faster but it is never significantly cheaper than IaaS. \u25b3 Less Submitted 17 May, 2021; originally announced May 2021. arXiv:2104.12615 [pdf, other] Evaluating Query Languages and Systems for High-Energy Physics Data [Extended Version] Authors: Dan Graur, Ingo M\u00fcller, Mason Proffitt, Ghislain Fourny, Gordon T. Watts, Gustavo Alonso Abstract: In the domain of high-energy physics (HEP), query languages in general and SQL in particular have found limited acceptance. This is surprising since HEP data analysis matches the SQL model well: the data is fully structured and queried using mostly standard operators. To gain insights on why this is the case, we perform a comprehensive analysis of six diverse, general-purpose data processing platf\u2026 \u25bd More In the domain of high-energy physics (HEP), query languages in general and SQL in particular have found limited acceptance. This is surprising since HEP data analysis matches the SQL model well: the data is fully structured and queried using mostly standard operators. To gain insights on why this is the case, we perform a comprehensive analysis of six diverse, general-purpose data processing platforms using an HEP benchmark. The result of the evaluation is an interesting and rather complex picture of existing solutions: Their query languages vary greatly in how natural and concise HEP query patterns can be expressed. Furthermore, most of them are also between one and two orders of magnitude slower than the domain-specific system used by particle physicists today. These observations suggest that, while database systems and their query languages are in principle viable tools for HEP, significant work remains to make them relevant to HEP researchers. \u25b3 Less Submitted 30 October, 2021; v1 submitted 26 April, 2021; originally announced April 2021. Comments: This is the extended version of a full paper to appear in PVLDB 15.2 (VLDB 2022) arXiv:2010.05894 [pdf, other] MicroRec: Efficient Recommendation Inference by Hardware and Data Structure Solutions Authors: Wenqi Jiang, Zhenhao He, Shuai Zhang, Thomas B. Preu\u00dfer, Kai Zeng, Liang Feng, Jiansong Zhang, Tongxuan Liu, Yong Li, Jingren Zhou, Ce Zhang, Gustavo Alonso Abstract: Deep neural networks are widely used in personalized recommendation systems. Unlike regular DNN inference workloads, recommendation inference is memory-bound due to the many random memory accesses needed to lookup the embedding tables. The inference is also heavily constrained in terms of latency because producing a recommendation for a user must be done in about tens of milliseconds. In this pape\u2026 \u25bd More Deep neural networks are widely used in personalized recommendation systems. Unlike regular DNN inference workloads, recommendation inference is memory-bound due to the many random memory accesses needed to lookup the embedding tables. The inference is also heavily constrained in terms of latency because producing a recommendation for a user must be done in about tens of milliseconds. In this paper, we propose MicroRec, a high-performance inference engine for recommendation systems. MicroRec accelerates recommendation inference by (1) redesigning the data structures involved in the embeddings to reduce the number of lookups needed and (2) taking advantage of the availability of High-Bandwidth Memory (HBM) in FPGA accelerators to tackle the latency by enabling parallel lookups. We have implemented the resulting design on an FPGA board including the embedding lookup step as well as the complete inference process. Compared to the optimized CPU baseline (16 vCPU, AVX2-enabled), MicroRec achieves 13.8~14.7x speedup on embedding lookup alone and 2.5$~5.4x speedup for the entire recommendation inference in terms of throughput. As for latency, CPU-based engines needs milliseconds for inferring a recommendation while MicroRec only takes microseconds, a significant advantage in real-time recommendation systems. \u25b3 Less Submitted 19 February, 2021; v1 submitted 12 October, 2020; originally announced October 2020. Comments: Accepted by MLSys'21 (the 4th Conference on Machine Learning and Systems) arXiv:2007.14394 [pdf, other] Signed Distance Fields Dynamic Diffuse Global Illumination Authors: Jinkai Hu, Milo Yip, G. Elias Alonso, Shihao Gu, Xiangjun Tang, Xiaogang Jin Abstract: Global Illumination (GI) is of utmost importance in the field of photo-realistic rendering. However, its computation has always been very complex, especially diffuse GI. State of the art real-time GI methods have limitations of different nature, such as light leaking, performance issues, special hardware requirements, noise corruption, bounce number limitations, among others. To overcome these lim\u2026 \u25bd More Global Illumination (GI) is of utmost importance in the field of photo-realistic rendering. However, its computation has always been very complex, especially diffuse GI. State of the art real-time GI methods have limitations of different nature, such as light leaking, performance issues, special hardware requirements, noise corruption, bounce number limitations, among others. To overcome these limitations, we propose a novel approach of computing dynamic diffuse GI with a signed distance fields approximation of the scene and discretizing the space domain of the irradiance function. With this approach, we are able to estimate real-time diffuse GI for dynamic lighting and geometry, without any precomputations and supporting multi-bounce GI, providing good quality lighting and high performance at the same time. Our algorithm is also able to achieve better scalability, and manage both large open scenes and indoor high-detailed scenes without being corrupted by noise. \u25b3 Less Submitted 28 July, 2020; originally announced July 2020. arXiv:2005.13332 [pdf, other] HyperLogLog Sketch Acceleration on FPGA Authors: Amit Kulkarni, Monica Chiosa, Thomas B. Preu\u00dfer, Kaan Kara, David Sidler, Gustavo Alonso Abstract: Data sketches are a set of widely used approximated data summarizing techniques. Their fundamental property is sub-linear memory complexity on the input cardinality, an important aspect when processing streams or data sets with a vast base domain (URLs, IP addresses, user IDs, etc.). Among the many data sketches available, HyperLogLog has become the reference for cardinality counting (how many dis\u2026 \u25bd More Data sketches are a set of widely used approximated data summarizing techniques. Their fundamental property is sub-linear memory complexity on the input cardinality, an important aspect when processing streams or data sets with a vast base domain (URLs, IP addresses, user IDs, etc.). Among the many data sketches available, HyperLogLog has become the reference for cardinality counting (how many distinct data items there are in a data set). Although it does not count every data item (to reduce memory consumption), it provides probabilistic guarantees on the result, and it is, thus, often used to analyze data streams. In this paper, we explore how to implement HyperLogLog on an FPGA to benefit from the parallelism available and the ability to process data streams coming from high-speed networks. Our multi-pipelined high-cardinality HyperLogLog implementation delivers 1.8x higher throughput than an optimized HyperLogLog running on a dual-socket Intel Xeon E5-2630 v3 system with a total of 16 cores and 32 hyper-threads. \u25b3 Less Submitted 20 October, 2020; v1 submitted 24 May, 2020; originally announced May 2020. Comments: This paper was accepted as a full paper to FPL 2020. The latest/full version of this paper is available: https://ieeexplore.ieee.org/document/9221525 arXiv:2005.04324 [pdf, other] Benchmarking High Bandwidth Memory on FPGAs Authors: Zeke Wang, Hongjing Huang, Jie Zhang, Gustavo Alonso Abstract: FPGAs are starting to be enhanced with High Bandwidth Memory (HBM) as a way to reduce the memory bandwidth bottleneck encountered in some applications and to give the FPGA more capacity to deal with application state. However, the performance characteristics of HBM are still not well specified, especially in the context of FPGAs. In this paper, we bridge the gap between nominal specifications and\u2026 \u25bd More FPGAs are starting to be enhanced with High Bandwidth Memory (HBM) as a way to reduce the memory bandwidth bottleneck encountered in some applications and to give the FPGA more capacity to deal with application state. However, the performance characteristics of HBM are still not well specified, especially in the context of FPGAs. In this paper, we bridge the gap between nominal specifications and actual performance by benchmarkingHBM on a state-of-the-art FPGA, i.e., a Xilinx Alveo U280 featuring a two-stack HBM subsystem. To this end, we propose Shuhai, a benchmarking tool that allows us to demystify all the underlying details of HBM on an FPGA. FPGA-based benchmarking should also provide a more accurate picture of HBM than doing so on CPUs/GPUs, since CPUs/GPUs are noisier systems due to their complex control logic and cache hierarchy. Since the memory itself is complex, leveraging custom hardware logic to benchmark inside an FPGA provides more details as well as accurate and deterministic measurements. We observe that 1) HBM is able to provide up to 425GB/s memory bandwidth, and 2) how HBM is used has a significant impact on performance, which in turn demonstrates the importance of unveiling the performance characteristics of HBM so as to select the best approach. As a yardstick, we also applyShuhaito DDR4to show the differences between HBM and DDR4.Shuhai can be easily generalized to other FPGA boards or other generations of memory, e.g., HBM3, and DDR3. We will makeShuhaiopen-source, benefiting the community \u25b3 Less Submitted 8 May, 2020; originally announced May 2020. arXiv:2004.11080 [pdf, ps, other] Using DSP Slices as Content-Addressable Update Queues Authors: Thomas B. Preu\u00dfer, Monica Chiosa, Alexander Weiss, Gustavo Alonso Abstract: Content-Addressable Memory (CAM) is a powerful abstraction for building memory caches, routing tables and hazard detection logic. Without a native CAM structure available on FPGA devices, their functionality must be emulated using the structural primitives at hand. Such an emulation causes significant overhead in the consumption of the underlying resources, typically general-purpose fabric and on-\u2026 \u25bd More Content-Addressable Memory (CAM) is a powerful abstraction for building memory caches, routing tables and hazard detection logic. Without a native CAM structure available on FPGA devices, their functionality must be emulated using the structural primitives at hand. Such an emulation causes significant overhead in the consumption of the underlying resources, typically general-purpose fabric and on-chip block RAM (BRAM). This often motivates mitigating trade-offs, such as the reduction of the associativity of memory caches. This paper describes a technique to implement the hazard resolution in a memory update queue that hides the off-chip memory readout latency of read-modify-write cycles while guaranteeing the delivery of the full memory bandwidth. The innovative use of DSP slices allows them to assume and combine the functions of (a) the tag and data storage, (b) the tag matching, and (c) the data update in this key-value storage scenario. The proposed approach provides designers with extra flexibility by adding this resource type as another option to implement CAM. \u25b3 Less Submitted 23 April, 2020; originally announced April 2020. Comments: Submitted to FPL 2020 arXiv:2004.03488 [pdf, other] Modularis: Modular Relational Analytics over Heterogeneous Distributed Platforms Authors: Dimitrios Koutsoukos, Ingo M\u00fcller, Renato Marroqu\u00edn, Ana Klimovic, Gustavo Alonso Abstract: The enormous quantity of data produced every day together with advances in data analytics has led to a proliferation of data management and analysis systems. Typically, these systems are built around highly specialized monolithic operators optimized for the underlying hardware. While effective in the short term, such an approach makes the operators cumbersome to port and adapt, which is increasing\u2026 \u25bd More The enormous quantity of data produced every day together with advances in data analytics has led to a proliferation of data management and analysis systems. Typically, these systems are built around highly specialized monolithic operators optimized for the underlying hardware. While effective in the short term, such an approach makes the operators cumbersome to port and adapt, which is increasingly required due to the speed at which algorithms and hardware evolve. To address this limitation, we present Modularis, an execution layer for data analytics based on sub-operators, i.e.,composable building blocks resembling traditional database operators but at a finer granularity. To demonstrate the advantages of our approach, we use Modularis to build a distributed query processing system supporting relational queries running on an RDMA cluster, a serverless cloud platform, and a smart storage engine. Modularis requires minimal code changes to execute queries across these three diverse hardware platforms, showing that the sub-operator approach reduces the amount and complexity of the code. In fact, changes in the platform affect only sub-operators that depend on the underlying hardware. We show the end-to-end performance of Modularis by comparing it with a framework for SQL processing (Presto), a commercial cluster database (SingleStore), as well as Query-as-a-Service systems (Athena, BigQuery). Modularis outperforms all these systems, proving that the design and architectural advantages of a modular design can be achieved without degrading performance. We also compare Modularis with a hand-optimized implementation of a join for RDMA clusters. We show that Modularis has the advantage of being easily extensible to a wider range of join variants and group by queries, all of which are not supported in the hand-tuned join. \u25b3 Less Submitted 29 September, 2021; v1 submitted 7 April, 2020; originally announced April 2020. Comments: Accepted at PVLDB vol. 14 arXiv:2004.01908 [pdf, other] The Collection Virtual Machine: An Abstraction for Multi-Frontend Multi-Backend Data Analysis Authors: Ingo M\u00fcller, Renato Marroqu\u00edn, Dimitrios Koutsoukos, Mike Wawrzoniak, Sabir Akhadov, Gustavo Alonso Abstract: Getting the best performance from the ever-increasing number of hardware platforms has been a recurring challenge for data processing systems. In recent years, the advent of data science with its increasingly numerous and complex types of analytics has made this challenge even more difficult. In practice, system designers are overwhelmed by the number of combinations and typically implement only o\u2026 \u25bd More Getting the best performance from the ever-increasing number of hardware platforms has been a recurring challenge for data processing systems. In recent years, the advent of data science with its increasingly numerous and complex types of analytics has made this challenge even more difficult. In practice, system designers are overwhelmed by the number of combinations and typically implement only one analysis/platform combination, leading to repeated implementation effort -- and a plethora of semi-compatible tools for data scientists. In this paper, we propose the \"Collection Virtual Machine\" (or CVM) -- an extensible compiler framework designed to keep the specialization process of data analytics systems tractable. It can capture at the same time the essence of a large span of low-level, hardware-specific implementation techniques as well as high-level operations of different types of analyses. At its core lies a language for defining nested, collection-oriented intermediate representations (IRs). Frontends produce programs in their IR flavors defined in that language, which get optimized through a series of rewritings (possibly changing the IR flavor multiple times) until the program is finally expressed in an IR of platform-specific operators. While reducing the overall implementation effort, this also improves the interoperability of both analyses and hardware platforms. We have used CVM successfully to build specialized backends for platforms as diverse as multi-core CPUs, RDMA clusters, and serverless computing infrastructure in the cloud and expect similar results for many more frontends and hardware platforms in the near future. \u25b3 Less Submitted 8 April, 2020; v1 submitted 4 April, 2020; originally announced April 2020. Comments: This paper is currently under review at DaMoN'20 arXiv:2004.01635 [pdf, other] High Bandwidth Memory on FPGAs: A Data Analytics Perspective Authors: Kaan Kara, Christoph Hagleitner, Dionysios Diamantopoulos, Dimitris Syrivelis, Gustavo Alonso Abstract: FPGA-based data processing in datacenters is increasing in popularity due to the demands of modern workloads and the ensuing necessity for specialization in hardware. Driven by this trend, vendors are rapidly adapting reconfigurable devices to suit data and compute intensive workloads. Inclusion of High Bandwidth Memory (HBM) in FPGA devices is a recent example. HBM promises overcoming the bandwid\u2026 \u25bd More FPGA-based data processing in datacenters is increasing in popularity due to the demands of modern workloads and the ensuing necessity for specialization in hardware. Driven by this trend, vendors are rapidly adapting reconfigurable devices to suit data and compute intensive workloads. Inclusion of High Bandwidth Memory (HBM) in FPGA devices is a recent example. HBM promises overcoming the bandwidth bottleneck, faced often by FPGA-based accelerators due to their throughput oriented design. In this paper, we study the usage and benefits of HBM on FPGAs from a data analytics perspective. We consider three workloads that are often performed in analytics oriented databases and implement them on FPGA showing in which cases they benefit from HBM: range selection, hash join, and stochastic gradient descent for linear model training. We integrate our designs into a columnar database (MonetDB) and show the trade-offs arising from the integration related to data movement and partitioning. In certain cases, FPGA+HBM based solutions are able to surpass the highest performance provided by either a 2-socket POWER9 system or a 14-core XeonE5 by up to 1.8x (selection), 12.9x (join), and 3.2x (SGD). \u25b3 Less Submitted 2 April, 2020; originally announced April 2020. arXiv:1912.00937 [pdf, other] Lambada: Interactive Data Analytics on Cold Data using Serverless Cloud Infrastructure Authors: Ingo M\u00fcller, Renato Marroqu\u00edn, Gustavo Alonso Abstract: The promise of ultimate elasticity and operational simplicity of serverless computing has recently lead to an explosion of research in this area. In the context of data analytics, the concept sounds appealing, but due to the limitations of current offerings, there is no consensus yet on whether or not this approach is technically and economically viable. In this paper, we identify interactive data\u2026 \u25bd More The promise of ultimate elasticity and operational simplicity of serverless computing has recently lead to an explosion of research in this area. In the context of data analytics, the concept sounds appealing, but due to the limitations of current offerings, there is no consensus yet on whether or not this approach is technically and economically viable. In this paper, we identify interactive data analytics on cold data as a use case where serverless computing excels. We design and implement Lambada, a system following a purely serverless architecture, in order to illustrate when and how serverless computing should be employed for data analytics. We propose several system components that overcome the previously known limitations inherent in the serverless paradigm as well as additional ones we identify in this work. We can show that, thanks to careful design, a serverless query processing system can be at the same time one order of magnitude faster and two orders of magnitude cheaper compared to commercial Query-as-a-Service systems, the only alternative with similar operational simplicity. \u25b3 Less Submitted 2 December, 2019; originally announced December 2019. Report number: https://doi.org/10.3929/ethz-b-000413183 arXiv:1910.11582 [pdf, other] Rumble: Data Independence for Large Messy Data Sets Authors: Ingo M\u00fcller, Ghislain Fourny, Stefan Irimescu, Can Berker Cikis, Gustavo Alonso Abstract: This paper introduces Rumble, a query execution engine for large, heterogeneous, and nested collections of JSON objects built on top of Apache Spark. While data sets of this type are more and more wide-spread, most existing tools are built around a tabular data model, creating an impedance mismatch for both the engine and the query interface. In contrast, Rumble uses JSONiq, a standardized languag\u2026 \u25bd More This paper introduces Rumble, a query execution engine for large, heterogeneous, and nested collections of JSON objects built on top of Apache Spark. While data sets of this type are more and more wide-spread, most existing tools are built around a tabular data model, creating an impedance mismatch for both the engine and the query interface. In contrast, Rumble uses JSONiq, a standardized language specifically designed for querying JSON documents. The key challenge in the design and implementation of Rumble is mapping the recursive structure of JSON documents and JSONiq queries onto Spark's execution primitives based on tabular data frames. Our solution is to translate a JSONiq expression into a tree of iterators that dynamically switch between local and distributed execution modes depending on the nesting level. By overcoming the impedance mismatch in the engine, Rumble frees the user from solving the same problem for every single query, thus increasing their productivity considerably. As we show in extensive experiments, Rumble is able to scale to large and complex data sets in the terabyte range with a similar or better performance than other engines. The results also illustrate that Codd's concept of data independence makes as much sense for heterogeneous, nested data sets as it does on highly structured tables. \u25b3 Less Submitted 19 October, 2020; v1 submitted 25 October, 2019; originally announced October 2019. Comments: In revision for PVLDB 14, 2020 MSC Class: 68N99 ACM Class: H.2.3; C.2.4 arXiv:1910.09017 [pdf, other] Demystifying Graph Databases: Analysis and Taxonomy of Data Organization, System Designs, and Graph Queries Authors: Maciej Besta, Robert Gerstenberger, Emanuel Peter, Marc Fischer, Micha\u0142 Podstawski, Claude Barthels, Gustavo Alonso, Torsten Hoefler Abstract: Graph processing has become an important part of multiple areas of computer science, such as machine learning, computational sciences, medical applications, social network analysis, and many others. Numerous graphs such as web or social networks may contain up to trillions of edges. Often, these graphs are also dynamic (their structure changes over time) and have domain-specific rich data associat\u2026 \u25bd More Graph processing has become an important part of multiple areas of computer science, such as machine learning, computational sciences, medical applications, social network analysis, and many others. Numerous graphs such as web or social networks may contain up to trillions of edges. Often, these graphs are also dynamic (their structure changes over time) and have domain-specific rich data associated with vertices and edges. Graph database systems such as Neo4j enable storing, processing, and analyzing such large, evolving, and rich datasets. Due to the sheer size of such datasets, combined with the irregular nature of graph processing, these systems face unique design challenges. To facilitate the understanding of this emerging domain, we present the first survey and taxonomy of graph database systems. We focus on identifying and analyzing fundamental categories of these systems (e.g., triple stores, tuple stores, native graph database systems, or object-oriented systems), the associated graph models (e.g., RDF or Labeled Property Graph), data organization techniques (e.g., storing graph data in indexing structures or dividing data into records), and different aspects of data distribution and query execution (e.g., support for sharding and ACID). 51 graph database systems are presented and compared, including Neo4j, OrientDB, or Virtuoso. We outline graph database queries and relationships with associated domains (NoSQL stores, graph streaming, and dynamic graph algorithms). Finally, we describe research and engineering challenges to outline the future of graph databases. \u25b3 Less Submitted 30 August, 2023; v1 submitted 20 October, 2019; originally announced October 2019. Comments: A shorter version of the survey was published in the journal ACM Computing Surveys arXiv:1904.03257 [pdf, ps, other] MLSys: The New Frontier of Machine Learning Systems Authors: Alexander Ratner, Dan Alistarh, Gustavo Alonso, David G. Andersen, Peter Bailis, Sarah Bird, Nicholas Carlini, Bryan Catanzaro, Jennifer Chayes, Eric Chung, Bill Dally, Jeff Dean, Inderjit S. Dhillon, Alexandros Dimakis, Pradeep Dubey, Charles Elkan, Grigori Fursin, Gregory R. Ganger, Lise Getoor, Phillip B. Gibbons, Garth A. Gibson, Joseph E. Gonzalez, Justin Gottschlich, Song Han, Kim Hazelwood , et al. (44 additional authors not shown) Abstract: Machine learning (ML) techniques are enjoying rapidly increasing adoption. However, designing and implementing the systems that support ML models in real-world deployments remains a significant obstacle, in large part due to the radically different development and deployment profile of modern ML methods, and the range of practical concerns that come with broader adoption. We propose to foster a ne\u2026 \u25bd More Machine learning (ML) techniques are enjoying rapidly increasing adoption. However, designing and implementing the systems that support ML models in real-world deployments remains a significant obstacle, in large part due to the radically different development and deployment profile of modern ML methods, and the range of practical concerns that come with broader adoption. We propose to foster a new systems machine learning research community at the intersection of the traditional systems and ML communities, focused on topics such as hardware systems for ML, software systems for ML, and ML optimized for metrics beyond predictive accuracy. To do this, we describe a new conference, MLSys, that explicitly targets research at the intersection of systems and machine learning with a program committee split evenly between experts in systems and ML, and an explicit focus on topics at the intersection of the two. \u25b3 Less Submitted 1 December, 2019; v1 submitted 29 March, 2019; originally announced April 2019. arXiv:1903.03404 [pdf, other] Accelerating Generalized Linear Models with MLWeaving: A One-Size-Fits-All System for Any-precision Learning (Technical Report) Authors: Zeke Wang, Kaan Kara, Hantian Zhang, Gustavo Alonso, Onur Mutlu, Ce Zhang Abstract: Learning from the data stored in a database is an important function increasingly available in relational engines. Methods using lower precision input data are of special interest given their overall higher efficiency but, in databases, these methods have a hidden cost: the quantization of the real value into a smaller number is an expensive step. To address the issue, in this paper we present MLW\u2026 \u25bd More Learning from the data stored in a database is an important function increasingly available in relational engines. Methods using lower precision input data are of special interest given their overall higher efficiency but, in databases, these methods have a hidden cost: the quantization of the real value into a smaller number is an expensive step. To address the issue, in this paper we present MLWeaving, a data structure and hardware acceleration technique intended to speed up learning of generalized linear models in databases. ML-Weaving provides a compact, in-memory representation enabling the retrieval of data at any level of precision. MLWeaving also takes advantage of the increasing availability of FPGA-based accelerators to provide a highly efficient implementation of stochastic gradient descent. The solution adopted in MLWeaving is more efficient than existing designs in terms of space (since it can process any resolution on the same design) and resources (via the use of bit-serial multipliers). MLWeaving also enables the runtime tuning of precision, instead of a fixed precision level during the training. We illustrate this using a simple, dynamic precision schedule. Experimental results show MLWeaving achieves up to16 performance improvement over low-precision CPU implementations of first-order methods. \u25b3 Less Submitted 28 March, 2019; v1 submitted 8 March, 2019; originally announced March 2019. Comments: 18 pages Journal ref: PVLDB, 2019 arXiv:1809.00159 [pdf, other] Pay One, Get Hundreds for Free: Reducing Cloud Costs through Shared Query Execution Authors: Renato Marroqu\u00edn, Ingo M\u00fcller, Darko Makreshanski, Gustavo Alonso Abstract: Cloud-based data analysis is nowadays common practice because of the lower system management overhead as well as the pay-as-you-go pricing model. The pricing model, however, is not always suitable for query processing as heavy use results in high costs. For example, in query-as-a-service systems, where users are charged per processed byte, collections of queries accessing the same data frequently\u2026 \u25bd More Cloud-based data analysis is nowadays common practice because of the lower system management overhead as well as the pay-as-you-go pricing model. The pricing model, however, is not always suitable for query processing as heavy use results in high costs. For example, in query-as-a-service systems, where users are charged per processed byte, collections of queries accessing the same data frequently can become expensive. The problem is compounded by the limited options for the user to optimize query execution when using declarative interfaces such as SQL. In this paper, we show how, without modifying existing systems and without the involvement of the cloud provider, it is possible to significantly reduce the overhead, and hence the cost, of query-as-a-service systems. Our approach is based on query rewriting so that multiple concurrent queries are combined into a single query. Our experiments show the aggregated amount of work done by the shared execution is smaller than in a query-at-a-time approach. Since queries are charged per byte processed, the cost of executing a group of queries is often the same as executing a single one of them. As an example, we demonstrate how the shared execution of the TPC-H benchmark is up to 100x and 16x cheaper in Amazon Athena and Google BigQuery than using a query-at-a-time approach while achieving a higher throughput. \u25b3 Less Submitted 1 September, 2018; originally announced September 2018. Journal ref: Proceedings of the ACM Symposium on Cloud Computing (SoCC) 2018, pages 439-450 arXiv:1802.09883 [pdf, other] Reproducible Floating-Point Aggregation in RDBMSs Authors: Ingo M\u00fcller, Andrea Arteaga, Torsten Hoefler, Gustavo Alonso Abstract: Industry-grade database systems are expected to produce the same result if the same query is repeatedly run on the same input. However, the numerous sources of non-determinism in modern systems make reproducible results difficult to achieve. This is particularly true if floating-point numbers are involved, where the order of the operations affects the final result. As part of a larger effort to\u2026 \u25bd More Industry-grade database systems are expected to produce the same result if the same query is repeatedly run on the same input. However, the numerous sources of non-determinism in modern systems make reproducible results difficult to achieve. This is particularly true if floating-point numbers are involved, where the order of the operations affects the final result. As part of a larger effort to extend database engines with data representations more suitable for machine learning and scientific applications, in this paper we explore the problem of making relational GroupBy over floating-point formats bit-reproducible, i.e., ensuring any execution of the operator produces the same result up to every single bit. To that aim, we first propose a numeric data type that can be used as drop-in replacement for other number formats and is---unlike standard floating-point formats---associative. We use this data type to make state-of-the-art GroupBy operators reproducible, but this approach incurs a slowdown between 4x and 12x compared to the same operator using conventional database number formats. We thus explore how to modify existing GroupBy algorithms to make them bit-reproducible and efficient. By using vectorized summation on batches and carefully balancing batch size, cache footprint, and preprocessing costs, we are able to reduce the slowdown due to reproducibility to a factor between 1.9x and 2.4x of aggregation in isolation and to a mere 2.7% of end-to-end query performance even on aggregation-intensive queries in MonetDB. We thereby provide a solid basis for supporting more reproducible operations directly in relational engines. This document is an extended version of an article currently in print for the proceedings of ICDE'18 with the same title and by the same authors. The main additions are more implementation details and experiments. \u25b3 Less Submitted 27 February, 2018; originally announced February 2018. Comments: This document is the extended version of an article in the Proceedings of the 34th IEEE International Conference on Data Engineering (ICDE) 2018 Journal ref: 34th IEEE International Conference on Data Engineering (ICDE) 2018, Paris, 2018, pp. 1049-1060 arXiv:1203.0056 [pdf, other] SharedDB: Killing One Thousand Queries With One Stone Authors: Georgios Giannikis, Gustavo Alonso, Donald Kossmann Abstract: Traditional database systems are built around the query-at-a-time model. This approach tries to optimize performance in a best-effort way. Unfortunately, best effort is not good enough for many modern applications. These applications require response time guarantees in high load situations. This paper describes the design of a new database architecture that is based on batching queries and shared\u2026 \u25bd More Traditional database systems are built around the query-at-a-time model. This approach tries to optimize performance in a best-effort way. Unfortunately, best effort is not good enough for many modern applications. These applications require response time guarantees in high load situations. This paper describes the design of a new database architecture that is based on batching queries and shared computation across possibly hundreds of concurrent queries and updates. Performance experiments with the TPC-W benchmark show that the performance of our implementation, SharedDB, is indeed robust across a wide range of dynamic workloads. \u25b3 Less Submitted 29 February, 2012; originally announced March 2012. Comments: VLDB2012 Journal ref: Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 6, pp. 526-537 (2012) arXiv:1103.5554 [pdf] Visual Localisation of Mobile Devices in an Indoor Environment under Network Delay Conditions Authors: Alberto Alonso Fern\u00e1ndez, Omar \u00c1lvarez Fres, Ignacio Gonz\u00e1lez Alonso, Huosheng Hu Abstract: Current progresses in home automation and service robotic environment have highlighted the need to develop interoperability mechanisms that allow a standard communication between the two systems. During the development of the DHCompliant protocol, the problem of locating mobile devices in an indoor environment has been investigated. The communication of the device with the location service has bee\u2026 \u25bd More Current progresses in home automation and service robotic environment have highlighted the need to develop interoperability mechanisms that allow a standard communication between the two systems. During the development of the DHCompliant protocol, the problem of locating mobile devices in an indoor environment has been investigated. The communication of the device with the location service has been carried out to study the time delay that web services offer in front of the sockets. The importance of obtaining data from real-time location systems portends that a basic tool for interoperability, such as web services, can be ineffective in this scenario because of the delays added in the invocation of services. This paper is focused on introducing a web service to resolve a coordinates request without any significant delay in comparison with the sockets. \u25b3 Less Submitted 29 March, 2011; originally announced March 2011. Journal ref: International Journal of Distributed and Parallel Systems (IJDPS) Vol.2, No.2, March 2011"
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Subramanian,+S",
    "title": "Showing 1\u201350 of 154 results for author: Subramanian, S",
    "author": "Authors:\nWenqi Jiang, \n      \n      Suvinay Subramanian, \n      \n      Cat Graves, \n      \n      Gustavo Alonso, \n      \n      Amir Yazdanbakhsh, \n      \n      Vidushi Dadu",
    "body": "arXiv:2503.14649 [pdf, other] RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving Authors: Wenqi Jiang, Suvinay Subramanian, Cat Graves, Gustavo Alonso, Amir Yazdanbakhsh, Vidushi Dadu Abstract: Retrieval-augmented generation (RAG), which combines large language models (LLMs) with retrievals from external knowledge databases, is emerging as a popular approach for reliable LLM serving. However, efficient RAG serving remains an open challenge due to the rapid emergence of many RAG variants and the substantial differences in workload characteristics across them. In this paper, we make three\u2026 \u25bd More Retrieval-augmented generation (RAG), which combines large language models (LLMs) with retrievals from external knowledge databases, is emerging as a popular approach for reliable LLM serving. However, efficient RAG serving remains an open challenge due to the rapid emergence of many RAG variants and the substantial differences in workload characteristics across them. In this paper, we make three fundamental contributions to advancing RAG serving. First, we introduce RAGSchema, a structured abstraction that captures the wide range of RAG algorithms, serving as a foundation for performance optimization. Second, we analyze several representative RAG workloads with distinct RAGSchema, revealing significant performance variability across these workloads. Third, to address this variability and meet diverse performance requirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a system optimization framework for efficient RAG serving. Our evaluation shows that RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in time-to-first-token latency compared to RAG systems built on LLM-system extensions. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. arXiv:2503.03866 [pdf, other] Learning to Negotiate via Voluntary Commitment Authors: Shuhui Zhu, Baoxiang Wang, Sriram Ganapathi Subramanian, Pascal Poupart Abstract: The partial alignment and conflict of autonomous agents lead to mixed-motive scenarios in many real-world applications. However, agents may fail to cooperate in practice even when cooperation yields a better outcome. One well known reason for this failure comes from non-credible commitments. To facilitate commitments among agents for better cooperation, we define Markov Commitment Games (MCGs), a\u2026 \u25bd More The partial alignment and conflict of autonomous agents lead to mixed-motive scenarios in many real-world applications. However, agents may fail to cooperate in practice even when cooperation yields a better outcome. One well known reason for this failure comes from non-credible commitments. To facilitate commitments among agents for better cooperation, we define Markov Commitment Games (MCGs), a variant of commitment games, where agents can voluntarily commit to their proposed future plans. Based on MCGs, we propose a learnable commitment protocol via policy gradients. We further propose incentive-compatible learning to accelerate convergence to equilibria with better social welfare. Experimental results in challenging mixed-motive tasks demonstrate faster empirical convergence and higher returns for our method compared with its counterparts. Our code is available at https://github.com/shuhui-zhu/DCL. \u25b3 Less Submitted 19 March, 2025; v1 submitted 5 March, 2025; originally announced March 2025. Comments: Accepted by AISTATS 2025 arXiv:2502.11517 [pdf, other] Learning to Keep a Promise: Scaling Language Model Decoding Parallelism with Learned Asynchronous Decoding Authors: Tian Jin, Ellie Y. Cheng, Zack Ankner, Nikunj Saunshi, Blake M. Elias, Amir Yazdanbakhsh, Jonathan Ragan-Kelley, Suvinay Subramanian, Michael Carbin Abstract: Decoding with autoregressive large language models (LLMs) traditionally occurs sequentially, generating one token after another. An emerging line of work explored parallel decoding by identifying and simultaneously generating semantically independent chunks of LLM responses. However, these techniques rely on hand-crafted heuristics tied to syntactic structures like lists and paragraphs, making the\u2026 \u25bd More Decoding with autoregressive large language models (LLMs) traditionally occurs sequentially, generating one token after another. An emerging line of work explored parallel decoding by identifying and simultaneously generating semantically independent chunks of LLM responses. However, these techniques rely on hand-crafted heuristics tied to syntactic structures like lists and paragraphs, making them rigid and imprecise. We present PASTA, a learning-based system that teaches LLMs to identify semantic independence and express parallel decoding opportunities in their own responses. At its core are PASTA-LANG and its interpreter: PASTA-LANG is an annotation language that enables LLMs to express semantic independence in their own responses; the language interpreter acts on these annotations to orchestrate parallel decoding on-the-fly at inference time. Through a two-stage finetuning process, we train LLMs to generate PASTA-LANG annotations that optimize both response quality and decoding speed. Evaluation on AlpacaEval, an instruction following benchmark, shows that our approach Pareto-dominates existing methods in terms of decoding speed and response quality; our results demonstrate geometric mean speedups ranging from 1.21x to 1.93x with corresponding quality changes of +2.2% to -7.1%, measured by length-controlled win rates against sequential decoding baseline. \u25b3 Less Submitted 21 February, 2025; v1 submitted 17 February, 2025; originally announced February 2025. Comments: 15 pages arXiv:2502.02683 [pdf, other] Streaming Speaker Change Detection and Gender Classification for Transducer-Based Multi-Talker Speech Translation Authors: Peidong Wang, Naoyuki Kanda, Jian Xue, Jinyu Li, Xiaofei Wang, Aswin Shanmugam Subramanian, Junkun Chen, Sunit Sivasankaran, Xiong Xiao, Yong Zhao Abstract: Streaming multi-talker speech translation is a task that involves not only generating accurate and fluent translations with low latency but also recognizing when a speaker change occurs and what the speaker's gender is. Speaker change information can be used to create audio prompts for a zero-shot text-to-speech system, and gender can help to select speaker profiles in a conventional text-to-speec\u2026 \u25bd More Streaming multi-talker speech translation is a task that involves not only generating accurate and fluent translations with low latency but also recognizing when a speaker change occurs and what the speaker's gender is. Speaker change information can be used to create audio prompts for a zero-shot text-to-speech system, and gender can help to select speaker profiles in a conventional text-to-speech model. We propose to tackle streaming speaker change detection and gender classification by incorporating speaker embeddings into a transducer-based streaming end-to-end speech translation model. Our experiments demonstrate that the proposed methods can achieve high accuracy for both speaker change detection and gender classification. \u25b3 Less Submitted 4 February, 2025; originally announced February 2025. arXiv:2501.12486 [pdf, other] The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws Authors: Tian Jin, Ahmed Imtiaz Humayun, Utku Evci, Suvinay Subramanian, Amir Yazdanbakhsh, Dan Alistarh, Gintare Karolina Dziugaite Abstract: Pruning eliminates unnecessary parameters in neural networks; it offers a promising solution to the growing computational demands of large language models (LLMs). While many focus on post-training pruning, sparse pre-training--which combines pruning and pre-training into a single phase--provides a simpler alternative. In this work, we present the first systematic exploration of optimal sparse pre-\u2026 \u25bd More Pruning eliminates unnecessary parameters in neural networks; it offers a promising solution to the growing computational demands of large language models (LLMs). While many focus on post-training pruning, sparse pre-training--which combines pruning and pre-training into a single phase--provides a simpler alternative. In this work, we present the first systematic exploration of optimal sparse pre-training configurations for LLMs through an examination of 80 unique pruning schedules across different sparsity levels and training durations. We find that initiating pruning at 25% of total training compute and concluding at 75% achieves near-optimal final evaluation loss. These findings provide valuable insights for efficient and effective sparse pre-training of LLMs. Furthermore, we propose a new scaling law that modifies the Chinchilla scaling law to use the average parameter count over pre-training. Through empirical and theoretical validation, we demonstrate that this modified scaling law accurately models evaluation loss for both sparsely and densely pre-trained LLMs, unifying scaling laws across pre-training paradigms. Our findings indicate that while sparse pre-training achieves the same final model quality as dense pre-training for equivalent compute budgets, it provides substantial benefits through reduced model size, enabling significant potential computational savings during inference. \u25b3 Less Submitted 15 March, 2025; v1 submitted 21 January, 2025; originally announced January 2025. Comments: 17 pages arXiv:2501.10084 [pdf, other] Two-level Solar Irradiance Clustering with Season Identification: A Comparative Analysis Authors: Roshni Agrawal, Sivakumar Subramanian, Venkataramana Runkana Abstract: Solar irradiance clustering can enhance solar power capacity planning and help improve forecasting models by identifying similar irradiance patterns influenced by seasonal and weather changes. In this study, we adopt an efficient two-level clustering approach to automatically identify seasons using the clear sky irradiance in first level and subsequently to identify daily cloud level as clear, clo\u2026 \u25bd More Solar irradiance clustering can enhance solar power capacity planning and help improve forecasting models by identifying similar irradiance patterns influenced by seasonal and weather changes. In this study, we adopt an efficient two-level clustering approach to automatically identify seasons using the clear sky irradiance in first level and subsequently to identify daily cloud level as clear, cloudy and partly cloudy within each season in second level. In the second level of clustering, three methods are compared, namely, Daily Irradiance Index (DII or $\u03b2$), Euclidean Distance (ED), and Dynamic Time Warping (DTW) distance. The DII is computed as the ratio of time integral of measured irradiance to time integral of the clear sky irradiance. The identified clusters were compared quantitatively using established clustering metrics and qualitatively by comparing the mean irradiance profiles. The results clearly establish the superiority of the $\u03b2$-based clustering approach as the leader, setting a new benchmark for solar irradiance clustering studies. Moreover, $\u03b2$-based clustering remains effective even for annual data unlike the time-series methods which suffer significant performance degradation. Interestingly, contrary to expectations, ED-based clustering outperforms the more compute-intensive DTW distance-based clustering. The method has been rigorously validated using data from two distinct US locations, demonstrating robust scalability for larger datasets and potential applicability for other locations. \u25b3 Less Submitted 17 January, 2025; originally announced January 2025. Comments: 30 pages, 9 figures, 6 tables arXiv:2501.05465 [pdf, other] Small Language Models (SLMs) Can Still Pack a Punch: A survey Authors: Shreyas Subramanian, Vikram Elango, Mecit Gungor Abstract: As foundation AI models continue to increase in size, an important question arises - is massive scale the only path forward? This survey of about 160 papers presents a family of Small Language Models (SLMs) in the 1 to 8 billion parameter range that demonstrate smaller models can perform as well, or even outperform large models. We explore task agnostic, general purpose SLMs, task-specific SLMs an\u2026 \u25bd More As foundation AI models continue to increase in size, an important question arises - is massive scale the only path forward? This survey of about 160 papers presents a family of Small Language Models (SLMs) in the 1 to 8 billion parameter range that demonstrate smaller models can perform as well, or even outperform large models. We explore task agnostic, general purpose SLMs, task-specific SLMs and techniques to create SLMs that can guide the community to build models while balancing performance, efficiency, scalability and cost. Furthermore we define and characterize SLMs' effective sizes, representing increased capability with respect to LLMs. \u25b3 Less Submitted 3 January, 2025; originally announced January 2025. arXiv:2501.00912 [pdf, other] AutoPresent: Designing Structured Visuals from Scratch Authors: Jiaxin Ge, Zora Zhiruo Wang, Xuhui Zhou, Yi-Hao Peng, Sanjay Subramanian, Qinyue Tan, Maarten Sap, Alane Suhr, Daniel Fried, Graham Neubig, Trevor Darrell Abstract: Designing structured visuals such as presentation slides is essential for communicative needs, necessitating both content creation and visual planning skills. In this work, we tackle the challenge of automated slide generation, where models produce slide presentations from natural language (NL) instructions. We first introduce the SlidesBench benchmark, the first benchmark for slide generation wit\u2026 \u25bd More Designing structured visuals such as presentation slides is essential for communicative needs, necessitating both content creation and visual planning skills. In this work, we tackle the challenge of automated slide generation, where models produce slide presentations from natural language (NL) instructions. We first introduce the SlidesBench benchmark, the first benchmark for slide generation with 7k training and 585 testing examples derived from 310 slide decks across 10 domains. SlidesBench supports evaluations that are (i)reference-based to measure similarity to a target slide, and (ii)reference-free to measure the design quality of generated slides alone. We benchmark end-to-end image generation and program generation methods with a variety of models, and find that programmatic methods produce higher-quality slides in user-interactable formats. Built on the success of program generation, we create AutoPresent, an 8B Llama-based model trained on 7k pairs of instructions paired with code for slide generation, and achieve results comparable to the closed-source model GPT-4o. We further explore iterative design refinement where the model is tasked to self-refine its own output, and we found that this process improves the slide's quality. We hope that our work will provide a basis for future work on generating structured visuals. \u25b3 Less Submitted 1 January, 2025; originally announced January 2025. arXiv:2411.05781 [pdf, other] Using Language Models to Disambiguate Lexical Choices in Translation Authors: Josh Barua, Sanjay Subramanian, Kayo Yin, Alane Suhr Abstract: In translation, a concept represented by a single word in a source language can have multiple variations in a target language. The task of lexical selection requires using context to identify which variation is most appropriate for a source text. We work with native speakers of nine languages to create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingual concept variation when tran\u2026 \u25bd More In translation, a concept represented by a single word in a source language can have multiple variations in a target language. The task of lexical selection requires using context to identify which variation is most appropriate for a source text. We work with native speakers of nine languages to create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingual concept variation when translating from English. We evaluate recent LLMs and neural machine translation systems on DTAiLS, with the best-performing model, GPT-4, achieving from 67 to 85% accuracy across languages. Finally, we use language models to generate English rules describing target-language concept variations. Providing weaker models with high-quality lexical rules improves accuracy substantially, in some cases reaching or outperforming GPT-4. \u25b3 Less Submitted 8 November, 2024; originally announced November 2024. Comments: Accepted to EMNLP 2024 arXiv:2410.12800 [pdf] Reproducibility Needs Reshape Scientific Data Governance Authors: Paul Meijer, Yousef Aggoune, Madeline Ambrose, Aldan Beaubien, James Harvey, Nicole Howard, Neelima Inala, Ed Johnson, Autumn Kelsey, Melissa Kinsey, Jessica Liang, Paul Mariz, Stark Pister, Sathya Subramanian, Vitalii Tereshchenko, Anne Vetto Abstract: Scientific data governance should prioritize maximizing the utility of data throughout the research lifecycle. Research software systems that enable analysis reproducibility inform data governance policies and assist administrators in setting clear guidelines for data reuse, data retention, and the management of scientific computing needs. Proactive analysis reproducibility and data governance are\u2026 \u25bd More Scientific data governance should prioritize maximizing the utility of data throughout the research lifecycle. Research software systems that enable analysis reproducibility inform data governance policies and assist administrators in setting clear guidelines for data reuse, data retention, and the management of scientific computing needs. Proactive analysis reproducibility and data governance are integral and interconnected components of research lifecycle management. \u25b3 Less Submitted 29 September, 2024; originally announced October 2024. arXiv:2410.07073 [pdf, other] Pixtral 12B Authors: Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, Soham Ghosh, Am\u00e9lie H\u00e9liou, Paul Jacob, Albert Q. Jiang, Kartik Khandelwal, Timoth\u00e9e Lacroix, Guillaume Lample, Diego Las Casas, Thibaut Lavril, Teven Le Scao, Andy Lo, William Marshall, Louis Martin, Arthur Mensch, Pavankumar Muddireddy , et al. (17 additional authors not shown) Abstract: We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to ex\u2026 \u25bd More We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license. \u25b3 Less Submitted 10 October, 2024; v1 submitted 9 October, 2024; originally announced October 2024. arXiv:2410.00273 [pdf, other] Comprehensive Performance Modeling and System Design Insights for Foundation Models Authors: Shashank Subramanian, Ermal Rrapaj, Peter Harrington, Smeet Chheda, Steven Farrell, Brian Austin, Samuel Williams, Nicholas Wright, Wahid Bhimji Abstract: Generative AI, in particular large transformer models, are increasingly driving HPC system design in science and industry. We analyze performance characteristics of such transformer models and discuss their sensitivity to the transformer type, parallelization strategy, and HPC system features (accelerators and interconnects). We utilize a performance model that allows us to explore this complex de\u2026 \u25bd More Generative AI, in particular large transformer models, are increasingly driving HPC system design in science and industry. We analyze performance characteristics of such transformer models and discuss their sensitivity to the transformer type, parallelization strategy, and HPC system features (accelerators and interconnects). We utilize a performance model that allows us to explore this complex design space and highlight its key components. We find that different transformer types demand different parallelism and system characteristics at different training regimes. Large Language Models are performant with 3D parallelism and amplify network needs only at pre-training scales with reduced dependence on accelerator capacity and bandwidth. On the other hand, long-sequence transformers, representative of scientific foundation models, place a more uniform dependence on network and capacity with necessary 4D parallelism. Our analysis emphasizes the need for closer performance modeling of different transformer types keeping system features in mind and demonstrates a path towards this. Our code is available as open-source. \u25b3 Less Submitted 30 September, 2024; originally announced October 2024. Comments: 17 pages, PMBS 2024 arXiv:2409.12566 [pdf, other] Quantum Channel Testing in Average-Case Distance Authors: Gregory Rosenthal, Hugo Aaronson, Sathyawageeswar Subramanian, Animesh Datta, Tom Gur Abstract: We study the complexity of testing properties of quantum channels. First, we show that testing identity to any channel $\\mathcal N: \\mathbb C^{d_{\\mathrm{in}} \\times d_{\\mathrm{in}}} \\to \\mathbb C^{d_{\\mathrm{out}} \\times d_{\\mathrm{out}}}$ in diamond norm distance requires $\u03a9(\\sqrt{d_{\\mathrm{in}}} / \\varepsilon)$ queries, even in the strongest algorithmic model that admits ancillae, coherence, a\u2026 \u25bd More We study the complexity of testing properties of quantum channels. First, we show that testing identity to any channel $\\mathcal N: \\mathbb C^{d_{\\mathrm{in}} \\times d_{\\mathrm{in}}} \\to \\mathbb C^{d_{\\mathrm{out}} \\times d_{\\mathrm{out}}}$ in diamond norm distance requires $\u03a9(\\sqrt{d_{\\mathrm{in}}} / \\varepsilon)$ queries, even in the strongest algorithmic model that admits ancillae, coherence, and adaptivity. This is due to the worst-case nature of the distance induced by the diamond norm. Motivated by this limitation and other theoretical and practical applications, we introduce an average-case analogue of the diamond norm, which we call the average-case imitation diamond (ACID) norm. In the weakest algorithmic model without ancillae, coherence, or adaptivity, we prove that testing identity to certain types of channels in ACID distance can be done with complexity independent of the dimensions of the channel, while for other types of channels the complexity depends on both the input and output dimensions. Building on previous work, we also show that identity to any fixed channel can be tested with $\\tilde O(d_{\\mathrm{in}} d_{\\mathrm{out}}^{3/2} / \\varepsilon^2)$ queries in ACID distance and $\\tilde O(d_{\\mathrm{in}}^2 d_{\\mathrm{out}}^{3/2} / \\varepsilon^2)$ queries in diamond distance in this model. Finally, we prove tight bounds on the complexity of channel tomography in ACID distance. \u25b3 Less Submitted 5 October, 2024; v1 submitted 19 September, 2024; originally announced September 2024. arXiv:2409.09493 [pdf] Hacking, The Lazy Way: LLM Augmented Pentesting Authors: Dhruva Goyal, Sitaraman Subramanian, Aditya Peela Abstract: Security researchers are continually challenged by the need to stay current with rapidly evolving cybersecurity research, tools, and techniques. This constant cycle of learning, unlearning, and relearning, combined with the repetitive tasks of sifting through documentation and analyzing data, often hinders productivity and innovation. This has led to a disparity where only organizations with subst\u2026 \u25bd More Security researchers are continually challenged by the need to stay current with rapidly evolving cybersecurity research, tools, and techniques. This constant cycle of learning, unlearning, and relearning, combined with the repetitive tasks of sifting through documentation and analyzing data, often hinders productivity and innovation. This has led to a disparity where only organizations with substantial resources can access top-tier security experts, while others rely on firms with less skilled researchers who focus primarily on compliance rather than actual security. We introduce \"LLM Augmented Pentesting,\" demonstrated through a tool named \"Pentest Copilot,\" to address this gap. This approach integrates Large Language Models into penetration testing workflows. Our research includes a \"chain of thought\" mechanism to streamline token usage and boost performance, as well as unique Retrieval Augmented Generation implementation to minimize hallucinations and keep models aligned with the latest techniques. Additionally, we propose a novel file analysis approach, enabling LLMs to understand files. Furthermore, we highlight a unique infrastructure system that supports if implemented, can support in-browser assisted penetration testing, offering a robust platform for cybersecurity professionals, These advancements mark a significant step toward bridging the gap between automated tools and human expertise, offering a powerful solution to the challenges faced by modern cybersecurity teams. \u25b3 Less Submitted 14 September, 2024; originally announced September 2024. Comments: 9 pages, 7 figures ACM Class: I.2.1 arXiv:2409.07569 [pdf, other] A Comprehensive Survey on Inverse Constrained Reinforcement Learning: Definitions, Progress and Challenges Authors: Guiliang Liu, Sheng Xu, Shicheng Liu, Ashish Gaurav, Sriram Ganapathi Subramanian, Pascal Poupart Abstract: Inverse Constrained Reinforcement Learning (ICRL) is the task of inferring the implicit constraints that expert agents adhere to, based on their demonstration data. As an emerging research topic, ICRL has received considerable attention in recent years. This article presents a categorical survey of the latest advances in ICRL. It serves as a comprehensive reference for machine learning researchers\u2026 \u25bd More Inverse Constrained Reinforcement Learning (ICRL) is the task of inferring the implicit constraints that expert agents adhere to, based on their demonstration data. As an emerging research topic, ICRL has received considerable attention in recent years. This article presents a categorical survey of the latest advances in ICRL. It serves as a comprehensive reference for machine learning researchers and practitioners, as well as starters seeking to comprehend the definitions, advancements, and important challenges in ICRL. We begin by formally defining the problem and outlining the algorithmic framework that facilitates constraint inference across various scenarios. These include deterministic or stochastic environments, environments with limited demonstrations, and multiple agents. For each context, we illustrate the critical challenges and introduce a series of fundamental methods to tackle these issues. This survey encompasses discrete, virtual, and realistic environments for evaluating ICRL agents. We also delve into the most pertinent applications of ICRL, such as autonomous driving, robot control, and sports analytics. To stimulate continuing research, we conclude the survey with a discussion of key unresolved questions in ICRL that can effectively foster a bridge between theoretical understanding and practical industrial applications. The papers referenced in this survey can be found at https://github.com/Jasonxu1225/Awesome-Constraint-Inference-in-RL. \u25b3 Less Submitted 31 January, 2025; v1 submitted 11 September, 2024; originally announced September 2024. Comments: 36 pages Journal ref: Transactions on Machine Learning Research, 2025 arXiv:2408.16378 [pdf, other] Unconditionally separating noisy $\\mathsf{QNC}^0$ from bounded polynomial threshold circuits of constant depth Authors: Min-Hsiu Hsieh, Leandro Mendes, Michael de Oliveira, Sathyawageeswar Subramanian Abstract: We study classes of constant-depth circuits with gates that compute restricted polynomial threshold functions, recently introduced by [Kum23] as a family that strictly generalizes $\\mathsf{AC}^0$. Denoting these circuit families $\\mathsf{bPTFC}^0[k]$ for $\\textit{bounded polynomial threshold circuits}$ parameterized by an integer-valued degree-bound $k$, we prove three hardness results separating\u2026 \u25bd More We study classes of constant-depth circuits with gates that compute restricted polynomial threshold functions, recently introduced by [Kum23] as a family that strictly generalizes $\\mathsf{AC}^0$. Denoting these circuit families $\\mathsf{bPTFC}^0[k]$ for $\\textit{bounded polynomial threshold circuits}$ parameterized by an integer-valued degree-bound $k$, we prove three hardness results separating these classes from constant-depth quantum circuits ($\\mathsf{QNC}^0$). $\\hspace{2em}$ - We prove that the parity halving problem [WKS+19], which $\\mathsf{QNC}^0$ over qubits can solve with certainty, remains average-case hard for polynomial size $\\mathsf{bPTFC}^0[k]$ circuits for all $k=\\mathcal{O}(n^{1/(5d)})$. $\\hspace{2em}$ - We construct a new family of relation problems based on computing $\\mathsf{mod}\\ p$ for each prime $p>2$, and prove a separation of $\\mathsf{QNC}^0$ circuits over higher dimensional quantum systems (`qupits') against $\\mathsf{bPTFC}^0[k]$ circuits for the same degree-bound parameter as above. $\\hspace{2em}$ - We prove that both foregoing results are noise-robust under the local stochastic noise model, by introducing fault-tolerant implementations of non-Clifford $\\mathsf{QNC}^0/|\\overline{T^{1/p}}>$ circuits, that use logical magic states as advice. $\\mathsf{bPTFC}^0[k]$ circuits can compute certain classes of Polynomial Threshold Functions (PTFs), which in turn serve as a natural model for neural networks and exhibit enhanced expressivity and computational capabilities. Furthermore, for large enough values of $k$, $\\mathsf{bPTFC}^0[k]$ contains $\\mathsf{TC}^0$ as a subclass. The main challenges we overcome include establishing classical average-case lower bounds, designing non-local games with quantum-classical gaps in winning probabilities and developing noise-resilient non-Clifford quantum circuits necessary to extend beyond qubits to higher dimensions. \u25b3 Less Submitted 29 August, 2024; originally announced August 2024. arXiv:2408.09103 [pdf] Provide Proactive Reproducible Analysis Transparency with Every Publication Authors: Paul Meijer, Nicole Howard, Jessica Liang, Autumn Kelsey, Sathya Subramanian, Ed Johnson, Paul Mariz, James Harvey, Madeline Ambrose, Vitalii Tereshchenko, Aldan Beaubien, Neelima Inala, Yousef Aggoune, Stark Pister, Anne Vetto, Melissa Kinsey, Tom Bumol, Ananda Goldrath, Xiaojun Li, Troy Torgerson, Peter Skene, Lauren Okada, Christian La France, Zach Thomson, Lucas Graybuck Abstract: The high incidence of irreproducible research has led to urgent appeals for transparency and equitable practices in open science. For the scientific disciplines that rely on computationally intensive analyses of large data sets, a granular understanding of the analysis methodology is an essential component of reproducibility. This paper discusses the guiding principles of a computational reproduci\u2026 \u25bd More The high incidence of irreproducible research has led to urgent appeals for transparency and equitable practices in open science. For the scientific disciplines that rely on computationally intensive analyses of large data sets, a granular understanding of the analysis methodology is an essential component of reproducibility. This paper discusses the guiding principles of a computational reproducibility framework that enables a scientist to proactively generate a complete reproducible trace as analysis unfolds, and share data, methods and executable tools as part of a scientific publication, allowing other researchers to verify results and easily re-execute the steps of the scientific investigation. \u25b3 Less Submitted 17 August, 2024; originally announced August 2024. arXiv:2408.03100 [pdf, other] Huge Ensembles Part I: Design of Ensemble Weather Forecasts using Spherical Fourier Neural Operators Authors: Ankur Mahesh, William Collins, Boris Bonev, Noah Brenowitz, Yair Cohen, Joshua Elms, Peter Harrington, Karthik Kashinath, Thorsten Kurth, Joshua North, Travis OBrien, Michael Pritchard, David Pruitt, Mark Risser, Shashank Subramanian, Jared Willard Abstract: Studying low-likelihood high-impact extreme weather events in a warming world is a significant and challenging task for current ensemble forecasting systems. While these systems presently use up to 100 members, larger ensembles could enrich the sampling of internal variability. They may capture the long tails associated with climate hazards better than traditional ensemble sizes. Due to computatio\u2026 \u25bd More Studying low-likelihood high-impact extreme weather events in a warming world is a significant and challenging task for current ensemble forecasting systems. While these systems presently use up to 100 members, larger ensembles could enrich the sampling of internal variability. They may capture the long tails associated with climate hazards better than traditional ensemble sizes. Due to computational constraints, it is infeasible to generate huge ensembles (comprised of 1,000-10,000 members) with traditional, physics-based numerical models. In this two-part paper, we replace traditional numerical simulations with machine learning (ML) to generate hindcasts of huge ensembles. In Part I, we construct an ensemble weather forecasting system based on Spherical Fourier Neural Operators (SFNO), and we discuss important design decisions for constructing such an ensemble. The ensemble represents model uncertainty through perturbed-parameter techniques, and it represents initial condition uncertainty through bred vectors, which sample the fastest growing modes of the forecast. Using the European Centre for Medium-Range Weather Forecasts Integrated Forecasting System (IFS) as a baseline, we develop an evaluation pipeline composed of mean, spectral, and extreme diagnostics. Using large-scale, distributed SFNOs with 1.1 billion learned parameters, we achieve calibrated probabilistic forecasts. As the trajectories of the individual members diverge, the ML ensemble mean spectra degrade with lead time, consistent with physical expectations. However, the individual ensemble members' spectra stay constant with lead time. Therefore, these members simulate realistic weather states, and the ML ensemble thus passes a crucial spectral test in the literature. The IFS and ML ensembles have similar Extreme Forecast Indices, and we show that the ML extreme weather forecasts are reliable and discriminating. \u25b3 Less Submitted 17 February, 2025; v1 submitted 6 August, 2024; originally announced August 2024. arXiv:2408.01581 [pdf, other] Huge Ensembles Part II: Properties of a Huge Ensemble of Hindcasts Generated with Spherical Fourier Neural Operators Authors: Ankur Mahesh, William Collins, Boris Bonev, Noah Brenowitz, Yair Cohen, Peter Harrington, Karthik Kashinath, Thorsten Kurth, Joshua North, Travis OBrien, Michael Pritchard, David Pruitt, Mark Risser, Shashank Subramanian, Jared Willard Abstract: In Part I, we created an ensemble based on Spherical Fourier Neural Operators. As initial condition perturbations, we used bred vectors, and as model perturbations, we used multiple checkpoints trained independently from scratch. Based on diagnostics that assess the ensemble's physical fidelity, our ensemble has comparable performance to operational weather forecasting systems. However, it require\u2026 \u25bd More In Part I, we created an ensemble based on Spherical Fourier Neural Operators. As initial condition perturbations, we used bred vectors, and as model perturbations, we used multiple checkpoints trained independently from scratch. Based on diagnostics that assess the ensemble's physical fidelity, our ensemble has comparable performance to operational weather forecasting systems. However, it requires several orders of magnitude fewer computational resources. Here in Part II, we generate a huge ensemble (HENS), with 7,424 members initialized each day of summer 2023. We enumerate the technical requirements for running huge ensembles at this scale. HENS precisely samples the tails of the forecast distribution and presents a detailed sampling of internal variability. For extreme climate statistics, HENS samples events 4$\u03c3$ away from the ensemble mean. At each grid cell, HENS improves the skill of the most accurate ensemble member and enhances coverage of possible future trajectories. As a weather forecasting model, HENS issues extreme weather forecasts with better uncertainty quantification. It also reduces the probability of outlier events, in which the verification value lies outside the ensemble forecast distribution. \u25b3 Less Submitted 17 February, 2025; v1 submitted 2 August, 2024; originally announced August 2024. arXiv:2407.21783 [pdf, other] The Llama 3 Herd of Models Authors: Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere , et al. (536 additional authors not shown) Abstract: Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical\u2026 \u25bd More Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development. \u25b3 Less Submitted 23 November, 2024; v1 submitted 31 July, 2024; originally announced July 2024. arXiv:2406.16782 [pdf, other] Confidence Aware Inverse Constrained Reinforcement Learning Authors: Sriram Ganapathi Subramanian, Guiliang Liu, Mohammed Elmahgiubi, Kasra Rezaee, Pascal Poupart Abstract: In coming up with solutions to real-world problems, humans implicitly adhere to constraints that are too numerous and complex to be specified completely. However, reinforcement learning (RL) agents need these constraints to learn the correct optimal policy in these settings. The field of Inverse Constraint Reinforcement Learning (ICRL) deals with this problem and provides algorithms that aim to es\u2026 \u25bd More In coming up with solutions to real-world problems, humans implicitly adhere to constraints that are too numerous and complex to be specified completely. However, reinforcement learning (RL) agents need these constraints to learn the correct optimal policy in these settings. The field of Inverse Constraint Reinforcement Learning (ICRL) deals with this problem and provides algorithms that aim to estimate the constraints from expert demonstrations collected offline. Practitioners prefer to know a measure of confidence in the estimated constraints, before deciding to use these constraints, which allows them to only use the constraints that satisfy a desired level of confidence. However, prior works do not allow users to provide the desired level of confidence for the inferred constraints. This work provides a principled ICRL method that can take a confidence level with a set of expert demonstrations and outputs a constraint that is at least as constraining as the true underlying constraint with the desired level of confidence. Further, unlike previous methods, this method allows a user to know if the number of expert trajectories is insufficient to learn a constraint with a desired level of confidence, and therefore collect more expert trajectories as required to simultaneously learn constraints with the desired level of confidence and a policy that achieves the desired level of performance. \u25b3 Less Submitted 24 June, 2024; originally announced June 2024. Comments: Paper to appear in ICML 2024 arXiv:2406.11704 [pdf, other] Nemotron-4 340B Technical Report Authors: Nvidia, :, Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H. Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, Sirshak Das, Ayush Dattagupta, Olivier Delalleau, Leon Derczynski, Yi Dong, Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman, Shaona Ghosh, Boris Ginsburg, Igor Gitman, Tomasz Grzegorzek , et al. (58 additional authors not shown) Abstract: We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base, Nemotron-4-340B-Instruct, and Nemotron-4-340B-Reward. Our models are open access under the NVIDIA Open Model License Agreement, a permissive model license that allows distribution, modification, and use of the models and its outputs. These models perform competitively to open access models on a wide range of evaluation be\u2026 \u25bd More We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base, Nemotron-4-340B-Instruct, and Nemotron-4-340B-Reward. Our models are open access under the NVIDIA Open Model License Agreement, a permissive model license that allows distribution, modification, and use of the models and its outputs. These models perform competitively to open access models on a wide range of evaluation benchmarks, and were sized to fit on a single DGX H100 with 8 GPUs when deployed in FP8 precision. We believe that the community can benefit from these models in various research studies and commercial applications, especially for generating synthetic data to train smaller language models. Notably, over 98% of data used in our model alignment process is synthetically generated, showcasing the effectiveness of these models in generating synthetic data. To further support open research and facilitate model development, we are also open-sourcing the synthetic data generation pipeline used in our model alignment process. \u25b3 Less Submitted 6 August, 2024; v1 submitted 17 June, 2024; originally announced June 2024. arXiv:2406.10276 [pdf, other] Soft Language Identification for Language-Agnostic Many-to-One End-to-End Speech Translation Authors: Peidong Wang, Jian Xue, Jinyu Li, Junkun Chen, Aswin Shanmugam Subramanian Abstract: Language-agnostic many-to-one end-to-end speech translation models can convert audio signals from different source languages into text in a target language. These models do not need source language identification, which improves user experience. In some cases, the input language can be given or estimated. Our goal is to use this additional language information while preserving the quality of the o\u2026 \u25bd More Language-agnostic many-to-one end-to-end speech translation models can convert audio signals from different source languages into text in a target language. These models do not need source language identification, which improves user experience. In some cases, the input language can be given or estimated. Our goal is to use this additional language information while preserving the quality of the other languages. We accomplish this by introducing a simple and effective linear input network. The linear input network is initialized as an identity matrix, which ensures that the model can perform as well as, or better than, the original model. Experimental results show that the proposed method can successfully enhance the specified language, while keeping the language-agnostic ability of the many-to-one ST models. \u25b3 Less Submitted 11 June, 2024; originally announced June 2024. arXiv:2406.06459 [pdf, other] How Useful is Intermittent, Asynchronous Expert Feedback for Bayesian Optimization? Authors: Agustinus Kristiadi, Felix Strieth-Kalthoff, Sriram Ganapathi Subramanian, Vincent Fortuin, Pascal Poupart, Geoff Pleiss Abstract: Bayesian optimization (BO) is an integral part of automated scientific discovery -- the so-called self-driving lab -- where human inputs are ideally minimal or at least non-blocking. However, scientists often have strong intuition, and thus human feedback is still useful. Nevertheless, prior works in enhancing BO with expert feedback, such as by incorporating it in an offline or online but blockin\u2026 \u25bd More Bayesian optimization (BO) is an integral part of automated scientific discovery -- the so-called self-driving lab -- where human inputs are ideally minimal or at least non-blocking. However, scientists often have strong intuition, and thus human feedback is still useful. Nevertheless, prior works in enhancing BO with expert feedback, such as by incorporating it in an offline or online but blocking (arrives at each BO iteration) manner, are incompatible with the spirit of self-driving labs. In this work, we study whether a small amount of randomly arriving expert feedback that is being incorporated in a non-blocking manner can improve a BO campaign. To this end, we run an additional, independent computing thread on top of the BO loop to handle the feedback-gathering process. The gathered feedback is used to learn a Bayesian preference model that can readily be incorporated into the BO thread, to steer its exploration-exploitation process. Experiments on toy and chemistry datasets suggest that even just a few intermittent, asynchronous expert feedback can be useful for improving or constraining BO. This can especially be useful for its implication in improving self-driving labs, e.g. making them more data-efficient and less costly. \u25b3 Less Submitted 10 June, 2024; originally announced June 2024. Comments: AABI 2024. Code: https://github.com/wiseodd/bo-async-feedback arXiv:2405.20935 [pdf, other] Effective Interplay between Sparsity and Quantization: From Theory to Practice Authors: Simla Burcu Harma, Ayan Chakraborty, Elizaveta Kostenok, Danila Mishin, Dongho Ha, Babak Falsafi, Martin Jaggi, Ming Liu, Yunho Oh, Suvinay Subramanian, Amir Yazdanbakhsh Abstract: The increasing size of deep neural networks (DNNs) necessitates effective model compression to reduce their computational and memory footprints. Sparsity and quantization are two prominent compression methods that have been shown to reduce DNNs' computational and memory footprints significantly while preserving model accuracy. However, how these two methods interact when combined together remains\u2026 \u25bd More The increasing size of deep neural networks (DNNs) necessitates effective model compression to reduce their computational and memory footprints. Sparsity and quantization are two prominent compression methods that have been shown to reduce DNNs' computational and memory footprints significantly while preserving model accuracy. However, how these two methods interact when combined together remains a key question for developers, as many tacitly assume that they are orthogonal, meaning that their combined use does not introduce additional errors beyond those introduced by each method independently. In this paper, we provide the first mathematical proof that sparsity and quantization are non-orthogonal. We corroborate these results with experiments spanning a range of large language models, including the OPT and LLaMA model families (with 125M to 8B parameters), and vision models like ViT and ResNet. We show that the order in which we apply these methods matters because applying quantization before sparsity may disrupt the relative importance of tensor elements, which may inadvertently remove significant elements from a tensor. More importantly, we show that even if applied in the correct order, the compounded errors from sparsity and quantization can significantly harm accuracy. Our findings extend to the efficient deployment of large models in resource-constrained compute platforms to reduce serving cost, offering insights into best practices for applying these compression methods to maximize hardware resource efficiency without compromising accuracy. \u25b3 Less Submitted 28 January, 2025; v1 submitted 31 May, 2024; originally announced May 2024. arXiv:2405.16129 [pdf, other] iREL at SemEval-2024 Task 9: Improving Conventional Prompting Methods for Brain Teasers Authors: Harshit Gupta, Manav Chaudhary, Tathagata Raha, Shivansh Subramanian, Vasudeva Varma Abstract: This paper describes our approach for SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense. The BRAINTEASER task comprises multiple-choice Question Answering designed to evaluate the models' lateral thinking capabilities. It consists of Sentence Puzzle and Word Puzzle subtasks that require models to defy default common-sense associations and exhibit unconventional thinking. We propo\u2026 \u25bd More This paper describes our approach for SemEval-2024 Task 9: BRAINTEASER: A Novel Task Defying Common Sense. The BRAINTEASER task comprises multiple-choice Question Answering designed to evaluate the models' lateral thinking capabilities. It consists of Sentence Puzzle and Word Puzzle subtasks that require models to defy default common-sense associations and exhibit unconventional thinking. We propose a unique strategy to improve the performance of pre-trained language models, notably the Gemini 1.0 Pro Model, in both subtasks. We employ static and dynamic few-shot prompting techniques and introduce a model-generated reasoning strategy that utilizes the LLM's reasoning capabilities to improve performance. Our approach demonstrated significant improvements, showing that it performed better than the baseline models by a considerable margin but fell short of performing as well as the human annotators, thus highlighting the efficacy of the proposed strategies. \u25b3 Less Submitted 25 May, 2024; originally announced May 2024. arXiv:2405.03689 [pdf, other] Pose Priors from Language Models Authors: Sanjay Subramanian, Evonne Ng, Lea M\u00fcller, Dan Klein, Shiry Ginosar, Trevor Darrell Abstract: We present a zero-shot pose optimization method that enforces accurate physical contact constraints when estimating the 3D pose of humans. Our central insight is that since language is often used to describe physical interaction, large pretrained text-based models can act as priors on pose estimation. We can thus leverage this insight to improve pose estimation by converting natural language des\u2026 \u25bd More We present a zero-shot pose optimization method that enforces accurate physical contact constraints when estimating the 3D pose of humans. Our central insight is that since language is often used to describe physical interaction, large pretrained text-based models can act as priors on pose estimation. We can thus leverage this insight to improve pose estimation by converting natural language descriptors, generated by a large multimodal model (LMM), into tractable losses to constrain the 3D pose optimization. Despite its simplicity, our method produces surprisingly compelling pose reconstructions of people in close contact, correctly capturing the semantics of the social and physical interactions. We demonstrate that our method rivals more complex state-of-the-art approaches that require expensive human annotation of contact points and training specialized models. Moreover, unlike previous approaches, our method provides a unified framework for resolving self-contact and person-to-person contact. \u25b3 Less Submitted 6 May, 2024; originally announced May 2024. arXiv:2404.19630 [pdf, other] Analyzing and Exploring Training Recipes for Large-Scale Transformer-Based Weather Prediction Authors: Jared D. Willard, Peter Harrington, Shashank Subramanian, Ankur Mahesh, Travis A. O'Brien, William D. Collins Abstract: The rapid rise of deep learning (DL) in numerical weather prediction (NWP) has led to a proliferation of models which forecast atmospheric variables with comparable or superior skill than traditional physics-based NWP. However, among these leading DL models, there is a wide variance in both the training settings and architecture used. Further, the lack of thorough ablation studies makes it hard to\u2026 \u25bd More The rapid rise of deep learning (DL) in numerical weather prediction (NWP) has led to a proliferation of models which forecast atmospheric variables with comparable or superior skill than traditional physics-based NWP. However, among these leading DL models, there is a wide variance in both the training settings and architecture used. Further, the lack of thorough ablation studies makes it hard to discern which components are most critical to success. In this work, we show that it is possible to attain high forecast skill even with relatively off-the-shelf architectures, simple training procedures, and moderate compute budgets. Specifically, we train a minimally modified SwinV2 transformer on ERA5 data, and find that it attains superior forecast skill when compared against IFS. We present some ablations on key aspects of the training pipeline, exploring different loss functions, model sizes and depths, and multi-step fine-tuning to investigate their effect. We also examine the model performance with metrics beyond the typical ACC and RMSE, and investigate how the performance scales with model size. \u25b3 Less Submitted 30 April, 2024; originally announced April 2024. Comments: 9 pages, 6 figures MSC Class: 68T07; 86A10 ACM Class: J.2; I.2.6 Journal ref: 23rd Conference on Artificial Intelligence for Environmental Science. Jan 2024. Abstract #437874 arXiv:2404.01476 [pdf, other] TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering Authors: Chuyi Shang, Amos You, Sanjay Subramanian, Trevor Darrell, Roei Herzig Abstract: Recently, image-based Large Multimodal Models (LMMs) have made significant progress in video question-answering (VideoQA) using a frame-wise approach by leveraging large-scale pretraining in a zero-shot manner. Nevertheless, these models need to be capable of finding relevant information, extracting it, and answering the question simultaneously. Currently, existing methods perform all of these ste\u2026 \u25bd More Recently, image-based Large Multimodal Models (LMMs) have made significant progress in video question-answering (VideoQA) using a frame-wise approach by leveraging large-scale pretraining in a zero-shot manner. Nevertheless, these models need to be capable of finding relevant information, extracting it, and answering the question simultaneously. Currently, existing methods perform all of these steps in a single pass without being able to adapt if insufficient or incorrect information is collected. To overcome this, we introduce a modular multi-LMM agent framework based on several agents with different roles, instructed by a Planner agent that updates its instructions using shared feedback from the other agents. Specifically, we propose TraveLER, a method that can create a plan to \"Traverse\" through the video, ask questions about individual frames to \"Locate\" and store key information, and then \"Evaluate\" if there is enough information to answer the question. Finally, if there is not enough information, our method is able to \"Replan\" based on its collected knowledge. Through extensive experiments, we find that the proposed TraveLER approach improves performance on several VideoQA benchmarks without the need to fine-tune on specific datasets. Our code is available at https://github.com/traveler-framework/TraveLER. \u25b3 Less Submitted 19 October, 2024; v1 submitted 1 April, 2024; originally announced April 2024. Comments: EMNLP 2024 (Main) arXiv:2402.16819 [pdf, other] Nemotron-4 15B Technical Report Authors: Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, Vibhu Jawa, Jiwei Liu, Ameya Mahabaleshwarkar, Osvald Nitski, Annika Brundyn, James Maki, Miguel Martinez, Jiaxuan You, John Kamalu, Patrick LeGresley, Denys Fridman, Jared Casper, Ashwath Aithal, Oleksii Kuchaiev, Mohammad Shoeybi , et al. (2 additional authors not shown) Abstract: We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remai\u2026 \u25bd More We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks. \u25b3 Less Submitted 27 February, 2024; v1 submitted 26 February, 2024; originally announced February 2024. arXiv:2402.15734 [pdf, other] Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning Authors: Wuyang Chen, Jialin Song, Pu Ren, Shashank Subramanian, Dmitriy Morozov, Michael W. Mahoney Abstract: Recent years have witnessed the promise of coupling machine learning methods and physical domain-specific insights for solving scientific problems based on partial differential equations (PDEs). However, being data-intensive, these methods still require a large amount of PDE data. This reintroduces the need for expensive numerical PDE solutions, partially undermining the original goal of avoiding\u2026 \u25bd More Recent years have witnessed the promise of coupling machine learning methods and physical domain-specific insights for solving scientific problems based on partial differential equations (PDEs). However, being data-intensive, these methods still require a large amount of PDE data. This reintroduces the need for expensive numerical PDE solutions, partially undermining the original goal of avoiding these expensive simulations. In this work, seeking data efficiency, we design unsupervised pretraining for PDE operator learning. To reduce the need for training data with heavy simulation costs, we mine unlabeled PDE data without simulated solutions, and we pretrain neural operators with physics-inspired reconstruction-based proxy tasks. To improve out-of-distribution performance, we further assist neural operators in flexibly leveraging a similarity-based method that learns in-context examples, without incurring extra training costs or designs. Extensive empirical evaluations on a diverse set of PDEs demonstrate that our method is highly data-efficient, more generalizable, and even outperforms conventional vision-pretrained models. We provide our code at https://github.com/delta-lab-ai/data_efficient_nopt. \u25b3 Less Submitted 27 October, 2024; v1 submitted 24 February, 2024; originally announced February 2024. arXiv:2402.04744 [pdf, other] Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers Authors: Abhimanyu Rajeshkumar Bambhaniya, Amir Yazdanbakhsh, Suvinay Subramanian, Sheng-Chun Kao, Shivani Agrawal, Utku Evci, Tushar Krishna Abstract: N:M Structured sparsity has garnered significant interest as a result of relatively modest overhead and improved efficiency. Additionally, this form of sparsity holds considerable appeal for reducing the memory footprint owing to their modest representation overhead. There have been efforts to develop training recipes for N:M structured sparsity, they primarily focus on low-sparsity regions (\u2026 \u25bd More N:M Structured sparsity has garnered significant interest as a result of relatively modest overhead and improved efficiency. Additionally, this form of sparsity holds considerable appeal for reducing the memory footprint owing to their modest representation overhead. There have been efforts to develop training recipes for N:M structured sparsity, they primarily focus on low-sparsity regions ($\\sim$50\\%). Nonetheless, performance of models trained using these approaches tends to decline when confronted with high-sparsity regions ($>$80\\%). In this work, we study the effectiveness of existing sparse training recipes at \\textit{high-sparsity regions} and argue that these methods fail to sustain the model quality on par with low-sparsity regions. We demonstrate that the significant factor contributing to this disparity is the presence of elevated levels of induced noise in the gradient magnitudes. To mitigate this undesirable effect, we employ decay mechanisms to progressively restrict the flow of gradients towards pruned elements. Our approach improves the model quality by up to 2$\\%$ and 5$\\%$ in vision and language models at high sparsity regime, respectively. We also evaluate the trade-off between model accuracy and training compute cost in terms of FLOPs. At iso-training FLOPs, our method yields better performance compared to conventional sparse training recipes, exhibiting an accuracy improvement of up to 2$\\%$. The source code is available at https://github.com/abhibambhaniya/progressive_gradient_flow_nm_sparsity. \u25b3 Less Submitted 7 February, 2024; originally announced February 2024. Comments: 18 pages, 8 figures, 17 tables. Code is available at https://github.com/abhibambhaniya/progressive_gradient_flow_nm_sparsity arXiv:2402.02006 [pdf, other] PresAIse, A Prescriptive AI Solution for Enterprises Authors: Wei Sun, Scott McFaddin, Linh Ha Tran, Shivaram Subramanian, Kristjan Greenewald, Yeshi Tenzin, Zack Xue, Youssef Drissi, Markus Ettl Abstract: Prescriptive AI represents a transformative shift in decision-making, offering causal insights and actionable recommendations. Despite its huge potential, enterprise adoption often faces several challenges. The first challenge is caused by the limitations of observational data for accurate causal inference which is typically a prerequisite for good decision-making. The second pertains to the inter\u2026 \u25bd More Prescriptive AI represents a transformative shift in decision-making, offering causal insights and actionable recommendations. Despite its huge potential, enterprise adoption often faces several challenges. The first challenge is caused by the limitations of observational data for accurate causal inference which is typically a prerequisite for good decision-making. The second pertains to the interpretability of recommendations, which is crucial for enterprise decision-making settings. The third challenge is the silos between data scientists and business users, hindering effective collaboration. This paper outlines an initiative from IBM Research, aiming to address some of these challenges by offering a suite of prescriptive AI solutions. Leveraging insights from various research papers, the solution suite includes scalable causal inference methods, interpretable decision-making approaches, and the integration of large language models (LLMs) to bridge communication gaps via a conversation agent. A proof-of-concept, PresAIse, demonstrates the solutions' potential by enabling non-ML experts to interact with prescriptive AI models via a natural language interface, democratizing advanced analytics for strategic decision-making. \u25b3 Less Submitted 12 February, 2024; v1 submitted 2 February, 2024; originally announced February 2024. Comments: 14 pages arXiv:2401.04088 [pdf, other] Mixtral of Experts Authors: Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix , et al. (1 additional authors not shown) Abstract: We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected e\u2026 \u25bd More We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license. \u25b3 Less Submitted 8 January, 2024; originally announced January 2024. Comments: See more details at https://mistral.ai/news/mixtral-of-experts/ arXiv:2312.02249 [pdf, other] Recursive Visual Programming Authors: Jiaxin Ge, Sanjay Subramanian, Baifeng Shi, Roei Herzig, Trevor Darrell Abstract: Visual Programming (VP) has emerged as a powerful framework for Visual Question Answering (VQA). By generating and executing bespoke code for each question, these methods demonstrate impressive compositional and reasoning capabilities, especially in few-shot and zero-shot scenarios. However, existing VP methods generate all code in a single function, resulting in code that is suboptimal in terms o\u2026 \u25bd More Visual Programming (VP) has emerged as a powerful framework for Visual Question Answering (VQA). By generating and executing bespoke code for each question, these methods demonstrate impressive compositional and reasoning capabilities, especially in few-shot and zero-shot scenarios. However, existing VP methods generate all code in a single function, resulting in code that is suboptimal in terms of both accuracy and interpretability. Inspired by human coding practices, we propose Recursive Visual Programming (RVP), which simplifies generated routines, provides more efficient problem solving, and can manage more complex data structures. RVP is inspired by human coding practices and approaches VQA tasks with an iterative recursive code generation approach, allowing decomposition of complicated problems into smaller parts. Notably, RVP is capable of dynamic type assignment, i.e., as the system recursively generates a new piece of code, it autonomously determines the appropriate return type and crafts the requisite code to generate that output. We show RVP's efficacy through extensive experiments on benchmarks including VSR, COVR, GQA, and NextQA, underscoring the value of adopting human-like recursive and modular programming techniques for solving VQA tasks through coding. \u25b3 Less Submitted 10 July, 2024; v1 submitted 4 December, 2023; originally announced December 2023. arXiv:2311.12391 [pdf, other] From Wrong To Right: A Recursive Approach Towards Vision-Language Explanation Authors: Jiaxin Ge, Sanjay Subramanian, Trevor Darrell, Boyi Li Abstract: Addressing the challenge of adapting pre-trained vision-language models for generating insightful explanations for visual reasoning tasks with limited annotations, we present ReVisE: a $\\textbf{Re}$cursive $\\textbf{Vis}$ual $\\textbf{E}$xplanation algorithm. Our method iteratively computes visual features (conditioned on the text input), an answer, and an explanation, to improve the explanation qua\u2026 \u25bd More Addressing the challenge of adapting pre-trained vision-language models for generating insightful explanations for visual reasoning tasks with limited annotations, we present ReVisE: a $\\textbf{Re}$cursive $\\textbf{Vis}$ual $\\textbf{E}$xplanation algorithm. Our method iteratively computes visual features (conditioned on the text input), an answer, and an explanation, to improve the explanation quality step by step until the answer converges. We find that this multi-step approach guides the model to correct its own answers and outperforms single-step explanation generation. Furthermore, explanations generated by ReVisE also serve as valuable annotations for few-shot self-training. Our approach outperforms previous methods while utilizing merely 5% of the human-annotated explanations across 10 metrics, demonstrating up to a 4.2 and 1.3 increase in BLEU-1 score on the VCR and VQA-X datasets, underscoring the efficacy and data-efficiency of our method. \u25b3 Less Submitted 21 November, 2023; originally announced November 2023. Comments: EMNLP 2023 Main arXiv:2311.05529 [pdf, other] Information-theoretic generalization bounds for learning from quantum data Authors: Matthias Caro, Tom Gur, Cambyse Rouz\u00e9, Daniel Stilck Fran\u00e7a, Sathyawageeswar Subramanian Abstract: Learning tasks play an increasingly prominent role in quantum information and computation. They range from fundamental problems such as state discrimination and metrology over the framework of quantum probably approximately correct (PAC) learning, to the recently proposed shadow variants of state tomography. However, the many directions of quantum learning theory have so far evolved separately. We\u2026 \u25bd More Learning tasks play an increasingly prominent role in quantum information and computation. They range from fundamental problems such as state discrimination and metrology over the framework of quantum probably approximately correct (PAC) learning, to the recently proposed shadow variants of state tomography. However, the many directions of quantum learning theory have so far evolved separately. We propose a general mathematical formalism for describing quantum learning by training on classical-quantum data and then testing how well the learned hypothesis generalizes to new data. In this framework, we prove bounds on the expected generalization error of a quantum learner in terms of classical and quantum information-theoretic quantities measuring how strongly the learner's hypothesis depends on the specific data seen during training. To achieve this, we use tools from quantum optimal transport and quantum concentration inequalities to establish non-commutative versions of decoupling lemmas that underlie recent information-theoretic generalization bounds for classical machine learning. Our framework encompasses and gives intuitively accessible generalization bounds for a variety of quantum learning scenarios such as quantum state discrimination, PAC learning quantum states, quantum parameter estimation, and quantumly PAC learning classical functions. Thereby, our work lays a foundation for a unifying quantum information-theoretic perspective on quantum learning. \u25b3 Less Submitted 18 June, 2024; v1 submitted 9 November, 2023; originally announced November 2023. Comments: 48+14 pages, 4 figures arXiv:2310.12183 [pdf, other] An Optimistic-Robust Approach for Dynamic Positioning of Omnichannel Inventories Authors: Pavithra Harsha, Shivaram Subramanian, Ali Koc, Mahesh Ramakrishna, Brian Quanz, Dhruv Shah, Chandra Narayanaswami Abstract: We introduce a new class of data-driven and distribution-free optimistic-robust bimodal inventory optimization (BIO) strategy to effectively allocate inventory across a retail chain to meet time-varying, uncertain omnichannel demand. While prior Robust optimization (RO) methods emphasize the downside, i.e., worst-case adversarial demand, BIO also considers the upside to remain resilient like RO wh\u2026 \u25bd More We introduce a new class of data-driven and distribution-free optimistic-robust bimodal inventory optimization (BIO) strategy to effectively allocate inventory across a retail chain to meet time-varying, uncertain omnichannel demand. While prior Robust optimization (RO) methods emphasize the downside, i.e., worst-case adversarial demand, BIO also considers the upside to remain resilient like RO while also reaping the rewards of improved average-case performance by overcoming the presence of endogenous outliers. This bimodal strategy is particularly valuable for balancing the tradeoff between lost sales at the store and the costs of cross-channel e-commerce fulfillment, which is at the core of our inventory optimization model. These factors are asymmetric due to the heterogenous behavior of the channels, with a bias towards the former in terms of lost-sales cost and a dependence on network effects for the latter. We provide structural insights about the BIO solution and how it can be tuned to achieve a preferred tradeoff between robustness and the average-case. Our experiments show that significant benefits can be achieved by rethinking traditional approaches to inventory management, which are siloed by channel and location. Using a real-world dataset from a large American omnichannel retail chain, a business value assessment during a peak period indicates over a 15% profitability gain for BIO over RO and other baselines while also preserving the (practical) worst case performance. \u25b3 Less Submitted 17 October, 2023; originally announced October 2023. arXiv:2310.10806 [pdf, other] Convolutional Neural Network Model for Diabetic Retinopathy Feature Extraction and Classification Authors: Sharan Subramanian, Leilani H. Gilpin Abstract: The application of Artificial Intelligence in the medical market brings up increasing concerns but aids in more timely diagnosis of silent progressing diseases like Diabetic Retinopathy. In order to diagnose Diabetic Retinopathy (DR), ophthalmologists use color fundus images, or pictures of the back of the retina, to identify small distinct features through a difficult and time-consuming process.\u2026 \u25bd More The application of Artificial Intelligence in the medical market brings up increasing concerns but aids in more timely diagnosis of silent progressing diseases like Diabetic Retinopathy. In order to diagnose Diabetic Retinopathy (DR), ophthalmologists use color fundus images, or pictures of the back of the retina, to identify small distinct features through a difficult and time-consuming process. Our work creates a novel CNN model and identifies the severity of DR through fundus image input. We classified 4 known DR features, including micro-aneurysms, cotton wools, exudates, and hemorrhages, through convolutional layers and were able to provide an accurate diagnostic without additional user input. The proposed model is more interpretable and robust to overfitting. We present initial results with a sensitivity of 97% and an accuracy of 71%. Our contribution is an interpretable model with similar accuracy to more complex models. With that, our model advances the field of DR detection and proves to be a key step towards AI-focused medical diagnosis. \u25b3 Less Submitted 16 October, 2023; originally announced October 2023. Comments: 9 pages, 2 tables, 5 figures arXiv:2310.03025 [pdf, other] Retrieval meets Long Context Large Language Models Authors: Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, Bryan Catanzaro Abstract: Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by stu\u2026 \u25bd More Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners. \u25b3 Less Submitted 23 January, 2024; v1 submitted 4 October, 2023; originally announced October 2023. Comments: Published at ICLR 2024 arXiv:2308.10897 [pdf, other] Can Language Models Learn to Listen? Authors: Evonne Ng, Sanjay Subramanian, Dan Klein, Angjoo Kanazawa, Trevor Darrell, Shiry Ginosar Abstract: We present a framework for generating appropriate facial responses from a listener in dyadic social interactions based on the speaker's words. Given an input transcription of the speaker's words with their timestamps, our approach autoregressively predicts a response of a listener: a sequence of listener facial gestures, quantized using a VQ-VAE. Since gesture is a language component, we propose t\u2026 \u25bd More We present a framework for generating appropriate facial responses from a listener in dyadic social interactions based on the speaker's words. Given an input transcription of the speaker's words with their timestamps, our approach autoregressively predicts a response of a listener: a sequence of listener facial gestures, quantized using a VQ-VAE. Since gesture is a language component, we propose treating the quantized atomic motion elements as additional language token inputs to a transformer-based large language model. Initializing our transformer with the weights of a language model pre-trained only on text results in significantly higher quality listener responses than training a transformer from scratch. We show that our generated listener motion is fluent and reflective of language semantics through quantitative metrics and a qualitative user study. In our evaluation, we analyze the model's ability to utilize temporal and semantic aspects of spoken text. Project page: https://people.eecs.berkeley.edu/~evonne_ng/projects/text2listen/ \u25b3 Less Submitted 21 August, 2023; originally announced August 2023. Comments: ICCV 2023; Project page: https://people.eecs.berkeley.edu/~evonne_ng/projects/text2listen/ arXiv:2307.15615 [pdf, other] A survey on deep learning in medical image registration: new technologies, uncertainty, evaluation metrics, and beyond Authors: Junyu Chen, Yihao Liu, Shuwen Wei, Zhangxing Bian, Shalini Subramanian, Aaron Carass, Jerry L. Prince, Yong Du Abstract: Deep learning technologies have dramatically reshaped the field of medical image registration over the past decade. The initial developments, such as regression-based and U-Net-based networks, established the foundation for deep learning in image registration. Subsequent progress has been made in various aspects of deep learning-based registration, including similarity measures, deformation regula\u2026 \u25bd More Deep learning technologies have dramatically reshaped the field of medical image registration over the past decade. The initial developments, such as regression-based and U-Net-based networks, established the foundation for deep learning in image registration. Subsequent progress has been made in various aspects of deep learning-based registration, including similarity measures, deformation regularizations, network architectures, and uncertainty estimation. These advancements have not only enriched the field of image registration but have also facilitated its application in a wide range of tasks, including atlas construction, multi-atlas segmentation, motion estimation, and 2D-3D registration. In this paper, we present a comprehensive overview of the most recent advancements in deep learning-based image registration. We begin with a concise introduction to the core concepts of deep learning-based image registration. Then, we delve into innovative network architectures, loss functions specific to registration, and methods for estimating registration uncertainty. Additionally, this paper explores appropriate evaluation metrics for assessing the performance of deep learning models in registration tasks. Finally, we highlight the practical applications of these novel techniques in medical imaging and discuss the future prospects of deep learning-based image registration. \u25b3 Less Submitted 1 November, 2024; v1 submitted 28 July, 2023; originally announced July 2023. Comments: Accepted to Medical Image Analysis ((c) MedIA). A list of open-sourced code from the papers reviewed has been organized and is available at https://bit.ly/3QgFJ9z arXiv:2306.14070 [pdf, other] SuperBench: A Super-Resolution Benchmark Dataset for Scientific Machine Learning Authors: Pu Ren, N. Benjamin Erichson, Shashank Subramanian, Omer San, Zarija Lukic, Michael W. Mahoney Abstract: Super-Resolution (SR) techniques aim to enhance data resolution, enabling the retrieval of finer details, and improving the overall quality and fidelity of the data representation. There is growing interest in applying SR methods to complex spatiotemporal systems within the Scientific Machine Learning (SciML) community, with the hope of accelerating numerical simulations and/or improving forecasts\u2026 \u25bd More Super-Resolution (SR) techniques aim to enhance data resolution, enabling the retrieval of finer details, and improving the overall quality and fidelity of the data representation. There is growing interest in applying SR methods to complex spatiotemporal systems within the Scientific Machine Learning (SciML) community, with the hope of accelerating numerical simulations and/or improving forecasts in weather, climate, and related areas. However, the lack of standardized benchmark datasets for comparing and validating SR methods hinders progress and adoption in SciML. To address this, we introduce SuperBench, the first benchmark dataset featuring high-resolution datasets (up to $2048\\times2048$ dimensions), including data from fluid flows, cosmology, and weather. Here, we focus on validating spatial SR performance from data-centric and physics-preserved perspectives, as well as assessing robustness to data degradation tasks. While deep learning-based SR methods (developed in the computer vision community) excel on certain tasks, despite relatively limited prior physics information, we identify limitations of these methods in accurately capturing intricate fine-scale features and preserving fundamental physical properties and constraints in scientific data. These shortcomings highlight the importance and subtlety of incorporating domain knowledge into ML models. We anticipate that SuperBench will significantly advance SR methods for scientific tasks. \u25b3 Less Submitted 24 June, 2023; originally announced June 2023. arXiv:2306.10619 [pdf, other] Towards Stability of Autoregressive Neural Operators Authors: Michael McCabe, Peter Harrington, Shashank Subramanian, Jed Brown Abstract: Neural operators have proven to be a promising approach for modeling spatiotemporal systems in the physical sciences. However, training these models for large systems can be quite challenging as they incur significant computational and memory expense -- these systems are often forced to rely on autoregressive time-stepping of the neural network to predict future temporal states. While this is effe\u2026 \u25bd More Neural operators have proven to be a promising approach for modeling spatiotemporal systems in the physical sciences. However, training these models for large systems can be quite challenging as they incur significant computational and memory expense -- these systems are often forced to rely on autoregressive time-stepping of the neural network to predict future temporal states. While this is effective in managing costs, it can lead to uncontrolled error growth over time and eventual instability. We analyze the sources of this autoregressive error growth using prototypical neural operator models for physical systems and explore ways to mitigate it. We introduce architectural and application-specific improvements that allow for careful control of instability-inducing operations within these models without inflating the compute/memory expense. We present results on several scientific systems that include Navier-Stokes fluid flow, rotating shallow water, and a high-resolution global weather forecasting system. We demonstrate that applying our design principles to neural operators leads to significantly lower errors for long-term forecasts as well as longer time horizons without qualitative signs of divergence compared to the original models for these systems. We open-source our \\href{https://github.com/mikemccabe210/stabilizing_neural_operators}{code} for reproducibility. \u25b3 Less Submitted 10 December, 2023; v1 submitted 18 June, 2023; originally announced June 2023. Journal ref: Transactions on Machine Learning Research. November 2023 arXiv:2306.05392 [pdf, other] Modular Visual Question Answering via Code Generation Authors: Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell, Dan Klein Abstract: We present a framework that formulates visual question answering as modular code generation. In contrast to prior work on modular approaches to VQA, our approach requires no additional training and relies on pre-trained language models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA examples used for in-context learning. The generated Python programs invoke and compose the o\u2026 \u25bd More We present a framework that formulates visual question answering as modular code generation. In contrast to prior work on modular approaches to VQA, our approach requires no additional training and relies on pre-trained language models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA examples used for in-context learning. The generated Python programs invoke and compose the outputs of the visual models using arithmetic and conditional logic. Our approach improves accuracy on the COVR dataset by at least 3% and on the GQA dataset by roughly 2% compared to the few-shot baseline that does not employ code generation. \u25b3 Less Submitted 8 June, 2023; originally announced June 2023. Comments: ACL 2023 arXiv:2306.00258 [pdf, other] Towards Foundation Models for Scientific Machine Learning: Characterizing Scaling and Transfer Behavior Authors: Shashank Subramanian, Peter Harrington, Kurt Keutzer, Wahid Bhimji, Dmitriy Morozov, Michael Mahoney, Amir Gholami Abstract: Pre-trained machine learning (ML) models have shown great performance for a wide range of applications, in particular in natural language processing (NLP) and computer vision (CV). Here, we study how pre-training could be used for scientific machine learning (SciML) applications, specifically in the context of transfer learning. We study the transfer behavior of these models as (i) the pre-trained\u2026 \u25bd More Pre-trained machine learning (ML) models have shown great performance for a wide range of applications, in particular in natural language processing (NLP) and computer vision (CV). Here, we study how pre-training could be used for scientific machine learning (SciML) applications, specifically in the context of transfer learning. We study the transfer behavior of these models as (i) the pre-trained model size is scaled, (ii) the downstream training dataset size is scaled, (iii) the physics parameters are systematically pushed out of distribution, and (iv) how a single model pre-trained on a mixture of different physics problems can be adapted to various downstream applications. We find that-when fine-tuned appropriately-transfer learning can help reach desired accuracy levels with orders of magnitude fewer downstream examples (across different tasks that can even be out-of-distribution) than training from scratch, with consistent behavior across a wide range of downstream examples. We also find that fine-tuning these models yields more performance gains as model size increases, compared to training from scratch on new downstream tasks. These results hold for a broad range of PDE learning tasks. All in all, our results demonstrate the potential of the \"pre-train and fine-tune\" paradigm for SciML problems, demonstrating a path towards building SciML foundation models. We open-source our code for reproducibility. \u25b3 Less Submitted 31 May, 2023; originally announced June 2023. Comments: 16 pages, 11 figures Journal ref: NeurIPS 2023 arXiv:2305.14177 [pdf, other] ChemGymRL: An Interactive Framework for Reinforcement Learning for Digital Chemistry Authors: Chris Beeler, Sriram Ganapathi Subramanian, Kyle Sprague, Nouha Chatti, Colin Bellinger, Mitchell Shahen, Nicholas Paquin, Mark Baula, Amanuel Dawit, Zihan Yang, Xinkai Li, Mark Crowley, Isaac Tamblyn Abstract: This paper provides a simulated laboratory for making use of Reinforcement Learning (RL) for chemical discovery. Since RL is fairly data intensive, training agents `on-the-fly' by taking actions in the real world is infeasible and possibly dangerous. Moreover, chemical processing and discovery involves challenges which are not commonly found in RL benchmarks and therefore offer a rich space to wor\u2026 \u25bd More This paper provides a simulated laboratory for making use of Reinforcement Learning (RL) for chemical discovery. Since RL is fairly data intensive, training agents `on-the-fly' by taking actions in the real world is infeasible and possibly dangerous. Moreover, chemical processing and discovery involves challenges which are not commonly found in RL benchmarks and therefore offer a rich space to work in. We introduce a set of highly customizable and open-source RL environments, ChemGymRL, based on the standard Open AI Gym template. ChemGymRL supports a series of interconnected virtual chemical benches where RL agents can operate and train. The paper introduces and details each of these benches using well-known chemical reactions as illustrative examples, and trains a set of standard RL algorithms in each of these benches. Finally, discussion and comparison of the performances of several standard RL methods are provided in addition to a list of directions for future work as a vision for the further development and usage of ChemGymRL. \u25b3 Less Submitted 23 May, 2023; originally announced May 2023. Comments: 19 pages, 13 figures, 2 tables arXiv:2304.14082 [pdf, other] JaxPruner: A concise library for sparsity research Authors: Joo Hyung Lee, Wonpyo Park, Nicole Mitchell, Jonathan Pilault, Johan Obando-Ceron, Han-Byul Kim, Namhoon Lee, Elias Frantar, Yun Long, Amir Yazdanbakhsh, Shivani Agrawal, Suvinay Subramanian, Xin Wang, Sheng-Chun Kao, Xingyao Zhang, Trevor Gale, Aart Bik, Woohyun Han, Milen Ferev, Zhonglin Han, Hong-Seok Kim, Yann Dauphin, Gintare Karolina Dziugaite, Pablo Samuel Castro, Utku Evci Abstract: This paper introduces JaxPruner, an open-source JAX-based pruning and sparse training library for machine learning research. JaxPruner aims to accelerate research on sparse neural networks by providing concise implementations of popular pruning and sparse training algorithms with minimal memory and latency overhead. Algorithms implemented in JaxPruner use a common API and work seamlessly with the\u2026 \u25bd More This paper introduces JaxPruner, an open-source JAX-based pruning and sparse training library for machine learning research. JaxPruner aims to accelerate research on sparse neural networks by providing concise implementations of popular pruning and sparse training algorithms with minimal memory and latency overhead. Algorithms implemented in JaxPruner use a common API and work seamlessly with the popular optimization library Optax, which, in turn, enables easy integration with existing JAX based libraries. We demonstrate this ease of integration by providing examples in four different codebases: Scenic, t5x, Dopamine and FedJAX and provide baseline experiments on popular benchmarks. \u25b3 Less Submitted 18 December, 2023; v1 submitted 27 April, 2023; originally announced April 2023. Comments: Jaxpruner is hosted at http://github.com/google-research/jaxpruner arXiv:2304.01433 [pdf] TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings Authors: Norman P. Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing, Brian Towles, Cliff Young, Xiang Zhou, Zongwei Zhou, David Patterson Abstract: In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and perfo\u2026 \u25bd More In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are <5% of system cost and <3% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x-7x yet use only 5% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus ~10x faster overall, which along with OCS flexibility helps large language models. For similar sized systems, it is ~4.3x-4.5x faster than the Graphcore IPU Bow and is 1.2x-1.7x faster and uses 1.3x-1.9x less power than the Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computers of Google Cloud use ~3x less energy and produce ~20x less CO2e than contemporary DSAs in a typical on-premise data center. \u25b3 Less Submitted 20 April, 2023; v1 submitted 3 April, 2023; originally announced April 2023. Comments: 15 pages; 16 figures; to be published at ISCA 2023 (the International Symposium on Computer Architecture) arXiv:2303.17951 [pdf, other] FP8 versus INT8 for efficient deep learning inference Authors: Mart van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren, Eric Mahurin, Chirag Patel, Sundar Subramanian, Sanghyuk Lee, Markus Nagel, Joseph Soriaga, Tijmen Blankevoort Abstract: Recently, the idea of using FP8 as a number format for neural network training has been floating around the deep learning world. Given that most training is currently conducted with entire networks in FP32, or sometimes FP16 with mixed-precision, the step to having some parts of a network run in FP8 with 8-bit weights is an appealing potential speed-up for the generally costly and time-intensive t\u2026 \u25bd More Recently, the idea of using FP8 as a number format for neural network training has been floating around the deep learning world. Given that most training is currently conducted with entire networks in FP32, or sometimes FP16 with mixed-precision, the step to having some parts of a network run in FP8 with 8-bit weights is an appealing potential speed-up for the generally costly and time-intensive training procedures in deep learning. A natural question arises regarding what this development means for efficient inference on edge devices. In the efficient inference device world, workloads are frequently executed in INT8. Sometimes going even as low as INT4 when efficiency calls for it. In this whitepaper, we compare the performance for both the FP8 and INT formats for efficient on-device inference. We theoretically show the difference between the INT and FP formats for neural networks and present a plethora of post-training quantization and quantization-aware-training results to show how this theory translates to practice. We also provide a hardware analysis showing that the FP formats are somewhere between 50-180% less efficient in terms of compute in dedicated hardware than the INT format. Based on our research and a read of the research field, we conclude that although the proposed FP8 format could be good for training, the results for inference do not warrant a dedicated implementation of FP8 in favor of INT8 for efficient inference. We show that our results are mostly consistent with previous findings but that important comparisons between the formats have thus far been lacking. Finally, we discuss what happens when FP8-trained networks are converted to INT8 and conclude with a brief discussion on the most efficient way for on-device deployment and an extensive suite of INT8 results for many models. \u25b3 Less Submitted 15 June, 2023; v1 submitted 31 March, 2023; originally announced March 2023."
  },
  {
    "url": "https://arxiv.org/search/cs?searchtype=author&query=Jiang,+W",
    "title": "Showing 1\u201350 of 642 results for author: Jiang, W",
    "author": "Authors:\nTengjin Weng, \n      \n      Jingyi Wang, \n      \n      Wenhao Jiang, \n      \n      Zhong Ming",
    "body": "arXiv:2503.14939 [pdf, other] VisNumBench: Evaluating Number Sense of Multimodal Large Language Models Authors: Tengjin Weng, Jingyi Wang, Wenhao Jiang, Zhong Ming Abstract: Can Multimodal Large Language Models (MLLMs) develop an intuitive number sense similar to humans? Targeting this problem, we introduce Visual Number Benchmark (VisNumBench) to evaluate the number sense abilities of MLLMs across a wide range of visual numerical tasks. VisNumBench consists of about 1,900 multiple-choice question-answer pairs derived from both synthetic and real-world visual data, co\u2026 \u25bd More Can Multimodal Large Language Models (MLLMs) develop an intuitive number sense similar to humans? Targeting this problem, we introduce Visual Number Benchmark (VisNumBench) to evaluate the number sense abilities of MLLMs across a wide range of visual numerical tasks. VisNumBench consists of about 1,900 multiple-choice question-answer pairs derived from both synthetic and real-world visual data, covering seven visual numerical attributes and four types of visual numerical estimation tasks. Our experiments on VisNumBench led to the following key findings: (i) The 17 MLLMs we tested, including open-source models such as Qwen2.5-VL and InternVL2.5, as well as proprietary models like GPT-4o and Gemini 2.0 Flash, perform significantly below human levels in number sense-related tasks. (ii) Multimodal mathematical models and multimodal chain-of-thought (CoT) models did not exhibit significant improvements in number sense abilities. (iii) Stronger MLLMs with larger parameter sizes and broader general abilities demonstrate modest gains in number sense abilities. We believe VisNumBench will serve as a valuable resource for the research community, encouraging further advancements in enhancing MLLMs' number sense abilities. All benchmark resources, including code and datasets, will be publicly available at https://wwwtttjjj.github.io/VisNumBench/. \u25b3 Less Submitted 19 March, 2025; originally announced March 2025. arXiv:2503.14649 [pdf, other] RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving Authors: Wenqi Jiang, Suvinay Subramanian, Cat Graves, Gustavo Alonso, Amir Yazdanbakhsh, Vidushi Dadu Abstract: Retrieval-augmented generation (RAG), which combines large language models (LLMs) with retrievals from external knowledge databases, is emerging as a popular approach for reliable LLM serving. However, efficient RAG serving remains an open challenge due to the rapid emergence of many RAG variants and the substantial differences in workload characteristics across them. In this paper, we make three\u2026 \u25bd More Retrieval-augmented generation (RAG), which combines large language models (LLMs) with retrievals from external knowledge databases, is emerging as a popular approach for reliable LLM serving. However, efficient RAG serving remains an open challenge due to the rapid emergence of many RAG variants and the substantial differences in workload characteristics across them. In this paper, we make three fundamental contributions to advancing RAG serving. First, we introduce RAGSchema, a structured abstraction that captures the wide range of RAG algorithms, serving as a foundation for performance optimization. Second, we analyze several representative RAG workloads with distinct RAGSchema, revealing significant performance variability across these workloads. Third, to address this variability and meet diverse performance requirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a system optimization framework for efficient RAG serving. Our evaluation shows that RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in time-to-first-token latency compared to RAG systems built on LLM-system extensions. \u25b3 Less Submitted 18 March, 2025; originally announced March 2025. arXiv:2503.12460 [pdf, other] Exploring Contextual Attribute Density in Referring Expression Counting Authors: Zhicheng Wang, Zhiyu Pan, Zhan Peng, Jian Cheng, Liwen Xiao, Wei Jiang, Zhiguo Cao Abstract: Referring expression counting (REC) algorithms are for more flexible and interactive counting ability across varied fine-grained text expressions. However, the requirement for fine-grained attribute understanding poses challenges for prior arts, as they struggle to accurately align attribute information with correct visual patterns. Given the proven importance of ''visual density'', it is presumed\u2026 \u25bd More Referring expression counting (REC) algorithms are for more flexible and interactive counting ability across varied fine-grained text expressions. However, the requirement for fine-grained attribute understanding poses challenges for prior arts, as they struggle to accurately align attribute information with correct visual patterns. Given the proven importance of ''visual density'', it is presumed that the limitations of current REC approaches stem from an under-exploration of ''contextual attribute density'' (CAD). In the scope of REC, we define CAD as the measure of the information intensity of one certain fine-grained attribute in visual regions. To model the CAD, we propose a U-shape CAD estimator in which referring expression and multi-scale visual features from GroundingDINO can interact with each other. With additional density supervision, we can effectively encode CAD, which is subsequently decoded via a novel attention procedure with CAD-refined queries. Integrating all these contributions, our framework significantly outperforms state-of-the-art REC methods, achieves $30\\%$ error reduction in counting metrics and a $10\\%$ improvement in localization accuracy. The surprising results shed light on the significance of contextual attribute density for REC. Code will be at github.com/Xu3XiWang/CAD-GD. \u25b3 Less Submitted 16 March, 2025; originally announced March 2025. Comments: CVPR25 arXiv:2503.11064 [pdf, other] MobiVital: Self-supervised Time-series Quality Estimation for Contactless Respiration Monitoring Using UWB Radar Authors: Ziqi Wang, Derek Hua, Wenjun Jiang, Tianwei Xing, Xun Chen, Mani Srivastava Abstract: Respiration waveforms are increasingly recognized as important biomarkers, offering insights beyond simple respiration rates, such as detecting breathing irregularities for disease diagnosis or monitoring breath patterns to guide rehabilitation training. Previous works in wireless respiration monitoring have primarily focused on estimating respiration rate, where the breath waveforms are often gen\u2026 \u25bd More Respiration waveforms are increasingly recognized as important biomarkers, offering insights beyond simple respiration rates, such as detecting breathing irregularities for disease diagnosis or monitoring breath patterns to guide rehabilitation training. Previous works in wireless respiration monitoring have primarily focused on estimating respiration rate, where the breath waveforms are often generated as a by-product. As a result, issues such as waveform deformation and inversion have largely been overlooked, reducing the signal's utility for applications requiring breathing waveforms. To address this problem, we present a novel approach, MobiVital, that improves the quality of respiration waveforms obtained from ultra-wideband (UWB) radar data. MobiVital combines a self-supervised autoregressive model for breathing waveform extraction with a biology-informed algorithm to detect and correct waveform inversions. To encourage reproducible research efforts for developing wireless vital signal monitoring systems, we also release a 12-person, 24-hour UWB radar vital signal dataset, with time-synchronized ground truth obtained from wearable sensors. Our results show that the respiration waveforms produced by our system exhibit a 7-34% increase in fidelity to the ground truth compared to the baselines and can benefit downstream tasks such as respiration rate estimation. \u25b3 Less Submitted 14 March, 2025; originally announced March 2025. arXiv:2503.11062 [pdf, other] Active Learning from Scene Embeddings for End-to-End Autonomous Driving Authors: Wenhao Jiang, Duo Li, Menghan Hu, Chao Ma, Ke Wang, Zhipeng Zhang Abstract: In the field of autonomous driving, end-to-end deep learning models show great potential by learning driving decisions directly from sensor data. However, training these models requires large amounts of labeled data, which is time-consuming and expensive. Considering that the real-world driving data exhibits a long-tailed distribution where simple scenarios constitute a majority part of the data,\u2026 \u25bd More In the field of autonomous driving, end-to-end deep learning models show great potential by learning driving decisions directly from sensor data. However, training these models requires large amounts of labeled data, which is time-consuming and expensive. Considering that the real-world driving data exhibits a long-tailed distribution where simple scenarios constitute a majority part of the data, we are thus inspired to identify the most challenging scenarios within it. Subsequently, we can efficiently improve the performance of the model by training with the selected data of the highest value. Prior research has focused on the selection of valuable data by empirically designed strategies. However, manually designed methods suffer from being less generalizable to new data distributions. Observing that the BEV (Bird's Eye View) features in end-to-end models contain all the information required to represent the scenario, we propose an active learning framework that relies on these vectorized scene-level features, called SEAD. The framework selects initial data based on driving-environmental information and incremental data based on BEV features. Experiments show that we only need 30\\% of the nuScenes training data to achieve performance close to what can be achieved with the full dataset. The source code will be released. \u25b3 Less Submitted 13 March, 2025; originally announced March 2025. Comments: 9 pages, 5 figures arXiv:2503.09158 [pdf, other] FaVChat: Unlocking Fine-Grained Facial Video Understanding with Multimodal Large Language Models Authors: Fufangchen Zhao, Ming Li, Linrui Xu, Wenhao Jiang, Jian Gao, Danfeng Yan Abstract: Video-based multimodal large language models (VMLLMs) have demonstrated remarkable potential in cross-modal video understanding. However, their abilities in fine-grained face comprehension remain largely underexplored. Given its pivotal role in human-centric intelligence, developing VMLLMs for facial understanding holds a fundamental problem. To address this gap, we propose FaVChat, the first VMLL\u2026 \u25bd More Video-based multimodal large language models (VMLLMs) have demonstrated remarkable potential in cross-modal video understanding. However, their abilities in fine-grained face comprehension remain largely underexplored. Given its pivotal role in human-centric intelligence, developing VMLLMs for facial understanding holds a fundamental problem. To address this gap, we propose FaVChat, the first VMLLM specifically designed for fine-grained facial video understanding. To facilitate its training, we construct a large-scale facial video dataset comprising over 60k videos, with the majority annotated with 83 fine-grained facial attributes. These attributes are incorporated to enrich GPT-4o-generated captions, yielding 60k high-quality video-summary pairs and an additional 170k fine-grained question-answering (QA) pairs. To effectively capture rich facial clues, we propose a hybrid model architecture composed of a general visual encoder, a dedicated facial encoder, and a mixture-of-experts-enhanced adapter for adaptive fusion of multi-source visual features. To mitigate information loss during feature transformation, we extract multi-granularity representations from the facial encoder and integrate them into the subsequent LLM. This design enhances the model's ability to comprehend and respond to questions involving diverse levels of visual details. We employ a progressive training paradigm, transitioning from video summarization to a high-quality subset of video QA, gradually increasing task complexity to enhance the model's fine-grained visual perception. We conduct extensive zero-shot evaluation on a couple of public benchmarks, demonstrating that FaVChat consistently surpasses existing VMLLMs across multiple tasks. \u25b3 Less Submitted 13 March, 2025; v1 submitted 12 March, 2025; originally announced March 2025. arXiv:2503.07482 [pdf, other] Efficient Membership Inference Attacks by Bayesian Neural Network Authors: Zhenlong Liu, Wenyu Jiang, Feng Zhou, Hongxin Wei Abstract: Membership Inference Attacks (MIAs) aim to estimate whether a specific data point was used in the training of a given model. Previous attacks often utilize multiple reference models to approximate the conditional score distribution, leading to significant computational overhead. While recent work leverages quantile regression to estimate conditional thresholds, it fails to capture epistemic uncert\u2026 \u25bd More Membership Inference Attacks (MIAs) aim to estimate whether a specific data point was used in the training of a given model. Previous attacks often utilize multiple reference models to approximate the conditional score distribution, leading to significant computational overhead. While recent work leverages quantile regression to estimate conditional thresholds, it fails to capture epistemic uncertainty, resulting in bias in low-density regions. In this work, we propose a novel approach - Bayesian Membership Inference Attack (BMIA), which performs conditional attack through Bayesian inference. In particular, we transform a trained reference model into Bayesian neural networks by Laplace approximation, enabling the direct estimation of the conditional score distribution by probabilistic model parameters. Our method addresses both epistemic and aleatoric uncertainty with only a reference model, enabling efficient and powerful MIA. Extensive experiments on five datasets demonstrate the effectiveness and efficiency of BMIA. \u25b3 Less Submitted 10 March, 2025; originally announced March 2025. Comments: 8 pages, under review arXiv:2503.06966 [pdf, other] MIGA: Mutual Information-Guided Attack on Denoising Models for Semantic Manipulation Authors: Guanghao Li, Mingzhi Chen, Hao Yu, Shuting Dong, Wenhao Jiang, Ming Tang, Chun Yuan Abstract: Deep learning-based denoising models have been widely employed in vision tasks, functioning as filters to eliminate noise while retaining crucial semantic information. Additionally, they play a vital role in defending against adversarial perturbations that threaten downstream tasks. However, these models can be intrinsically susceptible to adversarial attacks due to their dependence on specific no\u2026 \u25bd More Deep learning-based denoising models have been widely employed in vision tasks, functioning as filters to eliminate noise while retaining crucial semantic information. Additionally, they play a vital role in defending against adversarial perturbations that threaten downstream tasks. However, these models can be intrinsically susceptible to adversarial attacks due to their dependence on specific noise assumptions. Existing attacks on denoising models mainly aim at deteriorating visual clarity while neglecting semantic manipulation, rendering them either easily detectable or limited in effectiveness. In this paper, we propose Mutual Information-Guided Attack (MIGA), the first method designed to directly attack deep denoising models by strategically disrupting their ability to preserve semantic content via adversarial perturbations. By minimizing the mutual information between the original and denoised images, a measure of semantic similarity. MIGA forces the denoiser to produce perceptually clean yet semantically altered outputs. While these images appear visually plausible, they encode systematically distorted semantics, revealing a fundamental vulnerability in denoising models. These distortions persist in denoised outputs and can be quantitatively assessed through downstream task performance. We propose new evaluation metrics and systematically assess MIGA on four denoising models across five datasets, demonstrating its consistent effectiveness in disrupting semantic fidelity. Our findings suggest that denoising models are not always robust and can introduce security risks in real-world applications. \u25b3 Less Submitted 11 March, 2025; v1 submitted 10 March, 2025; originally announced March 2025. arXiv:2503.04550 [pdf, other] Benchmarking Reasoning Robustness in Large Language Models Authors: Tong Yu, Yongcheng Jing, Xikun Zhang, Wentao Jiang, Wenjie Wu, Yingjie Wang, Wenbin Hu, Bo Du, Dacheng Tao Abstract: Despite the recent success of large language models (LLMs) in reasoning such as DeepSeek, we for the first time identify a key dilemma in reasoning robustness and generalization: significant performance degradation on novel or incomplete data, suggesting a reliance on memorized patterns rather than systematic reasoning. Our closer examination reveals four key unique limitations underlying this iss\u2026 \u25bd More Despite the recent success of large language models (LLMs) in reasoning such as DeepSeek, we for the first time identify a key dilemma in reasoning robustness and generalization: significant performance degradation on novel or incomplete data, suggesting a reliance on memorized patterns rather than systematic reasoning. Our closer examination reveals four key unique limitations underlying this issue:(1) Positional bias--models favor earlier queries in multi-query inputs but answering the wrong one in the latter (e.g., GPT-4o's accuracy drops from 75.8 percent to 72.8 percent); (2) Instruction sensitivity--performance declines by 5.0 to 7.5 percent in the Qwen2.5 Series and by 5.0 percent in DeepSeek-V3 with auxiliary guidance; (3) Numerical fragility--value substitution sharply reduces accuracy (e.g., GPT-4o drops from 97.5 percent to 82.5 percent, GPT-o1-mini drops from 97.5 percent to 92.5 percent); and (4) Memory dependence--models resort to guesswork when missing critical data. These findings further highlight the reliance on heuristic recall over rigorous logical inference, demonstrating challenges in reasoning robustness. To comprehensively investigate these robustness challenges, this paper introduces a novel benchmark, termed as Math-RoB, that exploits hallucinations triggered by missing information to expose reasoning gaps. This is achieved by an instruction-based approach to generate diverse datasets that closely resemble training distributions, facilitating a holistic robustness assessment and advancing the development of more robust reasoning frameworks. Bad character(s) in field Abstract. \u25b3 Less Submitted 6 March, 2025; originally announced March 2025. arXiv:2503.03524 [pdf, other] Intrinsic and Extrinsic Factor Disentanglement for Recommendation in Various Context Scenarios Authors: Yixin Su, Wei Jiang, Fangquan Lin, Cheng Yang, Sarah M. Erfani, Junhao Gan, Yunxiang Zhao, Ruixuan Li, Rui Zhang Abstract: In recommender systems, the patterns of user behaviors (e.g., purchase, click) may vary greatly in different contexts (e.g., time and location). This is because user behavior is jointly determined by two types of factors: intrinsic factors, which reflect consistent user preference, and extrinsic factors, which reflect external incentives that may vary in different contexts. Differentiating between\u2026 \u25bd More In recommender systems, the patterns of user behaviors (e.g., purchase, click) may vary greatly in different contexts (e.g., time and location). This is because user behavior is jointly determined by two types of factors: intrinsic factors, which reflect consistent user preference, and extrinsic factors, which reflect external incentives that may vary in different contexts. Differentiating between intrinsic and extrinsic factors helps learn user behaviors better. However, existing studies have only considered differentiating them from a single, pre-defined context (e.g., time or location), ignoring the fact that a user's extrinsic factors may be influenced by the interplay of various contexts at the same time. In this paper, we propose the Intrinsic-Extrinsic Disentangled Recommendation (IEDR) model, a generic framework that differentiates intrinsic from extrinsic factors considering various contexts simultaneously, enabling more accurate differentiation of factors and hence the improvement of recommendation accuracy. IEDR contains a context-invariant contrastive learning component to capture intrinsic factors, and a disentanglement component to extract extrinsic factors under the interplay of various contexts. The two components work together to achieve effective factor learning. Extensive experiments on real-world datasets demonstrate IEDR's effectiveness in learning disentangled factors and significantly improving recommendation accuracy by up to 4% in NDCG. \u25b3 Less Submitted 15 March, 2025; v1 submitted 5 March, 2025; originally announced March 2025. Comments: 32 pages, 13 figures, 11 tables. Published on Transactions of Information Systems arXiv:2503.03276 [pdf, other] TrafficKAN-GCN: Graph Convolutional-based Kolmogorov-Arnold Network for Traffic Flow Optimization Authors: Jiayi Zhang, Yiming Zhang, Yuan Zheng, Yuchen Wang, Jinjiang You, Yuchen Xu, Wenxing Jiang, Soumyabrata Dev Abstract: Urban traffic optimization is critical for improving transportation efficiency and alleviating congestion, particularly in large-scale dynamic networks. Traditional methods, such as Dijkstra's and Floyd's algorithms, provide effective solutions in static settings, but they struggle with the spatial-temporal complexity of real-world traffic flows. In this work, we propose TrafficKAN-GCN, a hybrid d\u2026 \u25bd More Urban traffic optimization is critical for improving transportation efficiency and alleviating congestion, particularly in large-scale dynamic networks. Traditional methods, such as Dijkstra's and Floyd's algorithms, provide effective solutions in static settings, but they struggle with the spatial-temporal complexity of real-world traffic flows. In this work, we propose TrafficKAN-GCN, a hybrid deep learning framework combining Kolmogorov-Arnold Networks (KAN) with Graph Convolutional Networks (GCN), designed to enhance urban traffic flow optimization. By integrating KAN's adaptive nonlinear function approximation with GCN's spatial graph learning capabilities, TrafficKAN-GCN captures both complex traffic patterns and topological dependencies. We evaluate the proposed framework using real-world traffic data from the Baltimore Metropolitan area. Compared with baseline models such as MLP-GCN, standard GCN, and Transformer-based approaches, TrafficKAN-GCN achieves competitive prediction accuracy while demonstrating improved robustness in handling noisy and irregular traffic data. Our experiments further highlight the framework's ability to redistribute traffic flow, mitigate congestion, and adapt to disruptive events, such as the Francis Scott Key Bridge collapse. This study contributes to the growing body of work on hybrid graph learning for intelligent transportation systems, highlighting the potential of combining KAN and GCN for real-time traffic optimization. Future work will focus on reducing computational overhead and integrating Transformer-based temporal modeling for enhanced long-term traffic prediction. The proposed TrafficKAN-GCN framework offers a promising direction for data-driven urban mobility management, balancing predictive accuracy, robustness, and computational efficiency. \u25b3 Less Submitted 5 March, 2025; originally announced March 2025. Comments: 21 pages, 14 figures MSC Class: 90B20; 68T07; 05C85; 90C90 ACM Class: G.2.2; I.2.6; I.5.1; I.2.8; J.7 arXiv:2503.03056 [pdf, other] A2Perf: Real-World Autonomous Agents Benchmark Authors: Ikechukwu Uchendu, Jason Jabbour, Korneel Van den Berghe, Joel Runevic, Matthew Stewart, Jeffrey Ma, Srivatsan Krishnan, Izzeddin Gur, Austin Huang, Colton Bishop, Paige Bailey, Wenjie Jiang, Ebrahim M. Songhori, Sergio Guadarrama, Jie Tan, Jordan K. Terry, Aleksandra Faust, Vijay Janapa Reddi Abstract: Autonomous agents and systems cover a number of application areas, from robotics and digital assistants to combinatorial optimization, all sharing common, unresolved research challenges. It is not sufficient for agents to merely solve a given task; they must generalize to out-of-distribution tasks, perform reliably, and use hardware resources efficiently during training and inference, among other\u2026 \u25bd More Autonomous agents and systems cover a number of application areas, from robotics and digital assistants to combinatorial optimization, all sharing common, unresolved research challenges. It is not sufficient for agents to merely solve a given task; they must generalize to out-of-distribution tasks, perform reliably, and use hardware resources efficiently during training and inference, among other requirements. Several methods, such as reinforcement learning and imitation learning, are commonly used to tackle these problems, each with different trade-offs. However, there is a lack of benchmarking suites that define the environments, datasets, and metrics which can be used to provide a meaningful way for the community to compare progress on applying these methods to real-world problems. We introduce A2Perf--a benchmark with three environments that closely resemble real-world domains: computer chip floorplanning, web navigation, and quadruped locomotion. A2Perf provides metrics that track task performance, generalization, system resource efficiency, and reliability, which are all critical to real-world applications. Using A2Perf, we demonstrate that web navigation agents can achieve latencies comparable to human reaction times on consumer hardware, reveal reliability trade-offs between algorithms for quadruped locomotion, and quantify the energy costs of different learning approaches for computer chip-design. In addition, we propose a data cost metric to account for the cost incurred acquiring offline data for imitation learning and hybrid algorithms, which allows us to better compare these approaches. A2Perf also contains several standard baselines, enabling apples-to-apples comparisons across methods and facilitating progress in real-world autonomy. As an open-source benchmark, A2Perf is designed to remain accessible, up-to-date, and useful to the research community over the long term. \u25b3 Less Submitted 4 March, 2025; originally announced March 2025. Comments: 32 pages, 12 figures, preprint arXiv:2503.02268 [pdf, other] AppAgentX: Evolving GUI Agents as Proficient Smartphone Users Authors: Wenjia Jiang, Yangyang Zhuang, Chenxi Song, Xu Yang, Chi Zhang Abstract: Recent advancements in Large Language Models (LLMs) have led to the development of intelligent LLM-based agents capable of interacting with graphical user interfaces (GUIs). These agents demonstrate strong reasoning and adaptability, enabling them to perform complex tasks that traditionally required predefined rules. However, the reliance on step-by-step reasoning in LLM-based agents often results\u2026 \u25bd More Recent advancements in Large Language Models (LLMs) have led to the development of intelligent LLM-based agents capable of interacting with graphical user interfaces (GUIs). These agents demonstrate strong reasoning and adaptability, enabling them to perform complex tasks that traditionally required predefined rules. However, the reliance on step-by-step reasoning in LLM-based agents often results in inefficiencies, particularly for routine tasks. In contrast, traditional rule-based systems excel in efficiency but lack the intelligence and flexibility to adapt to novel scenarios. To address this challenge, we propose a novel evolutionary framework for GUI agents that enhances operational efficiency while retaining intelligence and flexibility. Our approach incorporates a memory mechanism that records the agent's task execution history. By analyzing this history, the agent identifies repetitive action sequences and evolves high-level actions that act as shortcuts, replacing these low-level operations and improving efficiency. This allows the agent to focus on tasks requiring more complex reasoning, while simplifying routine actions. Experimental results on multiple benchmark tasks demonstrate that our approach significantly outperforms existing methods in both efficiency and accuracy. The code will be open-sourced to support further research. \u25b3 Less Submitted 3 March, 2025; originally announced March 2025. arXiv:2502.21087 [pdf, other] PASemiQA: Plan-Assisted Agent for Question Answering on Semi-Structured Data with Text and Relational Information Authors: Hansi Yang, Qi Zhang, Wei Jiang, Jianguo Li Abstract: Large language models (LLMs) have shown impressive abilities in answering questions across various domains, but they often encounter hallucination issues on questions that require professional and up-to-date knowledge. To address this limitation, retrieval-augmented generation (RAG) techniques have been proposed, which retrieve relevant information from external sources to inform their responses.\u2026 \u25bd More Large language models (LLMs) have shown impressive abilities in answering questions across various domains, but they often encounter hallucination issues on questions that require professional and up-to-date knowledge. To address this limitation, retrieval-augmented generation (RAG) techniques have been proposed, which retrieve relevant information from external sources to inform their responses. However, existing RAG methods typically focus on a single type of external data, such as vectorized text database or knowledge graphs, and cannot well handle real-world questions on semi-structured data containing both text and relational information. To bridge this gap, we introduce PASemiQA, a novel approach that jointly leverages text and relational information in semi-structured data to answer questions. PASemiQA first generates a plan to identify relevant text and relational information to answer the question in semi-structured data, and then uses an LLM agent to traverse the semi-structured data and extract necessary information. Our empirical results demonstrate the effectiveness of PASemiQA across different semi-structured datasets from various domains, showcasing its potential to improve the accuracy and reliability of question answering systems on semi-structured data. \u25b3 Less Submitted 28 February, 2025; originally announced February 2025. arXiv:2502.20943 [pdf, other] BadRefSR: Backdoor Attacks Against Reference-based Image Super Resolution Authors: Xue Yang, Tao Chen, Lei Guo, Wenbo Jiang, Ji Guo, Yongming Li, Jiaming He Abstract: Reference-based image super-resolution (RefSR) represents a promising advancement in super-resolution (SR). In contrast to single-image super-resolution (SISR), RefSR leverages an additional reference image to help recover high-frequency details, yet its vulnerability to backdoor attacks has not been explored. To fill this research gap, we propose a novel attack framework called BadRefSR, which em\u2026 \u25bd More Reference-based image super-resolution (RefSR) represents a promising advancement in super-resolution (SR). In contrast to single-image super-resolution (SISR), RefSR leverages an additional reference image to help recover high-frequency details, yet its vulnerability to backdoor attacks has not been explored. To fill this research gap, we propose a novel attack framework called BadRefSR, which embeds backdoors in the RefSR model by adding triggers to the reference images and training with a mixed loss function. Extensive experiments across various backdoor attack settings demonstrate the effectiveness of BadRefSR. The compromised RefSR network performs normally on clean input images, while outputting attacker-specified target images on triggered input images. Our study aims to alert researchers to the potential backdoor risks in RefSR. Codes are available at https://github.com/xuefusiji/BadRefSR. \u25b3 Less Submitted 28 February, 2025; originally announced February 2025. Comments: 5 pages,4 figures arXiv:2502.20528 [pdf, other] ConfuGuard: Using Metadata to Detect Active and Stealthy Package Confusion Attacks Accurately and at Scale Authors: Wenxin Jiang, Berk \u00c7akar, Mikola Lysenko, James C. Davis Abstract: Package confusion attacks such as typosquatting threaten software supply chains. Attackers make packages with names that syntactically or semantically resemble legitimate ones, tricking engineers into installing malware. While prior work has developed defenses against package confusions in some software package registries, notably NPM, PyPI, and RubyGems, gaps remain: high false-positive rates; ge\u2026 \u25bd More Package confusion attacks such as typosquatting threaten software supply chains. Attackers make packages with names that syntactically or semantically resemble legitimate ones, tricking engineers into installing malware. While prior work has developed defenses against package confusions in some software package registries, notably NPM, PyPI, and RubyGems, gaps remain: high false-positive rates; generalization to more software package ecosystems; and insights from real-world deployment. In this work, we introduce ConfuGuard, a solution designed to address the challenges posed by package confusion threats. We begin by presenting the first empirical analysis of benign signals derived from prior package confusion data, uncovering their threat patterns, engineering practices, and measurable attributes. We observed that 13.3% of real package confusion attacks are initially stealthy, so we take that into consideration and refined the definitions. Building on state-of-the-art approaches, we extend support from three to six software package registries, and leverage package metadata to distinguish benign packages. Our approach significantly reduces 64% false-positive (from 77% to 13%), with acceptable additional overhead to filter out benign packages by analyzing the package metadata. ConfuGuard is in production at our industry partner, whose analysts have already confirmed 301 packages detected by ConfuGuard as real attacks. We share lessons learned from production and provide insights to researchers. \u25b3 Less Submitted 17 March, 2025; v1 submitted 27 February, 2025; originally announced February 2025. arXiv:2502.18904 [pdf, other] An Empirical Study on Commit Message Generation using LLMs via In-Context Learning Authors: Yifan Wu, Yunpeng Wang, Ying Li, Wei Tao, Siyu Yu, Haowen Yang, Wei Jiang, Jianguo Li Abstract: Commit messages concisely describe code changes in natural language and are important for software maintenance. Several approaches have been proposed to automatically generate commit messages, but they still suffer from critical limitations, such as time-consuming training and poor generalization ability. To tackle these limitations, we propose to borrow the weapon of large language models (LLMs)\u2026 \u25bd More Commit messages concisely describe code changes in natural language and are important for software maintenance. Several approaches have been proposed to automatically generate commit messages, but they still suffer from critical limitations, such as time-consuming training and poor generalization ability. To tackle these limitations, we propose to borrow the weapon of large language models (LLMs) and in-context learning (ICL). Our intuition is based on the fact that the training corpora of LLMs contain extensive code changes and their pairwise commit messages, which makes LLMs capture the knowledge about commits, while ICL can exploit the knowledge hidden in the LLMs and enable them to perform downstream tasks without model tuning. However, it remains unclear how well LLMs perform on commit message generation via ICL. In this paper, we conduct an empirical study to investigate the capability of LLMs to generate commit messages via ICL. Specifically, we first explore the impact of different settings on the performance of ICL-based commit message generation. We then compare ICL-based commit message generation with state-of-the-art approaches on a popular multilingual dataset and a new dataset we created to mitigate potential data leakage. The results show that ICL-based commit message generation significantly outperforms state-of-the-art approaches on subjective evaluation and achieves better generalization ability. We further analyze the root causes for LLM's underperformance and propose several implications, which shed light on future research directions for using LLMs to generate commit messages. \u25b3 Less Submitted 26 February, 2025; originally announced February 2025. Comments: Accepted by the 47th IEEE/ACM International Conference on Software Engineering (ICSE'25) arXiv:2502.18810 [pdf, other] Holistic Audit Dataset Generation for LLM Unlearning via Knowledge Graph Traversal and Redundancy Removal Authors: Weipeng Jiang, Juan Zhai, Shiqing Ma, Ziyan Lei, Xiaofei Xie, Yige Wang, Chao Shen Abstract: In recent years, Large Language Models (LLMs) have faced increasing demands to selectively remove sensitive information, protect privacy, and comply with copyright regulations through unlearning, by Machine Unlearning. While evaluating unlearning effectiveness is crucial, existing benchmarks are limited in scale and comprehensiveness, typically containing only a few hundred test cases. We identify\u2026 \u25bd More In recent years, Large Language Models (LLMs) have faced increasing demands to selectively remove sensitive information, protect privacy, and comply with copyright regulations through unlearning, by Machine Unlearning. While evaluating unlearning effectiveness is crucial, existing benchmarks are limited in scale and comprehensiveness, typically containing only a few hundred test cases. We identify two critical challenges in generating holistic audit datasets: ensuring audit adequacy and handling knowledge redundancy between forget and retain dataset. To address these challenges, we propose HANKER, an automated framework for holistic audit dataset generation leveraging knowledge graphs to achieve fine-grained coverage and eliminate redundant knowledge. Applying HANKER to the popular MUSE benchmark, we successfully generated over 69,000 and 111,000 audit cases for the News and Books datasets respectively, identifying thousands of knowledge memorization instances that the previous benchmark failed to detect. Our empirical analysis uncovers how knowledge redundancy significantly skews unlearning effectiveness metrics, with redundant instances artificially inflating the observed memorization measurements ROUGE from 19.7% to 26.1% and Entailment Scores from 32.4% to 35.2%, highlighting the necessity of systematic deduplication for accurate assessment. \u25b3 Less Submitted 25 February, 2025; originally announced February 2025. Comments: 11 pages, 4 figures ACM Class: I.2.7; D.2.5; I.2.0 arXiv:2502.17051 [pdf, other] A Frequency-Domain Opportunistic Approach for Spectral-Efficient Cell-Free Massive MIMO Authors: Wei Jiang, Hans Dieter Schotten Abstract: Constrained by weak signal strength and significant inter-cell interference, users located at the cell edge in a cellular network suffer from inferior service quality. Recently, cell-free massive MIMO (CFmMIMO) has gained considerable attention due to its capability to offer uniform quality of service, alleviating the cell-edge problem. In contrast to previous studies focused on narrow-band CFmMIM\u2026 \u25bd More Constrained by weak signal strength and significant inter-cell interference, users located at the cell edge in a cellular network suffer from inferior service quality. Recently, cell-free massive MIMO (CFmMIMO) has gained considerable attention due to its capability to offer uniform quality of service, alleviating the cell-edge problem. In contrast to previous studies focused on narrow-band CFmMIMO systems, this paper studies wideband CFmMIMO communications against channel frequency selectivity. By exploiting the frequency-domain flexibility offered by orthogonal frequency-division multiplexing (OFDM), and leveraging a particular spatial characteristic in the cell-free structure -- namely, the near-far effect among distributed access points (APs) -- we propose an opportunistic approach to boost spectral efficiency. The core concept lies in opportunistically activating nearby APs for certain users across their assigned OFDM subcarriers while deactivating distant APs to prevent power wastage and lower inter-user interference. Furthermore, this approach enables the use of downlink pilots by reducing the number of active APs per subcarrier to a small subset, thereby substantially improving downlink performance through coherent detection at the user receiver. Verified by numerical results, our proposed approach demonstrates considerable performance improvement compared to the two benchmark approaches. \u25b3 Less Submitted 24 February, 2025; originally announced February 2025. arXiv:2502.16235 [pdf, other] Dynamic Parallel Tree Search for Efficient LLM Reasoning Authors: Yifu Ding, Wentao Jiang, Shunyu Liu, Yongcheng Jing, Jinyang Guo, Yingjie Wang, Jing Zhang, Zengmao Wang, Ziwei Liu, Bo Du, Xianglong Liu, Dacheng Tao Abstract: Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by structuring problem-solving as a spanning tree. However, recent methods focus on search accuracy while overlooking computational efficiency. The challenges of accelerating the ToT lie in the frequent switching of reasoning focus, and the redundant exploration of suboptimal solutions. To alleviate this dilemma, we propose Dynam\u2026 \u25bd More Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by structuring problem-solving as a spanning tree. However, recent methods focus on search accuracy while overlooking computational efficiency. The challenges of accelerating the ToT lie in the frequent switching of reasoning focus, and the redundant exploration of suboptimal solutions. To alleviate this dilemma, we propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework that aims to dynamically optimize the reasoning path in inference. It includes the Parallelism Streamline in the generation phase to build up a flexible and adaptive parallelism with arbitrary paths by fine-grained cache management and alignment. Meanwhile, the Search and Transition Mechanism filters potential candidates to dynamically maintain the reasoning focus on more possible solutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with Math500 and GSM8K datasets show that DPTS significantly improves efficiency by 2-4x on average while maintaining or even surpassing existing reasoning algorithms in accuracy, making ToT-based reasoning more scalable and computationally efficient. \u25b3 Less Submitted 27 February, 2025; v1 submitted 22 February, 2025; originally announced February 2025. Comments: 17 pages, 11 figures arXiv:2502.15401 [pdf, other] Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning Authors: Xuetao Ma, Wenbin Jiang, Hua Huang Abstract: In-context learning (ICL) can significantly enhance the complex reasoning capabilities of large language models (LLMs), with the key lying in the selection and ordering of demonstration examples. Previous methods typically relied on simple features to measure the relevance between examples. We argue that these features are not sufficient to reflect the intrinsic connections between examples. In th\u2026 \u25bd More In-context learning (ICL) can significantly enhance the complex reasoning capabilities of large language models (LLMs), with the key lying in the selection and ordering of demonstration examples. Previous methods typically relied on simple features to measure the relevance between examples. We argue that these features are not sufficient to reflect the intrinsic connections between examples. In this study, we propose a curriculum ICL strategy guided by problem-solving logic. We select demonstration examples by analyzing the problem-solving logic and order them based on curriculum learning. Specifically, we constructed a problem-solving logic instruction set based on the BREAK dataset and fine-tuned a language model to analyze the problem-solving logic of examples. Subsequently, we selected appropriate demonstration examples based on problem-solving logic and assessed their difficulty according to the number of problem-solving steps. In accordance with the principles of curriculum learning, we ordered the examples from easy to hard to serve as contextual prompts. Experimental results on multiple benchmarks indicate that our method outperforms previous ICL approaches in terms of performance and efficiency, effectively enhancing the complex reasoning capabilities of LLMs. Our project will be publicly available subsequently. \u25b3 Less Submitted 21 February, 2025; originally announced February 2025. arXiv:2502.15271 [pdf, other] Omnidirectional Image Quality Captioning: A Large-scale Database and A New Model Authors: Jiebin Yan, Ziwen Tan, Yuming Fang, Junjie Chen, Wenhui Jiang, Zhou Wang Abstract: The fast growing application of omnidirectional images calls for effective approaches for omnidirectional image quality assessment (OIQA). Existing OIQA methods have been developed and tested on homogeneously distorted omnidirectional images, but it is hard to transfer their success directly to the heterogeneously distorted omnidirectional images. In this paper, we conduct the largest study so far\u2026 \u25bd More The fast growing application of omnidirectional images calls for effective approaches for omnidirectional image quality assessment (OIQA). Existing OIQA methods have been developed and tested on homogeneously distorted omnidirectional images, but it is hard to transfer their success directly to the heterogeneously distorted omnidirectional images. In this paper, we conduct the largest study so far on OIQA, where we establish a large-scale database called OIQ-10K containing 10,000 omnidirectional images with both homogeneous and heterogeneous distortions. A comprehensive psychophysical study is elaborated to collect human opinions for each omnidirectional image, together with the spatial distributions (within local regions or globally) of distortions, and the head and eye movements of the subjects. Furthermore, we propose a novel multitask-derived adaptive feature-tailoring OIQA model named IQCaption360, which is capable of generating a quality caption for an omnidirectional image in a manner of textual template. Extensive experiments demonstrate the effectiveness of IQCaption360, which outperforms state-of-the-art methods by a significant margin on the proposed OIQ-10K database. The OIQ-10K database and the related source codes are available at https://github.com/WenJuing/IQCaption360. \u25b3 Less Submitted 21 February, 2025; originally announced February 2025. arXiv:2502.12587 [pdf, other] RSMLP: A light Sampled MLP Structure for Incomplete Utterance Rewrite Authors: Lunjun Liu, Weilai Jiang, Yaonan Wang Abstract: The Incomplete Utterance Rewriting (IUR) task has garnered significant attention in recent years. Its goal is to reconstruct conversational utterances to better align with the current context, thereby enhancing comprehension. In this paper, we introduce a novel and versatile lightweight method, Rewritten-Sampled MLP (RSMLP). By employing an MLP based architecture with a carefully designed down-sam\u2026 \u25bd More The Incomplete Utterance Rewriting (IUR) task has garnered significant attention in recent years. Its goal is to reconstruct conversational utterances to better align with the current context, thereby enhancing comprehension. In this paper, we introduce a novel and versatile lightweight method, Rewritten-Sampled MLP (RSMLP). By employing an MLP based architecture with a carefully designed down-sampling strategy, RSMLP effectively extracts latent semantic information between utterances and makes appropriate edits to restore incomplete utterances. Due to its simple yet efficient structure, our method achieves competitive performance on public IUR datasets and in real-world applications. \u25b3 Less Submitted 18 February, 2025; originally announced February 2025. arXiv:2502.12214 [pdf, other] Zero Token-Driven Deep Thinking in LLMs: Unlocking the Full Potential of Existing Parameters via Cyclic Refinement Authors: Guanghao Li, Wenhao Jiang, Li Shen, Ming Tang, Chun Yuan Abstract: Resource limitations often constrain the parameter counts of Large Language Models (LLMs), hindering their performance. While existing methods employ parameter sharing to reuse the same parameter set under fixed budgets, such approaches typically force each layer to assume multiple roles with a predetermined number of iterations, restricting efficiency and adaptability. In this work, we propose th\u2026 \u25bd More Resource limitations often constrain the parameter counts of Large Language Models (LLMs), hindering their performance. While existing methods employ parameter sharing to reuse the same parameter set under fixed budgets, such approaches typically force each layer to assume multiple roles with a predetermined number of iterations, restricting efficiency and adaptability. In this work, we propose the Zero Token Transformer (ZTT), which features a head-tail decoupled parameter cycling method. We disentangle the first (head) and last (tail) layers from parameter cycling and iteratively refine only the intermediate layers. Furthermore, we introduce a Zero-Token Mechanism, an internal architectural component rather than an input token, to guide layer-specific computation. At each cycle, the model retrieves a zero token (with trainable key values) from a Zero-Token Pool, integrating it alongside regular tokens in the attention mechanism. The corresponding attention scores not only reflect each layer's computational importance but also enable dynamic early exits without sacrificing overall model accuracy. Our approach achieves superior performance under tight parameter budgets, effectively reduces computational overhead via early exits, and can be readily applied to fine-tune existing pre-trained models for enhanced efficiency and adaptability. \u25b3 Less Submitted 16 February, 2025; originally announced February 2025. arXiv:2502.08352 [pdf, other] Sat-DN: Implicit Surface Reconstruction from Multi-View Satellite Images with Depth and Normal Supervision Authors: Tianle Liu, Shuangming Zhao, Wanshou Jiang, Bingxuan Guo Abstract: With advancements in satellite imaging technology, acquiring high-resolution multi-view satellite imagery has become increasingly accessible, enabling rapid and location-independent ground model reconstruction. However, traditional stereo matching methods struggle to capture fine details, and while neural radiance fields (NeRFs) achieve high-quality reconstructions, their training time is prohibit\u2026 \u25bd More With advancements in satellite imaging technology, acquiring high-resolution multi-view satellite imagery has become increasingly accessible, enabling rapid and location-independent ground model reconstruction. However, traditional stereo matching methods struggle to capture fine details, and while neural radiance fields (NeRFs) achieve high-quality reconstructions, their training time is prohibitively long. Moreover, challenges such as low visibility of building facades, illumination and style differences between pixels, and weakly textured regions in satellite imagery further make it hard to reconstruct reasonable terrain geometry and detailed building facades. To address these issues, we propose Sat-DN, a novel framework leveraging a progressively trained multi-resolution hash grid reconstruction architecture with explicit depth guidance and surface normal consistency constraints to enhance reconstruction quality. The multi-resolution hash grid accelerates training, while the progressive strategy incrementally increases the learning frequency, using coarse low-frequency geometry to guide the reconstruction of fine high-frequency details. The depth and normal constraints ensure a clear building outline and correct planar distribution. Extensive experiments on the DFC2019 dataset demonstrate that Sat-DN outperforms existing methods, achieving state-of-the-art results in both qualitative and quantitative evaluations. The code is available at https://github.com/costune/SatDN. \u25b3 Less Submitted 12 February, 2025; originally announced February 2025. arXiv:2502.08277 [pdf, other] ChorusCVR: Chorus Supervision for Entire Space Post-Click Conversion Rate Modeling Authors: Wei Cheng, Yucheng Lu, Boyang Xia, Jiangxia Cao, Kuan Xu, Mingxing Wen, Wei Jiang, Jiaming Zhang, Zhaojie Liu, Liyin Hong, Kun Gai, Guorui Zhou Abstract: Post-click conversion rate (CVR) estimation is a vital task in many recommender systems of revenue businesses, e.g., e-commerce and advertising. In a perspective of sample, a typical CVR positive sample usually goes through a funnel of exposure to click to conversion. For lack of post-event labels for un-clicked samples, CVR learning task commonly only utilizes clicked samples, rather than all exp\u2026 \u25bd More Post-click conversion rate (CVR) estimation is a vital task in many recommender systems of revenue businesses, e.g., e-commerce and advertising. In a perspective of sample, a typical CVR positive sample usually goes through a funnel of exposure to click to conversion. For lack of post-event labels for un-clicked samples, CVR learning task commonly only utilizes clicked samples, rather than all exposed samples as for click-through rate (CTR) learning task. However, during online inference, CVR and CTR are estimated on the same assumed exposure space, which leads to a inconsistency of sample space between training and inference, i.e., sample selection bias (SSB). To alleviate SSB, previous wisdom proposes to design novel auxiliary tasks to enable the CVR learning on un-click training samples, such as CTCVR and counterfactual CVR, etc. Although alleviating SSB to some extent, none of them pay attention to the discrimination between ambiguous negative samples (un-clicked) and factual negative samples (clicked but un-converted) during modelling, which makes CVR model lacks robustness. To full this gap, we propose a novel ChorusCVR model to realize debiased CVR learning in entire-space. \u25b3 Less Submitted 14 February, 2025; v1 submitted 12 February, 2025; originally announced February 2025. Comments: Work in progress arXiv:2502.07160 [pdf, other] HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates Authors: Lei Lu, Yize Li, Yanzhi Wang, Wei Wang, Wei Jiang Abstract: Image compression under ultra-low bitrates remains challenging for both conventional learned image compression (LIC) and generative vector-quantized (VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy quantization, while generative VQ modeling gives poor fidelity due to the mismatch between learned generative priors and specific inputs. In this work, we propose Hybrid-Diffus\u2026 \u25bd More Image compression under ultra-low bitrates remains challenging for both conventional learned image compression (LIC) and generative vector-quantized (VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy quantization, while generative VQ modeling gives poor fidelity due to the mismatch between learned generative priors and specific inputs. In this work, we propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream framework that utilizes both generative VQ-modeling and diffusion models, as well as conventional LIC, to achieve both high fidelity and high perceptual quality. Different from previous hybrid methods that directly use pre-trained LIC models to generate low-quality fidelity-preserving information from heavily quantized latent, we use diffusion models to extract high-quality complimentary fidelity information from the ground-truth input, which can enhance the system performance in several aspects: improving indices map prediction, enhancing the fidelity-preserving output of the LIC stream, and refining conditioned image reconstruction with VQ-latent correction. In addition, our diffusion model is based on a dense representative vector (DRV), which is lightweight with very simple sampling schedulers. Extensive experiments demonstrate that our HDCompression outperforms the previous conventional LIC, generative VQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative visualization, providing balanced robust compression performance at ultra-low bitrates. \u25b3 Less Submitted 18 February, 2025; v1 submitted 10 February, 2025; originally announced February 2025. Comments: Under Review arXiv:2502.07016 [pdf, ps, other] Confidence Intervals for Evaluation of Data Mining Authors: Zheng Yuan, Wenxin Jiang Abstract: In data mining, when binary prediction rules are used to predict a binary outcome, many performance measures are used in a vast array of literature for the purposes of evaluation and comparison. Some examples include classification accuracy, precision, recall, F measures, and Jaccard index. Typically, these performance measures are only approximately estimated from a finite dataset, which may lead\u2026 \u25bd More In data mining, when binary prediction rules are used to predict a binary outcome, many performance measures are used in a vast array of literature for the purposes of evaluation and comparison. Some examples include classification accuracy, precision, recall, F measures, and Jaccard index. Typically, these performance measures are only approximately estimated from a finite dataset, which may lead to findings that are not statistically significant. In order to properly quantify such statistical uncertainty, it is important to provide confidence intervals associated with these estimated performance measures. We consider statistical inference about general performance measures used in data mining, with both individual and joint confidence intervals. These confidence intervals are based on asymptotic normal approximations and can be computed fast, without needs to do bootstrap resampling. We study the finite sample coverage probabilities for these confidence intervals and also propose a `blurring correction' on the variance to improve the finite sample performance. This 'blurring correction' generalizes the plus-four method from binomial proportion to general performance measures used in data mining. Our framework allows multiple performance measures of multiple classification rules to be inferred simultaneously for comparisons. \u25b3 Less Submitted 10 February, 2025; originally announced February 2025. arXiv:2502.06816 [pdf, other] DeepCell: Multiview Representation Learning for Post-Mapping Netlists Authors: Zhengyuan Shi, Chengyu Ma, Ziyang Zheng, Lingfeng Zhou, Hongyang Pan, Wentao Jiang, Fan Yang, Xiaoyan Yang, Zhufei Chu, Qiang Xu Abstract: Representation learning for post-mapping (PM) netlists is a critical challenge in Electronic Design Automation (EDA), driven by the diverse and complex nature of modern circuit designs. Existing approaches focus on intermediate representations like And-Inverter Graphs (AIGs), limiting their applicability to post-synthesis stages. We introduce DeepCell, a multiview representation learning framework\u2026 \u25bd More Representation learning for post-mapping (PM) netlists is a critical challenge in Electronic Design Automation (EDA), driven by the diverse and complex nature of modern circuit designs. Existing approaches focus on intermediate representations like And-Inverter Graphs (AIGs), limiting their applicability to post-synthesis stages. We introduce DeepCell, a multiview representation learning framework that integrates structural and functional insights from both PM netlists and AIGs to learn rich, generalizable embeddings. At its core, DeepCell employs the novel Mask Circuit Modeling (MCM) mechanism, which refines PM netlist representations in a self-supervised manner using pretrained AIG encoders. DeepCell sets a new benchmark in PM netlist representation, outperforming existing methods in predictive accuracy and reconstruction fidelity. To validate its efficacy, we apply DeepCell to functional Engineering Change Orders (ECO), achieving significant reductions in patch generation costs and runtime while improving patch quality. \u25b3 Less Submitted 4 February, 2025; originally announced February 2025. arXiv:2502.06557 [pdf, other] LiveForesighter: Generating Future Information for Live-Streaming Recommendations at Kuaishou Authors: Yucheng Lu, Jiangxia Cao, Xu Kuan, Wei Cheng, Wei Jiang, Jiaming Zhang, Yang Shuang, Liu Zhaojie, Liyin Hong Abstract: Live-streaming, as a new-generation media to connect users and authors, has attracted a lot of attention and experienced rapid growth in recent years. Compared with the content-static short-video recommendation, the live-streaming recommendation faces more challenges in giving our users a satisfactory experience: (1) Live-streaming content is dynamically ever-changing along time. (2) valuable beha\u2026 \u25bd More Live-streaming, as a new-generation media to connect users and authors, has attracted a lot of attention and experienced rapid growth in recent years. Compared with the content-static short-video recommendation, the live-streaming recommendation faces more challenges in giving our users a satisfactory experience: (1) Live-streaming content is dynamically ever-changing along time. (2) valuable behaviors (e.g., send digital-gift, buy products) always require users to watch for a long-time (>10 min). Combining the two attributes, here raising a challenging question for live-streaming recommendation: How to discover the live-streamings that the content user is interested in at the current moment, and further a period in the future? \u25b3 Less Submitted 10 February, 2025; originally announced February 2025. Comments: Work in progress arXiv:2502.01715 [pdf, other] Process-Supervised Reinforcement Learning for Code Generation Authors: Yufan Ye, Ting Zhang, Wenbin Jiang, Hua Huang Abstract: Existing reinforcement learning strategies based on outcome supervision have proven effective in enhancing the performance of large language models(LLMs) for code generation. While reinforcement learning based on process supervision has shown great promise in handling multi-step reasoning tasks, its effectiveness in code generation remains largely underexplored and underjustified. The primary obst\u2026 \u25bd More Existing reinforcement learning strategies based on outcome supervision have proven effective in enhancing the performance of large language models(LLMs) for code generation. While reinforcement learning based on process supervision has shown great promise in handling multi-step reasoning tasks, its effectiveness in code generation remains largely underexplored and underjustified. The primary obstacle stems from the resource-intensive nature of constructing high-quality process-supervised data, which demands substantial human expertise and computational resources. In response to this challenge, we propose a \"statement mutation/refactoring-compile and execution verification\" strategy: mutating and refactoring code line-by-line through a teacher model, and utilizing compiler execution results to automatically label each line, resulting in line-by-line process-supervised data, which is pivotal for training a process-supervised reward model. The trained reward model is then integrated into the PRLCoder framework, followed by experimental validation on several benchmarks. Experimental results demonstrate that process-supervised reinforcement learning significantly surpasses methods relying solely on outcome supervision. Notably, in tackling complex code generation tasks, process-supervised reinforcement learning shows a clear advantage, ensuring both the integrity of the code generation process and the correctness of the generation results. \u25b3 Less Submitted 3 February, 2025; originally announced February 2025. arXiv:2502.00870 [pdf, other] FedHPD: Heterogeneous Federated Reinforcement Learning via Policy Distillation Authors: Wenzheng Jiang, Ji Wang, Xiongtao Zhang, Weidong Bao, Cheston Tan, Flint Xiaofeng Fan Abstract: Federated Reinforcement Learning (FedRL) improves sample efficiency while preserving privacy; however, most existing studies assume homogeneous agents, limiting its applicability in real-world scenarios. This paper investigates FedRL in black-box settings with heterogeneous agents, where each agent employs distinct policy networks and training configurations without disclosing their internal detai\u2026 \u25bd More Federated Reinforcement Learning (FedRL) improves sample efficiency while preserving privacy; however, most existing studies assume homogeneous agents, limiting its applicability in real-world scenarios. This paper investigates FedRL in black-box settings with heterogeneous agents, where each agent employs distinct policy networks and training configurations without disclosing their internal details. Knowledge Distillation (KD) is a promising method for facilitating knowledge sharing among heterogeneous models, but it faces challenges related to the scarcity of public datasets and limitations in knowledge representation when applied to FedRL. To address these challenges, we propose Federated Heterogeneous Policy Distillation (FedHPD), which solves the problem of heterogeneous FedRL by utilizing action probability distributions as a medium for knowledge sharing. We provide a theoretical analysis of FedHPD's convergence under standard assumptions. Extensive experiments corroborate that FedHPD shows significant improvements across various reinforcement learning benchmark tasks, further validating our theoretical findings. Moreover, additional experiments demonstrate that FedHPD operates effectively without the need for an elaborate selection of public datasets. \u25b3 Less Submitted 2 February, 2025; originally announced February 2025. Comments: This preprint presents the full version of the Extended Abstract accepted by AAMAS 2025, including all the proofs and experiments ACM Class: I.2.11 arXiv:2502.00753 [pdf, other] Mirror Descent Under Generalized Smoothness Authors: Dingzhi Yu, Wei Jiang, Yuanyu Wan, Lijun Zhang Abstract: Smoothness is crucial for attaining fast rates in first-order optimization. However, many optimization problems in modern machine learning involve non-smooth objectives. Recent studies relax the smoothness assumption by allowing the Lipschitz constant of the gradient to grow with respect to the gradient norm, which accommodates a broad range of objectives in practice. Despite this progress, existi\u2026 \u25bd More Smoothness is crucial for attaining fast rates in first-order optimization. However, many optimization problems in modern machine learning involve non-smooth objectives. Recent studies relax the smoothness assumption by allowing the Lipschitz constant of the gradient to grow with respect to the gradient norm, which accommodates a broad range of objectives in practice. Despite this progress, existing generalizations of smoothness are restricted to Euclidean geometry with $\\ell_2$-norm and only have theoretical guarantees for optimization in the Euclidean space. In this paper, we address this limitation by introducing a new $\\ell*$-smoothness concept that measures the norm of Hessian in terms of a general norm and its dual, and establish convergence for mirror-descent-type algorithms, matching the rates under the classic smoothness. Notably, we propose a generalized self-bounding property that facilitates bounding the gradients via controlling suboptimality gaps, serving as a principal component for convergence analysis. Beyond deterministic optimization, we establish an anytime convergence for stochastic mirror descent based on a new bounded noise condition that encompasses the widely adopted bounded or affine noise assumptions. \u25b3 Less Submitted 2 February, 2025; originally announced February 2025. Comments: 59 pages, 2 figures arXiv:2501.13570 [pdf, other] Occamy: A Preemptive Buffer Management for On-chip Shared-memory Switches Authors: Danfeng Shan, Yunguang Li, Jinchao Ma, Zhenxing Zhang, Zeyu Liang, Xinyu Wen, Hao Li, Wanchun Jiang, Nan Li, Fengyuan Ren Abstract: Today's high-speed switches employ an on-chip shared packet buffer. The buffer is becoming increasingly insufficient as it cannot scale with the growing switching capacity. Nonetheless, the buffer needs to face highly intense bursts and meet stringent performance requirements for datacenter applications. This imposes rigorous demand on the Buffer Management (BM) scheme, which dynamically allocates\u2026 \u25bd More Today's high-speed switches employ an on-chip shared packet buffer. The buffer is becoming increasingly insufficient as it cannot scale with the growing switching capacity. Nonetheless, the buffer needs to face highly intense bursts and meet stringent performance requirements for datacenter applications. This imposes rigorous demand on the Buffer Management (BM) scheme, which dynamically allocates the buffer across queues. However, the de facto BM scheme, designed over two decades ago, is ill-suited to meet the requirements of today's network. In this paper, we argue that shallow-buffer switches, intense bursts, along with dynamic traffic call for a highly agile BM that can quickly adjust the buffer allocation as traffic changes. However, the agility of the current BM is fundamentally limited by its non-preemptive nature. Nonetheless, we find that preemptive BM, considered unrealizable in history, is now feasible on modern switch chips. We propose Occamy, a preemptive BM that can quickly adjust buffer allocation. Occamy utilizes the redundant memory bandwidth to actively reclaim and reallocate the over-allocated buffer. Testbed experiments and large-scale simulations show that Occamy can improve the end-to-end performance by up to ~55%. \u25b3 Less Submitted 23 January, 2025; originally announced January 2025. Comments: 18 pages, 23 figures arXiv:2501.12492 [pdf, ps, other] QuSplit: Achieving Both High Fidelity and Throughput via Job Splitting on Noisy Quantum Computers Authors: Jinyang Li, Yuhong Song, Yipei Liu, Jianli Pan, Lei Yang, Travis Humble, Weiwen Jiang Abstract: With the progression into the quantum utility era, computing is shifting toward quantum-centric architectures, where multiple quantum processors collaborate with classical computing resources. Platforms such as IBM Quantum and Amazon Braket exemplify this trend, enabling access to diverse quantum backends. However, efficient resource management remains a challenge, as quantum processors are highly\u2026 \u25bd More With the progression into the quantum utility era, computing is shifting toward quantum-centric architectures, where multiple quantum processors collaborate with classical computing resources. Platforms such as IBM Quantum and Amazon Braket exemplify this trend, enabling access to diverse quantum backends. However, efficient resource management remains a challenge, as quantum processors are highly susceptible to noise, which significantly impacts computation fidelity. Additionally, the heterogeneous noise characteristics across different processors add further complexity to scheduling and resource allocation. Existing scheduling strategies typically focus on mapping and scheduling jobs to these heterogeneous backends, which leads to some jobs suffering extremely low fidelity. Targeting quantum optimization jobs (e.g., VQC, VQE, QAOA) - among the most promising quantum applications in the NISQ era - we hypothesize that executing the later stages of a job on a high-fidelity quantum processor can significantly improve overall fidelity. To verify this, we use VQE as a case study and develop a Genetic Algorithm-based scheduling framework that incorporates job splitting to optimize fidelity and throughput. Experimental results demonstrate that our approach consistently maintains high fidelity across all jobs while significantly enhancing system throughput. Furthermore, the proposed algorithm exhibits excellent scalability in handling an increasing number of quantum processors and larger workloads, making it a robust and practical solution for emerging quantum computing platforms. To further substantiate its effectiveness, we conduct experiments on a real quantum processor, IBM Strasbourg, which confirm that job splitting improves fidelity and reduces the number of iterations required for convergence. \u25b3 Less Submitted 11 March, 2025; v1 submitted 21 January, 2025; originally announced January 2025. arXiv:2501.12032 [pdf, other] Multi-Tenant SmartNICs for In-Network Preprocessing of Recommender Systems Authors: Yu Zhu, Wenqi Jiang, Gustavo Alonso Abstract: Keeping ML-based recommender models up-to-date as data drifts and evolves is essential to maintain accuracy. As a result, online data preprocessing plays an increasingly important role in serving recommender systems. Existing solutions employ multiple CPU workers to saturate the input bandwidth of a single training node. Such an approach results in high deployment costs and energy consumption. For\u2026 \u25bd More Keeping ML-based recommender models up-to-date as data drifts and evolves is essential to maintain accuracy. As a result, online data preprocessing plays an increasingly important role in serving recommender systems. Existing solutions employ multiple CPU workers to saturate the input bandwidth of a single training node. Such an approach results in high deployment costs and energy consumption. For instance, a recent report from industrial deployments shows that data storage and ingestion pipelines can account for over 60\\% of the power consumption in a recommender system. In this paper, we tackle the issue from a hardware perspective by introducing Piper, a flexible and network-attached accelerator that executes data loading and preprocessing pipelines in a streaming fashion. As part of the design, we define MiniPipe, the smallest pipeline unit enabling multi-pipeline implementation by executing various data preprocessing tasks across the single board, giving Piper the ability to be reconfigured at runtime. Our results, using publicly released commercial pipelines, show that Piper, prototyped on a power-efficient FPGA, achieves a 39$\\sim$105$\\times$ speedup over a server-grade, 128-core CPU and 3$\\sim$17$\\times$ speedup over GPUs like RTX 3090 and A100 in multiple pipelines. The experimental analysis demonstrates that Piper provides advantages in both latency and energy efficiency for preprocessing tasks in recommender systems, providing an alternative design point for systems that today are in very high demand. \u25b3 Less Submitted 24 January, 2025; v1 submitted 21 January, 2025; originally announced January 2025. arXiv:2501.09466 [pdf, other] DEFOM-Stereo: Depth Foundation Model Based Stereo Matching Authors: Hualie Jiang, Zhiqiang Lou, Laiyan Ding, Rui Xu, Minglang Tan, Wenjie Jiang, Rui Huang Abstract: Stereo matching is a key technique for metric depth estimation in computer vision and robotics. Real-world challenges like occlusion and non-texture hinder accurate disparity estimation from binocular matching cues. Recently, monocular relative depth estimation has shown remarkable generalization using vision foundation models. Thus, to facilitate robust stereo matching with monocular depth cues,\u2026 \u25bd More Stereo matching is a key technique for metric depth estimation in computer vision and robotics. Real-world challenges like occlusion and non-texture hinder accurate disparity estimation from binocular matching cues. Recently, monocular relative depth estimation has shown remarkable generalization using vision foundation models. Thus, to facilitate robust stereo matching with monocular depth cues, we incorporate a robust monocular relative depth model into the recurrent stereo-matching framework, building a new framework for depth foundation model-based stereo-matching, DEFOM-Stereo. In the feature extraction stage, we construct the combined context and matching feature encoder by integrating features from conventional CNNs and DEFOM. In the update stage, we use the depth predicted by DEFOM to initialize the recurrent disparity and introduce a scale update module to refine the disparity at the correct scale. DEFOM-Stereo is verified to have comparable performance on the Scene Flow dataset with state-of-the-art (SOTA) methods and notably shows much stronger zero-shot generalization. Moreover, DEFOM-Stereo achieves SOTA performance on the KITTI 2012, KITTI 2015, Middlebury, and ETH3D benchmarks, ranking 1st on many metrics. In the joint evaluation under the robust vision challenge, our model simultaneously outperforms previous models on the individual benchmarks. Both results demonstrate the outstanding capabilities of the proposed model. \u25b3 Less Submitted 16 January, 2025; originally announced January 2025. Comments: Code: https://github.com/Insta360-Research-Team/DEFOM-Stereo arXiv:2501.07849 [pdf, other] Unveiling Provider Bias in Large Language Models for Code Generation Authors: Xiaoyu Zhang, Juan Zhai, Shiqing Ma, Qingshuang Bao, Weipeng Jiang, Chao Shen, Yang Liu Abstract: Large Language Models (LLMs) have emerged as the new recommendation engines, outperforming traditional methods in both capability and scope, particularly in code generation applications. Our research reveals a novel provider bias in LLMs, namely without explicit input prompts, these models show systematic preferences for services from specific providers in their recommendations (e.g., favoring Goo\u2026 \u25bd More Large Language Models (LLMs) have emerged as the new recommendation engines, outperforming traditional methods in both capability and scope, particularly in code generation applications. Our research reveals a novel provider bias in LLMs, namely without explicit input prompts, these models show systematic preferences for services from specific providers in their recommendations (e.g., favoring Google Cloud over Microsoft Azure). This bias holds significant implications for market dynamics and societal equilibrium, potentially promoting digital monopolies. It may also deceive users and violate their expectations, leading to various consequences. This paper presents the first comprehensive empirical study of provider bias in LLM code generation. We develop a systematic methodology encompassing an automated pipeline for dataset generation, incorporating 6 distinct coding task categories and 30 real-world application scenarios. Our analysis encompasses over 600,000 LLM-generated responses across seven state-of-the-art models, utilizing approximately 500 million tokens (equivalent to \\$5,000+ in computational costs). The study evaluates both the generated code snippets and their embedded service provider selections to quantify provider bias. Additionally, we conduct a comparative analysis of seven debiasing prompting techniques to assess their efficacy in mitigating these biases. Our findings demonstrate that LLMs exhibit significant provider preferences, predominantly favoring services from Google and Amazon, and can autonomously modify input code to incorporate their preferred providers without users' requests. Notably, we observe discrepancies between providers recommended in conversational contexts versus those implemented in generated code. The complete dataset and analysis results are available in our repository. \u25b3 Less Submitted 14 January, 2025; originally announced January 2025. Comments: 21 pages, 15 figures arXiv:2501.06997 [pdf, other] TFLAG:Towards Practical APT Detection via Deviation-Aware Learning on Temporal Provenance Graph Authors: Wenhan Jiang, Tingting Chai, Hongri Liu, Kai Wang, Hongke Zhang Abstract: Advanced Persistent Threat (APT) have grown increasingly complex and concealed, posing formidable challenges to existing Intrusion Detection Systems in identifying and mitigating these attacks. Recent studies have incorporated graph learning techniques to extract detailed information from provenance graphs, enabling the detection of attacks with greater granularity. Nevertheless, existing studies\u2026 \u25bd More Advanced Persistent Threat (APT) have grown increasingly complex and concealed, posing formidable challenges to existing Intrusion Detection Systems in identifying and mitigating these attacks. Recent studies have incorporated graph learning techniques to extract detailed information from provenance graphs, enabling the detection of attacks with greater granularity. Nevertheless, existing studies have largely overlooked the continuous yet subtle temporal variations in the structure of provenance graphs, which may correspond to surreptitious perturbation anomalies in ongoing APT attacks. Therefore, we introduce TFLAG, an advanced anomaly detection framework that for the first time integrates the structural dynamic extraction capabilities of temporal graph model with the anomaly delineation abilities of deviation networks to pinpoint covert attack activities in provenance graphs. This self-supervised integration framework leverages the graph model to extract neighbor interaction data under continuous temporal changes from historical benign behaviors within provenance graphs, while simultaneously utilizing deviation networks to accurately distinguish authentic attack activities from false positive deviations due to unexpected subtle perturbations. The experimental results indicate that, through a comprehensive design that utilizes both attribute and temporal information, it can accurately identify the time windows associated with APT attack behaviors without prior knowledge (e.g., labeled data samples), demonstrating superior accuracy compared to current state-of-the-art methods in differentiating between attack events and system false positive events. \u25b3 Less Submitted 12 January, 2025; originally announced January 2025. arXiv:2501.06533 [pdf, other] DivTrackee versus DynTracker: Promoting Diversity in Anti-Facial Recognition against Dynamic FR Strategy Authors: Wenshu Fan, Minxing Zhang, Hongwei Li, Wenbo Jiang, Hanxiao Chen, Xiangyu Yue, Michael Backes, Xiao Zhang Abstract: The widespread adoption of facial recognition (FR) models raises serious concerns about their potential misuse, motivating the development of anti-facial recognition (AFR) to protect user facial privacy. In this paper, we argue that the static FR strategy, predominantly adopted in prior literature for evaluating AFR efficacy, cannot faithfully characterize the actual capabilities of determined tra\u2026 \u25bd More The widespread adoption of facial recognition (FR) models raises serious concerns about their potential misuse, motivating the development of anti-facial recognition (AFR) to protect user facial privacy. In this paper, we argue that the static FR strategy, predominantly adopted in prior literature for evaluating AFR efficacy, cannot faithfully characterize the actual capabilities of determined trackers who aim to track a specific target identity. In particular, we introduce \\emph{\\ourAttack}, a dynamic FR strategy where the model's gallery database is iteratively updated with newly recognized target identity images. Surprisingly, such a simple approach renders all the existing AFR protections ineffective. To mitigate the privacy threats posed by DynTracker, we advocate for explicitly promoting diversity in the AFR-protected images. We hypothesize that the lack of diversity is the primary cause of the failure of existing AFR methods. Specifically, we develop \\emph{DivTrackee}, a novel method for crafting diverse AFR protections that builds upon a text-guided image generation framework and diversity-promoting adversarial losses. Through comprehensive experiments on various facial image benchmarks and feature extractors, we demonstrate DynTracker's strength in breaking existing AFR methods and the superiority of DivTrackee in preventing user facial images from being identified by dynamic FR strategies. We believe our work can act as an important initial step towards developing more effective AFR methods for protecting user facial privacy against determined trackers. \u25b3 Less Submitted 11 January, 2025; originally announced January 2025. arXiv:2501.06224 [pdf] Detection, Retrieval, and Explanation Unified: A Violence Detection System Based on Knowledge Graphs and GAT Authors: Wen-Dong Jiang, Chih-Yung Chang, Diptendu Sinha Roy Abstract: Recently, violence detection systems developed using unified multimodal models have achieved significant success and attracted widespread attention. However, most of these systems face two critical challenges: the lack of interpretability as black-box models and limited functionality, offering only classification or retrieval capabilities. To address these challenges, this paper proposes a novel i\u2026 \u25bd More Recently, violence detection systems developed using unified multimodal models have achieved significant success and attracted widespread attention. However, most of these systems face two critical challenges: the lack of interpretability as black-box models and limited functionality, offering only classification or retrieval capabilities. To address these challenges, this paper proposes a novel interpretable violence detection system, termed the Three-in-One (TIO) System. The TIO system integrates knowledge graphs (KG) and graph attention networks (GAT) to provide three core functionalities: detection, retrieval, and explanation. Specifically, the system processes each video frame along with text descriptions generated by a large language model (LLM) for videos containing potential violent behavior. It employs ImageBind to generate high-dimensional embeddings for constructing a knowledge graph, uses GAT for reasoning, and applies lightweight time series modules to extract video embedding features. The final step connects a classifier and retriever for multi-functional outputs. The interpretability of KG enables the system to verify the reasoning process behind each output. Additionally, the paper introduces several lightweight methods to reduce the resource consumption of the TIO system and enhance its efficiency. Extensive experiments conducted on the XD-Violence and UCF-Crime datasets validate the effectiveness of the proposed system. A case study further reveals an intriguing phenomenon: as the number of bystanders increases, the occurrence of violent behavior tends to decrease. \u25b3 Less Submitted 5 February, 2025; v1 submitted 7 January, 2025; originally announced January 2025. Comments: This work has been submitted to the IEEE for possible publication arXiv:2501.05460 [pdf, other] Efficiently Serving Large Multimodal Models Using EPD Disaggregation Authors: Gursimran Singh, Xinglu Wang, Yifan Hu, Timothy Yu, Linzi Xing, Wei Jiang, Zhefeng Wang, Xiaolong Bai, Yi Li, Ying Xiong, Yong Zhang, Zhenan Fan Abstract: Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by handling diverse inputs such as images, audio, and video, but at the cost of adding a multimodal encoding stage that increases both computational and memory overhead. This step negatively impacting key Service Level Objectives (SLOs) like time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce Encode-Prefill-D\u2026 \u25bd More Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by handling diverse inputs such as images, audio, and video, but at the cost of adding a multimodal encoding stage that increases both computational and memory overhead. This step negatively impacting key Service Level Objectives (SLOs) like time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates the encoding, prefill, and decode stages onto dedicated resources. Unlike current systems, which bundle encoding and prefill together, our approach decouple these steps unlocking new opportunities and optimizations. These include a new mechanism to cache multimedia tokens for efficient transfer, a novel way to parallelize encoding load within a request, a module to find the optimal resource allocation for disaggregated serving, and a novel role switching method to handle changing workload characteristics. Experimental evaluations with popular LMMs show substantial gains in memory efficiency (up to 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger), 10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it leads to significant improvements in latency metrics (TTFT up to 71\\% reduction) and end-to-end throughput (up to 57\\% reduction), compared to systems that do not disaggregate. \u25b3 Less Submitted 5 February, 2025; v1 submitted 25 December, 2024; originally announced January 2025. Comments: 16 pages, 11 figures arXiv:2501.01773 [pdf, other] Compressed Domain Prior-Guided Video Super-Resolution for Cloud Gaming Content Authors: Qizhe Wang, Qian Yin, Zhimeng Huang, Weijia Jiang, Yi Su, Siwei Ma, Jiaqi Zhang Abstract: Cloud gaming is an advanced form of Internet service that necessitates local terminals to decode within limited resources and time latency. Super-Resolution (SR) techniques are often employed on these terminals as an efficient way to reduce the required bit-rate bandwidth for cloud gaming. However, insufficient attention has been paid to SR of compressed game video content. Most SR networks amplif\u2026 \u25bd More Cloud gaming is an advanced form of Internet service that necessitates local terminals to decode within limited resources and time latency. Super-Resolution (SR) techniques are often employed on these terminals as an efficient way to reduce the required bit-rate bandwidth for cloud gaming. However, insufficient attention has been paid to SR of compressed game video content. Most SR networks amplify block artifacts and ringing effects in decoded frames while ignoring edge details of game content, leading to unsatisfactory reconstruction results. In this paper, we propose a novel lightweight network called Coding Prior-Guided Super-Resolution (CPGSR) to address the SR challenges in compressed game video content. First, we design a Compressed Domain Guided Block (CDGB) to extract features of different depths from coding priors, which are subsequently integrated with features from the U-net backbone. Then, a series of re-parameterization blocks are utilized for reconstruction. Ultimately, inspired by the quantization in video coding, we propose a partitioned focal frequency loss to effectively guide the model's focus on preserving high-frequency information. Extensive experiments demonstrate the advancement of our approach. \u25b3 Less Submitted 3 January, 2025; originally announced January 2025. Comments: 10 pages, 4 figures, Data Compression Conference2025 arXiv:2501.00330 [pdf, other] Exploring the Implicit Semantic Ability of Multimodal Large Language Models: A Pilot Study on Entity Set Expansion Authors: Hebin Wang, Yangning Li, Yinghui Li, Hai-Tao Zheng, Wenhao Jiang, Hong-Gee Kim Abstract: The rapid development of multimodal large language models (MLLMs) has brought significant improvements to a wide range of tasks in real-world applications. However, LLMs still exhibit certain limitations in extracting implicit semantic information. In this paper, we apply MLLMs to the Multi-modal Entity Set Expansion (MESE) task, which aims to expand a handful of seed entities with new entities be\u2026 \u25bd More The rapid development of multimodal large language models (MLLMs) has brought significant improvements to a wide range of tasks in real-world applications. However, LLMs still exhibit certain limitations in extracting implicit semantic information. In this paper, we apply MLLMs to the Multi-modal Entity Set Expansion (MESE) task, which aims to expand a handful of seed entities with new entities belonging to the same semantic class, and multi-modal information is provided with each entity. We explore the capabilities of MLLMs to understand implicit semantic information at the entity-level granularity through the MESE task, introducing a listwise ranking method LUSAR that maps local scores to global rankings. Our LUSAR demonstrates significant improvements in MLLM's performance on the MESE task, marking the first use of generative MLLM for ESE tasks and extending the applicability of listwise ranking. \u25b3 Less Submitted 31 December, 2024; originally announced January 2025. Comments: ICASSP 2025 arXiv:2412.20201 [pdf] Injecting Explainability and Lightweight Design into Weakly Supervised Video Anomaly Detection Systems Authors: Wen-Dong Jiang, Chih-Yung Chang, Hsiang-Chuan Chang, Ji-Yuan Chen, Diptendu Sinha Roy Abstract: Weakly Supervised Monitoring Anomaly Detection (WSMAD) utilizes weak supervision learning to identify anomalies, a critical task for smart city monitoring. However, existing multimodal approaches often fail to meet the real-time and interpretability requirements of edge devices due to their complexity. This paper presents TCVADS (Two-stage Cross-modal Video Anomaly Detection System), which leverag\u2026 \u25bd More Weakly Supervised Monitoring Anomaly Detection (WSMAD) utilizes weak supervision learning to identify anomalies, a critical task for smart city monitoring. However, existing multimodal approaches often fail to meet the real-time and interpretability requirements of edge devices due to their complexity. This paper presents TCVADS (Two-stage Cross-modal Video Anomaly Detection System), which leverages knowledge distillation and cross-modal contrastive learning to enable efficient, accurate, and interpretable anomaly detection on edge devices.TCVADS operates in two stages: coarse-grained rapid classification and fine-grained detailed analysis. In the first stage, TCVADS extracts features from video frames and inputs them into a time series analysis module, which acts as the teacher model. Insights are then transferred via knowledge distillation to a simplified convolutional network (student model) for binary classification. Upon detecting an anomaly, the second stage is triggered, employing a fine-grained multi-class classification model. This stage uses CLIP for cross-modal contrastive learning with text and images, enhancing interpretability and achieving refined classification through specially designed triplet textual relationships. Experimental results demonstrate that TCVADS significantly outperforms existing methods in model performance, detection efficiency, and interpretability, offering valuable contributions to smart city monitoring applications. \u25b3 Less Submitted 28 December, 2024; originally announced December 2024. Comments: IEEE TETC-CS (Under review) arXiv:2412.18972 [pdf, other] Recommending Pre-Trained Models for IoT Devices Authors: Parth V. Patil, Wenxin Jiang, Huiyun Peng, Daniel Lugo, Kelechi G. Kalu, Josh LeBlanc, Lawrence Smith, Hyeonwoo Heo, Nathanael Aou, James C. Davis Abstract: The availability of pre-trained models (PTMs) has enabled faster deployment of machine learning across applications by reducing the need for extensive training. Techniques like quantization and distillation have further expanded PTM applicability to resource-constrained IoT hardware. Given the many PTM options for any given task, engineers often find it too costly to evaluate each model's suitabil\u2026 \u25bd More The availability of pre-trained models (PTMs) has enabled faster deployment of machine learning across applications by reducing the need for extensive training. Techniques like quantization and distillation have further expanded PTM applicability to resource-constrained IoT hardware. Given the many PTM options for any given task, engineers often find it too costly to evaluate each model's suitability. Approaches such as LogME, LEEP, and ModelSpider help streamline model selection by estimating task relevance without exhaustive tuning. However, these methods largely leave hardware constraints as future work-a significant limitation in IoT settings. In this paper, we identify the limitations of current model recommendation approaches regarding hardware constraints and introduce a novel, hardware-aware method for PTM selection. We also propose a research agenda to guide the development of effective, hardware-conscious model recommendation systems for IoT applications. \u25b3 Less Submitted 25 December, 2024; originally announced December 2024. Comments: Accepted at SERP4IOT'25 arXiv:2412.18419 [pdf] Research on the Proximity Relationships of Psychosomatic Disease Knowledge Graph Modules Extracted by Large Language Models Authors: Zihan Zhou, Ziyi Zeng, Wenhao Jiang, Yihui Zhu, Jiaxin Mao, Yonggui Yuan, Min Xia, Shubin Zhao, Mengyu Yao, Yunqian Chen Abstract: As social changes accelerate, the incidence of psychosomatic disorders has significantly increased, becoming a major challenge in global health issues. This necessitates an innovative knowledge system and analytical methods to aid in diagnosis and treatment. Here, we establish the ontology model and entity types, using the BERT model and LoRA-tuned LLM for named entity recognition, constructing th\u2026 \u25bd More As social changes accelerate, the incidence of psychosomatic disorders has significantly increased, becoming a major challenge in global health issues. This necessitates an innovative knowledge system and analytical methods to aid in diagnosis and treatment. Here, we establish the ontology model and entity types, using the BERT model and LoRA-tuned LLM for named entity recognition, constructing the knowledge graph with 9668 triples. Next, by analyzing the network distances between disease, symptom, and drug modules, it was found that closer network distances among diseases can predict greater similarities in their clinical manifestations, treatment approaches, and psychological mechanisms, and closer distances between symptoms indicate that they are more likely to co-occur. Lastly, by comparing the proximity d and proximity z score, it was shown that symptom-disease pairs in primary diagnostic relationships have a stronger association and are of higher referential value than those in diagnostic relationships. The research results revealed the potential connections between diseases, co-occurring symptoms, and similarities in treatment strategies, providing new perspectives for the diagnosis and treatment of psychosomatic disorders and valuable information for future mental health research and practice. \u25b3 Less Submitted 24 December, 2024; originally announced December 2024. arXiv:2412.15639 [pdf, other] Tacit Learning with Adaptive Information Selection for Cooperative Multi-Agent Reinforcement Learning Authors: Lunjun Liu, Weilai Jiang, Yaonan Wang Abstract: In multi-agent reinforcement learning (MARL), the centralized training with decentralized execution (CTDE) framework has gained widespread adoption due to its strong performance. However, the further development of CTDE faces two key challenges. First, agents struggle to autonomously assess the relevance of input information for cooperative tasks, impairing their decision-making abilities. Second,\u2026 \u25bd More In multi-agent reinforcement learning (MARL), the centralized training with decentralized execution (CTDE) framework has gained widespread adoption due to its strong performance. However, the further development of CTDE faces two key challenges. First, agents struggle to autonomously assess the relevance of input information for cooperative tasks, impairing their decision-making abilities. Second, in communication-limited scenarios with partial observability, agents are unable to access global information, restricting their ability to collaborate effectively from a global perspective. To address these challenges, we introduce a novel cooperative MARL framework based on information selection and tacit learning. In this framework, agents gradually develop implicit coordination during training, enabling them to infer the cooperative behavior of others in a discrete space without communication, relying solely on local information. Moreover, we integrate gating and selection mechanisms, allowing agents to adaptively filter information based on environmental changes, thereby enhancing their decision-making capabilities. Experiments on popular MARL benchmarks show that our framework can be seamlessly integrated with state-of-the-art algorithms, leading to significant performance improvements. \u25b3 Less Submitted 24 December, 2024; v1 submitted 20 December, 2024; originally announced December 2024. Comments: Accepted by AAMAS 2025 (Extended Abstract) arXiv:2412.15529 [pdf, other] XRAG: eXamining the Core -- Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation Authors: Qianren Mao, Yangyifei Luo, Jinlong Zhang, Hanwen Hao, Zhilong Cao, Xiaolong Wang, Xiao Guan, Zhenting Huang, Weifeng Jiang, Shuyu Guo, Zhentao Han, Qili Zhang, Siyuan Tao, Yujie Liu, Junnan Liu, Zhixing Tan, Jie Sun, Bo Li, Xudong Liu, Richong Zhang, Jianxin Li Abstract: Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent data with the generative capabilities of Large Language Models (LLMs), ensuring that the generated output is not only contextually relevant but also accurate and current. We introduce XRAG, an open-source, modular codebase that facilitates exhaustive evaluation of the performance of foundational components of advanced RAG m\u2026 \u25bd More Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent data with the generative capabilities of Large Language Models (LLMs), ensuring that the generated output is not only contextually relevant but also accurate and current. We introduce XRAG, an open-source, modular codebase that facilitates exhaustive evaluation of the performance of foundational components of advanced RAG modules. These components are systematically categorized into four core phases: pre-retrieval, retrieval, post-retrieval, and generation. We systematically analyse them across reconfigured datasets, providing a comprehensive benchmark for their effectiveness. As the complexity of RAG systems continues to escalate, we underscore the critical need to identify potential failure points in RAG systems. We formulate a suite of experimental methodologies and diagnostic testing protocols to dissect the failure points inherent in RAG engineering. Subsequently, we proffer bespoke solutions aimed at bolstering the overall performance of these modules. Our work thoroughly evaluates the performance of advanced core components in RAG systems, providing insights into optimizations for prevalent failure points. \u25b3 Less Submitted 24 December, 2024; v1 submitted 19 December, 2024; originally announced December 2024. arXiv:2412.14426 [pdf, other] All-in-One Tuning and Structural Pruning for Domain-Specific LLMs Authors: Lei Lu, Zhepeng Wang, Runxue Bao, Mengbing Wang, Fangyi Li, Yawen Wu, Weiwen Jiang, Jie Xu, Yanzhi Wang, Shangqian Gao Abstract: Existing pruning techniques for large language models (LLMs) targeting domain-specific applications typically follow a two-stage process: pruning the pretrained general-purpose LLMs and then fine-tuning the pruned LLMs on specific domains. However, the pruning decisions, derived from the pretrained weights, remain unchanged during fine-tuning, even if the weights have been updated. Therefore, such\u2026 \u25bd More Existing pruning techniques for large language models (LLMs) targeting domain-specific applications typically follow a two-stage process: pruning the pretrained general-purpose LLMs and then fine-tuning the pruned LLMs on specific domains. However, the pruning decisions, derived from the pretrained weights, remain unchanged during fine-tuning, even if the weights have been updated. Therefore, such a combination of the pruning decisions and the finetuned weights may be suboptimal, leading to non-negligible performance degradation. To address these limitations, we propose ATP: All-in-One Tuning and Structural Pruning, a unified one-stage structural pruning and fine-tuning approach that dynamically identifies the current optimal substructure throughout the fine-tuning phase via a trainable pruning decision generator. Moreover, given the limited available data for domain-specific applications, Low-Rank Adaptation (LoRA) becomes a common technique to fine-tune the LLMs. In ATP, we introduce LoRA-aware forward and sparsity regularization to ensure that the substructures corresponding to the learned pruning decisions can be directly removed after the ATP process. ATP outperforms the state-of-the-art two-stage pruning methods on tasks in the legal and healthcare domains. More specifically, ATP recovers up to 88% and 91% performance of the dense model when pruning 40% parameters of LLaMA2-7B and LLaMA3-8B models, respectively. \u25b3 Less Submitted 20 December, 2024; v1 submitted 18 December, 2024; originally announced December 2024. Comments: Updated a typo in the author list;"
  }
]